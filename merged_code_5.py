


#==============================================================================
# File: 提现人行征信模型fpd30标签v4.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[3]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[4]:


# 设置数据存储目录
directory = f'./result_v4'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[5]:


task_name = '提现人行征信模型fpd30标签'


# # 函数定义

# In[6]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[ ]:


df_sample_dict = {}


# In[ ]:


# # 计算今天的时间
# from datetime import datetime, timedelta, date

# today = datetime.now().strftime('%Y-%m-%d')
# print(today)

# this_day =datetime.strptime('2025-03-21', '%Y-%m-%d')
# end_day = datetime.strptime('2024-09-01', '%Y-%m-%d')

# while this_day >= end_day:
#     run_day = this_day.strftime('%Y-%m-%d')
#     sql = f'''
# select *
# from znzz_fintech_ads.lxl_tmp_pboc_model_sample_250522
# where apply_time>=apply_time_auth
#   and apply_date='{run_day}'
# '''
#     print(f'=========================={run_day}=============================')
#     df_sample_dict[run_day] = get_data(sql)
#     this_day = this_day - timedelta(days=1)


# In[115]:


sql = """
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.apply_time
,t.apply_date_auth
,t.apply_time_auth
,t.diff_days
,t.target_fpd10
,t.target_fpd20
,t.target_fpd30
,t.target_cpd30
,t.target_mob3dpd30
,t.target_mob4dpd30
,all_loan_approval_rate_l1m
,pboc_base_sex
,candnlted1m10cnycas
,inquire_organizations_nbank_average_wave_dividing_l360d
,pboc_latest1monthselfinquirequerynum
,creditcard_inquire_date_to_now_day
,creditcard_all_loan_approval_rate_l1m
,candnlted1mnbcnyua_uuam
,s00jdn
,swwjstoas
,tisr1b_arl6mr
,query_recentfirst_afterloan_count
,candnlted1mnbcnycas
,s00jhdn
,mom_add_inquire_reason_exclude_dhgl_num
,loan_sum_amount_wave_dividing_l90d
,redg_zbvb_xave_mc
,inloan_balance_of_all_ratio
,pboc_all_account_cnt
,q03_q12_ncfcqc_r
,pbocv_md_c_ulmt_pct_p_1_6
,agaa_zbvg_xawd_bbvh_me
,inquire_reason_exclude_plm_num_wave_dividing_l120d
,loan_normal_account_type_d1_balance_dividing_amount_ratio
,all_loan_amount_less_than_30000_actual_repayment_of_all_ratio
,all_loan_exclude_homeloan_carloan_sum_permonth_repayments
,pboc_summary_unstl_nrev_act_cnt
,pboc_education
,pboc_nrev_lmt_use_ratio
,pboc_summary_query_monitor_cnt_l24m
,q06_ncfcqc_rrgr_r
,loan_repay_normal_account_num_l12m
,all_actual_repayment_of_permonth_repayments_ratio
,loan_remaining_periods_exceed_6_permonth_repayments_ratio
,loan_other_loans_min
,credit_average_repay_balance
,t24callr1xcdr
,candnlted2mnbcnycas
,pboc_rebn_zcva_xbvb_bavf_mcq8
,owwcwwxcdr
,q01_tqc_ncfcqc_r
,pbocv_cc_min_creditlimit
,min_gap_day_of_opening_date_to_close_date
,q01_q24_rfioorg_gea_qc_r
,query_2m_nobank_ratio
,pboc_cc_avg_amt_last3
,twwcallr1xcdr
,query_9m_dksp_stddev
,inquire_slc_num_l6m
,loan_normal_account_type_r1_balance_dividing_amount_ratio
,candlted6m10cnyua_uuaa
,s02jhcdr
,loan_nbank_max_amount
,inquire_slc_num_of_all_ratio_l3m
,loan_normal_account_type_d1_min_amount_opening_date_gap
,cral_zbvg_xave_bbvf_mfn11
,pboc_summary_ovd_cc_amt_max_by_mth
,s02jhstoas
,add_loan_account_num_wave_dividing_l360d
,pboc_pbcotherloanrepaysum
,candnltedallmnbcnycaa
,lcdoddil
,all_loan_actual_repayment_of_permonth_repayments_ratio
,loan_normal_account_type_d1_max_amount_opening_date_gap
,pboc_summary_cc_acount_cnt
,pboc_coaa_zbvg_xawd_bbvd_n3
,query_afterloan_debts_orgcnt
,pboc_summary_unstl_rev_lmt
,q01_q24_fgcqc_r
,add_inquire_organizations_num_l3m_exclude_l4m_l12m
,account_loan_open3mp_orgratio
,s01jhcdr
,q09_qccv_r
,callolldn
,t01cnncdr
,pboc_overdueamtonemonthdebitcard
,q24_rficca_ldst_t
,d07_q24_ncfcqc_r
,pboc_summary_query_grr_cnt_l24m
,query_3m_ccln_loanorg_ratio
,loan_normal_bank_balance_average
,inquire_reason_loan_num_wave_dividing_l90d
,loan_cfc_permonth_repayments_average
,query_2m_dksp_loanorg_ratio
,loan_normal_cfc_opening_date_to_now_max
,deaa_zbva_xavc_bbvf_mdo2
,pboc_debitcardmaxline
,account_ccln_3m_status_c_ratio
,q01_q09_nbfiqc_r
,add_inquire_num_lst_special_transaction_tqjq_tqhk
,loan_normal_repayments_num_l6m
,s01jhstoas
,creditcard_all_loan_approval_rate_l3m
,pboc_latest24overduetimestotal
,agaa_zbva_xawd_bbvf_me
,creditcard_approval_rate_l24m
,q09_qitncfc_qccv_r
,twwcallm1toas
,t12cnr1xcdr
,inquire_slc_num_of_all_ratio
,candnlted1mnbcnycam
,loan_max_amount_l1m
,q12_tqi_rfila_nqi_r
,pboc_cur_loan_business_ltv
,pbocv_query_guaranchkorg_p3m
,loan_slc_balance_average
,pboc_summary_opn_cc_min_lmt
,loan_sum_amount_wave_sub_l90d
,mom_add_credit_account_amount_l1m
,inquire_reason_exclude_plm_num_wave_dividing_l90d
,t24cnr1xchr
,inquire_reason_exclude_plm_num_wave_dividing_l30d
,loan_normal_slc_opening_date_to_now_sum
,q09_qitnbfi_qccv_r
,acda_zbvb_n2
,pboc_overduemonthloan
,never_overdue_all_registered_time_exceed_6m_account
,d07_q03_nbfiqc_r
,add_loan_account_num_wave_sub_l180d
,pboc_summary_query_org_type_lst
,pboc_normal_loan_act_cnt_l24m
,candnltedallmbcnyua_uuam
,q06_q24_fgc_noqi_r
,cral_zbvg_xawe_bbvf_mcn9
,credit_balance_of_all_balance_ratio
,cral_zbvg_xavc_bbvj_mdn11
,pboc_cc_rcy_ovdmonth
,creditcard_max_use_ratio_amount_greater_than_amount_account_ratio
,account_business_loan_mob_avg
,twwcnxcdr
,cral_zbvg_xave_bbvf_mcn5
,loan_cfc_balance_max
,estimated_revenue_max
,credit_repay_method_debx_max_int
,pboc_summary_nrev_org_cnt
,pboc_loan_lastrepay_days
,never_overdue_all_loan_registered_time_exceed_12m_account
,cral_zbvg_xawh_bbvf_mfn3
,twwcallnmchr
,loan_normal_bank_opening_date_to_now_min
,cadnlt10momrtmc
,credit_loan_average_balance
,candnlted1mnbcnycaa
,inquire_num_diff_reason_l1m
,organizations_slc_cfc_balance_of_all_ratio
,q12_tqi_cfc_nqi_r
,pboc_latest2overduetimestotal
,pboc_xd_24m_query_ins_cnt
,pboc_pbccredituseminusper
,q01_q24_fgcqc_oorg_r
,pboc_summary_unstl_nrev_repay_avg_l6m
,loan_normal_bank_max_balance_opening_date_gap
,redg_zbvb_xave_md
,inloan_bank_max_balance
,query_1m_ccln_loanorg_ratio
,q12_tqi_ncfc_nqi_r
,pbocv_loan_noncrhs_avgamt_p12m
,candnlted24_allmnbcnyua_ar
,s12jhcd
,pboc_summary_unstl_rev_repay_avg_l6m
,o02cncho06cnchr
,callt00m1dn
,pboc_query_auth_3v12m_pct
,add_amount_less_than_30000_of_all_account_ratio_l12m
,add_amount_less_than_30000_of_all_amount_ratio_l6m
,loan_credit_amount_less_than_30000_amount
,identity_phone_mob_min
,credit_repay_method_xxhb_debx_sub_int
,loan_consumer_balance_dividing_amount_ratio
,t02callcht06callchr
,candnlted1_2m10cnycar
,pboc_last12n_zhjqp
,deaa_zbvg_xawh_bbvf_mdo9
,pboc_pbcdebitcardthismonthrepaymentsumline
,credit_average_gap_month_of_aod_to_acd
,pboc_dksp_3m_nbank_cnt
,c02crchr
,loan_max_amount_wave_dividing_l180d
,pboc_dksp_1m_nbank_cnt
,q12_rficca_qc_rrg_s
,creditcard_sum_average_useamt_l6m_of_sum_max_useamt_ratio
,d15_q24_ncfcqc_r
,q01_tqi_ncfc_nqi_r
,pboc_loan_ovdmonthcnt
,pboc_overduemonthdebitcard
,credit_permonth_repayments_of_actual_repayment_max_ratio
,creditcard_overdue_exceed_m1_to_now_month
,candnltedallm10cnyua_caa
,all_permonth_repayments_of_actual_repayment_max_ratio
,pboc_acct_rvlv_loan_avgrp_in6m
,inquire_reason_loan_max_wave_dividing_l180d
,candnlted1mnbcnyua_cal
,q06_q24_rfigea_nqi_r
,q12_rfila_qc_rrgr_r
,loan_slc_amount_average
,q03_q09_mfc_nqi_r
,all_loan_approval_rate_l24m
,pboc_cc_maxovd_monthratio
,pboc_summary_ovd_rev_mth_cnt
,pboc_summary_unstl_nrev_act_org_cnt
,inquire_slc_num_l3m
,pboc_summary_org_cnt
,account_ccln_orgcnt
,q03_ncfc_qry_c
,tisr1ras
,q24_qitncfc_top2mqc_avg_m
,q24_qitsifi_maxdi_t
,candltedallm10cnyua_uuar
,tisr2b_arl6mr
,inquire_reason_exclude_plm_l1m_dividing_l12m_ratio
,pboc_zx_age
,candnlted12m10cnycam
,account_ccln_6m_status_c_ratio
,candnlted6m10moc
,pboc_debts_sgac_mc
,loan_norma_bank_amount_min
,account_loan_12m_amount_max
,add_inquire_num_lst_overdue
,pboc_3m_xdspcxyy_num
,q24_qitcfc_maxdi_t
,add_credit_sum_amount_l6m
,credit_average_repay_num
,cclnsum_6mavguseamount_sum
,inquire_organizations_nbank_max_wave_sub_l90d
,candnlted2_3mnbcnycar
,owwcwwncd
,pboc_debitcard7accountnum
,d15_q06_rfila_nqi_r
,s24jstoam
,redg_zbvb_xawd_md
,o03cnxchr
,pboc_summary_unstl_rev_sub_repay_avg_l6m
,inquire_all_max_contin_num_l6m
,q03_tqi_rfila_nqi_r
,add_all_loan_max_amount_l12m
,pboc_datedebitcardfirstactive
,creditcard_average_useamt_l6m_of_permonth_repayments_max_ratio
,d07_q06_rfila_qc_r
,candnltedallm10cnycll
,t01cnd1r1r4ncdr
,q06_rfigeaqc_cmos_max
,creditcard_average_useamt_l6m_of_sum_amount_max_ratio
,pboc_guar_querycnt
,agaa_zbvg_xawd_bbvf_mf
,all_normal_balance_of_sum_permonth_repayments_ratio
,agaa_zbvg_xawd_bbvf_me
,d07_q06_nbfiqc_r
,candnlted1m10cnyua_clr
,loan_normal_credit_average_opening_days
,inquire_reason_loan_l1m_dividing_l12m_ratio
,credit_never_overdue_permonth_repayments_of_all_ratio
,tisr1b_rar
,o12cwwncd
,pboc_loan_unstl_fst_inval_mths
,candnltedallm10eddfm
,rh_loan_overdue_31d_his_all_mon
,loan_cfc_balance_sum
,pboc_summary_opn_cc_use_amt
,q12_qitcfcqc_cmos_max
,add_amount_less_than_30000_of_all_account_ratio_l6m
,q03_fgc_nqi_rrgr_r
,loan_cfc_balance_average
,loan_credit_all_loan_repay_num
,pboc_repay_dhaa_md
,s03jhcd
,pboc_loan_total_use_ratio
,all_loan_min_amount
,creditcard_sum_average_useamt_of_max_useamt_ratio
,pboc_sycpsyed
,twwcallm2cd
,pboc_loan_personal_lmt_l12m
,loan_consumer_balance_exceed_0_account_num
,loan_remaining_periods_exceed_6_permonth_repayments
,pboc_xd_3m_query_ins_cnt
,pboc_base_degree
,inloan_consumer_organizations_num
,account_loanfirst_open1m_orgratio
,d15_q24_nbfiqc_r
,pboc_summary_unstl_nrev_lmt
,never_overdue_all_loan_sum_account_l24m
,pboc_xd_1m_nbank_cnt
,pboc_1m_cxcs_dksp
,carloan_amount_max
,pboc_pettyloan_cnt
,settled_loan_sec_wcp_account
,add_all_loan_average_amount_l12m
,pboc_query_cnt_l1m
,pbocv_cc_rmb_unclsed_cnt
,inquire_all_max_contin_num_l12m
,repay_24m_ccln_nooverdue_count
,occupation_industry_type
,pboc_deaa_zbvg_bbvb_o4
,creditcard_12periods_repay_neednot_account_num
,pboc_summary_unstl_nrev_bal
,pboc_query_org_cnt_l6m
,pboc_dkzhzjycyq_month_num
,all_loan_sum_permonth_repayments_of_sum_actual_repayment_ratio
,account_loan_frequency_amount_sum
,creditcard_min_amount_of_max_use_amount_ratio
,inquire_bank_num_l12m
,loan_remaining_periods_exceed_12_cfc_permonth_repayments_sum
,pboc_djk_edsyl
,pboc_ovd_max_inval_mths
,pboc_loan_act_cnt_l9m
,pbocv_loan_otherspayamtmonth
,loan_remaining_periods_exceed_12_permonth_repayments_ratio
,pboc_query_org_cnt_l3m
,twwcallm1ch
,pbocv_query_loanappr_cnt_p1m
,pboc_repay_dcal_tmd
,pboc_latest2monthcreditapprovalquerynum
,pboc_cur_nomal_act_cnt
,q09_rfila_mqc_c
,yoy_add_overdue_account_num_l12m
,pboc_history_ovdstatus
,pboc_cc_total_ovdcnt
,pboc_all_edsyl
,pboc_query_loan_cnt_l12m
,candlted24m30ua_a9r
,rh_card_his_all
,pboc_good_event_cnt
,pboc_wjqfyjggrxfdkbs
,all_useamt_ratio_exceed_50pct_account
,add_loan_account_num_l24m
,pboc_pboc_zdye
,repaid_loan_homeloan_carloan_amount_of_all_ratio
,loan_normal_credit_sum_balance
,s24jhcd
,pboc_credittotalamount
,cclnsum_mob_max
,pboc_fzss
,account_consumer_loans_balance_sum
,pboc_query_cc_org_cnt_l6m
,never_overdue_all_sum_account
,account_creditcard_normal_useamount_sum
,never_overdue_all_account_l24m
,pboc_dkcpxzhzz_zxzl
,pbocv_cc_lmtutlrate_p6m
,creditcard_all_loan_maximum_remaining_repayment
,pboc_query_cc_cnt_l12m
,pboc_query_loan_cnt_l7d
,overdue_credit_max_amount_month
,pboc_debitcardclosedsum
,pboc_crecard_ovdcnt
,pboc_loan3accountnum
,inquire_all_num_is_increasing_l3m
,settled_loan_homeloan_carloan_amount_of_all_ratio
,rh_house_his
,inquire_reason_loan_num_wave_sub_l90d
,pbocv_loan_other_cnt
,pboc_debitcardissuersinstitutionnum
,noncredit_amount_of_all_ratio
,query_mob_min
,inquire_num_weight_on_month
,inquire_slc_num_l1m
,loan_remaining_periods_exceed_12_bank_permonth_repayments_sum
,pboc_summary_all_act_cnt
,t12callnmchr
,s03hstoas
,pboc_loan_open_mths_max
,pboc_query_cnt_l12m
,cclnsum_creditcard_mob_max
,acao_bbvf_ldve_n12
,pboc_pbccredituseelimper
,pboc_summary_cc_fst_opn_inval_mths
,q03_rfi_la_c
,pboc_loan_total_ovdcnt
,pboc_settle_loan_to_now_month
,loan_normal_bank_amount_sum
,q01_tqc_rfigea_qc_r
,pboc_xd_1m_query_ins_cnt
,boc_credit_dqrq_hkrq_max_gap_day
,credit_loan_average_amount
,pboc_yjqjkqs_36m_xyd
,rh_loan_xfqt_cnt
,creditcard_approval_rate_l12m
,inquire_reason_exclude_plm_num_l15d
,all_loan_overdue_exceed_m1_to_now_month
,pboc_acct_nrvlv_cre_avgrp_in6m
,pboc_accfundcompercent
,never_overdue_creditcard_sum_account_l12m
,all_overdue_amount_less_than_30000_of_all_account_ratio
,pboc_24m_crecard_max_ovdperiod
,pboc_debitcardnoactivesum
,pboc_query_loan_org_cnt_l3m
,pboc_query_loan_org_cnt_l12m
,never_overdue_all_account_l12m
,pboc_query_loan_org_cnt_l6m
,rh_house_loan_now
,pbocv_cc_utlrate75up_accts
,acaf_zbvb_bevl_n3
,q06_rfi_c
,pboc_loansum
,pboc_acct_rvlv_loan_bal
,pboc_last6m_zhjkpd
,rh_loan_nohouse_cnt
,creditcard_average_useamt_l6m_less_than_25pct_ratio
,pbocv_house_repay_amt_mthly
,pboc_latest12overduetimestotal
,pboc_curr_1m_mortgage_repay_amount
,pboc_consumerloansum
,q12_qitcfc_mqc_c
,inquire_bank_num_l1m
,pboc_summary_opn_cc_act_cnt
,pboc_cc_normal_open_mths_max
,pboc_overdue_months
,repay_60m_creditcard_overduem1_count
,pboc_house_fund
,pboc_xd_6m_nbank_cnt
,pboc_moreovduemonth
,never_overdue_creditcard_account
,pboc_query_org_cnt_l1m
,loan_credit_amount_less_than_30000_account
,pboc_cpf_last_pay_income
,pboc_xd_24m_nbank_cnt
,pboc_latest24paymentfind2times
,candnltedallm10cnyual
,pboc_summary_ovd_nrev_max_mths
,pboc_open_acc_cnt_l6m
,creditcard_max_use_ratio_amount_greater_than_amount_account
,pboc_cpf_max_personal_ratio
,pboc_xyk_yyed
,pboc_summary_ovd_rev_act_cnt
,pboc_cpf_amt_base
,pboc_summary_opn_cc_lmt_sum
,pboc_summary_unstl_rev_sub_act_org_cnt
,pboc_query_cc_org_cnt_l12m
,pbocv_loan_guacredunpaidloanorg
,pboc_overdueaccountloan
,pboc_query_cnt_l3m
,pboc_mortgagerepaymentmonths
,d1momn
,pboc_cur_unstl_personal_business_sum_bal
,creditcard_max_amount_estimated_revenue
,pboc_mortgage_residue
,repay_60m_creditcard_cny_overduem1_count
,insurance_housing_fund_estimated_revenue
,rh_loan_overdue_61d_his_all_mon
,pbocv_cc_utlrate80up_accts
,pboc_unclear_oprloan_orgcnt
,t24callm1chr
,pboc_latest2monthloanapprovalquerynum
,inquire_bank_num_l3m
,pboc_query_nobank_cnt
,loan_remaining_periods_exceed_12_nbank_permonth_repayments_sum
,loan_normal_mortgages_max_amount
,t24cnd1r1r4ncd
,pboc_xd_3m_nbank_cnt
,all_loan_inquire_date_to_now_day
,pboc_acct_nrvlv_credit_amt_sum
,pboc_query_loan_cnt_l6m
,pboc_dksp_1m_query_ins_cnt
,never_overdue_all_loan_registered_time_exceed_3m_account
,all_overdue_exceed_m1_to_now_month
,inquire_sum_month_l6m
,pboc_summary_other_loan_act_cnt
,pboc_cc_max_records_l24m_bycard
,pboc_quae_xcva_p5
,pboc_xd_12m_query_ins_cnt
,all_normal_loan_sum_balance
,pboc_summary_ovd_rev_sub_amt_max_by_mth
,q09_qitcfc_mqc_c
,creditcard_approval_rate_l6m
,loan_cfc_balance_exceed_0_account_num
,pboc_query_cnt_l7d
,pboc_loan_rcy_ovdmonth
,pboc_dksp_12m_nbank_cnt
,creditcard_never_overdue_permonth_repayments_of_all_ratio
,pboc_curr_bal
,pboc_summary_query_rsn
,pboc_summary_ovd_rev_amt_max_by_mth
,account_loan_nocarandnohouse_12m_amount_max
,creditcard_approval_rate_l3m
,pboc_max_creperiod
,pboc_latest24maxoverduemonthtotal
,pboc_open_loan_acc_cnt_l6m
,inquire_num_diff_reason_l3m
,pboc_dksp_6m_nbank_cnt
,pboc_query_cc_org_cnt_l3m
,pboc_latest24overduetimesdebitcard
,pboc_base_living_address_cnt_l36m
,cram_bbvf_ldve_mfn9
,pboc_summary_ovd_nrev_mth_cnt
,loan_normal_slc_opening_days_sum
,pboc_latest1querytimedebitcard
,pboc_summary_unstl_rev_act_cnt
,q24_qitfl_ldst_t
,pboc_cpf_last_pay_amt
,rh_ifhousecarcard
,pboc_summary_ovd_rev_sub_mth_cnt
,pboc_summary_ovd_nrev_act_cnt
,pbocv_cc_utlrate50up_accts
,homeloan_amount_max
,pboc_latest2querytimedebitcard
,q24_qitcfc_mqc_c
,pboc_loan1accountnum
,settled_loan_homeloan_carloan_amount
,pboc_quota_amount
,creditcard_all_loan_never_overdue_permonth_repayments_l3m
,pboc_query_org_cnt_l12m
,never_overdue_all_loan_account_l12m
,never_overdue_all_loan_registered_time_exceed_6m_account
,ccln_commercialloan_cnt
,add_loan_account_num_l12m
,pboc_6m_dkpjyhk
,all_overdue_exceed_m3_to_now_month
,pboc_query_org_cnt_l7d
,inquire_reason_exclude_plm_num_wave_sub_l90d
,redg_zbvb_xawd_mc
,account_loan_bank_repayment_sum
,all_loan_average_balance
,o01callcho02callchr
,q06_q24_bfiqc_r
,candnlted1m10eddfm
,candlted24mbcnyua_caa
,loan_normal_trusts_opening_date_to_now_max
,pboc_debts_ywab_tmg
,query_1m_ratio
,inquire_reason_exclude_plm_num_wave_dividing_l180d
,deaa_zbvg_xawd_bbvf_mfo2
,pboc_confin_orgcnt
,yoy_add_credit_account_amount_l12m
,loan_cfc_amount_average
,query_12m_securities_insurance_ratio
,q03_tqc_bfiqc_r
,all_loan_creditcard_sum_actual_repayment_of_sum_amount_ratio
,account_creditcard_normal_uesaratio_median
,all_khrq_hkrq_average_gap_day
,inquire_reason_exclude_plm_num_wave_sub_l150d
,all_useamt_ratio_exceed_75pct_of_all_account_ratio
,loan_normal_slc_opening_date_to_now_max
,loan_sum_amount_wave_sub_l330d
,inquire_reason_loan_num_wave_dividing_l210d
,q12_tqi_rfiap_nqi_r
,account_loan_open6mp_orgratio
,loan_normal_cfc_opening_days_sum
,q24_qitfgc_difl_t
,callt00xndn
,all_loan_sum_balance_of_sum_amount_ratio
,candnltdd_ednbdfl
,loan_consumer_organizations_num_dividing_account_ratio
,add_credit_sum_amount_l12m
,loan_remaining_periods_less_than_12_slc_permonth_repayments_var
,loan_normal_bank_amount_max
,creditcard_all_loan_approval_rate_l6m
,deaa_zbvg_xava_bbvf_mco9
,nature_of_work_category_min_value
,loan_credit_amount_less_than_30000_of_all_account_ratio
,add_all_loan_amount_lst_overdue
,inloan_creditcard_organizations_num_dividing_account_ratio
,pboc_base_age
,account_business_loan_mob_min
,cram_bbvf_ldvi_mfn11
,inquire_reason_exclude_plm_num_wave_sub_l210d
,loan_creditcard_useamt_exceed_80pct_organizations_ratio
,loan_credit_average_amount_l12m
,q09_tqc_cfcqc_r
,creditcard_actual_repayment_of_permonth_repayments_ratio
,candnlted12mnbcnyuam
,candlted24_allmbcnyua_ar
,add_loan_account_num_l3m_dividing_l12m_ratio
,rev_creditcard_average_useamt_less_than_25pct_ratio
,gap_month_of_first_work_to_now
,q24_qitncfc_fdst_t
,pboc_creditavgline
,t12cnr1xchr
,creditcard_all_loan_never_overdue_actual_repayment_permonth_repayments_ratio_l3m
,callt00dzmgbdn
,pboc_summary_loan_fst_opn_inval_mths
,s02jhstoam
,creditloan_permonth_repayments_estimated_revenue
,loan_average_amount_wave_sub_l360d
,query_3m_dksp_approv_ratio
,q09_tqi_cfc_nqi_r
,q24_qitbfi_l2di_t
,candnltdd_rtnbdfl
,loan_remaining_periods_exceed_12_permonth_repayments
,loan_average_permonth_repayments_l6m_dividing_sum_balance_ratio
,loan_repay_progress_amount
,loan_normal_bank_min_balance_opening_date_gap
,loan_normal_account_type_r1_max_balance_opening_date_gap
,pbocv_md_c_orgcnt_lmt08_p
,candnltdd_ed10dfl
,q12_tqc_rfiap_qc_r
,inquire_slc_num_of_all_ratio_l6m
,candnlted1mnbcnyua_uuaa
,organizations_bank_balance_of_all_ratio
,all_sum_permonth_repayments_of_max_actual_repayment_ratio
,q03_nqi_rrgr_r
,creditcard_average_useamt_l6m_of_all_amount_ratio
,candnlted6mbcnyual
,loan_normal_bank_max_amount_opening_date_gap
,pboc_com_loan_bal
,inquire_reason_loan_num_wave_dividing_l180d
,loan_credit_amount_less_than_30000_of_all_amount_ratio
,add_amount_less_than_30000_of_all_amount_ratio_l12m
,loan_repay_progress_amount_average
,inquire_reason_loan_num_wave_sub_l180d
,add_loan_account_num_wave_dividing_l180d
,inquire_organizations_nbank_average_wave_sub_l180d
,pbocv_cc_max_lmtutlrate
,pboc_base_job_status
,candltedallmnbcnycam
,all_useamt_ratio_exceed_90pct_of_all_account_ratio
,candnltedallmnbcnyua_caa
,loan_normal_account_type_r4_max_amount_opening_date_gap
,account_creditcard_1m_useamount_median
,all_amount_less_than_10000_of_all_account_ratio
,q12_q24_rfigea_qc_r
,candnlted2mnbcnycam
,cral_zbva_xave_bbvf_mfn11
,tisd1b_ar
,inquire_reason_exclude_plm_num_wave_sub_l330d
,q06_q24_rfigea_qc_r
,creditcard_all_loan_approval_rate_l24m
,loan_repayment_frequency_month_remaining_periods_ratio
,credit_average_permonth_repayments_amount
,cral_zbva_xawh_bbvf_mdn2
,organizations_slc_cfc_permonth_repayments_of_all_ratio
,pboc_xyk_edsyl
,account_creditcard_useamount_ratio
,cral_zbva_xawd_bbvf_mfn2
,candltrt_ed30dfm
,pboc_cc_max_bal_inval_mth
,inquire_reason_loan_num_wave_sub_l330d
,candnlted6_12mnbcnyuar
,add_loan_account_num_wave_sub_l360d
,organizations_slc_cfc_permonth_repayments_amount
,inquire_loan_organizations_nbank_wave_dividing_l90d
,account_loanfirst_open6m_orgratio
,pboc_occupation
,all_loan_sum_actual_repayment_of_all_amount_ratio
,agaa_zbvg_xava_bbvh_md
,swwhcdr
,pboc_earliestrecordcredi
,cram_bbvh_ldve_mfn9
,creditcard_all_loan_actual_repayment_sum_cur
,pboc_summary_ovd_nrev_amt_max_by_mth
,pboc_summary_unstl_rev_sub_bal
,pboc_cc_amt_avg
,inloan_consumer_organizations_num_dividing_account_ratio
,account_ccln_balance500p_orgcnt
,pboc_base_phone_cnt
,account_ccln_nonguaran_balance_sum
,cral_zbvg_xawh_bbvf_mdn4
,s01jhstoam
,all_useamt_ratio_exceed_75pct_account
,pboc_6m_newcreloan_amt
,pboc_ovd_act_cnt_rate
,d15_q01_rfiap_nqi_r
,inquire_reason_creditcard_num_l900d
,pboc_crah_mfo1
,d07_qry_c
,candnlted2_3mnbcnyuuar
,pboc_totalcreditloanamount
,pboc_debitcardaccountnum
,loan_normal_account_type_d1_max_balance
,account_ccn_nocarandnohouse_balance_max
,pboc_coaa_zbva_xavh_bbvf_n9
,inquire_num_l150d
,account_loan__noclosure_mob_min
,inquire_bank_num_l6m
,s06jhcd
,pboc_dksp_24m_nbank_cnt
,loan_remaining_periods_exceed_6_permonth_repayments_sum
,creditcard_max_use_amount_lst_6m_average_amount_ratio
,pboc_summary_unstl_rev_sub_act_cnt
,pboc_nrev_open_l12m_stl_amt
,pboc_acct_nrvlv_credit_amt_sum_div_orgs
,o01cncho02cnchr
,account_lnfirsit_overdue_mob_min
,creditcard_aging_median_of_day
,pboc_account_cnt
,pboc_loan_ovd_act_pct
,tisd1bs
,inquire_organizations_nbank_num_wave_sub_l90d
,creditcard_permonth_repayments_of_actual_repayment_max_ratio
,pboc_dksp_24m_query_ins_cnt
,pboc_query_loan_cnt_l24m
,pboc_overdueamtonemonthloan
,q01_nbfiqc_rrg_s
,loan_bank_sum_amount
,dfied24isial
,pboc_com_loan_small_act_cnt
,creditcard_all_loan_never_overdue_actual_repayment_l3m
,all_useamt_ratio_exceed_90pct_account
,pboc_acct_cc_avg_useamt_in6m
,creditcard_average_permonth_actual_repayment
,agaa_zbvg_xawh_bbvf_me
,d15_q01_rfiap_qc_r
,pboc_cc_total_amt
,q01_tqc_rfioorg_gea_qc_r
,all_amount_less_than_30000_of_all_account_ratio
,s03jhstoas
,q12_qitnbfiqc_cmos_max
,pbocv_md_l_cnt_l3_12
,account_consumer_loans_3m_balance_sum
,candnlted24_allmnbcnycar
,all_loan_approval_rate_l12m
,d07_q12_nbfiqc_r
,inquire_reason_loan_max_wave_sub_l180d
,inquire_nbank_num_l420d
,pboc_base_marriage_status
,account_creditcard_mob_median
,candltedallmbcnyua_uual
,pboc_summary_unstl_rev_bal
,pboc_loan_amt
,pboc_12m_ffbjq_num
,pboc_latest6querytimedebitcard
,pboc_cpf_max_amt
,creditcard_normal_average_opening_days
,all_loan_approval_rate_l6m
,pboc_total_querycnt
,d07_q09_oorgqc_r
,candnlted12mnbcnyua_uual
,pboc_dkcpxzhzz_pjzl
,inloan_account_type_r1_max_amount
,pboc_com_loan_act_cnt
,pboc_loan_com_bal_total
,all_loan_approval_rate_l3m
,estimated_revenue_average
,pbocv_credit_1stopen_mths
,credit_repay_method_xxhb_debx_average_int
,repay_ccln_overdue_mob_min
,all_loan_creditcard_approval_rate_lst_overdue
,creditcard_use_amount_exceed_actual_repayment_account_ratio
,q03_bfiqc_rrgr_r
,d15_q09_cfcqc_r
,pboc_6m_newloan_amt
,add_loan_account_num_l6m_dividing_l24m_ratio
,d15_q09_nbfiqc_r
,pboc_acct_rvlv_amt_sum_div_orgs
,candnlted24mnbcnyuas
,loan_normal_nbank_balance_average
,account_creditcard_1m_useamount_sum
,pboc_debts_sgad_mc
,query_6m_securities_insurance_ratio
,creditcard_max_use_amount_of_sum_max_use_amount_ratio
,d07_q09_nbfiqc_r
,creditcard_average_useamt_l6m_of_max_useamt_max_ratio
,credit_repay_method_debx_min_int
,pboc_query_tfau_mg
,inquire_slc_num_l12m
,pboc_dkywzlwgrxfdk_num
,pboc_query_cnt_l6m
,pboc_summary_unstl_rev_sub_lmt
,never_overdue_all_registered_time_exceed_12m_account
,pboc_mob_mx
,candnlted24mbcnyclm
,cral_zbvg_xavh_bbvf_mdn5
,inloan_account_type_r1_num_dividing_account_ratio
,creditcard_all_loan_approval_rate_l12m
,creditcard_max_amount_of_average_use_amount_ratio
,all_loan_max_actual_repayment_of_all_amount_ratio
,loan_nbank_min_amount
,d15_q03_nbfiqc_r
,pboc_datedebitcardfirstall
,loan_remaining_periods_less_than_12_cfc_permonth_repayments_var
,pboc_total_use_ratio
,pbocv_loan_noncarhouse_bal
,pboc_12m_total_querycnt


from znzz_fintech_ads.lxl_tmp_pboc_model_sample_250522 as t
where apply_time>=apply_time_auth 
  and apply_date>='2024-10-01'
  and channel_id in (227, 231, 245, 213, 241, 240, 302, 233, 246, 251, 249,247, 80038)
"""
df_sample_1 = get_data(sql)


# In[139]:


df_sample_ = df_sample_1.copy()


# In[140]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
# df_sample_ = pd.read_csv('df_sample_0525.csv')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[ ]:


# df_sample_.to_csv('df_sample_0525.csv',index=False)


# In[141]:


print(df_sample_['apply_date'].max(),df_sample_['apply_date'].min())


# In[142]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())
print(df_sample_.shape[0], df_sample_['user_id'].nunique())


# In[143]:


df_sample_.dropna(how='all', axis=1, inplace=True)
print(df_sample_.shape[1])


# In[144]:


print(df_sample_.columns.to_list()[:15])


# In[145]:


varsname = df_sample_.columns.to_list()[15:]

# print(varsname)
print("初始特征变量个数：",len(varsname))


# In[ ]:


# categorical_vars = ['pboc_all_class5_status_cur','pboc_rpt_query_date','pboc_base_birth_date',
#                  'pboc_cc_repay_exception',
#             'pboc_djk_isdj','pboc_djk_iszf','pboc_cc_24m_g','pboc_cc_account_exception',
#             'pboc_datedebitcardfirstactive','pboc_datedebitcardfirstall','pboc_loan_repay_exception',
#             'pboc_dkwjfl_new','pboc_loan_class5_status_cur','pboc_loan_fiveclass_status','pboc_loan_24m_g',
#             'pboc_loan_account_exception','pboc_loantopfivelevel','pboc_danbao_class5_status','rh_ifhousecarcard',
#             'pboc_curr_ovdstatus','pboc_digital_score_explain','pboc_employer1gettime','pboc_employer2gettime',
#             'pboc_employer3gettime','pboc_cpf_last_pay_state','pboc_base_birth_address',
#             'pboc_base_marriage_status','pboc_edulevel','pboc_specialdeal_status','rh_month6_loan_ifnull',
#             'rh_month6_loan_ifnormal','pboc_base_job_status','pboc_occupation','pboc_summary_query_org_type_lst',
#             'pboc_summary_query_date_lst','pboc_summary_query_rsn','pboc_is_dbh','pboc_is_car_loan',
#             'pboc_is_house_loan','pboc_is_cpf_loan','pboc_is_student_loan','pboc_szjdf',
#             'pboc_base_contact_address','pboc_repayduty_fiveclass_status2','pboc_repayduty_fiveclass_status1',
#             'pboc_zhzt_zxjl','pboc_base_sex','pboc_base_gender','pboc_education','pboc_base_education',
#             'pboc_base_degree','occupation_industry_type','pboc_whitestuatus','rh_querystate','pboc_is_bh',
#             'pboc_qc_24m_g','rh_danwei']
# categorical_vars = [col for col in categorical_vars if col in varsname]
# print(f"分类变量的个数：{len(categorical_vars)}")

# numerical_vars = [col for col in varsname if col not in categorical_vars]
# print(f"数值变量的个数：{len(numerical_vars)}")


# In[ ]:


# df_sample_[categorical_vars].info(show_counts=True)
# df_sample_[categorical_vars].head()


# In[ ]:


# df_sample_[varsname].select_dtypes(include=['object']).info(show_counts=True)
# df_sample_[varsname].select_dtypes(include=['object']).head()


# In[146]:


# 筛选出所有数据类型为'object'的列
object_cols = list(df_sample_[varsname].select_dtypes(include=['object']).columns)
print(len(object_cols))


# In[147]:


keep_object_cols = ['pboc_loantopfivelevel','pboc_cpf_last_pay_state',
             'pboc_base_marriage_status','pboc_base_job_status','pboc_occupation',
            'pboc_summary_query_org_type_lst','pboc_summary_query_rsn','pboc_base_sex',
             'pboc_education','pboc_base_degree']
print(len(keep_object_cols))
keep_object_cols = [col for col in object_cols if col in keep_object_cols]
print(len(keep_object_cols))
# drop_object_cols = [col for col in object_cols if col not in keep_object_cols]
# print(len(drop_object_cols))


# In[ ]:


# print(df_sample_.columns.to_list()[:14])


# In[ ]:


# varsname = df_sample_.columns.to_list()[14:]

# # print(varsname)
# print("初始特征变量个数：",len(varsname))


# In[149]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[505]:


df_sample[['order_no','y_prob_base_v4']+list(feat['model_name'])].head(200).to_csv('test.csv')


# In[150]:


df_sample_[keep_object_cols] = df_sample_[keep_object_cols].replace(['-1', '-9999'], np.nan)


# In[151]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[152]:


for col in keep_object_cols:
    df_sample_[col + '_c'] = df_sample_[col]


# In[153]:


categorical_vars = keep_object_cols[:]
print(f"分类变量的个数：{len(categorical_vars)}")
print(f"分类变量的个数：{len(categorical_vars)}")


# In[154]:


mappings = {}
cat_code = {}
for col in categorical_vars:
    # 获取类别和编码的映射关系
    mappings[col] = dict(enumerate(df_sample_[col].astype('category').cat.categories))
    df_sample_[col] = df_sample_[col].astype('category').cat.codes
    # 获取类别和编码的映射关系
    cat_code[col] = dict(enumerate(df_sample_[col+'_c'].astype('category').cat.categories))


# In[488]:


df_sample.query("order_no=='25010318581423325836'")[['pboc_base_sex_c','pboc_education_c','pboc_summary_query_org_type_lst_c','pboc_occupation_c']]


# In[155]:


mappings


# In[156]:


cat_code


# In[157]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[158]:


# for col in categorical_vars:
#     print(df_sample_[col + '_c'].value_counts(dropna=False))
#     print(df_sample_[col].value_counts())


# In[160]:


import json

with open(result_path + 'category_mapping.json', 'w') as f:
    json.dump(mappings, f)


# In[161]:


import json

with open(result_path + 'cat_code.json', 'w') as f:
    json.dump(cat_code, f)


# In[ ]:


# with open(result_path + 'category_mapping.json', 'r') as f:
#     loaded_map = json.load(f)

# # 注意：JSON 的 key 是字符串，要转换成 int
# loaded_map = {int(k): v for k, v in loaded_map.items()}


# In[162]:


numerical_vars = [col for col in varsname if col not in categorical_vars]
print(f"数值变量的个数：{len(numerical_vars)}")


# In[163]:


for i, col in enumerate(numerical_vars):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col],errors='coerce')


# In[164]:


df_sample_[df_sample_[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample_[df_sample_[varsname].isna().all(axis=1)].info(show_counts=True)


# In[ ]:


# df_sample_[df_sample_[varsname].isna().all(axis=1)].head(3)


# In[ ]:


# df_sample_.drop(index=df_sample_[df_sample_[varsname].isna().all(axis=1)].index,inplace=True)
# df_sample_.shape


# In[165]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[166]:


df_sample_['target_fpd30'].value_counts()


# In[167]:


df_sample_['target_fpd30'].value_counts(normalize=True)


# In[ ]:


# df_sample_.to_csv('df_sample_原始_250516.csv',index=False)


# In[168]:


df_sample = df_sample_.copy()


# In[ ]:


# df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[ ]:


# df_sample.info(show_counts=True)


# In[170]:


df_sample['apply_date'] = df_sample['apply_date'].apply(str)
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[171]:


df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date< '2025-03-01'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-20'").index, 'data_set']='3_oot2'


# In[ ]:


# df_sample.to_csv(result_path + '提现人行征信模型fpd30标签2411_2502_v1_0527.csv',index=False)
# print(result_path + '提现人行征信模型fpd30标签2411_2502_v1_0527.csv')


# In[ ]:


# df_sample[df_sample[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index, inplace=True)


# In[172]:


target = 'target_fpd30'


# # 1. 样本概况

# In[173]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[174]:


print(df_sample[target].value_counts())


# In[175]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[176]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[177]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_v4_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_v4_{timestamp}.xlsx")


# In[178]:


df_vars_des = pd.read_excel('人行线上变量清单.xlsx')
print(df_vars_des.shape)
df_vars_des.head()


# In[ ]:





# # 2.数据探索性分析

# In[179]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor = pd.merge(df_vars_des.set_index('变量编码'), df_explor, how='right',left_index=True,right_index=True)
df_explor.head()


# ## 2.1缺失值处理

# In[180]:


len(numerical_vars)


# In[181]:


for col in numerical_vars:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[182]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[183]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[184]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_v4_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_v4_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[185]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[186]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[187]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[188]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_变量缺失率_{task_name}_v4_{timestamp}.xlsx') as writer:
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_变量缺失率_{task_name}_v4_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[189]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=1.0).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=1.0].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

# to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
# print("to_drop_mode:", len(to_drop_mode))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique ))
print(f"删除的变量有{len(to_drop1)}个")


# In[190]:


to_drop_recent


# In[191]:


varsname_v1 = varsname[:]
# varsname_v1 = [col for col in varsname if col not in to_drop1] 
# print(f"保留的变量有{len(varsname_v1)}个")
# print(varsname_v1[:10])


# In[192]:


len(varsname_v1)


# ## 3.2 基于变量重要性/相关性删除变量
# 

# In[ ]:


# # 2.3 快速查看特征重要性
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='dt', min_samples=0.05, n_bins=6)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.02, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:].head()


# In[226]:


varsname_v2 = varsname_v1[:]
# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[429]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df



def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[430]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[439]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_base_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[440]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[441]:


categorical_vars


# In[442]:


len(varsname_base_v2)


# In[ ]:


# numerical_vars_v2 = [col for col in varsname_v2 if col in numerical_vars]
# print(f"数值变量个数:{len(numerical_vars_v2)}")
# categorical_vars_v2 = [col for col in varsname_v2 if col in categorical_vars]
# print(f"数值变量个数:{len(categorical_vars_v2)}")


# In[ ]:


# numerical_vars_v2.append('pboc_datedebitcardfirstactive')
# categorical_vars_v2.remove('pboc_datedebitcardfirstactive')


# In[445]:


numerical_vars_base = [ col for col in varsname_base_v2 if col not in categorical_vars]
len(numerical_vars_base)


# In[ ]:


# print(f"数值变量个数:{len(numerical_vars_v2)}")
# print(f"数值变量个数:{len(categorical_vars_v2)}")


# In[446]:


new_bins_dict = {}
to_drop_mode = []

for i, col in enumerate(numerical_vars_base):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[ ]:





# In[447]:


new_bins_dict


# In[448]:


categorical_vars_base = [ col for col in varsname_base_v2 if col in categorical_vars]


# In[449]:


for col in categorical_vars_base:
    new_bins_dict[col] = existing_bins_dict[col]


# In[450]:


combiner.load(new_bins_dict)


# In[451]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[452]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[454]:


varsname_v2 = varsname_base_v2[:]


# In[455]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

print('---------')
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print('---------')


# In[456]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[457]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[458]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[459]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
drop_by_psi = list(set(drop_by_psi_month + drop_by_psi_set))
print("drop_by_psi: ", len(drop_by_psi))


# In[ ]:


df_psi_by_month.loc[drop_by_psi,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi,:]


# In[ ]:


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<0.01].dropna(how='all').index)
drop_by_iv = list(set(drop_by_iv_month + drop_by_iv_set))
print("drop_by_iv: ", len(drop_by_iv))


# In[ ]:


len(drop_by_iv_set)


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[ ]:


to_drop3 = list(set(drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")
print(varsname_v3)


# ## 4.2 Y标签相关性删除

# In[460]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v2]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)

print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[461]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[462]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v2].corr(method='spearman')
df_corr_matrix.info()
df_corr_matrix.head(2)


# In[463]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[472]:


df_high_corr.info()
df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


df_iv_by_set.loc[to_drop4,:]


# In[465]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
# print(f"保留变量{len(varsname_v4)}个")
# print(varsname_v4)


# In[466]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
#         all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
#     return (all_corrs, all_pvalue)
    return all_corrs


# In[467]:


# 调用函数
# df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
#                                                                     'apply_month',
#                                                                     varsname_v4,
#                                                                     target,
#                                                                     method='pointbiserialr'
#                                                                    )
df_corr_vars_target = calculate_correlations(df_sample_woe,
                                            'apply_month',
                                            varsname_v2,
                                            target,
                                            method='pointbiserialr'
                                           )
# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[470]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[471]:


df_corr_vars_target.loc[to_drop5,:]


# In[ ]:


varsname_v5 = [col for col in varsname_v4 if col not in to_drop5]
print(f"保留变量{len(varsname_v5)}个")
print(varsname_v5)


# In[469]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set')
#         df_iv.to_excel(writer, sheet_name='df_iv')
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_distribute_iv_psi_corr_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:


gc.collect()


# # 5.模型训练

# ## 5.0 函数定义

# In[193]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[194]:


target = 'target_fpd30'


# In[195]:


df_sample[target].value_counts()


# In[196]:


df_sample['target_fpd30_1'] = 1 - df_sample[target]
modeltrian_target = 'target_fpd30_1'


# In[197]:


df_sample[modeltrian_target].value_counts()


# In[198]:


df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[199]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[200]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[201]:


print("最优参数opt_params: ", opt_params)


# In[ ]:


# print(len(varsname_v5))
# print(varsname_v5)


# In[ ]:


# print(varsname_v2)


# In[203]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[204]:


varsname_base = varsname_v1[:]
print(len(varsname_base))


# In[ ]:


# df_sample['pboc_base_sex'].head()


# In[ ]:


# df_sample['pboc_base_sex'] = df_sample['pboc_base_sex'].astype('category').cat.codes
# df_sample['pboc_base_sex'].head()
# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# df['pboc_base_sex'] = le.fit_transform(df['pboc_base_sex'])


# In[ ]:


# df_sample['pboc_base_sex'].value_counts()


# In[205]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[206]:


df_sample['data_set'].value_counts(dropna=False)


# In[207]:


categorical_vars


# In[208]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars)


# In[210]:


lgb_model.params


# In[209]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[211]:


# 最初评估模型效果 
df_ks_auc_set_base = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v1', 'data_set')
# df_ks_auc_set_base['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base = pd.concat([tmp, df_ks_auc_set_base], axis=1)
print(df_ks_auc_set_base)


# In[212]:


df_ks_auc_month_base = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_base


# In[213]:


# 最初评估模型效果 
df_sample_jk30 = df_sample.query("diff_days>30 & channel_types=='金科渠道'")


# In[214]:


# 最初评估模型效果 
df_ks_auc_set_base_30 = model_ks_auc(df_sample_jk30, modeltrian_target, 'y_prob_base_v1', 'data_set')
# df_ks_auc_set_base['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30 = pd.concat([tmp, df_ks_auc_set_base_30], axis=1)
print(df_ks_auc_set_base_30)


# In[215]:


df_ks_auc_month_base_30 = calculate_ks_auc(df_sample_jk30, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_base_30


# In[ ]:


df_vars_des.head()


# In[217]:


df_vars_des.rename(columns={'变量编码':'feature'},inplace=True)


# In[218]:


# 模型变量重要性
# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base = feature_importance(lgb_model) 
# df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_base = df_importance_base.reset_index()
df_importance_base = pd.merge(df_vars_des, df_importance_base, how='right',on='feature')
df_importance_base


# In[224]:


# 模型变量重要性
# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_set_base = feature_importance(lgb_model) 
# df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base = df_importance_set_base.reset_index()
df_importance_set_base = pd.merge(df_vars_des, df_importance_set_base, how='right',on='feature')
df_importance_set_base


# In[ ]:


df_model_corr_base = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
df_model_corr_base.info()
df_model_corr_base.head()


# In[225]:


# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v1_{timestamp}.xlsx') as writer:
    df_importance_set_base.to_excel(writer, sheet_name='df_importance_set_base')
    df_importance_base.to_excel(writer, sheet_name='df_importance_base')
    df_ks_auc_set_base.to_excel(writer, sheet_name='df_ks_auc_set_base')
    df_ks_auc_month_base.to_excel(writer, sheet_name='df_ks_auc_month_base')
    df_ks_auc_set_base_30.to_excel(writer, sheet_name='df_ks_auc_set_base_30')
    df_ks_auc_month_base_30.to_excel(writer, sheet_name='df_ks_auc_month_base_30')
print(result_path + f'4_模型训练_{task_name}_base4_v1_{timestamp}.xlsx')


# ### 5.2.2 模型优化

# In[ ]:


# df_sample = pd.read_csv(result_path + '提现人行征信模型fpd30标签2411_2501_v1_0515.csv')


# #### 5.2.2.1 特征优化

# In[226]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[227]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# varsname_base_v2 = varsname_v4[:]
# print(len(varsname_base_v2))


# In[228]:


varsname_base_v2 = list(df_importance_set_base.query("gain>0")['feature'])
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[85]:


varsname_base_v2 = list(df_importance_set_base.query("gain>0")['feature'])
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[ ]:


# drop_cols_2_1 = ['creditcard_all_loan_approval_rate_l3m','q03_q12_ncfcqc_r','o01callcho02callchr','agaa_zbvg_xawh_bbvf_me','agaa_zbvg_xawd_bbvh_me','loan_max_amount_l1m','q09_qitncfc_qccv_r']
# varsname_base_v2_1 = ['creditcard_all_loan_approval_rate_l3m','agaa_zbvg_xawh_bbvf_me','q09_qitncfc_qccv_r','candnlted1mnbcnycas','s02jhcdr','pboc_rebn_zcva_xbvb_bavf_mcq8','candnlted1m10cnycas','inquire_reason_exclude_plm_l1m_dividing_l12m_ratio','twwcallm1toas','pboc_summary_unstl_rev_repay_avg_l6m','t24callr1xcdr','q01_q24_fgcqc_oorg_r','pboc_pbccredituseminusper','pboc_base_sex','q01_tqc_ncfcqc_r','candltedallmnbcnycam','pboc_cc_total_ovdcnt','candnlted1mnbcnyua_uuam','tisr2b_arl6mr','cclnsum_6mavguseamount_sum','owwcwwxcdr','redg_zbvb_xave_mc','pboc_com_loan_bal','s00jhdn','candltrt_ed30dfm','loan_normal_cfc_opening_days_sum','candnltedallmnbcnycaa','d15_q09_nbfiqc_r','q12_tqi_ncfc_nqi_r','c02crchr','candnlted12m10cnycam','candltedallmbcnyua_uual','loan_slc_amount_average','estimated_revenue_max','pboc_xyk_edsyl']
# len(varsname_base_v2_1)


# In[ ]:


# varsname_base_v2_2 = varsname_base_v2[:]
# varsname_base_v2_2.remove('pbocv_cc_max_lmtutlrate')
# varsname_base_v2_2.remove('pboc_acct_rvlv_amt_sum_div_orgs')
# varsname_base_v2_2.remove('candltrt_ed30dfm')
# print(varsname_base_v2_2)
# print(len(varsname_base_v2_2))


# In[ ]:


# df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date< '2025-03-01'").index, 'data_set']='1_train'
# df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
# df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-20'").index, 'data_set']='3_oot2'


# In[229]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:





# In[230]:


# 查看训练数据集
df_sample['data_set'].value_counts(dropna=False)


# In[231]:


categorical_vars


# In[232]:


categorical_vars_v2 = [col for col in categorical_vars if col in varsname_base_v2]
categorical_vars_v2


# In[233]:



# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[112]:


# 1. 各模型打分
lgb_model1 = load_model_from_pkl('./result_v4/提现人行征信模型fpd30标签_base4_v2_20250526140948.pkl')
feature_names = lgb_model1.feature_name()
df_sample['y_prob_base_v2'] = lgb_model1.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[234]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[235]:


# 最初评估模型效果 
df_ks_auc_set_base_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'data_set')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v2 = pd.concat([tmp, df_ks_auc_set_base_v2], axis=1)
print(df_ks_auc_set_base_v2)


# In[236]:


# 最初评估模型效果 
df_ks_auc_month_base_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v2 = pd.concat([tmp, df_ks_auc_month_base_v2], axis=1)
print(df_ks_auc_month_base_v2)


# In[237]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")
# 最初评估模型效果 
df_ks_auc_set_base_30_v2 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v2', 'data_set')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v2 = pd.concat([tmp, df_ks_auc_set_base_30_v2], axis=1)
print(df_ks_auc_set_base_30_v2)


# In[238]:


# 最初评估模型效果 
df_ks_auc_month_base_30_v2 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_30_v2 = pd.concat([tmp, df_ks_auc_month_base_30_v2], axis=1)
print(df_ks_auc_month_base_30_v2)


# In[239]:


df_ks_auc_month_base_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_base_v2


# In[240]:


df_ks_auc_month_base_30_v2 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_base_30_v2


# In[241]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v2 = feature_importance(lgb_model) 
# df_importance_base_v2 = pd.merge(df_importance_base_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v2 = df_importance_base_v2.reset_index()
df_importance_base_v2 = pd.merge(df_vars_des, df_importance_base_v2, how='right',on='feature')
df_importance_base_v2


# In[242]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v2 = feature_importance(lgb_model) 
# df_importance_set_base_v2 = pd.merge(df_importance_set_base_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v2 = df_importance_set_base_v2.reset_index()
df_importance_set_base_v2 = pd.merge(df_vars_des, df_importance_set_base_v2, how='right',on='feature')
df_importance_set_base_v2


# In[ ]:


# df_model_corr_base_v2 = df_corr_matrix.loc[varsname_base_v2,varsname_base_v2].reset_index()
# df_model_corr_base_v2.info()
# df_model_corr_base_v2.head()


# In[243]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v2_{timestamp}.xlsx') as writer:
    df_importance_set_base_v2.to_excel(writer, sheet_name='df_importance_set_base_v2')
    df_importance_base_v2.to_excel(writer, sheet_name='df_importance_base_v2')
    df_ks_auc_set_base_v2.to_excel(writer, sheet_name='df_ks_auc_set_base_v2')
    df_ks_auc_month_base_v2.to_excel(writer, sheet_name='df_ks_auc_month_base_v2')
    df_ks_auc_set_base_30_v2.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v2')
    df_ks_auc_month_base_30_v2.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v2')
print(result_path + f'4_模型训练_{task_name}_base4_v2_{timestamp}.xlsx')


# In[ ]:





# #### 5.2.2.2 参数优化

# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.6


# In[ ]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


len(varsname_base_v2)


# In[ ]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


categorical_vars_v2


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[ ]:


# 优化后评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[ ]:



# 最初评估模型效果 
df_ks_auc_set_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3 = pd.concat([tmp, df_ks_auc_set_base_v3], axis=1)
print(df_ks_auc_set_base_v3)


# In[ ]:


df_ks_auc_month_base_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_v3


# In[ ]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[ ]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v3 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v3', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v3 = pd.concat([tmp, df_ks_auc_set_base_30_v3], axis=1)
print(df_ks_auc_set_base_30_v3)


# In[ ]:


df_ks_auc_month_base_30_v3 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_30_v3


# In[ ]:





# In[ ]:





# In[ ]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v3 = feature_importance(lgb_model) 
# df_importance_base_v3 = pd.merge(df_importance_base_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v3 = df_importance_base_v3.reset_index()
df_importance_base_v3 = pd.merge(df_vars_des, df_importance_base_v3, how='right',on='feature')
df_importance_base_v3


# In[ ]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v3 = feature_importance(lgb_model) 
# df_importance_set_base_v3 = pd.merge(df_importance_set_base_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v3 = df_importance_set_base_v3.reset_index()
df_importance_set_base_v3 = pd.merge(df_vars_des, df_importance_set_base_v3, how='right',on='feature')
df_importance_set_base_v3


# In[ ]:





# In[ ]:


# df_model_corr_base_v3 = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
# df_model_corr_base_v3.info()
# df_model_corr_base_v3.head()


# In[ ]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v3_{timestamp}.xlsx') as writer:
    df_importance_set_base_v3.to_excel(writer, sheet_name='df_importance_set_base_v3')
    df_importance_base_v3.to_excel(writer, sheet_name='df_importance_base_v3')
    df_ks_auc_set_base_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_v3')
    df_ks_auc_month_base_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_v3')
    df_ks_auc_set_base_30_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v3')
    df_ks_auc_month_base_30_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v3')
print(result_path + f'4_模型训练_{task_name}_base4_v3_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[ ]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[ ]:


# 优化后评估模型效果
df_sample['y_prob_base_v3_1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3_1'].head()


# In[ ]:


# 最初评估模型效果 
df_ks_auc_set_base_v3_1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3_1', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3_1 = pd.concat([tmp, df_ks_auc_set_base_v3_1], axis=1)
print(df_ks_auc_set_base_v3_1)


# In[ ]:


df_ks_auc_month_base_v3_1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3_1', 'apply_month')
df_ks_auc_month_base_v3_1


# In[ ]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[ ]:


df_ks_auc_month_base_30_v3_1 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v3_1', 'apply_month')
df_ks_auc_month_base_30_v3_1


# #### 5.2.2.3 特征优化v2

# In[491]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[492]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[246]:


varsname_base_v3 = list(df_importance_set_base_v2.query("gain>0")['feature'])
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[489]:


lgb_model= load_model_from_pkl('./result_v4/提现人行征信模型fpd30标签_base4_v4_20250527182448.pkl')


# In[493]:


varsname_base_v3 = lgb_model.feature_name()
len(varsname_base_v3)


# In[380]:


varsname_base_v3 = list(df_importance_set_base_v4.query("gain>0")['feature'])
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[494]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[495]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[496]:


print(categorical_vars_v2)


# In[497]:


categorical_vars_v3 = [col for col in categorical_vars_v2 if col in varsname_base_v3]
print(categorical_vars_v3)


# In[498]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v3)


# In[307]:


# lgb_model.params


# In[289]:


# 优化后评估模型效果
# df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
# df_sample['y_prob_base_v4'].head()


# In[499]:


df_sample['y_prob_base_v5'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v5'].head()


# In[500]:


# 最初评估模型效果 
df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v5', 'apply_month')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[394]:


# 最初评估模型效果 
df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v5', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[393]:


df_ks_auc_month_base_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v5', 'apply_month')
df_ks_auc_month_base_v4


# In[501]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[502]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v4 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v5', 'apply_month')
# df_ks_auc_set_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'apply_month').set_index('bins')
df_ks_auc_set_base_30_v4 = pd.concat([tmp, df_ks_auc_set_base_30_v4], axis=1)
print(df_ks_auc_set_base_30_v4)


# In[503]:


lgb_model.save_model('M1A0043.txt')


# In[391]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v4 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v5', 'data_set')
# df_ks_auc_set_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v4 = pd.concat([tmp, df_ks_auc_set_base_30_v4], axis=1)
print(df_ks_auc_set_base_30_v4)


# In[395]:


df_ks_auc_month_base_30_v4 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v5', 'apply_month')
df_ks_auc_month_base_30_v4


# In[396]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v4 = feature_importance(lgb_model) 
# df_importance_base_v4 = pd.merge(df_importance_base_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v4 = df_importance_base_v4.reset_index()
df_importance_base_v4 = pd.merge(df_vars_des, df_importance_base_v4, how='right',on='feature')
df_importance_base_v4


# In[390]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v4 = feature_importance(lgb_model) 
# df_importance_set_base_v4 = pd.merge(df_importance_set_base_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v4 = df_importance_set_base_v4.reset_index()
df_importance_set_base_v4 = pd.merge(df_vars_des, df_importance_set_base_v4, how='right',on='feature')
df_importance_set_base_v4


# In[ ]:


# df_model_corr_base_v4 = df_corr_matrix.loc[varsname_base_v4,varsname_base_v4].reset_index()
# df_model_corr_base_v4.head()


# In[299]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v4_{timestamp}.xlsx') as writer:
    df_importance_set_base_v4.to_excel(writer, sheet_name='df_importance_set_base_v4')
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v4')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v4')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v4')
    df_ks_auc_set_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v4')
    df_ks_auc_month_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v4')
print(result_path + f'4_模型训练_{task_name}_base4_v4_{timestamp}.xlsx')


# In[397]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v5_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v5_{timestamp}.xlsx') as writer:
    df_importance_set_base_v4.to_excel(writer, sheet_name='df_importance_set_base_v5')
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v5')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v5')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v5')
    df_ks_auc_set_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v5')
    df_ks_auc_month_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v5')
print(result_path + f'4_模型训练_{task_name}_base4_v5_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[ ]:


sql = """
select 
 t.order_no
-- 深圳团队子分
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,t_high_p_f30_2504
,t_high_p_m3d30_2025
,M1A0017
,M1A0028
,M1A0031

-- 北京团队模型子分
,t_mix_pboc2_dpd20
,t_pboc_dpd20

from 
    (
    select 
     t1.order_no
    from znzz_fintech_ads.dm_f_zzj_test_order_target as t1 
    where dt='2025-05-22'

    ) as t 
-- 北京团队的子分
inner join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date >= '2024-09-01'
      and apply_date <= '2025-03-20'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no 
    ,max(case when variable_code = 't_high_p_f30_2504' then good_score else null end) as t_high_p_f30_2504 
    ,max(case when variable_code = 't_high_p_m3d30_2025' then good_score else null end) as t_high_p_m3d30_2025 
    ,max(case when variable_code = 'M1A0017' then good_score else null end) as M1A0017 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031 
  from znzz_fintech_ads.lending_model01_scores_off as t 
    where lending_time >= '2024-09-01'
      and lending_time <= '2025-03-20'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
left join 
    (
    select 
     order_no 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
  from znzz_fintech_ads.lending_model01_scores_vars as t 
    where lending_time >= '2024-09-01'
      and lending_time <= '2025-03-20'
    group by order_no
    ) as t3 on t.order_no=t3.order_no 
;


"""
df_bj = get_data(sql)
df_bj.info(show_counts=True)


# In[398]:


df_bj = pd.read_csv('df_bj.csv')
df_bj.info(show_counts=True)
df_bj.head()


# In[403]:


print(df_bj.shape[0],df_bj['order_no'].nunique())


# In[404]:


df_bj.drop_duplicates(subset=['order_no'],keep='first',inplace=True)


# ### 5.3.1数据处理

# In[ ]:


# # 1. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v2_20250516151629.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[ ]:


# # 2. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v3_20250519141155.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[ ]:


# # 3. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v4_20250520141208.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[405]:


df_sample.columns[-10:]


# In[406]:


usecols = ['order_no','diff_days','target_mob4dpd30'] + list(df_sample.columns[-9:]) 

print(df_sample.shape)
df_evalue = pd.merge(df_sample[usecols], df_bj, how='inner', on='order_no')
print(df_evalue.shape)
df_evalue.info(show_counts=True)


# In[407]:


print(df_evalue.shape[0],df_evalue['order_no'].nunique())


# In[408]:


pd.set_option('display.max_columns',None)
df_evalue.head()


# In[409]:


df_evalue[list(df_evalue.columns[8:])].describe()


# In[410]:


for col in ['all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','m1a0028']:
    df_evalue[col] = 1 - df_evalue[col]


# In[411]:


df_evalue.head()


# ### 5.3.2 效果对比

# In[412]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
#         df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
#         df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[413]:


print(df_evalue.columns[8:].to_list())


# In[ ]:





# In[414]:



score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_fpd30_1'] 
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

print(df_evalue.shape[0])
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[415]:



score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_fpd30_1'] 
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

print(df_evalue.shape[0])
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
tmp_df_evalue = tmp_df_evalue.query("channel_types=='金科渠道' & diff_days>30")
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1_30 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1_30


# #### 长目标

# In[416]:


df_evalue_dpd30 = df_evalue.query("target_mob4dpd30 >= 0 ")
df_evalue_dpd30.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v6', 'apply_month')
# df_ks_auc_month_pboc.head()


# In[ ]:


# df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v6', 'data_set')
# df_ks_auc_set_pboc['渠道'] = '全渠道'
# tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
# df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
# df_ks_auc_set_pboc


# In[417]:


df_evalue_dpd30['target_mob4dpd30_1'] = 1 - df_evalue_dpd30['target_mob4dpd30']


# In[418]:


score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

print(df_evalue_dpd30.shape[0])
tmp_df_evalue = df_evalue_dpd30.loc[df_evalue_dpd30[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1, df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2


# In[419]:


score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

print(df_evalue_dpd30.shape[0])
tmp_df_evalue = df_evalue_dpd30.loc[df_evalue_dpd30[score_list].notna().all(axis=1),:]
tmp_df_evalue = tmp_df_evalue.query("channel_types=='金科渠道' & diff_days>30")
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_30_v2 = pd.concat([df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_30_v2


# In[420]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_pboc_2_v2.to_excel(writer, sheet_name='mob4dpd30')
    df_ksauc_all_1_30.to_excel(writer, sheet_name='30_fpd30')
    df_ksauc_pboc_2_30_v2.to_excel(writer, sheet_name='30_mob4dpd30')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[421]:


df_evalue.to_csv(result_path + r'提现全渠道人行fpd30模型2411_2502_evalue.csv',index=False)


# In[473]:


print(result_path + r'提现全渠道人行fpd30模型2411_2502_evalue.csv')


# # 6. 评分分布

# In[422]:


df_sample['apply_month'].value_counts()


# In[423]:


score = 'y_prob_base_v4'


# In[424]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[425]:


df_sample['score_bins'].head()


# In[426]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[427]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[431]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[432]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[433]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_base4_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_base4_{timestamp}.xlsx')


# In[434]:


df_sample.to_csv(result_path + '提现全渠道人行fpd30模型_2411_2502_base4.csv',index=False)




#==============================================================================
# File: 提现人行征信模型mob4dpd30标签v1.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储目录
directory = f'./result_v5'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[3]:


task_name = '提现人行征信模型mob4dpd30标签'


# # 函数定义

# In[4]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[ ]:


df_sample_dict = {}


# In[ ]:


# # 计算今天的时间
# from datetime import datetime, timedelta, date

# today = datetime.now().strftime('%Y-%m-%d')
# print(today)

# this_day =datetime.strptime('2025-03-21', '%Y-%m-%d')
# end_day = datetime.strptime('2024-09-01', '%Y-%m-%d')

# while this_day >= end_day:
#     run_day = this_day.strftime('%Y-%m-%d')
#     sql = f'''
# select *
# from znzz_fintech_ads.lxl_tmp_pboc_model_sample_250522
# where apply_time>=apply_time_auth
#   and apply_date='{run_day}'
# '''
#     print(f'=========================={run_day}=============================')
#     df_sample_dict[run_day] = get_data(sql)
#     this_day = this_day - timedelta(days=1)


# In[115]:


sql = """
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.apply_time
,t.apply_date_auth
,t.apply_time_auth
,t.diff_days
,t.target_fpd10
,t.target_fpd20
,t.target_fpd30
,t.target_cpd30
,t.target_mob3dpd30
,t.target_mob4dpd30
,all_loan_approval_rate_l1m
,pboc_base_sex
,candnlted1m10cnycas
,inquire_organizations_nbank_average_wave_dividing_l360d
,pboc_latest1monthselfinquirequerynum
,creditcard_inquire_date_to_now_day
,creditcard_all_loan_approval_rate_l1m
,candnlted1mnbcnyua_uuam
,s00jdn
,swwjstoas
,tisr1b_arl6mr
,query_recentfirst_afterloan_count
,candnlted1mnbcnycas
,s00jhdn
,mom_add_inquire_reason_exclude_dhgl_num
,loan_sum_amount_wave_dividing_l90d
,redg_zbvb_xave_mc
,inloan_balance_of_all_ratio
,pboc_all_account_cnt
,q03_q12_ncfcqc_r
,pbocv_md_c_ulmt_pct_p_1_6
,agaa_zbvg_xawd_bbvh_me
,inquire_reason_exclude_plm_num_wave_dividing_l120d
,loan_normal_account_type_d1_balance_dividing_amount_ratio
,all_loan_amount_less_than_30000_actual_repayment_of_all_ratio
,all_loan_exclude_homeloan_carloan_sum_permonth_repayments
,pboc_summary_unstl_nrev_act_cnt
,pboc_education
,pboc_nrev_lmt_use_ratio
,pboc_summary_query_monitor_cnt_l24m
,q06_ncfcqc_rrgr_r
,loan_repay_normal_account_num_l12m
,all_actual_repayment_of_permonth_repayments_ratio
,loan_remaining_periods_exceed_6_permonth_repayments_ratio
,loan_other_loans_min
,credit_average_repay_balance
,t24callr1xcdr
,candnlted2mnbcnycas
,pboc_rebn_zcva_xbvb_bavf_mcq8
,owwcwwxcdr
,q01_tqc_ncfcqc_r
,pbocv_cc_min_creditlimit
,min_gap_day_of_opening_date_to_close_date
,q01_q24_rfioorg_gea_qc_r
,query_2m_nobank_ratio
,pboc_cc_avg_amt_last3
,twwcallr1xcdr
,query_9m_dksp_stddev
,inquire_slc_num_l6m
,loan_normal_account_type_r1_balance_dividing_amount_ratio
,candlted6m10cnyua_uuaa
,s02jhcdr
,loan_nbank_max_amount
,inquire_slc_num_of_all_ratio_l3m
,loan_normal_account_type_d1_min_amount_opening_date_gap
,cral_zbvg_xave_bbvf_mfn11
,pboc_summary_ovd_cc_amt_max_by_mth
,s02jhstoas
,add_loan_account_num_wave_dividing_l360d
,pboc_pbcotherloanrepaysum
,candnltedallmnbcnycaa
,lcdoddil
,all_loan_actual_repayment_of_permonth_repayments_ratio
,loan_normal_account_type_d1_max_amount_opening_date_gap
,pboc_summary_cc_acount_cnt
,pboc_coaa_zbvg_xawd_bbvd_n3
,query_afterloan_debts_orgcnt
,pboc_summary_unstl_rev_lmt
,q01_q24_fgcqc_r
,add_inquire_organizations_num_l3m_exclude_l4m_l12m
,account_loan_open3mp_orgratio
,s01jhcdr
,q09_qccv_r
,callolldn
,t01cnncdr
,pboc_overdueamtonemonthdebitcard
,q24_rficca_ldst_t
,d07_q24_ncfcqc_r
,pboc_summary_query_grr_cnt_l24m
,query_3m_ccln_loanorg_ratio
,loan_normal_bank_balance_average
,inquire_reason_loan_num_wave_dividing_l90d
,loan_cfc_permonth_repayments_average
,query_2m_dksp_loanorg_ratio
,loan_normal_cfc_opening_date_to_now_max
,deaa_zbva_xavc_bbvf_mdo2
,pboc_debitcardmaxline
,account_ccln_3m_status_c_ratio
,q01_q09_nbfiqc_r
,add_inquire_num_lst_special_transaction_tqjq_tqhk
,loan_normal_repayments_num_l6m
,s01jhstoas
,creditcard_all_loan_approval_rate_l3m
,pboc_latest24overduetimestotal
,agaa_zbva_xawd_bbvf_me
,creditcard_approval_rate_l24m
,q09_qitncfc_qccv_r
,twwcallm1toas
,t12cnr1xcdr
,inquire_slc_num_of_all_ratio
,candnlted1mnbcnycam
,loan_max_amount_l1m
,q12_tqi_rfila_nqi_r
,pboc_cur_loan_business_ltv
,pbocv_query_guaranchkorg_p3m
,loan_slc_balance_average
,pboc_summary_opn_cc_min_lmt
,loan_sum_amount_wave_sub_l90d
,mom_add_credit_account_amount_l1m
,inquire_reason_exclude_plm_num_wave_dividing_l90d
,t24cnr1xchr
,inquire_reason_exclude_plm_num_wave_dividing_l30d
,loan_normal_slc_opening_date_to_now_sum
,q09_qitnbfi_qccv_r
,acda_zbvb_n2
,pboc_overduemonthloan
,never_overdue_all_registered_time_exceed_6m_account
,d07_q03_nbfiqc_r
,add_loan_account_num_wave_sub_l180d
,pboc_summary_query_org_type_lst
,pboc_normal_loan_act_cnt_l24m
,candnltedallmbcnyua_uuam
,q06_q24_fgc_noqi_r
,cral_zbvg_xawe_bbvf_mcn9
,credit_balance_of_all_balance_ratio
,cral_zbvg_xavc_bbvj_mdn11
,pboc_cc_rcy_ovdmonth
,creditcard_max_use_ratio_amount_greater_than_amount_account_ratio
,account_business_loan_mob_avg
,twwcnxcdr
,cral_zbvg_xave_bbvf_mcn5
,loan_cfc_balance_max
,estimated_revenue_max
,credit_repay_method_debx_max_int
,pboc_summary_nrev_org_cnt
,pboc_loan_lastrepay_days
,never_overdue_all_loan_registered_time_exceed_12m_account
,cral_zbvg_xawh_bbvf_mfn3
,twwcallnmchr
,loan_normal_bank_opening_date_to_now_min
,cadnlt10momrtmc
,credit_loan_average_balance
,candnlted1mnbcnycaa
,inquire_num_diff_reason_l1m
,organizations_slc_cfc_balance_of_all_ratio
,q12_tqi_cfc_nqi_r
,pboc_latest2overduetimestotal
,pboc_xd_24m_query_ins_cnt
,pboc_pbccredituseminusper
,q01_q24_fgcqc_oorg_r
,pboc_summary_unstl_nrev_repay_avg_l6m
,loan_normal_bank_max_balance_opening_date_gap
,redg_zbvb_xave_md
,inloan_bank_max_balance
,query_1m_ccln_loanorg_ratio
,q12_tqi_ncfc_nqi_r
,pbocv_loan_noncrhs_avgamt_p12m
,candnlted24_allmnbcnyua_ar
,s12jhcd
,pboc_summary_unstl_rev_repay_avg_l6m
,o02cncho06cnchr
,callt00m1dn
,pboc_query_auth_3v12m_pct
,add_amount_less_than_30000_of_all_account_ratio_l12m
,add_amount_less_than_30000_of_all_amount_ratio_l6m
,loan_credit_amount_less_than_30000_amount
,identity_phone_mob_min
,credit_repay_method_xxhb_debx_sub_int
,loan_consumer_balance_dividing_amount_ratio
,t02callcht06callchr
,candnlted1_2m10cnycar
,pboc_last12n_zhjqp
,deaa_zbvg_xawh_bbvf_mdo9
,pboc_pbcdebitcardthismonthrepaymentsumline
,credit_average_gap_month_of_aod_to_acd
,pboc_dksp_3m_nbank_cnt
,c02crchr
,loan_max_amount_wave_dividing_l180d
,pboc_dksp_1m_nbank_cnt
,q12_rficca_qc_rrg_s
,creditcard_sum_average_useamt_l6m_of_sum_max_useamt_ratio
,d15_q24_ncfcqc_r
,q01_tqi_ncfc_nqi_r
,pboc_loan_ovdmonthcnt
,pboc_overduemonthdebitcard
,credit_permonth_repayments_of_actual_repayment_max_ratio
,creditcard_overdue_exceed_m1_to_now_month
,candnltedallm10cnyua_caa
,all_permonth_repayments_of_actual_repayment_max_ratio
,pboc_acct_rvlv_loan_avgrp_in6m
,inquire_reason_loan_max_wave_dividing_l180d
,candnlted1mnbcnyua_cal
,q06_q24_rfigea_nqi_r
,q12_rfila_qc_rrgr_r
,loan_slc_amount_average
,q03_q09_mfc_nqi_r
,all_loan_approval_rate_l24m
,pboc_cc_maxovd_monthratio
,pboc_summary_ovd_rev_mth_cnt
,pboc_summary_unstl_nrev_act_org_cnt
,inquire_slc_num_l3m
,pboc_summary_org_cnt
,account_ccln_orgcnt
,q03_ncfc_qry_c
,tisr1ras
,q24_qitncfc_top2mqc_avg_m
,q24_qitsifi_maxdi_t
,candltedallm10cnyua_uuar
,tisr2b_arl6mr
,inquire_reason_exclude_plm_l1m_dividing_l12m_ratio
,pboc_zx_age
,candnlted12m10cnycam
,account_ccln_6m_status_c_ratio
,candnlted6m10moc
,pboc_debts_sgac_mc
,loan_norma_bank_amount_min
,account_loan_12m_amount_max
,add_inquire_num_lst_overdue
,pboc_3m_xdspcxyy_num
,q24_qitcfc_maxdi_t
,add_credit_sum_amount_l6m
,credit_average_repay_num
,cclnsum_6mavguseamount_sum
,inquire_organizations_nbank_max_wave_sub_l90d
,candnlted2_3mnbcnycar
,owwcwwncd
,pboc_debitcard7accountnum
,d15_q06_rfila_nqi_r
,s24jstoam
,redg_zbvb_xawd_md
,o03cnxchr
,pboc_summary_unstl_rev_sub_repay_avg_l6m
,inquire_all_max_contin_num_l6m
,q03_tqi_rfila_nqi_r
,add_all_loan_max_amount_l12m
,pboc_datedebitcardfirstactive
,creditcard_average_useamt_l6m_of_permonth_repayments_max_ratio
,d07_q06_rfila_qc_r
,candnltedallm10cnycll
,t01cnd1r1r4ncdr
,q06_rfigeaqc_cmos_max
,creditcard_average_useamt_l6m_of_sum_amount_max_ratio
,pboc_guar_querycnt
,agaa_zbvg_xawd_bbvf_mf
,all_normal_balance_of_sum_permonth_repayments_ratio
,agaa_zbvg_xawd_bbvf_me
,d07_q06_nbfiqc_r
,candnlted1m10cnyua_clr
,loan_normal_credit_average_opening_days
,inquire_reason_loan_l1m_dividing_l12m_ratio
,credit_never_overdue_permonth_repayments_of_all_ratio
,tisr1b_rar
,o12cwwncd
,pboc_loan_unstl_fst_inval_mths
,candnltedallm10eddfm
,rh_loan_overdue_31d_his_all_mon
,loan_cfc_balance_sum
,pboc_summary_opn_cc_use_amt
,q12_qitcfcqc_cmos_max
,add_amount_less_than_30000_of_all_account_ratio_l6m
,q03_fgc_nqi_rrgr_r
,loan_cfc_balance_average
,loan_credit_all_loan_repay_num
,pboc_repay_dhaa_md
,s03jhcd
,pboc_loan_total_use_ratio
,all_loan_min_amount
,creditcard_sum_average_useamt_of_max_useamt_ratio
,pboc_sycpsyed
,twwcallm2cd
,pboc_loan_personal_lmt_l12m
,loan_consumer_balance_exceed_0_account_num
,loan_remaining_periods_exceed_6_permonth_repayments
,pboc_xd_3m_query_ins_cnt
,pboc_base_degree
,inloan_consumer_organizations_num
,account_loanfirst_open1m_orgratio
,d15_q24_nbfiqc_r
,pboc_summary_unstl_nrev_lmt
,never_overdue_all_loan_sum_account_l24m
,pboc_xd_1m_nbank_cnt
,pboc_1m_cxcs_dksp
,carloan_amount_max
,pboc_pettyloan_cnt
,settled_loan_sec_wcp_account
,add_all_loan_average_amount_l12m
,pboc_query_cnt_l1m
,pbocv_cc_rmb_unclsed_cnt
,inquire_all_max_contin_num_l12m
,repay_24m_ccln_nooverdue_count
,occupation_industry_type
,pboc_deaa_zbvg_bbvb_o4
,creditcard_12periods_repay_neednot_account_num
,pboc_summary_unstl_nrev_bal
,pboc_query_org_cnt_l6m
,pboc_dkzhzjycyq_month_num
,all_loan_sum_permonth_repayments_of_sum_actual_repayment_ratio
,account_loan_frequency_amount_sum
,creditcard_min_amount_of_max_use_amount_ratio
,inquire_bank_num_l12m
,loan_remaining_periods_exceed_12_cfc_permonth_repayments_sum
,pboc_djk_edsyl
,pboc_ovd_max_inval_mths
,pboc_loan_act_cnt_l9m
,pbocv_loan_otherspayamtmonth
,loan_remaining_periods_exceed_12_permonth_repayments_ratio
,pboc_query_org_cnt_l3m
,twwcallm1ch
,pbocv_query_loanappr_cnt_p1m
,pboc_repay_dcal_tmd
,pboc_latest2monthcreditapprovalquerynum
,pboc_cur_nomal_act_cnt
,q09_rfila_mqc_c
,yoy_add_overdue_account_num_l12m
,pboc_history_ovdstatus
,pboc_cc_total_ovdcnt
,pboc_all_edsyl
,pboc_query_loan_cnt_l12m
,candlted24m30ua_a9r
,rh_card_his_all
,pboc_good_event_cnt
,pboc_wjqfyjggrxfdkbs
,all_useamt_ratio_exceed_50pct_account
,add_loan_account_num_l24m
,pboc_pboc_zdye
,repaid_loan_homeloan_carloan_amount_of_all_ratio
,loan_normal_credit_sum_balance
,s24jhcd
,pboc_credittotalamount
,cclnsum_mob_max
,pboc_fzss
,account_consumer_loans_balance_sum
,pboc_query_cc_org_cnt_l6m
,never_overdue_all_sum_account
,account_creditcard_normal_useamount_sum
,never_overdue_all_account_l24m
,pboc_dkcpxzhzz_zxzl
,pbocv_cc_lmtutlrate_p6m
,creditcard_all_loan_maximum_remaining_repayment
,pboc_query_cc_cnt_l12m
,pboc_query_loan_cnt_l7d
,overdue_credit_max_amount_month
,pboc_debitcardclosedsum
,pboc_crecard_ovdcnt
,pboc_loan3accountnum
,inquire_all_num_is_increasing_l3m
,settled_loan_homeloan_carloan_amount_of_all_ratio
,rh_house_his
,inquire_reason_loan_num_wave_sub_l90d
,pbocv_loan_other_cnt
,pboc_debitcardissuersinstitutionnum
,noncredit_amount_of_all_ratio
,query_mob_min
,inquire_num_weight_on_month
,inquire_slc_num_l1m
,loan_remaining_periods_exceed_12_bank_permonth_repayments_sum
,pboc_summary_all_act_cnt
,t12callnmchr
,s03hstoas
,pboc_loan_open_mths_max
,pboc_query_cnt_l12m
,cclnsum_creditcard_mob_max
,acao_bbvf_ldve_n12
,pboc_pbccredituseelimper
,pboc_summary_cc_fst_opn_inval_mths
,q03_rfi_la_c
,pboc_loan_total_ovdcnt
,pboc_settle_loan_to_now_month
,loan_normal_bank_amount_sum
,q01_tqc_rfigea_qc_r
,pboc_xd_1m_query_ins_cnt
,boc_credit_dqrq_hkrq_max_gap_day
,credit_loan_average_amount
,pboc_yjqjkqs_36m_xyd
,rh_loan_xfqt_cnt
,creditcard_approval_rate_l12m
,inquire_reason_exclude_plm_num_l15d
,all_loan_overdue_exceed_m1_to_now_month
,pboc_acct_nrvlv_cre_avgrp_in6m
,pboc_accfundcompercent
,never_overdue_creditcard_sum_account_l12m
,all_overdue_amount_less_than_30000_of_all_account_ratio
,pboc_24m_crecard_max_ovdperiod
,pboc_debitcardnoactivesum
,pboc_query_loan_org_cnt_l3m
,pboc_query_loan_org_cnt_l12m
,never_overdue_all_account_l12m
,pboc_query_loan_org_cnt_l6m
,rh_house_loan_now
,pbocv_cc_utlrate75up_accts
,acaf_zbvb_bevl_n3
,q06_rfi_c
,pboc_loansum
,pboc_acct_rvlv_loan_bal
,pboc_last6m_zhjkpd
,rh_loan_nohouse_cnt
,creditcard_average_useamt_l6m_less_than_25pct_ratio
,pbocv_house_repay_amt_mthly
,pboc_latest12overduetimestotal
,pboc_curr_1m_mortgage_repay_amount
,pboc_consumerloansum
,q12_qitcfc_mqc_c
,inquire_bank_num_l1m
,pboc_summary_opn_cc_act_cnt
,pboc_cc_normal_open_mths_max
,pboc_overdue_months
,repay_60m_creditcard_overduem1_count
,pboc_house_fund
,pboc_xd_6m_nbank_cnt
,pboc_moreovduemonth
,never_overdue_creditcard_account
,pboc_query_org_cnt_l1m
,loan_credit_amount_less_than_30000_account
,pboc_cpf_last_pay_income
,pboc_xd_24m_nbank_cnt
,pboc_latest24paymentfind2times
,candnltedallm10cnyual
,pboc_summary_ovd_nrev_max_mths
,pboc_open_acc_cnt_l6m
,creditcard_max_use_ratio_amount_greater_than_amount_account
,pboc_cpf_max_personal_ratio
,pboc_xyk_yyed
,pboc_summary_ovd_rev_act_cnt
,pboc_cpf_amt_base
,pboc_summary_opn_cc_lmt_sum
,pboc_summary_unstl_rev_sub_act_org_cnt
,pboc_query_cc_org_cnt_l12m
,pbocv_loan_guacredunpaidloanorg
,pboc_overdueaccountloan
,pboc_query_cnt_l3m
,pboc_mortgagerepaymentmonths
,d1momn
,pboc_cur_unstl_personal_business_sum_bal
,creditcard_max_amount_estimated_revenue
,pboc_mortgage_residue
,repay_60m_creditcard_cny_overduem1_count
,insurance_housing_fund_estimated_revenue
,rh_loan_overdue_61d_his_all_mon
,pbocv_cc_utlrate80up_accts
,pboc_unclear_oprloan_orgcnt
,t24callm1chr
,pboc_latest2monthloanapprovalquerynum
,inquire_bank_num_l3m
,pboc_query_nobank_cnt
,loan_remaining_periods_exceed_12_nbank_permonth_repayments_sum
,loan_normal_mortgages_max_amount
,t24cnd1r1r4ncd
,pboc_xd_3m_nbank_cnt
,all_loan_inquire_date_to_now_day
,pboc_acct_nrvlv_credit_amt_sum
,pboc_query_loan_cnt_l6m
,pboc_dksp_1m_query_ins_cnt
,never_overdue_all_loan_registered_time_exceed_3m_account
,all_overdue_exceed_m1_to_now_month
,inquire_sum_month_l6m
,pboc_summary_other_loan_act_cnt
,pboc_cc_max_records_l24m_bycard
,pboc_quae_xcva_p5
,pboc_xd_12m_query_ins_cnt
,all_normal_loan_sum_balance
,pboc_summary_ovd_rev_sub_amt_max_by_mth
,q09_qitcfc_mqc_c
,creditcard_approval_rate_l6m
,loan_cfc_balance_exceed_0_account_num
,pboc_query_cnt_l7d
,pboc_loan_rcy_ovdmonth
,pboc_dksp_12m_nbank_cnt
,creditcard_never_overdue_permonth_repayments_of_all_ratio
,pboc_curr_bal
,pboc_summary_query_rsn
,pboc_summary_ovd_rev_amt_max_by_mth
,account_loan_nocarandnohouse_12m_amount_max
,creditcard_approval_rate_l3m
,pboc_max_creperiod
,pboc_latest24maxoverduemonthtotal
,pboc_open_loan_acc_cnt_l6m
,inquire_num_diff_reason_l3m
,pboc_dksp_6m_nbank_cnt
,pboc_query_cc_org_cnt_l3m
,pboc_latest24overduetimesdebitcard
,pboc_base_living_address_cnt_l36m
,cram_bbvf_ldve_mfn9
,pboc_summary_ovd_nrev_mth_cnt
,loan_normal_slc_opening_days_sum
,pboc_latest1querytimedebitcard
,pboc_summary_unstl_rev_act_cnt
,q24_qitfl_ldst_t
,pboc_cpf_last_pay_amt
,rh_ifhousecarcard
,pboc_summary_ovd_rev_sub_mth_cnt
,pboc_summary_ovd_nrev_act_cnt
,pbocv_cc_utlrate50up_accts
,homeloan_amount_max
,pboc_latest2querytimedebitcard
,q24_qitcfc_mqc_c
,pboc_loan1accountnum
,settled_loan_homeloan_carloan_amount
,pboc_quota_amount
,creditcard_all_loan_never_overdue_permonth_repayments_l3m
,pboc_query_org_cnt_l12m
,never_overdue_all_loan_account_l12m
,never_overdue_all_loan_registered_time_exceed_6m_account
,ccln_commercialloan_cnt
,add_loan_account_num_l12m
,pboc_6m_dkpjyhk
,all_overdue_exceed_m3_to_now_month
,pboc_query_org_cnt_l7d
,inquire_reason_exclude_plm_num_wave_sub_l90d
,redg_zbvb_xawd_mc
,account_loan_bank_repayment_sum
,all_loan_average_balance
,o01callcho02callchr
,q06_q24_bfiqc_r
,candnlted1m10eddfm
,candlted24mbcnyua_caa
,loan_normal_trusts_opening_date_to_now_max
,pboc_debts_ywab_tmg
,query_1m_ratio
,inquire_reason_exclude_plm_num_wave_dividing_l180d
,deaa_zbvg_xawd_bbvf_mfo2
,pboc_confin_orgcnt
,yoy_add_credit_account_amount_l12m
,loan_cfc_amount_average
,query_12m_securities_insurance_ratio
,q03_tqc_bfiqc_r
,all_loan_creditcard_sum_actual_repayment_of_sum_amount_ratio
,account_creditcard_normal_uesaratio_median
,all_khrq_hkrq_average_gap_day
,inquire_reason_exclude_plm_num_wave_sub_l150d
,all_useamt_ratio_exceed_75pct_of_all_account_ratio
,loan_normal_slc_opening_date_to_now_max
,loan_sum_amount_wave_sub_l330d
,inquire_reason_loan_num_wave_dividing_l210d
,q12_tqi_rfiap_nqi_r
,account_loan_open6mp_orgratio
,loan_normal_cfc_opening_days_sum
,q24_qitfgc_difl_t
,callt00xndn
,all_loan_sum_balance_of_sum_amount_ratio
,candnltdd_ednbdfl
,loan_consumer_organizations_num_dividing_account_ratio
,add_credit_sum_amount_l12m
,loan_remaining_periods_less_than_12_slc_permonth_repayments_var
,loan_normal_bank_amount_max
,creditcard_all_loan_approval_rate_l6m
,deaa_zbvg_xava_bbvf_mco9
,nature_of_work_category_min_value
,loan_credit_amount_less_than_30000_of_all_account_ratio
,add_all_loan_amount_lst_overdue
,inloan_creditcard_organizations_num_dividing_account_ratio
,pboc_base_age
,account_business_loan_mob_min
,cram_bbvf_ldvi_mfn11
,inquire_reason_exclude_plm_num_wave_sub_l210d
,loan_creditcard_useamt_exceed_80pct_organizations_ratio
,loan_credit_average_amount_l12m
,q09_tqc_cfcqc_r
,creditcard_actual_repayment_of_permonth_repayments_ratio
,candnlted12mnbcnyuam
,candlted24_allmbcnyua_ar
,add_loan_account_num_l3m_dividing_l12m_ratio
,rev_creditcard_average_useamt_less_than_25pct_ratio
,gap_month_of_first_work_to_now
,q24_qitncfc_fdst_t
,pboc_creditavgline
,t12cnr1xchr
,creditcard_all_loan_never_overdue_actual_repayment_permonth_repayments_ratio_l3m
,callt00dzmgbdn
,pboc_summary_loan_fst_opn_inval_mths
,s02jhstoam
,creditloan_permonth_repayments_estimated_revenue
,loan_average_amount_wave_sub_l360d
,query_3m_dksp_approv_ratio
,q09_tqi_cfc_nqi_r
,q24_qitbfi_l2di_t
,candnltdd_rtnbdfl
,loan_remaining_periods_exceed_12_permonth_repayments
,loan_average_permonth_repayments_l6m_dividing_sum_balance_ratio
,loan_repay_progress_amount
,loan_normal_bank_min_balance_opening_date_gap
,loan_normal_account_type_r1_max_balance_opening_date_gap
,pbocv_md_c_orgcnt_lmt08_p
,candnltdd_ed10dfl
,q12_tqc_rfiap_qc_r
,inquire_slc_num_of_all_ratio_l6m
,candnlted1mnbcnyua_uuaa
,organizations_bank_balance_of_all_ratio
,all_sum_permonth_repayments_of_max_actual_repayment_ratio
,q03_nqi_rrgr_r
,creditcard_average_useamt_l6m_of_all_amount_ratio
,candnlted6mbcnyual
,loan_normal_bank_max_amount_opening_date_gap
,pboc_com_loan_bal
,inquire_reason_loan_num_wave_dividing_l180d
,loan_credit_amount_less_than_30000_of_all_amount_ratio
,add_amount_less_than_30000_of_all_amount_ratio_l12m
,loan_repay_progress_amount_average
,inquire_reason_loan_num_wave_sub_l180d
,add_loan_account_num_wave_dividing_l180d
,inquire_organizations_nbank_average_wave_sub_l180d
,pbocv_cc_max_lmtutlrate
,pboc_base_job_status
,candltedallmnbcnycam
,all_useamt_ratio_exceed_90pct_of_all_account_ratio
,candnltedallmnbcnyua_caa
,loan_normal_account_type_r4_max_amount_opening_date_gap
,account_creditcard_1m_useamount_median
,all_amount_less_than_10000_of_all_account_ratio
,q12_q24_rfigea_qc_r
,candnlted2mnbcnycam
,cral_zbva_xave_bbvf_mfn11
,tisd1b_ar
,inquire_reason_exclude_plm_num_wave_sub_l330d
,q06_q24_rfigea_qc_r
,creditcard_all_loan_approval_rate_l24m
,loan_repayment_frequency_month_remaining_periods_ratio
,credit_average_permonth_repayments_amount
,cral_zbva_xawh_bbvf_mdn2
,organizations_slc_cfc_permonth_repayments_of_all_ratio
,pboc_xyk_edsyl
,account_creditcard_useamount_ratio
,cral_zbva_xawd_bbvf_mfn2
,candltrt_ed30dfm
,pboc_cc_max_bal_inval_mth
,inquire_reason_loan_num_wave_sub_l330d
,candnlted6_12mnbcnyuar
,add_loan_account_num_wave_sub_l360d
,organizations_slc_cfc_permonth_repayments_amount
,inquire_loan_organizations_nbank_wave_dividing_l90d
,account_loanfirst_open6m_orgratio
,pboc_occupation
,all_loan_sum_actual_repayment_of_all_amount_ratio
,agaa_zbvg_xava_bbvh_md
,swwhcdr
,pboc_earliestrecordcredi
,cram_bbvh_ldve_mfn9
,creditcard_all_loan_actual_repayment_sum_cur
,pboc_summary_ovd_nrev_amt_max_by_mth
,pboc_summary_unstl_rev_sub_bal
,pboc_cc_amt_avg
,inloan_consumer_organizations_num_dividing_account_ratio
,account_ccln_balance500p_orgcnt
,pboc_base_phone_cnt
,account_ccln_nonguaran_balance_sum
,cral_zbvg_xawh_bbvf_mdn4
,s01jhstoam
,all_useamt_ratio_exceed_75pct_account
,pboc_6m_newcreloan_amt
,pboc_ovd_act_cnt_rate
,d15_q01_rfiap_nqi_r
,inquire_reason_creditcard_num_l900d
,pboc_crah_mfo1
,d07_qry_c
,candnlted2_3mnbcnyuuar
,pboc_totalcreditloanamount
,pboc_debitcardaccountnum
,loan_normal_account_type_d1_max_balance
,account_ccn_nocarandnohouse_balance_max
,pboc_coaa_zbva_xavh_bbvf_n9
,inquire_num_l150d
,account_loan__noclosure_mob_min
,inquire_bank_num_l6m
,s06jhcd
,pboc_dksp_24m_nbank_cnt
,loan_remaining_periods_exceed_6_permonth_repayments_sum
,creditcard_max_use_amount_lst_6m_average_amount_ratio
,pboc_summary_unstl_rev_sub_act_cnt
,pboc_nrev_open_l12m_stl_amt
,pboc_acct_nrvlv_credit_amt_sum_div_orgs
,o01cncho02cnchr
,account_lnfirsit_overdue_mob_min
,creditcard_aging_median_of_day
,pboc_account_cnt
,pboc_loan_ovd_act_pct
,tisd1bs
,inquire_organizations_nbank_num_wave_sub_l90d
,creditcard_permonth_repayments_of_actual_repayment_max_ratio
,pboc_dksp_24m_query_ins_cnt
,pboc_query_loan_cnt_l24m
,pboc_overdueamtonemonthloan
,q01_nbfiqc_rrg_s
,loan_bank_sum_amount
,dfied24isial
,pboc_com_loan_small_act_cnt
,creditcard_all_loan_never_overdue_actual_repayment_l3m
,all_useamt_ratio_exceed_90pct_account
,pboc_acct_cc_avg_useamt_in6m
,creditcard_average_permonth_actual_repayment
,agaa_zbvg_xawh_bbvf_me
,d15_q01_rfiap_qc_r
,pboc_cc_total_amt
,q01_tqc_rfioorg_gea_qc_r
,all_amount_less_than_30000_of_all_account_ratio
,s03jhstoas
,q12_qitnbfiqc_cmos_max
,pbocv_md_l_cnt_l3_12
,account_consumer_loans_3m_balance_sum
,candnlted24_allmnbcnycar
,all_loan_approval_rate_l12m
,d07_q12_nbfiqc_r
,inquire_reason_loan_max_wave_sub_l180d
,inquire_nbank_num_l420d
,pboc_base_marriage_status
,account_creditcard_mob_median
,candltedallmbcnyua_uual
,pboc_summary_unstl_rev_bal
,pboc_loan_amt
,pboc_12m_ffbjq_num
,pboc_latest6querytimedebitcard
,pboc_cpf_max_amt
,creditcard_normal_average_opening_days
,all_loan_approval_rate_l6m
,pboc_total_querycnt
,d07_q09_oorgqc_r
,candnlted12mnbcnyua_uual
,pboc_dkcpxzhzz_pjzl
,inloan_account_type_r1_max_amount
,pboc_com_loan_act_cnt
,pboc_loan_com_bal_total
,all_loan_approval_rate_l3m
,estimated_revenue_average
,pbocv_credit_1stopen_mths
,credit_repay_method_xxhb_debx_average_int
,repay_ccln_overdue_mob_min
,all_loan_creditcard_approval_rate_lst_overdue
,creditcard_use_amount_exceed_actual_repayment_account_ratio
,q03_bfiqc_rrgr_r
,d15_q09_cfcqc_r
,pboc_6m_newloan_amt
,add_loan_account_num_l6m_dividing_l24m_ratio
,d15_q09_nbfiqc_r
,pboc_acct_rvlv_amt_sum_div_orgs
,candnlted24mnbcnyuas
,loan_normal_nbank_balance_average
,account_creditcard_1m_useamount_sum
,pboc_debts_sgad_mc
,query_6m_securities_insurance_ratio
,creditcard_max_use_amount_of_sum_max_use_amount_ratio
,d07_q09_nbfiqc_r
,creditcard_average_useamt_l6m_of_max_useamt_max_ratio
,credit_repay_method_debx_min_int
,pboc_query_tfau_mg
,inquire_slc_num_l12m
,pboc_dkywzlwgrxfdk_num
,pboc_query_cnt_l6m
,pboc_summary_unstl_rev_sub_lmt
,never_overdue_all_registered_time_exceed_12m_account
,pboc_mob_mx
,candnlted24mbcnyclm
,cral_zbvg_xavh_bbvf_mdn5
,inloan_account_type_r1_num_dividing_account_ratio
,creditcard_all_loan_approval_rate_l12m
,creditcard_max_amount_of_average_use_amount_ratio
,all_loan_max_actual_repayment_of_all_amount_ratio
,loan_nbank_min_amount
,d15_q03_nbfiqc_r
,pboc_datedebitcardfirstall
,loan_remaining_periods_less_than_12_cfc_permonth_repayments_var
,pboc_total_use_ratio
,pbocv_loan_noncarhouse_bal
,pboc_12m_total_querycnt


from znzz_fintech_ads.lxl_tmp_pboc_model_sample_250522 as t
where apply_time>=apply_time_auth 
  and apply_date>='2024-10-01'
  and channel_id in (227, 231, 245, 213, 241, 240, 302, 233, 246, 251, 249,247,80038)
"""
df_sample_1 = get_data(sql)


# In[139]:


df_sample_ = df_sample_1.copy()


# In[140]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
# df_sample_ = pd.read_csv('df_sample_0525.csv')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[ ]:


# df_sample_.to_csv('df_sample_0525.csv',index=False)


# In[141]:


print(df_sample_['apply_date'].max(),df_sample_['apply_date'].min())


# In[142]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())
print(df_sample_.shape[0], df_sample_['user_id'].nunique())


# In[143]:


df_sample_.dropna(how='all', axis=1, inplace=True)
print(df_sample_.shape[1])


# In[144]:


print(df_sample_.columns.to_list()[:15])


# In[145]:


varsname = df_sample_.columns.to_list()[15:]

# print(varsname)
print("初始特征变量个数：",len(varsname))


# In[ ]:


# categorical_vars = ['pboc_all_class5_status_cur','pboc_rpt_query_date','pboc_base_birth_date',
#                  'pboc_cc_repay_exception',
#             'pboc_djk_isdj','pboc_djk_iszf','pboc_cc_24m_g','pboc_cc_account_exception',
#             'pboc_datedebitcardfirstactive','pboc_datedebitcardfirstall','pboc_loan_repay_exception',
#             'pboc_dkwjfl_new','pboc_loan_class5_status_cur','pboc_loan_fiveclass_status','pboc_loan_24m_g',
#             'pboc_loan_account_exception','pboc_loantopfivelevel','pboc_danbao_class5_status','rh_ifhousecarcard',
#             'pboc_curr_ovdstatus','pboc_digital_score_explain','pboc_employer1gettime','pboc_employer2gettime',
#             'pboc_employer3gettime','pboc_cpf_last_pay_state','pboc_base_birth_address',
#             'pboc_base_marriage_status','pboc_edulevel','pboc_specialdeal_status','rh_month6_loan_ifnull',
#             'rh_month6_loan_ifnormal','pboc_base_job_status','pboc_occupation','pboc_summary_query_org_type_lst',
#             'pboc_summary_query_date_lst','pboc_summary_query_rsn','pboc_is_dbh','pboc_is_car_loan',
#             'pboc_is_house_loan','pboc_is_cpf_loan','pboc_is_student_loan','pboc_szjdf',
#             'pboc_base_contact_address','pboc_repayduty_fiveclass_status2','pboc_repayduty_fiveclass_status1',
#             'pboc_zhzt_zxjl','pboc_base_sex','pboc_base_gender','pboc_education','pboc_base_education',
#             'pboc_base_degree','occupation_industry_type','pboc_whitestuatus','rh_querystate','pboc_is_bh',
#             'pboc_qc_24m_g','rh_danwei']
# categorical_vars = [col for col in categorical_vars if col in varsname]
# print(f"分类变量的个数：{len(categorical_vars)}")

# numerical_vars = [col for col in varsname if col not in categorical_vars]
# print(f"数值变量的个数：{len(numerical_vars)}")


# In[ ]:


# df_sample_[categorical_vars].info(show_counts=True)
# df_sample_[categorical_vars].head()


# In[ ]:


# df_sample_[varsname].select_dtypes(include=['object']).info(show_counts=True)
# df_sample_[varsname].select_dtypes(include=['object']).head()


# In[146]:


# 筛选出所有数据类型为'object'的列
object_cols = list(df_sample_[varsname].select_dtypes(include=['object']).columns)
print(len(object_cols))


# In[147]:


keep_object_cols = ['pboc_loantopfivelevel','pboc_cpf_last_pay_state',
             'pboc_base_marriage_status','pboc_base_job_status','pboc_occupation',
            'pboc_summary_query_org_type_lst','pboc_summary_query_rsn','pboc_base_sex',
             'pboc_education','pboc_base_degree']
print(len(keep_object_cols))
keep_object_cols = [col for col in object_cols if col in keep_object_cols]
print(len(keep_object_cols))
# drop_object_cols = [col for col in object_cols if col not in keep_object_cols]
# print(len(drop_object_cols))


# In[ ]:


# print(df_sample_.columns.to_list()[:14])


# In[ ]:


# varsname = df_sample_.columns.to_list()[14:]

# # print(varsname)
# print("初始特征变量个数：",len(varsname))


# In[149]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[150]:


df_sample_[keep_object_cols] = df_sample_[keep_object_cols].replace(['-1', '-9999'], np.nan)


# In[151]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[152]:


for col in keep_object_cols:
    df_sample_[col + '_c'] = df_sample_[col]


# In[153]:


categorical_vars = keep_object_cols[:]
print(f"分类变量的个数：{len(categorical_vars)}")
print(f"分类变量的个数：{len(categorical_vars)}")


# In[154]:


mappings = {}
cat_code = {}
for col in categorical_vars:
    # 获取类别和编码的映射关系
    mappings[col] = dict(enumerate(df_sample_[col].astype('category').cat.categories))
    df_sample_[col] = df_sample_[col].astype('category').cat.codes
    # 获取类别和编码的映射关系
    cat_code[col] = dict(enumerate(df_sample_[col+'_c'].astype('category').cat.categories))


# In[155]:


mappings


# In[156]:


cat_code


# In[157]:


df_sample_['pboc_base_sex'].value_counts(dropna=False)


# In[158]:


# for col in categorical_vars:
#     print(df_sample_[col + '_c'].value_counts(dropna=False))
#     print(df_sample_[col].value_counts())


# In[160]:


import json

with open(result_path + 'category_mapping.json', 'w') as f:
    json.dump(mappings, f)


# In[161]:


import json

with open(result_path + 'cat_code.json', 'w') as f:
    json.dump(cat_code, f)


# In[ ]:


# with open(result_path + 'category_mapping.json', 'r') as f:
#     loaded_map = json.load(f)

# # 注意：JSON 的 key 是字符串，要转换成 int
# loaded_map = {int(k): v for k, v in loaded_map.items()}


# In[162]:


numerical_vars = [col for col in varsname if col not in categorical_vars]
print(f"数值变量的个数：{len(numerical_vars)}")


# In[163]:


for i, col in enumerate(numerical_vars):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col],errors='coerce')


# In[164]:


df_sample_[df_sample_[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample_[df_sample_[varsname].isna().all(axis=1)].info(show_counts=True)


# In[ ]:


# df_sample_[df_sample_[varsname].isna().all(axis=1)].head(3)


# In[ ]:


# df_sample_.drop(index=df_sample_[df_sample_[varsname].isna().all(axis=1)].index,inplace=True)
# df_sample_.shape


# In[165]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[166]:


df_sample_['target_fpd30'].value_counts()


# In[167]:


df_sample_['target_fpd30'].value_counts(normalize=True)


# In[ ]:


# df_sample_.to_csv('df_sample_原始_250516.csv',index=False)


# In[168]:


df_sample = df_sample_.copy()


# In[ ]:


# df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[ ]:


# df_sample.info(show_counts=True)


# In[170]:


df_sample['apply_date'] = df_sample['apply_date'].apply(str)
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[171]:


df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date< '2025-03-01'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-20'").index, 'data_set']='3_oot2'


# In[ ]:


# df_sample.to_csv(result_path + '提现人行征信模型fpd30标签2411_2502_v1_0527.csv',index=False)
# print(result_path + '提现人行征信模型fpd30标签2411_2502_v1_0527.csv')


# In[ ]:


# df_sample[df_sample[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index, inplace=True)


# In[172]:


target = 'target_fpd30'


# # 1. 样本概况

# In[173]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[174]:


print(df_sample[target].value_counts())


# In[175]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[176]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[177]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_v4_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_v4_{timestamp}.xlsx")


# In[178]:


df_vars_des = pd.read_excel('人行线上变量清单.xlsx')
print(df_vars_des.shape)
df_vars_des.head()


# In[ ]:





# # 2.数据探索性分析

# In[179]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor = pd.merge(df_vars_des.set_index('变量编码'), df_explor, how='right',left_index=True,right_index=True)
df_explor.head()


# ## 2.1缺失值处理

# In[180]:


len(numerical_vars)


# In[181]:


for col in numerical_vars:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[182]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[183]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[184]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_v4_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_v4_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[185]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[186]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[187]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[188]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_变量缺失率_{task_name}_v4_{timestamp}.xlsx') as writer:
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_变量缺失率_{task_name}_v4_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[189]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=1.0).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=1.0].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

# to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
# print("to_drop_mode:", len(to_drop_mode))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique ))
print(f"删除的变量有{len(to_drop1)}个")


# In[190]:


to_drop_recent


# In[191]:


varsname_v1 = varsname[:]
# varsname_v1 = [col for col in varsname if col not in to_drop1] 
# print(f"保留的变量有{len(varsname_v1)}个")
# print(varsname_v1[:10])


# In[192]:


len(varsname_v1)


# ## 3.2 基于变量重要性/相关性删除变量
# 

# In[ ]:


# # 2.3 快速查看特征重要性
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='dt', min_samples=0.05, n_bins=6)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.02, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:].head()


# In[226]:


varsname_v2 = varsname_v1[:]
# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[429]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df



def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[430]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[439]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_base_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[440]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[441]:


categorical_vars


# In[442]:


len(varsname_base_v2)


# In[ ]:


# numerical_vars_v2 = [col for col in varsname_v2 if col in numerical_vars]
# print(f"数值变量个数:{len(numerical_vars_v2)}")
# categorical_vars_v2 = [col for col in varsname_v2 if col in categorical_vars]
# print(f"数值变量个数:{len(categorical_vars_v2)}")


# In[ ]:


# numerical_vars_v2.append('pboc_datedebitcardfirstactive')
# categorical_vars_v2.remove('pboc_datedebitcardfirstactive')


# In[445]:


numerical_vars_base = [ col for col in varsname_base_v2 if col not in categorical_vars]
len(numerical_vars_base)


# In[ ]:


# print(f"数值变量个数:{len(numerical_vars_v2)}")
# print(f"数值变量个数:{len(categorical_vars_v2)}")


# In[446]:


new_bins_dict = {}
to_drop_mode = []

for i, col in enumerate(numerical_vars_base):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[ ]:





# In[447]:


new_bins_dict


# In[448]:


categorical_vars_base = [ col for col in varsname_base_v2 if col in categorical_vars]


# In[449]:


for col in categorical_vars_base:
    new_bins_dict[col] = existing_bins_dict[col]


# In[450]:


combiner.load(new_bins_dict)


# In[451]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[452]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[454]:


varsname_v2 = varsname_base_v2[:]


# In[455]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

print('---------')
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print('---------')


# In[456]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[457]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[458]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[459]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
drop_by_psi = list(set(drop_by_psi_month + drop_by_psi_set))
print("drop_by_psi: ", len(drop_by_psi))


# In[ ]:


df_psi_by_month.loc[drop_by_psi,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi,:]


# In[ ]:


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<0.01].dropna(how='all').index)
drop_by_iv = list(set(drop_by_iv_month + drop_by_iv_set))
print("drop_by_iv: ", len(drop_by_iv))


# In[ ]:


len(drop_by_iv_set)


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[ ]:


to_drop3 = list(set(drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")
print(varsname_v3)


# ## 4.2 Y标签相关性删除

# In[460]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v2]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)

print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[461]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[462]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v2].corr(method='spearman')
df_corr_matrix.info()
df_corr_matrix.head(2)


# In[463]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[472]:


df_high_corr.info()
df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


df_iv_by_set.loc[to_drop4,:]


# In[465]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
# print(f"保留变量{len(varsname_v4)}个")
# print(varsname_v4)


# In[466]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
#         all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
#     return (all_corrs, all_pvalue)
    return all_corrs


# In[467]:


# 调用函数
# df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
#                                                                     'apply_month',
#                                                                     varsname_v4,
#                                                                     target,
#                                                                     method='pointbiserialr'
#                                                                    )
df_corr_vars_target = calculate_correlations(df_sample_woe,
                                            'apply_month',
                                            varsname_v2,
                                            target,
                                            method='pointbiserialr'
                                           )
# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[470]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[471]:


df_corr_vars_target.loc[to_drop5,:]


# In[ ]:


varsname_v5 = [col for col in varsname_v4 if col not in to_drop5]
print(f"保留变量{len(varsname_v5)}个")
print(varsname_v5)


# In[469]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set')
#         df_iv.to_excel(writer, sheet_name='df_iv')
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_distribute_iv_psi_corr_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:


gc.collect()


# # 5.模型训练

# ## 5.0 函数定义

# In[193]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[194]:


target = 'target_fpd30'


# In[195]:


df_sample[target].value_counts()


# In[196]:


df_sample['target_fpd30_1'] = 1 - df_sample[target]
modeltrian_target = 'target_fpd30_1'


# In[197]:


df_sample[modeltrian_target].value_counts()


# In[198]:


df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[199]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[200]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[201]:


print("最优参数opt_params: ", opt_params)


# In[ ]:


# print(len(varsname_v5))
# print(varsname_v5)


# In[ ]:


# print(varsname_v2)


# In[203]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[204]:


varsname_base = varsname_v1[:]
print(len(varsname_base))


# In[ ]:


# df_sample['pboc_base_sex'].head()


# In[ ]:


# df_sample['pboc_base_sex'] = df_sample['pboc_base_sex'].astype('category').cat.codes
# df_sample['pboc_base_sex'].head()
# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# df['pboc_base_sex'] = le.fit_transform(df['pboc_base_sex'])


# In[ ]:


# df_sample['pboc_base_sex'].value_counts()


# In[205]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[206]:


df_sample['data_set'].value_counts(dropna=False)


# In[207]:


categorical_vars


# In[208]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars)


# In[210]:


lgb_model.params


# In[209]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[211]:


# 最初评估模型效果 
df_ks_auc_set_base = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v1', 'data_set')
# df_ks_auc_set_base['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base = pd.concat([tmp, df_ks_auc_set_base], axis=1)
print(df_ks_auc_set_base)


# In[212]:


df_ks_auc_month_base = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_base


# In[213]:


# 最初评估模型效果 
df_sample_jk30 = df_sample.query("diff_days>30 & channel_types=='金科渠道'")


# In[214]:


# 最初评估模型效果 
df_ks_auc_set_base_30 = model_ks_auc(df_sample_jk30, modeltrian_target, 'y_prob_base_v1', 'data_set')
# df_ks_auc_set_base['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30 = pd.concat([tmp, df_ks_auc_set_base_30], axis=1)
print(df_ks_auc_set_base_30)


# In[215]:


df_ks_auc_month_base_30 = calculate_ks_auc(df_sample_jk30, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_base_30


# In[ ]:


df_vars_des.head()


# In[217]:


df_vars_des.rename(columns={'变量编码':'feature'},inplace=True)


# In[218]:


# 模型变量重要性
# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base = feature_importance(lgb_model) 
# df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_base = df_importance_base.reset_index()
df_importance_base = pd.merge(df_vars_des, df_importance_base, how='right',on='feature')
df_importance_base


# In[224]:


# 模型变量重要性
# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_set_base = feature_importance(lgb_model) 
# df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base = df_importance_set_base.reset_index()
df_importance_set_base = pd.merge(df_vars_des, df_importance_set_base, how='right',on='feature')
df_importance_set_base


# In[ ]:


df_model_corr_base = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
df_model_corr_base.info()
df_model_corr_base.head()


# In[225]:


# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v1_{timestamp}.xlsx') as writer:
    df_importance_set_base.to_excel(writer, sheet_name='df_importance_set_base')
    df_importance_base.to_excel(writer, sheet_name='df_importance_base')
    df_ks_auc_set_base.to_excel(writer, sheet_name='df_ks_auc_set_base')
    df_ks_auc_month_base.to_excel(writer, sheet_name='df_ks_auc_month_base')
    df_ks_auc_set_base_30.to_excel(writer, sheet_name='df_ks_auc_set_base_30')
    df_ks_auc_month_base_30.to_excel(writer, sheet_name='df_ks_auc_month_base_30')
print(result_path + f'4_模型训练_{task_name}_base4_v1_{timestamp}.xlsx')


# ### 5.2.2 模型优化

# In[ ]:


# df_sample = pd.read_csv(result_path + '提现人行征信模型fpd30标签2411_2501_v1_0515.csv')


# #### 5.2.2.1 特征优化

# In[226]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[227]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# varsname_base_v2 = varsname_v4[:]
# print(len(varsname_base_v2))


# In[228]:


varsname_base_v2 = list(df_importance_set_base.query("gain>0")['feature'])
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[85]:


varsname_base_v2 = list(df_importance_set_base.query("gain>0")['feature'])
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[ ]:


# drop_cols_2_1 = ['creditcard_all_loan_approval_rate_l3m','q03_q12_ncfcqc_r','o01callcho02callchr','agaa_zbvg_xawh_bbvf_me','agaa_zbvg_xawd_bbvh_me','loan_max_amount_l1m','q09_qitncfc_qccv_r']
# varsname_base_v2_1 = ['creditcard_all_loan_approval_rate_l3m','agaa_zbvg_xawh_bbvf_me','q09_qitncfc_qccv_r','candnlted1mnbcnycas','s02jhcdr','pboc_rebn_zcva_xbvb_bavf_mcq8','candnlted1m10cnycas','inquire_reason_exclude_plm_l1m_dividing_l12m_ratio','twwcallm1toas','pboc_summary_unstl_rev_repay_avg_l6m','t24callr1xcdr','q01_q24_fgcqc_oorg_r','pboc_pbccredituseminusper','pboc_base_sex','q01_tqc_ncfcqc_r','candltedallmnbcnycam','pboc_cc_total_ovdcnt','candnlted1mnbcnyua_uuam','tisr2b_arl6mr','cclnsum_6mavguseamount_sum','owwcwwxcdr','redg_zbvb_xave_mc','pboc_com_loan_bal','s00jhdn','candltrt_ed30dfm','loan_normal_cfc_opening_days_sum','candnltedallmnbcnycaa','d15_q09_nbfiqc_r','q12_tqi_ncfc_nqi_r','c02crchr','candnlted12m10cnycam','candltedallmbcnyua_uual','loan_slc_amount_average','estimated_revenue_max','pboc_xyk_edsyl']
# len(varsname_base_v2_1)


# In[ ]:


# varsname_base_v2_2 = varsname_base_v2[:]
# varsname_base_v2_2.remove('pbocv_cc_max_lmtutlrate')
# varsname_base_v2_2.remove('pboc_acct_rvlv_amt_sum_div_orgs')
# varsname_base_v2_2.remove('candltrt_ed30dfm')
# print(varsname_base_v2_2)
# print(len(varsname_base_v2_2))


# In[ ]:


# df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date< '2025-03-01'").index, 'data_set']='1_train'
# df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
# df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-20'").index, 'data_set']='3_oot2'


# In[229]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:





# In[230]:


# 查看训练数据集
df_sample['data_set'].value_counts(dropna=False)


# In[231]:


categorical_vars


# In[232]:


categorical_vars_v2 = [col for col in categorical_vars if col in varsname_base_v2]
categorical_vars_v2


# In[233]:



# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[112]:


# 1. 各模型打分
lgb_model1 = load_model_from_pkl('./result_v4/提现人行征信模型fpd30标签_base4_v2_20250526140948.pkl')
feature_names = lgb_model1.feature_name()
df_sample['y_prob_base_v2'] = lgb_model1.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[234]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[235]:


# 最初评估模型效果 
df_ks_auc_set_base_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'data_set')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v2 = pd.concat([tmp, df_ks_auc_set_base_v2], axis=1)
print(df_ks_auc_set_base_v2)


# In[236]:


# 最初评估模型效果 
df_ks_auc_month_base_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v2 = pd.concat([tmp, df_ks_auc_month_base_v2], axis=1)
print(df_ks_auc_month_base_v2)


# In[237]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")
# 最初评估模型效果 
df_ks_auc_set_base_30_v2 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v2', 'data_set')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v2 = pd.concat([tmp, df_ks_auc_set_base_30_v2], axis=1)
print(df_ks_auc_set_base_30_v2)


# In[238]:


# 最初评估模型效果 
df_ks_auc_month_base_30_v2 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_set_base_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_30_v2 = pd.concat([tmp, df_ks_auc_month_base_30_v2], axis=1)
print(df_ks_auc_month_base_30_v2)


# In[239]:


df_ks_auc_month_base_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_base_v2


# In[240]:


df_ks_auc_month_base_30_v2 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_base_30_v2


# In[241]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v2 = feature_importance(lgb_model) 
# df_importance_base_v2 = pd.merge(df_importance_base_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v2 = df_importance_base_v2.reset_index()
df_importance_base_v2 = pd.merge(df_vars_des, df_importance_base_v2, how='right',on='feature')
df_importance_base_v2


# In[242]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v2 = feature_importance(lgb_model) 
# df_importance_set_base_v2 = pd.merge(df_importance_set_base_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v2 = df_importance_set_base_v2.reset_index()
df_importance_set_base_v2 = pd.merge(df_vars_des, df_importance_set_base_v2, how='right',on='feature')
df_importance_set_base_v2


# In[ ]:


# df_model_corr_base_v2 = df_corr_matrix.loc[varsname_base_v2,varsname_base_v2].reset_index()
# df_model_corr_base_v2.info()
# df_model_corr_base_v2.head()


# In[243]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v2_{timestamp}.xlsx') as writer:
    df_importance_set_base_v2.to_excel(writer, sheet_name='df_importance_set_base_v2')
    df_importance_base_v2.to_excel(writer, sheet_name='df_importance_base_v2')
    df_ks_auc_set_base_v2.to_excel(writer, sheet_name='df_ks_auc_set_base_v2')
    df_ks_auc_month_base_v2.to_excel(writer, sheet_name='df_ks_auc_month_base_v2')
    df_ks_auc_set_base_30_v2.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v2')
    df_ks_auc_month_base_30_v2.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v2')
print(result_path + f'4_模型训练_{task_name}_base4_v2_{timestamp}.xlsx')


# In[ ]:





# #### 5.2.2.2 参数优化

# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.6


# In[ ]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


len(varsname_base_v2)


# In[ ]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


categorical_vars_v2


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[ ]:


# 优化后评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[ ]:



# 最初评估模型效果 
df_ks_auc_set_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3 = pd.concat([tmp, df_ks_auc_set_base_v3], axis=1)
print(df_ks_auc_set_base_v3)


# In[ ]:


df_ks_auc_month_base_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_v3


# In[ ]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[ ]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v3 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v3', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v3 = pd.concat([tmp, df_ks_auc_set_base_30_v3], axis=1)
print(df_ks_auc_set_base_30_v3)


# In[ ]:


df_ks_auc_month_base_30_v3 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_30_v3


# In[ ]:





# In[ ]:





# In[ ]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v3 = feature_importance(lgb_model) 
# df_importance_base_v3 = pd.merge(df_importance_base_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v3 = df_importance_base_v3.reset_index()
df_importance_base_v3 = pd.merge(df_vars_des, df_importance_base_v3, how='right',on='feature')
df_importance_base_v3


# In[ ]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v3 = feature_importance(lgb_model) 
# df_importance_set_base_v3 = pd.merge(df_importance_set_base_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v3 = df_importance_set_base_v3.reset_index()
df_importance_set_base_v3 = pd.merge(df_vars_des, df_importance_set_base_v3, how='right',on='feature')
df_importance_set_base_v3


# In[ ]:





# In[ ]:


# df_model_corr_base_v3 = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
# df_model_corr_base_v3.info()
# df_model_corr_base_v3.head()


# In[ ]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v3_{timestamp}.xlsx') as writer:
    df_importance_set_base_v3.to_excel(writer, sheet_name='df_importance_set_base_v3')
    df_importance_base_v3.to_excel(writer, sheet_name='df_importance_base_v3')
    df_ks_auc_set_base_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_v3')
    df_ks_auc_month_base_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_v3')
    df_ks_auc_set_base_30_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v3')
    df_ks_auc_month_base_30_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v3')
print(result_path + f'4_模型训练_{task_name}_base4_v3_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[ ]:



# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v2)


# In[ ]:


# 优化后评估模型效果
df_sample['y_prob_base_v3_1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3_1'].head()


# In[ ]:


# 最初评估模型效果 
df_ks_auc_set_base_v3_1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3_1', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3_1 = pd.concat([tmp, df_ks_auc_set_base_v3_1], axis=1)
print(df_ks_auc_set_base_v3_1)


# In[ ]:


df_ks_auc_month_base_v3_1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3_1', 'apply_month')
df_ks_auc_month_base_v3_1


# In[ ]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[ ]:


df_ks_auc_month_base_30_v3_1 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v3_1', 'apply_month')
df_ks_auc_month_base_30_v3_1


# #### 5.2.2.3 特征优化v2

# In[244]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[245]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[246]:


varsname_base_v3 = list(df_importance_set_base_v2.query("gain>0")['feature'])
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[380]:


varsname_base_v3 = list(df_importance_set_base_v4.query("gain>0")['feature'])
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[381]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[382]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[383]:


print(categorical_vars_v2)


# In[384]:


categorical_vars_v3 = [col for col in categorical_vars_v2 if col in varsname_base_v3]
print(categorical_vars_v3)


# In[385]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000,categorical_feature=categorical_vars_v3)


# In[307]:


# lgb_model.params


# In[289]:


# 优化后评估模型效果
# df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
# df_sample['y_prob_base_v4'].head()


# In[386]:


df_sample['y_prob_base_v5'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v5'].head()


# In[376]:


# 最初评估模型效果 
df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v5', 'apply_month')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[394]:


# 最初评估模型效果 
df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v5', 'data_set')
# df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[393]:


df_ks_auc_month_base_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v5', 'apply_month')
df_ks_auc_month_base_v4


# In[388]:


# 最初评估模型效果 
df_sample_jk_30 = df_sample.query("channel_types=='金科渠道' & diff_days>30")


# In[389]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v4 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v5', 'apply_month')
# df_ks_auc_set_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'apply_month').set_index('bins')
df_ks_auc_set_base_30_v4 = pd.concat([tmp, df_ks_auc_set_base_30_v4], axis=1)
print(df_ks_auc_set_base_30_v4)


# In[391]:


# 最初评估模型效果 
df_ks_auc_set_base_30_v4 = model_ks_auc(df_sample_jk_30, modeltrian_target, 'y_prob_base_v5', 'data_set')
# df_ks_auc_set_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_jk_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base_30_v4 = pd.concat([tmp, df_ks_auc_set_base_30_v4], axis=1)
print(df_ks_auc_set_base_30_v4)


# In[395]:


df_ks_auc_month_base_30_v4 = calculate_ks_auc(df_sample_jk_30, modeltrian_target, target, 'y_prob_base_v5', 'apply_month')
df_ks_auc_month_base_30_v4


# In[396]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base_v4 = feature_importance(lgb_model) 
# df_importance_base_v4 = pd.merge(df_importance_base_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_base_v4 = df_importance_base_v4.reset_index()
df_importance_base_v4 = pd.merge(df_vars_des, df_importance_base_v4, how='right',on='feature')
df_importance_base_v4


# In[390]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base_v4 = feature_importance(lgb_model) 
# df_importance_set_base_v4 = pd.merge(df_importance_set_base_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base_v4 = df_importance_set_base_v4.reset_index()
df_importance_set_base_v4 = pd.merge(df_vars_des, df_importance_set_base_v4, how='right',on='feature')
df_importance_set_base_v4


# In[ ]:


# df_model_corr_base_v4 = df_corr_matrix.loc[varsname_base_v4,varsname_base_v4].reset_index()
# df_model_corr_base_v4.head()


# In[299]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v4_{timestamp}.xlsx') as writer:
    df_importance_set_base_v4.to_excel(writer, sheet_name='df_importance_set_base_v4')
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v4')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v4')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v4')
    df_ks_auc_set_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v4')
    df_ks_auc_month_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v4')
print(result_path + f'4_模型训练_{task_name}_base4_v4_{timestamp}.xlsx')


# In[397]:



# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base4_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base4_v5_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base4_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_base4_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base4_v5_{timestamp}.xlsx') as writer:
    df_importance_set_base_v4.to_excel(writer, sheet_name='df_importance_set_base_v5')
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v5')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v5')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v5')
    df_ks_auc_set_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_30_v5')
    df_ks_auc_month_base_30_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_30_v5')
print(result_path + f'4_模型训练_{task_name}_base4_v5_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[ ]:


sql = """
select 
 t.order_no
-- 深圳团队子分
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,t_high_p_f30_2504
,t_high_p_m3d30_2025
,M1A0017
,M1A0028
,M1A0031

-- 北京团队模型子分
,t_mix_pboc2_dpd20
,t_pboc_dpd20

from 
    (
    select 
     t1.order_no
    from znzz_fintech_ads.dm_f_zzj_test_order_target as t1 
    where dt='2025-05-22'

    ) as t 
-- 北京团队的子分
inner join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date >= '2024-09-01'
      and apply_date <= '2025-03-20'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no 
    ,max(case when variable_code = 't_high_p_f30_2504' then good_score else null end) as t_high_p_f30_2504 
    ,max(case when variable_code = 't_high_p_m3d30_2025' then good_score else null end) as t_high_p_m3d30_2025 
    ,max(case when variable_code = 'M1A0017' then good_score else null end) as M1A0017 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031 
  from znzz_fintech_ads.lending_model01_scores_off as t 
    where lending_time >= '2024-09-01'
      and lending_time <= '2025-03-20'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
left join 
    (
    select 
     order_no 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
  from znzz_fintech_ads.lending_model01_scores_vars as t 
    where lending_time >= '2024-09-01'
      and lending_time <= '2025-03-20'
    group by order_no
    ) as t3 on t.order_no=t3.order_no 
;


"""
df_bj = get_data(sql)
df_bj.info(show_counts=True)


# In[398]:


df_bj = pd.read_csv('df_bj.csv')
df_bj.info(show_counts=True)
df_bj.head()


# In[403]:


print(df_bj.shape[0],df_bj['order_no'].nunique())


# In[404]:


df_bj.drop_duplicates(subset=['order_no'],keep='first',inplace=True)


# ### 5.3.1数据处理

# In[ ]:


# # 1. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v2_20250516151629.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[ ]:


# # 2. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v3_20250519141155.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[ ]:


# # 3. 各模型打分
# lgb_model = load_model_from_pkl('./result/提现人行征信模型fpd30标签_base_v4_20250520141208.pkl')
# feature_names = lgb_model.feature_name()
# df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[feature_names], num_iteration=lgb_model.best_iteration)


# In[405]:


df_sample.columns[-10:]


# In[406]:


usecols = ['order_no','diff_days','target_mob4dpd30'] + list(df_sample.columns[-9:]) 

print(df_sample.shape)
df_evalue = pd.merge(df_sample[usecols], df_bj, how='inner', on='order_no')
print(df_evalue.shape)
df_evalue.info(show_counts=True)


# In[407]:


print(df_evalue.shape[0],df_evalue['order_no'].nunique())


# In[408]:


pd.set_option('display.max_columns',None)
df_evalue.head()


# In[409]:


df_evalue[list(df_evalue.columns[8:])].describe()


# In[410]:


for col in ['all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','m1a0028']:
    df_evalue[col] = 1 - df_evalue[col]


# In[411]:


df_evalue.head()


# ### 5.3.2 效果对比

# In[412]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
#         df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
#         df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[413]:


print(df_evalue.columns[8:].to_list())


# In[ ]:





# In[414]:



score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_fpd30_1'] 
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

print(df_evalue.shape[0])
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[415]:



score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_fpd30_1'] 
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

print(df_evalue.shape[0])
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
tmp_df_evalue = tmp_df_evalue.query("channel_types=='金科渠道' & diff_days>30")
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1_30 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1_30


# #### 长目标

# In[416]:


df_evalue_dpd30 = df_evalue.query("target_mob4dpd30 >= 0 ")
df_evalue_dpd30.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v6', 'apply_month')
# df_ks_auc_month_pboc.head()


# In[ ]:


# df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v6', 'data_set')
# df_ks_auc_set_pboc['渠道'] = '全渠道'
# tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
# df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
# df_ks_auc_set_pboc


# In[417]:


df_evalue_dpd30['target_mob4dpd30_1'] = 1 - df_evalue_dpd30['target_mob4dpd30']


# In[418]:


score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

print(df_evalue_dpd30.shape[0])
tmp_df_evalue = df_evalue_dpd30.loc[df_evalue_dpd30[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1, df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2


# In[419]:


score_list = ['y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v4',
       'y_prob_base_v5','m1a0028', 'm1a0031','t_pboc_dpd20','t_high_p_m3d30_2025','t_mix_pboc2_dpd20',
       'all_a_rh_fpd0_v1_p', 'all_a_rh_fpd10_v1_p', 'all_a_rh_fpd10_v2_p',
       'all_a_rh_fpd30_v1_p', 'all_a_rh_fpd6_v1_p', 't_high_p_f30_2504']
print(len(score_list),score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

print(df_evalue_dpd30.shape[0])
tmp_df_evalue = df_evalue_dpd30.loc[df_evalue_dpd30[score_list].notna().all(axis=1),:]
tmp_df_evalue = tmp_df_evalue.query("channel_types=='金科渠道' & diff_days>30")
print(tmp_df_evalue.shape[0])

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_30_v2 = pd.concat([df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_30_v2


# In[420]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_pboc_2_v2.to_excel(writer, sheet_name='mob4dpd30')
    df_ksauc_all_1_30.to_excel(writer, sheet_name='30_fpd30')
    df_ksauc_pboc_2_30_v2.to_excel(writer, sheet_name='30_mob4dpd30')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[421]:


df_evalue.to_csv(result_path + r'提现全渠道人行fpd30模型2411_2502_evalue.csv',index=False)


# In[473]:


print(result_path + r'提现全渠道人行fpd30模型2411_2502_evalue.csv')


# # 6. 评分分布

# In[422]:


df_sample['apply_month'].value_counts()


# In[423]:


score = 'y_prob_base_v4'


# In[424]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[425]:


df_sample['score_bins'].head()


# In[426]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[427]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[431]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[432]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[433]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_base4_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_base4_{timestamp}.xlsx')


# In[434]:


df_sample.to_csv(result_path + '提现全渠道人行fpd30模型_2411_2502_base4.csv',index=False)




#==============================================================================
# File: 提现全渠道fpd30融合模型_2411_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储目录
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[3]:


task_name = '提现全渠道fpd30融合模型_2411_2502'


# # 函数定义

# In[4]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[7]:


sql = """
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,target_fpd10
,target_fpd20
,target_fpd30
,target_cpd30
,target_mob3dpd30
,target_mob4dpd30
-- 深圳团队子分
,M1A0007
,M1A0009
,M1A0011
,M1A0016
-- ,M1A0017
-- ,M1A0018
,M1A0020
,M1A0021
,M1A0022
,M1A0023
,M1A0026
,M1A0027
,M1A0028
,M1A0031
,M1A0033
,M1A0034
,M1A0035
,M1A0036
,M1A0037
,M1A0038
-- ,M1A0039
,M1A0040
,M1A0041
-- ,M1A0042
,M1A0043
,M1A0044
,M1B0001
,M1B0002
,M1B0004
,M1B0007
,M1B0011
,M1B0012
,M1B0016
,M1B0024
,M1B0025
--,M1B0027
--,M1B0028
,M1B0029
,M1B0030
,M1B0031
,M1B0032
-- ,M1B0035
-- ,M1B0037
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,a_pboc_fpd6_v1
,a_pdv3_fpd30_v1
,off_f30_2504
,off_f30_2506
,off_m4d30_2504
,off_m4d30_2506
,t_high_p_f30_2504
,t_high_p_m3d30_2025
,t_off_f30_2504
,t_off_f30_2506
,t_off_m3d30_2506
,t_off_m4d30_2504

-- 三方数据子分
,aliyun_5
,duxiaoman_6
,hengpu_4
,hengpu_5
,rong360_4
,tengxun_1
,tianchuang_7
,pudao_68
,pudao_54
,baihang_28
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,ppcm_behav_score
,hengpu_7
,tengxun_cash_score
,dianhuabang_score
,duxiaoman_cash_score
,haluo_cto_score

-- 北京团队模型子分
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_gen_fpd
,t_gen_mob4
,t_gen3_fpd
,t_gen3_mob4
,t_mix_low_fpd
,t_mix_low_mob4
,t_mix_nopboc_fpd
,t_mix_nopboc_mob4
,t_dz_fpd
,t_xz_fpd
,t_br3_fpd
,t_br3_mob4
,t_br4_fpd
,t_free_f30_202504
,t_xz_mob4
,t30_xz_mob4
,t_dz_mob4
,t_free_m4d30_2504
,t_low_m4d30_2504
,t_low_f30_2504
,t_pd_fpd
,t_pboc_dpd20
,t_mix_pboc2_dpd20

from 
    (
    select
     t1.order_no
    ,t1.id_no_des
    ,t1.user_id
    ,t1.channel_id
    ,t1.target_fpd10
    ,t1.target_fpd20
    ,t1.target_fpd30
    ,t1.target_cpd30
    ,t1.target_mob3dpd30
    ,t1.target_mob4dpd30
    ,t2.apply_date
    ,t2.apply_time
    ,t3.apply_date as apply_date_auth
    ,t3.apply_time as apply_time_auth
    ,t3.order_no as order_no_auth
    ,datediff(t2.apply_date, t3.apply_date) as diff_days
    from znzz_fintech_ads.dm_f_zzj_test_order_target as t1
    
    inner join znzz_fintech_dwd.dwd_beforeloan_order_examine_fd as t2 
    on t1.order_no = t2.order_no and t1.dt=t2.dt
    
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t3 
    on t2.user_id = t3.user_id and t2.channel_id = t3.channel_id and t2.id_no_des=t3.id_no_des and t2.dt=t3.dt

    where t1.dt = date_sub(current_date(), 1)
      and t1.channel_id != 1
      and t2.dt = date_sub(current_date(), 1)  
      and t2.channel_id != 1
      and t2.apply_date >= '2024-11-01'
      and t2.apply_date <= '2025-04-18' 
      and t3.dt = date_sub(current_date(), 1)
      and t3.channel_id != 1
      and datediff(t2.apply_date, t3.apply_date)>30
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date >= '2024-11-01'
      and apply_date <= '2025-04-18'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'M1A0007' then good_score else null end) as M1A0007 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011 
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016 
    -- ,max(case when variable_code = 'M1A0017' then good_score else null end) as M1A0017 
    -- ,max(case when variable_code = 'M1A0018' then good_score else null end) as M1A0018 
    -- ,max(case when variable_code = 'M1A0019' then good_score else null end) as M1A0019
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022 
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023 
    -- ,max(case when variable_code = 'M1A0024' then good_score else null end) as M1A0024 
    -- ,max(case when variable_code = 'M1A0025' then good_score else null end) as M1A0025 
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028 
    -- ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029
    -- ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031 
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035 
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036 
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037 
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038 
    -- ,max(case when variable_code = 'M1A0039' then good_score else null end) as M1A0039 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041 
    -- ,max(case when variable_code = 'M1A0042' then good_score else null end) as M1A0042 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044 
    ,max(case when variable_code = 'M1B0001' then good_score else null end) as M1B0001 
    ,max(case when variable_code = 'M1B0002' then good_score else null end) as M1B0002 
    ,max(case when variable_code = 'M1B0004' then good_score else null end) as M1B0004 
    ,max(case when variable_code = 'M1B0007' then good_score else null end) as M1B0007 
    -- ,max(case when variable_code = 'M1B0008' then good_score else null end) as M1B0008 
    ,max(case when variable_code = 'M1B0011' then good_score else null end) as M1B0011 
    ,max(case when variable_code = 'M1B0012' then good_score else null end) as M1B0012 
    ,max(case when variable_code = 'M1B0016' then good_score else null end) as M1B0016 
    ,max(case when variable_code = 'M1B0024' then good_score else null end) as M1B0024 
    ,max(case when variable_code = 'M1B0025' then good_score else null end) as M1B0025 
    -- ,max(case when variable_code = 'M1B0027' then good_score else null end) as M1B0027 
    -- ,max(case when variable_code = 'M1B0028' then good_score else null end) as M1B0028 
    ,max(case when variable_code = 'M1B0029' then good_score else null end) as M1B0029   
    ,max(case when variable_code = 'M1B0030' then good_score else null end) as M1B0030 
    ,max(case when variable_code = 'M1B0031' then good_score else null end) as M1B0031 
    ,max(case when variable_code = 'M1B0032' then good_score else null end) as M1B0032 
    -- ,max(case when variable_code = 'M1B0035' then good_score else null end) as M1B0035 
    -- ,max(case when variable_code = 'M1B0037' then good_score else null end) as M1B0037 
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1 
    ,max(case when variable_code = 'a_pdv3_fpd30_v1' then good_score else null end) as a_pdv3_fpd30_v1   
    ,max(case when variable_code = 'off_f30_2504' then good_score else null end) as off_f30_2504 
    ,max(case when variable_code = 'off_f30_2506' then good_score else null end) as off_f30_2506 
    ,max(case when variable_code = 'off_m4d30_2504' then good_score else null end) as off_m4d30_2504 
    ,max(case when variable_code = 'off_m4d30_2506' then good_score else null end) as off_m4d30_2506   
    ,max(case when variable_code = 't_high_p_f30_2504' then good_score else null end) as t_high_p_f30_2504 
    ,max(case when variable_code = 't_high_p_m3d30_2025' then good_score else null end) as t_high_p_m3d30_2025 
    ,max(case when variable_code = 't_off_f30_2504' then good_score else null end) as t_off_f30_2504 
    ,max(case when variable_code = 't_off_f30_2506' then good_score else null end) as t_off_f30_2506 
    ,max(case when variable_code = 't_off_m3d30_2506' then good_score else null end) as t_off_m3d30_2506   
    ,max(case when variable_code = 't_off_m4d30_2504' then good_score else null end) as t_off_m4d30_2504   

    from znzz_fintech_ads.lending_model01_scores_off as t 
    where lending_time >= '2024-11-01'
      and lending_time <= '2025-04-18'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_t_r30_three_score_data as t 
    where dt >= '2024-11-01'
      and dt <= '2025-04-18'
    ) as t3 on t.order_no=t3.order_no

;

"""
df_sample_ = get_data(sql)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[8]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[9]:


df_sample_['order_no'].value_counts().head(3)


# In[11]:


df_sample_.query("order_no=='O0323250409203312935'").head(3)


# In[12]:


df_sample_.drop_duplicates(inplace=True)
print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[13]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[14]:


df_sample_.sort_values(by=['order_no', 'target_fpd30'], ascending=[True, False], inplace=True)
df_sample_.drop_duplicates(subset=['order_no'], keep='first', inplace=True)
df_sample_.shape


# In[15]:


varsname = df_sample_.columns.to_list()[11:]

print(varsname)
print("初始特征变量个数：",len(varsname))


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[17]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[18]:


df_sample = df_sample_.query("target_fpd30>=0 ").reset_index(drop=True)


# In[ ]:


# df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[19]:


df_sample.info(show_counts=True)
df_sample.head()


# In[21]:


df_sample['apply_date'] = df_sample['apply_date'].apply(str)
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[22]:


df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2025-02-28'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-04-01' & apply_date<='2025-04-18'").index, 'data_set']='3_oot2'


# In[23]:


df_sample.to_csv(result_path + '提现全渠道fpd30融合模型_2411_2502.csv',index=False)
print(result_path + '提现全渠道fpd30融合模型_2411_2502.csv')


# In[24]:


df_sample[df_sample[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index, inplace=True)


# In[25]:


target = 'target_fpd30'


# # 1. 样本概况

# In[26]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[27]:


print(df_sample[target].value_counts())


# In[28]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[29]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[30]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[31]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[32]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[33]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[34]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[35]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[36]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[37]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[ ]:





# In[38]:


# 2.2 缺失率按数据集，按渠道分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[39]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[40]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[41]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[42]:


to_drop_recent


# In[43]:


df_iv.loc[to_drop_iv,:]


# In[44]:


varsname_v1 = [col for col in varsname if col not in to_drop_recent] 
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[45]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.95, iv=0.00, corr=0.75, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[46]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:]


# In[ ]:


df_explor.loc[to_drop2,:]


# In[47]:


print(to_drop2)


# In[48]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[49]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df



def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[52]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[53]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[112]:


df_sample[varsname_v2+[target]].head()


# In[ ]:


df_sample[['t_beha4_mob4','t_beha5_mob4','t_beha4_fpd','t_beha5_fpd']]


# In[117]:


# 计算分布前先变量分箱
combiner1 = toad.transform.Combiner()
combiner1.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=20, empty_separate=True) 


# In[119]:


existing_bins_dict1 = combiner1.export()


# In[120]:


new_bins_dict1 = {}
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict1[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict1[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict1[col] = cutbins + empty


# In[121]:


new_bins_dict1


# In[123]:


for k in new_bins_dict1.keys():
    print(f"变量{k}的分箱数：{len(new_bins_dict1[k])}")


# In[125]:


for k in existing_bins_dict1.keys():
    print(f"变量{k}的分箱数：{len(existing_bins_dict1[k])}")


# In[124]:


for k in new_bins_dict.keys():
    print(f"变量{k}的分箱数：{len(new_bins_dict[k])}")


# In[126]:


combiner1.load(new_bins_dict1)


# In[127]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'3_变量分箱字典_{task_name}_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict1, f)
print(result_path + f'3_变量分箱字典_{task_name}_{timestamp}.pkl')


# In[54]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[56]:


new_bins_dict = {}
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[57]:


new_bins_dict


# In[58]:


combiner.load(new_bins_dict)


# In[59]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'3_变量分箱字典_{task_name}_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'3_变量分箱字典_{task_name}_{timestamp}.pkl')


# In[128]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner1, return_frame = False)

print('---------')
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner1, return_frame = False)
print('---------')


# In[ ]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

print('---------')
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print('---------')


# In[129]:



df_bins = combiner1.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[ ]:



df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[130]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[131]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[132]:


print(df_group_month.shape)
df_group_month.head()


# In[133]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[134]:


df_group_set.head()


# In[135]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[136]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
drop_by_psi = drop_by_psi_month + drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
drop_by_iv = drop_by_iv_month + drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[137]:


df_iv_by_set.loc[drop_by_iv,:]


# In[138]:


df_psi_by_set.loc[drop_by_psi,:]


# In[ ]:


# to_drop3 = []
print("剔除的变量有: ", len(to_drop3))


# In[139]:


# varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")
print(varsname_v3)


# ## 4.2 Y标签相关性删除

# In[140]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[141]:


df_sample_woe.head()


# In[142]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[143]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample[varsname_v3].corr(method='pearson')
df_corr_matrix.info()


# In[144]:


df_corr_matrix.head()


# In[145]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.75)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[146]:


df_high_corr.info()
df_high_corr.head()


# In[ ]:


print(to_drop4)


# In[147]:


df_high_corr


# In[148]:


df_iv_by_set.loc[to_drop4,:]


# In[149]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[150]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            binary_var = group[group[column].notna()][target]
            binary_cols = group[group[column].notna()][column]
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, binary_cols)[0]
                result_pvalue[column] =  pointbiserialr(binary_var, binary_cols)[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, binary_cols)[0]
                result_pvalue[column] =  pearsonr(binary_var, binary_cols)[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, binary_cols)[0]
                result_pvalue[column] =  spearmanr(binary_var, binary_cols)[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, binary_cols)[0]
                result_pvalue[column] =  kendalltau(binary_var, binary_cols)[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[151]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[152]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[153]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[154]:


varsname_v5 = [col for col in varsname_v4 if col not in to_drop5]
print(f"保留变量{len(varsname_v5)}个")
print(varsname_v5)


# In[155]:


# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[156]:


gc.collect()


# # 5.模型训练

# ## 5.0 函数定义

# In[157]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[158]:


df_sample[target].value_counts()


# In[159]:


df_sample['target_fpd30_1'] = 1 - df_sample[target]
modeltrian_target = 'target_fpd30_1'


# In[160]:


df_sample[modeltrian_target].value_counts()


# In[161]:


df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[162]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[163]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[164]:


print("最优参数opt_params: ", opt_params)


# In[165]:


print(len(varsname_v5))
print(varsname_v5)


# In[166]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[167]:


drop_cols_1_v1 = ['m1b0007', 'm1b0016', 'm1b0024','off_f30_2506', 'off_m4d30_2504', 't_off_f30_2506', 't_off_m3d30_2506']
drop_cols_1_v2 = ['t_gen3_fpd', 't_mix_nopboc_fpd','t_low_f30_2504','t_pboc_dpd20', 't_mix_pboc2_dpd20']
drop_cols_1 = drop_cols_1_v1 + drop_cols_1_v2
varsname_base = [col for col in varsname_v5 if col not in drop_cols_1]
varsname_base.append('m1b0004')
print(len(varsname_base))
print(varsname_base) 


# In[168]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[169]:


df_sample['data_set'].value_counts()


# In[170]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[171]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[172]:


df_ks_auc_set_base = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_base


# In[173]:


df_ks_auc_month_base = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_base


# In[ ]:


df_vars_list = pd.read_csv('df_vars_des.csv',encoding='gbk')
df_vars_list


# In[175]:


# 模型变量重要性
# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base = feature_importance(lgb_model) 
df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_base = df_importance_base.reset_index()
# df_importance_base = pd.merge(df_vars_list, df_importance_base, how='right',on='feature')
df_importance_base


# In[176]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base = feature_importance(lgb_model) 
df_importance_set_base = pd.merge(df_importance_set_base, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base = df_importance_set_base.reset_index()
# df_importance_set_base = pd.merge(df_vars_list, df_importance_set_base, how='right',on='feature')
df_importance_set_base


# In[177]:


df_model_corr_base = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
df_model_corr_base.info()
df_model_corr_base.head()


# In[178]:


# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base_v1_{timestamp}.xlsx') as writer:
    df_importance_set_base.to_excel(writer, sheet_name='df_importance_set_base')
    df_importance_base.to_excel(writer, sheet_name='df_importance_base')
    df_ks_auc_set_base.to_excel(writer, sheet_name='df_ks_auc_set_base')
    df_ks_auc_month_base.to_excel(writer, sheet_name='df_ks_auc_month_base')
    df_model_corr_base.to_excel(writer, sheet_name='df_model_corr_base')
print(result_path + f'4_模型训练_{task_name}_base_v1_{timestamp}.xlsx')


# #### 5.2.1.1 模型优化1

# In[244]:


varsname_base_v2 = ['m1a0022','m1a0023', 'm1a0027',  'm1a0031', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0025', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'pudao_68', 'pudao_54', 'baihang_28', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'tengxun_cash_score', 'dianhuabang_score','t_br_fpd','t_br_mob4','t_br2_fpd','t_br2_mob4', 't_dz_fpd', 't_xz_fpd', 't_br3_fpd','t_br3_mob4','t_br4_fpd', 't_xz_mob4','t30_xz_mob4','t_dz_mob4', 't_pd_fpd']
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[246]:


df_high_corr1, _ = find_high_correlation_pairs(df_corr_matrix.loc[varsname_base_v2,varsname_base_v2],
                                                     df_iv_by_set['3_oot1'],
                                                     threshold=0.75)
df_high_corr1


# In[247]:


df_high_corr2, _ = find_high_correlation_pairs(df_corr_matrix.loc[varsname_base_v2,varsname_base_v2],
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.75)
df_high_corr2


# In[ ]:





# In[249]:



### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[250]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape,X_test.shape)
df_sample['data_set'].value_counts()


# In[251]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[252]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[253]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[254]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[255]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[256]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[257]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# #### 5.2.1.2 模型优化2

# In[493]:


varsname_base_v2 = varsname_base_v2 + ['t_beha4_fpd','t_beha4_mob4','t_beha5_fpd','t_beha5_mob4']
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[494]:


df_corr_matrix_new = df_sample[varsname_base_v2].corr()


# In[497]:



# df_high_corr1, _ = find_high_correlation_pairs(df_corr_matrix.loc[varsname_base_v2,varsname_base_v2],
#                                                      df_iv_by_set['3_oot1'],
#                                                      threshold=0.75)
# df_high_corr1


# In[492]:



df_high_corr2, _ = find_high_correlation_pairs(df_corr_matrix.loc[varsname_base_v2,varsname_base_v2],
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.75)
df_high_corr2


# In[456]:


# drop_cols_2_v1 = ['m1a0007', 'm1a0009', 'm1a0011', 'm1a0016', 'm1a0020','m1a0021','m1a0026', 'm1a0028','m1a0034', 'm1a0035', 'm1a0036', 'm1a0037', 'm1a0040','a_bhdj_fpd10_v1', 'a_pboc_fpd30_v1', 'a_pboc_fpd6_v1', 'a_pdv3_fpd30_v1']
# print(len(drop_cols_2_v1))
# varsname_base_v2 = ['m1a0022','m1a0023', 'm1a0027',  'm1a0031', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0025', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'pudao_68', 'pudao_54', 'baihang_28', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'tengxun_cash_score', 'dianhuabang_score','t_br_fpd','t_br_mob4','t_br2_fpd','t_br2_mob4', 't_dz_fpd', 't_xz_fpd', 't_br3_fpd','t_br3_mob4','t_br4_fpd', 't_xz_mob4','t30_xz_mob4','t_dz_mob4', 't_pd_fpd']

# drop_cols_3_v1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br_fpd','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v1]
# print(len(['m1a0023', 'm1a0027',  'm1a0031', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tianchuang_7', 'pudao_68', 'pudao_54', 'baihang_28', 'tengxun_cash_score', 'dianhuabang_score', 't_br2_mob4','t_br3_mob4', 't_dz_fpd', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))

# drop_cols_3_v2 =  ['pudao_68','pudao_54', 'hengpu_5'] + ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br_fpd','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v2]
# print(len(['m1a0023', 'm1a0027',  'm1a0031', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'tianchuang_7', 'baihang_28', 'tengxun_cash_score', 'dianhuabang_score', 't_br2_mob4','t_br3_mob4', 't_dz_fpd', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)


# drop_cols_3_v3 =  ['m1a0023','m1a0027','m1b0030','m1a0044','m1b0012','tengxun_cash_score']+['pudao_68','pudao_54', 'hengpu_5'] + ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v3]
# print(len(['m1a0031', 'm1a0041', 'm1a0043', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0029', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'tianchuang_7', 'baihang_28', 'dianhuabang_score', 't_br_fpd','t_br3_mob4', 't_dz_fpd', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)

# drop_cols_3_v4 = ['hengpu_5']
# varsname_base_v3 = ['m1a0023', 'm1a0027', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'pudao_68', 'pudao_54', 'baihang_28', 'dianhuabang_score', 'tengxun_cash_score','t_br_fpd','t_br3_mob4', 't_xz_fpd', 't_dz_mob4']
# drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# drop_cols_3_v4 =  drop4+ drop3+drop2 + drop1
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v4]
# print(len(['m1a0041', 'm1a0043', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'tianchuang_7', 'baihang_28', 'dianhuabang_score', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)

# drop5 = ['dianhuabang_score']
# drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# drop_cols_3_v5 =  drop5 + drop4+ drop3+drop2 + drop1
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v5]
# print(len(['m1a0041', 'm1a0043', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'tianchuang_7', 'baihang_28', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)

# drop6 = ['tianchuang_7','m1b0001']
# drop5 = ['dianhuabang_score']
# drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# drop_cols_3_v5 =  drop6 + drop5 + drop4+ drop3+drop2 + drop1
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v5]
# print(len(['m1a0041', 'm1a0043', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'baihang_28', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)
# drop7 = ['t_pd_fpd']
# # drop6 = ['tianchuang_7','m1b0001']
# drop5 = ['dianhuabang_score']
# drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# drop_cols_3_v6 = drop7 + drop5 + drop4+ drop3+drop2 + drop1
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v6]
# print(len(['m1a0041', 'm1a0043', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'baihang_28', 't_xz_fpd', 't_dz_mob4']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)

# drop7 = ['t_pd_fpd']
# drop6 = ['tianchuang_7','m1b0001']
# drop5 = ['dianhuabang_score']
# drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# drop_cols_3_v7 = drop7 + drop6+drop5 + drop4+ drop3+drop2 + drop1
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v7]
# print(len(['m1a0041', 'm1a0043', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'baihang_28', 't_xz_fpd', 't_dz_mob4']))
# print(len(varsname_base_v3))
# print(varsname_base_v3)

varsname_base_v3 = varsname_base_v3 + ['t_beha4_fpd','t_beha4_mob4','t_beha5_fpd','t_beha5_mob4']
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[546]:


# drop_cols_8_v1 =  ['pudao_68','pudao_54', 'hengpu_5'] + ['ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score']
varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v2]
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[555]:


# tianchuang_7,m1b0030,m1b0002,duxiaoman_6
# m1a0023 m1b0031 duxiaoman_6 tengxun_cash_score m1b0002 t_br2_mob4 m1a0027
# varsname_base_v3.append('duxiaoman_6')
varsname_base_v3.remove('m1a0023')
varsname_base_v3.remove('m1b0031')
varsname_base_v3.remove('duxiaoman_6')
varsname_base_v3.remove('tengxun_cash_score')
varsname_base_v3.remove('m1b0002')
varsname_base_v3.remove('t_br2_mob4')
varsname_base_v3.remove('m1a0027')
varsname_base_v3.remove('t_beha4_mob4')
varsname_base_v3.remove('t_beha4_fpd')
# ['t_beha4_mob4','t_beha5_mob4','t_beha4_fpd','t_beha5_fpd'] 't_br_fpd','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4'
# varsname_base_v3.remove('m1a0043')
# varsname_base_v3.remove('m1a0031')
# varsname_base_v3.append('hengpu_5')
# varsname_base_v3.append('tengxun_1')
# varsname_base_v3.append('t_br_fpd')
# varsname_base_v3.append('t_br2_fpd')
# varsname_base_v3.append('t_br3_fpd')
# varsname_base_v3.append('t_br4_fpd')
# varsname_base_v3.append('t_br_mob4')
# varsname_base_v3.append('t_xz_mob4')
# varsname_base_v3.append('t30_xz_mob4')


# In[562]:


varsname_base_v3.remove('m1b0001')
varsname_base_v3.remove('dianhuabang_score')
varsname_base_v3.remove('m1b0012')
varsname_base_v3.remove('tengxun_1')
varsname_base_v3.remove('t_pd_fpd')
varsname_base_v3.remove('m1b0004')
varsname_base_v3.remove('m1b0011')
varsname_base_v3.remove('tianchuang_7')
varsname_base_v3.remove('t_dz_fpd')
varsname_base_v3.remove('m1b0030')


# In[689]:


# varsname_base_v3.append('m1a0043')
# varsname_base_v3.remove('m1a0043')
# varsname_base_v3.append('m1a0031')
# varsname_base_v3.append('t_dz_fpd')
# varsname_base_v3.append('dianhuabang_score')
# varsname_base_v3.append('m1b0001')
# varsname_base_v3.append('t_pd_fpd')
# varsname_base_v3.append('m1b0004')
varsname_base_v3.remove('tengxun_1')
varsname_base_v3.remove('tianchuang_7')
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[ ]:


# varsname_base_v3=['m1a0041', 'm1a0043', 'm1b0002', 'm1b0004', 'm1b0029', 'aliyun_5', 'hengpu_4', 'rong360_4', 'baihang_28', 't_xz_fpd', 't_dz_mob4', 't_beha4_mob4', 't_beha5_fpd'] + ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012'] +['tianchuang_7','dianhuabang_score']

# drop_cols_3_v1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 't_br_fpd','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4','t30_xz_mob4']
# varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v1]
# print(len(['m1a0023', 'm1a0027',  'm1a0031', 'm1a0041', 'm1a0043', 'm1a0044', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0011', 'm1b0012', 'm1b0029', 'm1b0030', 'm1b0031',  'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tianchuang_7', 'pudao_68', 'pudao_54', 'baihang_28', 'tengxun_cash_score', 'dianhuabang_score', 't_br2_mob4','t_br3_mob4', 't_dz_fpd', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# print(len(varsname_base_v3))


# In[516]:


# df_model_corr = df_sample[varsname_base_v3].corr()
# df_model_corr


# In[690]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[691]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[692]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[687]:


# 优化后评估模型效果
df_sample['y_prob_base_v3_15'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3_15'].head()


# In[688]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3_15', 'data_set')
df_ks_auc_set_v3


# In[630]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3_14', 'data_set')
df_ks_auc_set_v3


# In[631]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3_14', 'apply_month')
df_ks_auc_month_v3


# In[675]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[ ]:





# In[603]:


# # 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
# df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
# df_importance_month_v3 = df_importance_month_v3.reset_index()
# # df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
# df_importance_month_v3


# In[ ]:


tianchuang_7,m1b0030,m1b0002,duxiaoman_6


# In[648]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[523]:


df_sample[['t_beha4_mob4','t_beha5_mob4','t_beha4_fpd','t_beha5_fpd']].corr()


# In[634]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_14_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_14_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_14_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_14_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_14_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_14_{timestamp}.xlsx')


# In[624]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_13_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_13_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_13_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_13_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_13_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_13_{timestamp}.xlsx')


# In[ ]:





# In[605]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_12_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_12_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_12_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_12_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_12_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_12_{timestamp}.xlsx')


# In[571]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_11_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_11_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_11_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_11_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_11_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_11_{timestamp}.xlsx')


# In[538]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_10_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_10_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_10_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_10_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_10_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_10_{timestamp}.xlsx')


# In[526]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_9_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_9_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_9_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_9_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_9_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_9_{timestamp}.xlsx')


# In[425]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_7_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_7_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_7_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_7_{timestamp}.xlsx')


# In[414]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_6_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_6_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_6_{timestamp}.xlsx')


# In[398]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_5_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_5_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_5_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_5_{timestamp}.xlsx')


# In[381]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_4_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_4_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_4_{timestamp}.xlsx')


# In[320]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_3_{timestamp}.xlsx')


# In[306]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_2_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_2_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_model_corr.to_excel(writer, sheet_name='df_model_corr')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_2_{timestamp}.xlsx')


# In[283]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_1_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_1_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_1_{timestamp}.xlsx')


# In[270]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# In[ ]:





# ### 5.2.2 base加入离线融合模型

# In[694]:


# varsname_base_v4 = varsname_base_v2 + ['m1b0007', 'm1b0016', 'm1b0024','off_f30_2504','off_f30_2506','off_m4d30_2504','off_m4d30_2506','t_off_f30_2504','t_off_f30_2506','t_off_m3d30_2506','t_off_m4d30_2504']
# print(len(varsname_base_v4))
# print(varsname_base_v4)

varsname_base_v4 = varsname_base_v3 + ['m1b0007', 'm1b0024','off_f30_2504','off_f30_2506','off_m4d30_2504','off_m4d30_2506','t_off_f30_2504','t_off_f30_2506','t_off_m3d30_2506','t_off_m4d30_2504']
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[697]:


rh_vars = ['m1b0007', 'm1b0024','off_f30_2504','off_f30_2506','off_m4d30_2504','off_m4d30_2506','t_off_f30_2504','t_off_f30_2506','t_off_m3d30_2506','t_off_m4d30_2504']


# In[699]:


df_high_corr1, _ = find_high_correlation_pairs(df_corr_matrix.loc[rh_vars,rh_vars],
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)
df_high_corr1


# In[701]:


df_high_corr2, _ = find_high_correlation_pairs(df_corr_matrix.loc[rh_vars,rh_vars],
                                                     df_iv_by_set['3_oot1'],
                                                     threshold=0.70)
df_high_corr2


# In[ ]:





# In[730]:


# drop_cols_4_v1 = ['m1b0004', 'm1b0016', 'off_f30_2504', 'off_m4d30_2506']
# varsname_base_v5 = [col for col in varsname_base_v4 if col not in drop_cols_4_v1]
# print(len(varsname_base_v5))
# print(varsname_base_v5)
# # drop5 = ['duxiaoman_6','t_off_m4d30_2504','off_f30_2504','t_dz_fpd','m1b0011','m1b0012']
# # drop4 = ['m1a0031','t_br3_mob4','t_br_fpd','t_dz_fpd','m1b0011','m1b0012']
# # drop3 = ['m1a0023','m1a0027','m1b0031','m1a0044','m1b0012','tengxun_cash_score']
# # drop2 = ['pudao_68','pudao_54', 'hengpu_5']
# # drop1 = ['m1a0022', 'm1b0025', 'tengxun_1', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5',
# #          'ppcm_behav_score', 't_br2_mob4','t_br_mob4','t_br2_fpd','t_br3_fpd','t_br4_fpd', 't_xz_mob4',
# #          't30_xz_mob4']
# # drop_cols_3_v4 =  drop4+ drop3+drop2 + drop1
# # varsname_base_v3 = [col for col in varsname_base_v2 if col not in drop_cols_3_v4]
# # print(len(['m1a0041', 'm1a0043', 'm1b0001', 'm1b0002', 'm1b0004', 'm1b0029', 'm1b0030', 'aliyun_5', 'duxiaoman_6', 'hengpu_4', 'rong360_4', 'tianchuang_7', 'baihang_28', 'dianhuabang_score', 't_xz_fpd', 't_dz_mob4', 't_pd_fpd']))
# # print(len(varsname_base_v3))
# # print(varsname_base_v3)
drop_cols_4_v1 = ['t_off_m4d30_2504','off_m4d30_2506', 'off_f30_2504','off_m4d30_2504','m1b0007','off_f30_2506']
drop_cols_4_v2 = ['t_beha5_mob4','dianhuabang_score','hengpu_4','hengpu_5']

drop_cols_4 = drop_cols_4_v1 + drop_cols_4_v2
varsname_base_v4 = [col for col in varsname_base_v4 if col not in drop_cols_4]

print(len(varsname_base_v4))
print(varsname_base_v4)


# In[754]:


# varsname_base_v4.remove('t_beha5_mob4')
varsname_base_v4.append('dianhuabang_score')
varsname_base_v4.append('hengpu_4')
varsname_base_v4.append('hengpu_5')
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[755]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[756]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'

df_sample['data_set'].value_counts()


# In[757]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[758]:


# 优化后评估模型效果
df_sample['y_prob_base_v4_2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v4_2'].head()


# In[759]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v4_2', 'data_set')
df_ks_auc_set_v4


# In[749]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v4_4', 'data_set')
df_ks_auc_set_v4


# In[760]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v4_2', 'apply_month')
df_ks_auc_month_v4


# In[761]:



# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
df_importance_set_v4 = pd.merge(df_importance_set_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v4 = pd.merge(df_vars_list, df_importance_set_v4, how='right',left_on='name',right_on='feature')
df_importance_set_v4


# In[708]:


df_high_corr1


# In[709]:


df_high_corr2


# In[762]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4= feature_importance(lgb_model) 
df_importance_month_v4 = pd.merge(df_importance_month_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v4 = pd.merge(df_vars_list, df_importance_month_v4, how='right',left_on='name',right_on='feature')
df_importance_month_v4


# In[753]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_4_{timestamp}.xlsx')


# In[729]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_3_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_3_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_3_{timestamp}.xlsx')


# In[763]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_2_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_2_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_2_{timestamp}.xlsx')


# In[719]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_1_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_1_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_1_{timestamp}.xlsx')


# In[371]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[333]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = ['apply_month','channel_types']
df_miss_month1 = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month1.index.name = 'variable'
print(df_miss_month1.head())


# In[336]:


df_miss_month1.loc['t_pd_fpd',:]


# ## 5.3 模型效果对比

# In[ ]:


df_sample.info(show_counts=True)


# In[427]:


sql = """
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date >= '2024-11-01'
      and apply_date <= '2025-04-22'
      and dt>=''
"""
df_bj = get_data(sql)
df_bj.info(show_counts=True)


# In[428]:


df_bj.to_csv('df_bj.csv',index=False)


# ### 5.3.1数据处理

# In[430]:


print(df_sample.columns.to_list())


# In[771]:


usecols1 = list(df_sample.columns[:11]) +['apply_month', 'data_set', 'target_fpd30_1', 'channel_types',
                                           'channel_rates', 'y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v3',
                                           'y_prob_base_v3_1', 'y_prob_base_v3_2', 'y_prob_base_v3_3',
                                            'y_prob_base_v3_4', 'y_prob_base_v3_5',
                                           'y_prob_base_v3_6', 'y_prob_base_v3_7', 'y_prob_base_v3_8',
                                            'y_prob_base_v3_9', 'y_prob_base_v3_10', 'y_prob_base_v3_11',
                                         'y_prob_base_v3_12', 'y_prob_base_v3_13', 'y_prob_base_v3_14',
                                         'y_prob_base_v4','y_prob_base_v4_1','y_prob_base_v4_2','y_prob_base_v4_3',
                                          'y_prob_base_v4_4']
usecols2 = ['t_high_p_f30_2504','t_high_p_m3d30_2025', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506',
            't_off_m4d30_2504','t_free_f30_202504','t_free_m4d30_2504','t_low_m4d30_2504','t_low_f30_2504']
usecols_bj = ['order_no','t_free_f30_2506','t_free_m4d30_2506','t_low_m4d30_2506','t_low_f30_2506']
df_evalue = pd.merge(df_sample[usecols1+usecols2], df_bj[usecols_bj], how='left', on='order_no')
# df_evalue = df_sample[usecols1]
df_evalue.info(show_counts=True)


# In[797]:


corr_cols = ['y_prob_base_v3_14','y_prob_base_v4_4'] + usecols2 + usecols_bj
corr_cols.remove('order_no')
df_corr_models = df_evalue[corr_cols].corr()
df_corr_models.to_csv('df_corr_models.csv')


# In[434]:


for col in df_evalue.columns[16:]:
    df_evalue[col] = pd.to_numeric(df_evalue[col])


# In[ ]:


for col in df_bj.columns[4:]:
    if df_bj[col].dtypes=='object':
        print(col)
        df_bj[col] = pd.to_numeric(df_bj[col])


# In[452]:


usecols = ['order_no','t_beha4_fpd','t_beha4_mob4','t_beha5_fpd','t_beha5_mob4']
# df_sample.drop(columns=['t_beha4_fpd','t_beha4_mob4','t_beha4_fpd','t_beha5_mob4'],inplace=True)
df_sample = pd.merge(df_sample, df_bj[usecols], how='left',on='order_no')


# In[453]:


print(df_sample.shape)


# In[454]:


df_sample[usecols].head()


# ### 5.3.2 效果对比

# In[766]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)     
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[773]:


model_score = ['y_prob_base_v3_14','y_prob_base_v4_4']
vars_score = ['t_high_p_f30_2504','t_low_f30_2506','t_free_f30_2506', 't_off_f30_2506', 't_off_f30_2504'
            ,'t_off_m3d30_2506','t_high_p_m3d30_2025','t_free_m4d30_2506','t_low_m4d30_2506',
            't_off_m4d30_2504','t_free_f30_202504','t_free_m4d30_2504','t_low_m4d30_2504','t_low_f30_2504']

score_list = model_score + vars_score
# score_list = list(df_evalue.columns[16:])
print(len(score_list),score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# #### 调用征信渠道

# In[774]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[ ]:


df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v6', 'apply_month')
df_ks_auc_month_pboc.head()


# In[ ]:


df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v6', 'data_set')
df_ks_auc_set_pboc['渠道'] = '全渠道'
tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
df_ks_auc_set_pboc


# In[775]:


model_score = ['y_prob_base_v3_14','y_prob_base_v4_4']
vars_score = ['t_high_p_f30_2504','t_low_f30_2506','t_free_f30_2506', 't_off_f30_2506', 't_off_f30_2504'
            ,'t_off_m3d30_2506','t_high_p_m3d30_2025','t_free_m4d30_2506','t_low_m4d30_2506',
            't_off_m4d30_2504','t_free_f30_202504','t_free_m4d30_2504','t_low_m4d30_2504','t_low_f30_2504']

score_list = model_score + vars_score
# score_list = list(df_evalue.columns[16:])
print(len(score_list),score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2


# In[770]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_base_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_pboc_2_v2.to_excel(writer, sheet_name='df_ksauc_pboc_2_v2')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_base_{timestamp}.xlsx')


# In[776]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_pboc_2_v2.to_excel(writer, sheet_name='df_ksauc_pboc_2_v2')
#     df_ks_auc_month_pboc.to_excel(writer, sheet_name='df_ks_auc_month_pboc')
#     df_ks_auc_set_pboc.to_excel(writer, sheet_name='df_ks_auc_set_pboc')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:


df_evalue.to_csv(result_path + r'提现全渠道高成本子分融合fpd30模型2410_2412_evalue.csv',index=False)


# # 6. 评分分布

# In[777]:


df_sample['apply_month'].value_counts()


# In[786]:


# score = 'y_prob_base_v3_14'

score = 'y_prob_base_v4_4'


# In[787]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[788]:


df_sample['score_bins'].head()


# In[789]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[790]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[791]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[792]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[793]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_有离线融合_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_有离线融合_{timestamp}.xlsx')


# In[785]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_无离线融合_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_无离线融合_{timestamp}.xlsx')


# In[ ]:





# In[794]:


df_sample.to_csv(result_path + '提现全渠道fpd30融合模型_2411_2502_report.csv',index=False)
print(result_path + '提现全渠道fpd30融合模型_2411_2502_report.csv')




#==============================================================================
# File: 提现全渠道人行衍生M6D30离线模型_2409_2411.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '提现全渠道人行衍生mob6dpd30离线模型'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行 SQL 查询
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# In[4]:



def process_big_data_lgb(sql, target, to_drop=[]):
    import pandas as pd
    import lightgbm as lgb
    from odps import ODPS
    from datetime import datetime
    import time
    import multiprocessing

    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    # Batch 大小
    BATCH_SIZE = 550000

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()

    # 执行 SQL 查询
    instance = conn.execute_sql(sql)
    reader = instance.open_reader()

    # 初始化存储：累积 importance 和样本权重
    feature_importances = {}  # {feature: [ (weight, importance), ... ]}
    total_samples = 0

    # LightGBM 参数配置
        ### 模型参数
    opt_params = {}
    opt_params['boosting'] = 'gbdt'
    opt_params['objective'] = 'binary'
    opt_params['metric'] = 'auc'
    opt_params['bagging_freq'] = 1
    opt_params['scale_pos_weight'] = 1 
    opt_params['seed'] = 1 
    opt_params['num_threads'] = -1 
    # 调参时设置成不用调参的参数
    opt_params['learning_rate'] = 0.1
    ## 正则参数，防止过拟合
    opt_params['bagging_fraction'] = 0.8628008772208227     
    opt_params['feature_fraction'] = 0.6177619614753441
    opt_params['lambda_l1'] = 0
    opt_params['lambda_l2'] = 300
    opt_params['early_stopping_rounds'] = 50

    # 调参后的参数需要变成整数型
    opt_params['num_leaves'] = 21
    opt_params['min_data_in_leaf'] = 103
    opt_params['max_depth'] = 2
    # 调参后的其他参
    opt_params['min_gain_to_split'] = 10

    # 分块读取
    for i, batch in enumerate(reader.to_pandas_batches(batch_size=BATCH_SIZE, n_process=n_process)):
        print(f"处理第 {i+1} 批数据，batch shape: {batch.shape}")

        if 'target' not in batch.columns:
            raise ValueError(f"目标列 '{target}' 不存在")

        X_batch = batch.drop(columns=to_drop)
        y_batch = batch[target].astype(int)
        n_samples = len(batch)

        # 构建 LightGBM Dataset
        lgb_dataset = lgb.Dataset(X_batch, label=y_batch)

        # 训练模型（使用 train 接口，便于控制参数）
        booster = lgb.train(
            params=opt_params,
            train_set=lgb_dataset,
            num_boost_round=10000,  # 控制树的数量
            valid_sets=[lgb_dataset],
            verbose_eval=-1,
            keep_training_booster=True
        )

        # 提取基于 'gain' 的特征重要性
        importance_gain = booster.feature_importance(importance_type='gain')  # 使用 gain
        feature_names = X_batch.columns.tolist()

        # 累积：按样本数加权
        for name, imp in zip(feature_names, importance_gain):
            if name not in feature_importances:
                feature_importances[name] = []
            feature_importances[name].append((n_samples, imp))  # (权重, 值)

        total_samples += n_samples

    # === 计算加权平均重要性 ===
    weighted_avg_importance = {}
    for feature, weight_imp_list in feature_importances.items():
        weighted_sum = sum(n * imp for n, imp in weight_imp_list)
        total_weight = sum(n for n, _ in weight_imp_list)
        weighted_avg_importance[feature] = weighted_sum / total_weight

    # 转为 DataFrame 并排序
    importance_df = pd.DataFrame(
        list(weighted_avg_importance.items()),
        columns=['feature', 'importance_avg']
    ).sort_values(by='importance_avg', ascending=False).reset_index(drop=True)

    print(f"共处理 {total_samples} 个样本")
    print(f"共提取 {len(importance_df)} 个特征的平均重要性（基于 'gain'，加权）")

    # 结束时间
    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行时间：{}秒".format(round(end - start, 2)))
    
    return importance_df


def process_big_data_rf(sql, target, to_drop=[]):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    from sklearn.feature_selection import SelectFromModel
    from sklearn.ensemble import RandomForestClassifier
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行 SQL 查询
    instance = conn.execute_sql(sql)
    reader = instance.open_reader()
    # 初始化模型
    selector = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=42))

    # 用于累积特征重要性
    feature_importances = {}
    
    BATCH_SIZE=550000
    # 分块读取并处理
    for batch in reader.to_pandas_batches(batch_size=BATCH_SIZE,n_process=n_process):
        print(f"处理第 {i+1} 批数据，batch shape: {batch.shape}")
        if target not in batch.columns:
            raise ValueError(f"目标列 '{target}' 不存在")
        # 假设 'target' 是目标列
        X_batch = batch.drop(columns=to_drop)
        y_batch = batch[target]

        # 训练模型并获取特征重要性
        selector.fit(X_batch, y_batch)

        # 累积特征重要性
        for feature, importance in zip(X_batch.columns, selector.estimator_.feature_importances_):
            if feature not in feature_importances:
                feature_importances[feature] = []
            feature_importances[feature].append(importance)

    # 计算平均特征重要性
    avg_feature_importances = {feature: sum(importances)/len(importances) 
                               for feature, importances in feature_importances.items()}

    # 根据平均重要性选择特征
    selected_features = [feature for feature, avg_imp in avg_feature_importances.items() if avg_imp > 10]
    
    return selected_features


# # 0. 数据读取

# In[6]:


sql = """
select order_no,diff_days,diff_days_flag
from 
(
    select 
     t1.order_no
    ,datediff(t1.apply_date,t2.apply_date) as diff_days
    ,case when datediff(t1.apply_date,t2.apply_date)=0  then '0'
          when datediff(t1.apply_date,t2.apply_date)<=30 then '(0, 30]'
     else '30+' end as diff_days_flag
    ,ROW_NUMBER() OVER (PARTITION BY t1.order_no  ORDER BY datediff(t1.apply_date,t2.apply_date)) AS rk
    from znzz_fintech_dwd.dwd_beforeloan_order_examine_fd as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2 
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des  
    where t1.dt = date_sub(current_date(), 1) 
      and t1.channel_id != 1
      and t2.dt = date_sub(current_date(), 1) 
      and t2.channel_id != 1
      and t1.order_status = 6
      and t2.auth_status = 6
      and t1.apply_date>= '2024-09-01'
      and t1.apply_date<= '2024-12-31'
      and t1.apply_date>=t2.apply_date
) as t 
where rk=1
"""
df_diff_days= get_data(sql)
df_diff_days.info(show_counts=True)
df_diff_days.head()


# In[7]:


print(df_diff_days.shape, df_diff_days['order_no'].nunique())


# In[ ]:


sql = """
select * from znzz_fintech_ads.lxl_order_tmp_pboc_model_sample

"""
df_sample_ = get_data(sql)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[10]:


df_sample_.info(show_counts=True)
df_sample_.head()


# In[11]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[12]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[13]:


df_sample_.drop_duplicates(subset=None,inplace=True)


# In[14]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[15]:


df_sample_.drop_duplicates(subset=['order_no'],inplace=True)


# In[16]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[17]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[18]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col],errors='coerce')


# In[19]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[ ]:





# In[20]:


df_sample = pd.merge(df_sample_.query("target_mob6dpd30>=0"),df_diff_days, how='left',on='order_no')
df_sample.info(show_counts=True)
df_sample.head()


# In[21]:


df_sample.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[23]:


df_sample['apply_month'] = df_sample['apply_date'].apply(str).str[0:7]
df_sample['apply_month'].head()


# In[ ]:





# In[33]:


df_sample['apply_date'] = df_sample['apply_date'].apply(str)


# In[34]:


df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-31'").index, 'data_set']='3_oot2'


# In[35]:


df_sample['diff_days_flag'].value_counts(dropna=False)


# In[36]:


df_sample.to_csv('df_sample.csv',index=False)


# In[37]:


target = 'target_mob6dpd30'


# # 1. 样本概况

# In[38]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[39]:


print(df_sample[target].value_counts())


# In[40]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[41]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[42]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[ ]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[ ]:





# In[43]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[44]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# ## 2.2 数据探索

# In[46]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[47]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[48]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[ ]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[50]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
#         df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
#         df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[11]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[12]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[13]:


varsname_v2 = lgb_model.feature_name()


# In[14]:


target


# In[15]:


df_sample['data_set'].value_counts()


# In[16]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[20]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[ ]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}, {len(existing_bins_dict[col])}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[28]:


new_bins_dict


# In[29]:


combiner.load(new_bins_dict)


# In[30]:


combiner.export()


# In[31]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[33]:


# 计算psi
# df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2, \
#                                    'apply_month', combiner, return_frame = False)
# print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[34]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[36]:


varsname_v2_copy = varsname_v2[:]


# In[37]:


varsname_v2 = varsname_v2[:200]


# In[40]:


df_importance_set_v3 = feature_importance(lgb_model).reset_index()
varsname_v2 = list(df_importance_set_v3.loc[range(200),'feature'])
len(varsname_v2)


# In[ ]:


# 计算iv
# df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
# print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[ ]:


df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")

# df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
# print("-------")


# In[ ]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[ ]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample[varsname_v2].corr(method='pearson')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
#         df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
#         df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
#         df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
#         df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
#         pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[38]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[52]:


target


# In[53]:


df_sample[target].value_counts()


# In[54]:


modeltrian_target = 'target_mob6dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[55]:


df_sample[modeltrian_target].value_counts()


# In[56]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[58]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[59]:


df_sample['channel_types'].value_counts()


# In[60]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[61]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[62]:


print("最优参数opt_params: ", opt_params)


# In[63]:


varsname_base = varsname[:]
print(len(varsname_base))
# print(varsname_base)


# In[64]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[67]:


del df_sample_
gc.collect()


# In[72]:


# 6，训练/保存/评估模型
# 优化训练模型
# train_set = lgb.Dataset(X_train, label=y_train)
# valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
# del X_train, X_test, y_train, y_test
# del X_train_, y_train_
# gc.collect()
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[78]:


# 优化后评估模型效果
# df_sample['y_pred_v1'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
# df_sample['y_pred_v1'].head()


# In[79]:


gc.collect()


# In[ ]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[ ]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[ ]:


df_vars_desc = pd.read_excel('人行变量清单2506.xlsx',sheet_name='离线变量清单')
df_vars_desc.info(show_counts=True)
df_vars_desc.head()


# In[ ]:


df_vars_desc.rename(columns={'var':'feature'},inplace=True)
df_vars_desc['feature']=df_vars_desc['feature'].str.lower()

df_vars_desc = df_vars_desc[['feature','comment']]


# In[ ]:


df_iv_psi_miss = pd.concat([df_iv_by_month, df_psi_by_month, df_explor.loc[:,'missing']], axis=1)
df_iv_psi_miss.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + ['total_na']
df_iv_psi_miss.head()


# In[74]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_desc, df_importance_month_v1, how='right',on='feature')
df_importance_month_v1


# In[ ]:





# In[ ]:


# df_iv_by_set.columns = [f'{col}_iv' for col in df_iv_by_set.columns]
# df_psi_by_set.columns = [f'{col}_psi' for col in df_psi_by_set.columns]
# df_miss = df_explor.loc[:,'missing']

# df_set_iv_psi_miss = pd.concat([df_iv_by_set, df_psi_by_set, df_miss], axis=1)
df_set_iv_psi_miss.columns=['1_train_iv','3_oot1_iv','3_oot2_iv','1_train_psi','3_oot1_psi','3_oot2_psi','missing']
df_set_iv_psi_miss.head()


# In[76]:


# 模型变量重要性
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_desc, df_importance_set_v1, how='right',on='feature')
df_importance_set_v1


# In[77]:


# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
#     df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
#     df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 特征变量优化1

# In[80]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[81]:


print("最优参数opt_params: ", opt_params)


# In[82]:


varsname_base_v2 = df_importance_month_v1[df_importance_month_v1['gain']>0]['feature'].to_list()
print(len(varsname_base_v2))
# print(varsname_base_v2)
to_drop = [col for col in varsname_base if col not in varsname_base_v2]
print(len(to_drop))


# In[83]:


df_sample.drop(columns=to_drop, inplace=True)
df_sample.shape


# In[84]:


df_sample.info(show_counts=True)


# In[85]:


gc.collect()


# In[86]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[87]:


df_sample['data_set'].value_counts()


# In[88]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
del X_train, X_test, y_train, y_test
gc.collect()
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[89]:


# 优化后评估模型效果
df_sample['y_pred_v2'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v2'].head()


# In[90]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'apply_month')
df_ks_auc_month_v2


# In[91]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'data_set')
df_ks_auc_set_v2


# In[93]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_desc, df_importance_month_v2, how='right',on='feature')
df_importance_month_v2


# In[95]:


# 模型变量重要性
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_desc, df_importance_set_v2, how='right',on='feature')
df_importance_set_v2


# In[96]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 特征变量优化2

# In[97]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[98]:


varsname_base_v3 = df_importance_month_v2[df_importance_month_v2['gain']>0]['feature'].to_list()
print(len(varsname_base_v3))


# In[99]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )

print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[100]:


df_sample['data_set'].value_counts()


# In[101]:


del X_train_,y_train_
gc.collect()


# In[ ]:


# 定义一个函数来进行递归特征消除
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # 使用当前最佳特征集训练模型
        # ✅ 构建当前特征子集的数据
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # 如果是 numpy array，用位置索引
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # 更新最佳特征集
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features

# 执行 RFE 过程
best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, opt_params)

print(f"Selected feature count: {len(best_features)}")
# print("Selected features:", best_features)


# In[107]:


print(f"Selected feature count: {len(best_features)}")


# In[106]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train[best_features], label=y_train)
valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[108]:


# 优化后评估模型效果
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[109]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'apply_month')
df_ks_auc_month_v3


# In[110]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'data_set')
df_ks_auc_set_v3


# In[111]:


# 模型变量重要性
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_desc, df_importance_month_v3, how='right',on='feature')
df_importance_month_v3


# In[112]:


# 模型变量重要性
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_desc, df_importance_set_v3, how='right',on='feature')
df_importance_set_v3


# In[113]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# ### 5.3.1数据处理

# In[5]:


with open('./result/提现全渠道人行衍生mob6dpd30离线模型_v3_20250804000025.pkl','rb') as f:
    lgb_model = pickle.load(f)


# In[6]:


uscols = ['order_no','user_id','id_no_des','channel_id','apply_date','apply_month','diff_days_flag','data_set','target_fpd30','target_cpd30','target_mob4dpd30','target_mob6dpd30'] + lgb_model.feature_name()
          


# In[7]:


df_sample = pd.read_csv('df_sample.csv',usecols=uscols)
df_sample.info(show_counts=True)
df_sample.head()


# In[8]:


target='target_mob6dpd30'


# In[9]:


df_sample['y_pred_v3'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[10]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[ ]:


usecols = ['order_no','channel_id','apply_date','apply_month', 'data_set', 'target_mob6dpd30', 'target_mob6dpd30_1','channel_types', 'channel_rates', 'y_pred_v3']
print(len(usecols))


# In[11]:


df_evalue = df_sample.drop(columns=lgb_model.feature_name())
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 效果对比

# In[12]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)       
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[13]:


df_evalue['target_mob6dpd30'].value_counts()


# In[14]:


df_evalue['target_mob6dpd30_1'] = 1 - df_evalue['target_mob6dpd30']


# In[15]:


score_list = ['y_pred_v3']
print(len(score_list))
print(score_list)

target_list = ['target_mob6dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all1


# In[16]:



df_evalue['diff_days_flag'].value_counts()


# In[23]:


score_list = ['y_pred_v3']
print(len(score_list))
print(score_list)

target_list = ['target_mob6dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['diff_days_flag','channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['diff_days_flag','channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['diff_days_flag','apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2=df_ksauc_all2.sort_values(['diff_days_flag','target_type'])
df_ksauc_all2


# In[24]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='分客群')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# # 6. 评分分布

# In[25]:


df_sample['apply_month'].value_counts()


# In[28]:


score = 'y_pred_v3'
target = 'target_mob6dpd30'


# In[29]:


c = toad.transform.Combiner()
c.fit(df_sample[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[30]:


df_sample['score_bins'].head()


# In[31]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[33]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[ ]:


# df_psi_matrix = df_psi_matrix.loc['1_train',:]
# df_psi_matrix


# In[35]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[36]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[37]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[38]:


df_evalue.to_csv(result_path + '提现全渠道人行衍生M6D30离线模型_2409_2411.csv',index=False)
print(result_path + '提现全渠道人行衍生M6D30离线模型_2409_2411.csv')


# # 7. 模型部署和回溯

# In[ ]:







#==============================================================================
# File: 提现全渠道高成本子分融合模型fpd30标签v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储目录
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[3]:


task_name = '提现全渠道高成本子分融合模型fpd30标签'


# # 函数定义

# In[4]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[5]:


# sql = """
# create table znzz_fintech_ads.lxl_tmp_tx_merge_250424 as 
# select 
#  t.order_no
# ,t.user_id
# ,t.id_no_des
# ,t.channel_id
# ,t.apply_date
# ,target_fpd10
# ,target_fpd20
# ,target_fpd30
# ,target_cpd30
# ,target_mob3dpd30
# ,target_mob4dpd30
# -- 深圳团队子分
# ,all_a_bhdj_fpd10_v1_p
# ,all_a_br_derived_fpd30_202408_g_p
# ,all_a_br_derived_v1_mob4dpd30_202502_st_p
# ,all_a_br_derived_v2_fpd30_202411_g_p
# ,all_a_br_derived_v3_fpd30_202412_g_p
# ,all_a_dz_derived_v1_fpd30_202502_g_p
# ,all_a_dz_derived_v2_fpd30_202502_g_p
# ,all_a_rh_fpd0_v1_p
# ,all_a_rh_fpd10_v1_p
# ,all_a_rh_fpd10_v2_p
# ,all_a_rh_fpd30_v1_p
# ,all_a_rh_fpd6_v1_p
# ,all_a_third_pdv3_fpd30_v_p
# ,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
# ,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
# ,HLV_D_HOLO_certNo_variableCode_standard_BD003
# ,HLV_D_HOLO_jk_certNo_score_fpd7_v1
# ,M1A0022_g_p
# ,M1A0023_g_p
# ,M1A0026_g_p
# ,M1A0027_g_p
# ,M1A0028_g_p
# ,M1B0011_st_score
# ,M1B0012_st_score
# ,M1B0013_st_score
# ,M1B0014_st_score
# ,M1B0015_st_score
# ,ypy_bhxz_a_fpd30_v1_prob_good
# ,ypy_pboc_a_fpd7_v1_prob_good

# -- 三方数据子分
# ,zhirongfen
# ,bh_lx_115
# ,pudao_68
# ,ruizhi_6
# ,baihang_28
# ,pudao_34
# ,feicuifen
# ,hengpu_5
# ,tengxun_1
# ,tianchuang_7
# ,bileizhenv1
# ,duxiaoman_6
# ,hengpu_4
# ,rong360_4
# ,aliyun_5

# -- 北京团队模型子分
# ,t_br_fpd
# ,t_br_mob4
# ,t_br2_fpd
# ,t_br2_mob4
# ,t_beha3_fpd
# ,t_beha3_mob4
# ,t_pboc_dpd20
# ,t_dz_fpd
# ,t_xz_fpd
# ,t_br3_fpd
# ,t_br3_mob4
# ,t_beha4_fpd
# ,t_beha4_mob4
# ,t_br4_fpd
# ,t_xz_mob4
# ,t30_xz_mob4
# ,t_dz_mob4

# from 
#     (
#     select 
#      t1.order_no
#     ,t1.user_id
#     ,t1.id_no_des
#     ,t1.channel_id
#     ,t2.apply_date
#     ,target_fpd10
#     ,target_fpd20
#     ,target_fpd30
#     ,target_cpd30
#     ,target_mob3dpd30
#     ,target_mob4dpd30
#     from znzz_fintech_ads.dm_f_zzj_test_order_target as t1 
#     inner join znzz_fintech_ads.lxl_tmp_auth_order as t2 on t1.order_no=t2.order_no
#     where t1.dt = '2025-04-23'
#       and t2.apply_date >= '2024-08-01'
#       and t2.apply_date <= '2025-02-22'
#       and t2.diff_days>30
#     ) as t 
# -- 北京团队的子分
# left join 
#     (
#     select t.*
#     from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
#     where apply_date >= '2024-08-01'
#       and apply_date <= '2025-02-22'
#       and dt>=''
#     ) as t1 on t.order_no=t1.order_no

# -- 深圳团队的子分
# left join 
#     (
#     select 
#      order_no
#     ,max(case when variable_code = 'all_a_bhdj_fpd10_v1_p' then variable_value else null end) as all_a_bhdj_fpd10_v1_p 
#     ,max(case when variable_code = 'all_a_br_derived_fpd30_202408_g_p' then variable_value else null end) as all_a_br_derived_fpd30_202408_g_p 
#     ,max(case when variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p' then variable_value else null end) as all_a_br_derived_v1_mob4dpd30_202502_st_p 
#     ,max(case when variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p' then variable_value else null end) as all_a_br_derived_v2_fpd30_202411_g_p 
#     ,max(case when variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p' then variable_value else null end) as all_a_br_derived_v3_fpd30_202412_g_p 
#     ,max(case when variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v1_fpd30_202502_g_p 
#     ,max(case when variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v2_fpd30_202502_g_p 
#     ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
#     ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
#     ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
#     ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
#     ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
#     ,max(case when variable_code = 'all_a_third_pdv3_fpd30_v_p' then variable_value else null end) as all_a_third_pdv3_fpd30_v_p 
#     ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard 
#     ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard 
#     ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_standard_BD003 
#     ,max(case when variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1' then variable_value else null end) as HLV_D_HOLO_jk_certNo_score_fpd7_v1 
#     ,max(case when variable_code = 'M1A0022_g_p' then variable_value else null end) as M1A0022_g_p 
#     ,max(case when variable_code = 'M1A0023_g_p' then variable_value else null end) as M1A0023_g_p 
#     ,max(case when variable_code = 'M1A0026_g_p' then variable_value else null end) as M1A0026_g_p 
#     ,max(case when variable_code = 'M1A0027_g_p' then variable_value else null end) as M1A0027_g_p 
#     ,max(case when variable_code = 'M1A0028_g_p' then variable_value else null end) as M1A0028_g_p 
#     ,max(case when variable_code = 'M1B0011_st_score' then variable_value else null end) as M1B0011_st_score 
#     ,max(case when variable_code = 'M1B0012_st_score' then variable_value else null end) as M1B0012_st_score 
#     ,max(case when variable_code = 'M1B0013_st_score' then variable_value else null end) as M1B0013_st_score 
#     ,max(case when variable_code = 'M1B0014_st_score' then variable_value else null end) as M1B0014_st_score 
#     ,max(case when variable_code = 'M1B0015_st_score' then variable_value else null end) as M1B0015_st_score 
#     ,max(case when variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good' then variable_value else null end) as ypy_bhxz_a_fpd30_v1_prob_good 
#     ,max(case when variable_code = 'ypy_pboc_a_fpd7_v1_prob_good' then variable_value else null end) as ypy_pboc_a_fpd7_v1_prob_good 
#     from znzz_fintech_ads.lending_model01_scores_vars as t 
#     where lending_time >= '2024-08-01'
#       and lending_time <= '2025-02-22'
#       and variable_value is not null 
#     group by order_no
#     ) as t2 on t.order_no=t2.order_no 
 
# ------------------三方数据-----------------   
# left join 
#     (
#     select t.*
#     from znzz_fintech_ads.lxl_t_r30_three_score_data as t 
#     where dt >= '2024-08-01'
#       and dt <= '2025-02-22'
#     ) as t3 on t.order_no=t3.order_no
# ;
# """

# sql = """
# select * 
# from znzz_fintech_ads.lxl_tmp_tx_merge_250424 
# where apply_date>='2024-10-01' and channel_id != 1
# """

# df_sample_ = get_data(sql)


# In[6]:


sql = """
    select 
    order_no
    ,max(case when variable_code = 't_off_f30_2504' then good_score else null end) as t_off_f30_2504_g_p 
    from znzz_fintech_ads.lending_model01_scores_off  as t 
    where lending_time >= '2024-08-01'
      and lending_time <= '2025-02-22'
      and variable_code in ('t_off_f30_2504')
    group by order_no

"""
df_m1b = get_data(sql)
df_m1b.info(show_counts=True)


# In[7]:


df_jll = pd.read_csv('提现高成本融合_to_磊磊.csv')
df_jll.info(show_counts=True)


# In[8]:


df_sample_ = pd.merge(df_jll, df_m1b, how='left',on='order_no')
df_sample_.info(show_counts=True)


# In[9]:


df_sample_ = df_sample_.query("apply_date>='2024-10-01'").reset_index(drop=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[10]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[11]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[12]:


df_sample_.sort_values(by=['order_no', 'target_fpd30'], ascending=[True, False], inplace=True)
df_sample_.drop_duplicates(subset=['order_no'], keep='first', inplace=True)
df_sample_.shape


# In[13]:


varsname = df_sample_.columns.to_list()[11:]

print(varsname)
print("初始特征变量个数：",len(varsname))


# In[14]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[15]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[16]:


df_sample = df_sample_.query("target_fpd30>=0 ").reset_index(drop=True)


# In[17]:


df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[18]:


df_sample.info(show_counts=True)


# In[19]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[20]:


df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2025-01-31'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-02-01' & apply_date<='2025-02-22'").index, 'data_set']='3_oot2'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'


# In[21]:


df_sample.to_csv(result_path + '提现全渠道高成本子分融合模型fpd30标签2410_2412_v2_0428.csv',index=False)
print(result_path + '提现全渠道高成本子分融合模型fpd30标签2410_2412_v2_0428.csv')


# In[22]:


df_sample[df_sample[varsname].isna().all(axis=1)].shape


# In[ ]:


# df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index, inplace=True)


# In[23]:


target = 'target_fpd30'


# # 1. 样本概况

# In[24]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[25]:


print(df_sample[target].value_counts())


# In[26]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[27]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[28]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_v2_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_v2_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[29]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[30]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[31]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[32]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[33]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_v2_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_v2_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[34]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[35]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[36]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[37]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[38]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_v2_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_v2_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[39]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.90).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[40]:


to_drop_recent


# In[41]:


df_iv.loc[to_drop_iv,:]


# In[42]:


varsname_v1 = [col for col in varsname if col not in to_drop1] 
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[43]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.75, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[44]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[45]:


df_iv.loc[to_drop2,:]


# In[46]:


df_explor.loc[to_drop2,:]


# In[47]:



varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[48]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df



def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[49]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[59]:


df_sample.rename(columns={'t_off_f30_2504_g_p_y':'t_off_f30_2504_g_p'},inplace=True)


# In[60]:


varsname_v2.remove('t_off_f30_2504_g_p_y')
varsname_v2.append('t_off_f30_2504_g_p')


# In[61]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[62]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[63]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[64]:


new_bins_dict


# In[65]:


combiner.load(new_bins_dict)


# In[66]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[67]:


to_drop_mode


# In[68]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

print('---------')
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print('---------')


# In[69]:



df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[70]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[71]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[72]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[73]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[74]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
drop_by_psi = drop_by_psi_month + drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
drop_by_iv = drop_by_iv_month + drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[75]:


df_iv_by_set.loc[drop_by_iv,:]


# In[76]:


df_psi_by_set.loc[drop_by_psi,:]


# In[77]:


to_drop3 = []
print("剔除的变量有: ", len(to_drop3))


# In[78]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")
print(varsname_v3)


# ## 4.2 Y标签相关性删除

# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.75)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr.info()
df_high_corr.head()


# In[ ]:


print(to_drop4)


# In[ ]:


df_iv_by_set.loc[to_drop4,:]


# In[ ]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


varsname_v5 = [col for col in varsname_v4 if col not in to_drop5]
print(f"保留变量{len(varsname_v5)}个")
print(varsname_v5)


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[ ]:


gc.collect()


# # 5.模型训练

# ## 5.0 函数定义

# In[97]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[98]:


df_sample[target].value_counts()


# In[99]:


df_sample['target_fpd30_1'] = 1 - df_sample[target]
modeltrian_target = 'target_fpd30_1'


# In[100]:


df_sample[modeltrian_target].value_counts()


# In[101]:


df_sample['data_set'].value_counts()


# In[102]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[103]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[104]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[105]:


print("最优参数opt_params: ", opt_params)


# In[106]:


print(len(varsname_v5))
print(varsname_v5)


# In[107]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[127]:


varsname_base = ['t_off_f30_2504_g_p','t_dz_fpd','m1a0027_g_p','m1b0024_g_p','t_beha4_mob4','all_a_br_derived_fpd30_202408_g_p','baihang_28','hengpu_4','aliyun_5','t_dz_mob4','all_a_rh_fpd10_v1_p','hengpu_5','m1a0031_g_p','m1b0007_g_p','rong360_4','m1b0012_st_score','m1a0028_g_p','all_a_br_derived_v2_fpd30_202411_g_p','t_beha3_fpd','tianchuang_7']
varsname_base.remove('tianchuang_7')
varsname_base.remove('m1b0007_g_p')
print(len(varsname_base))
print(varsname_base)


# In[137]:


varsname_base


# In[138]:


# 确定数据集参数后，训练模型
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[139]:


df_sample['data_set'].value_counts()


# In[140]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[141]:


# 优化后评估模型效果
df_sample['y_prob_base_v6'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v6'].head()


# In[142]:


# 最初评估模型效果 
df_ks_auc_set_base = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v6', 'data_set')
df_ks_auc_set_base['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base = pd.concat([tmp, df_ks_auc_set_base], axis=1)
print(df_ks_auc_set_base)


# In[143]:


df_ks_auc_month_base = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v6', 'apply_month')
df_ks_auc_month_base.head()


# In[144]:


df_vars_list = pd.read_csv('df_vars_des.csv',encoding='gbk')
df_vars_list


# In[145]:


# 模型变量重要性
# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_base = feature_importance(lgb_model) 
df_importance_base = pd.merge(df_importance_base, tmp, how='left', left_index=True, right_index=True)
df_importance_base = df_importance_base.reset_index()
# df_importance_base = pd.merge(df_vars_list, df_importance_base, how='right',on='feature')
df_importance_base


# In[146]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_base = feature_importance(lgb_model) 
df_importance_set_base = pd.merge(df_importance_set_base, tmp, how='left', left_index=True, right_index=True)
df_importance_set_base = df_importance_set_base.reset_index()
df_importance_set_base = pd.merge(df_vars_list, df_importance_set_base, how='right',on='feature')
df_importance_set_base


# In[147]:


df_model_corr_base = df_corr_matrix.loc[varsname_base,varsname_base].reset_index()
df_model_corr_base.info()
df_model_corr_base.head()


# In[148]:


# 效果评估后保存模型 
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_base_v6_{timestamp}.xlsx') as writer:
    df_importance_set_base.to_excel(writer, sheet_name='df_importance_set_base')
    df_importance_base.to_excel(writer, sheet_name='df_importance_base')
    df_ks_auc_set_base.to_excel(writer, sheet_name='df_ks_auc_set_base')
    df_ks_auc_month_base.to_excel(writer, sheet_name='df_ks_auc_month_base')
    df_model_corr_base.to_excel(writer, sheet_name='df_model_corr_base')
print(result_path + f'4_模型训练_{task_name}_base_v6_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[149]:


df_sample.info(show_counts=True)


# In[150]:


sql = """
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-02-22'
      and dt>=''
"""
df_bj = get_data(sql)
df_bj.info(show_counts=True)


# ### 5.3.1数据处理

# In[152]:


usecols = ['order_no','t_free_f30_202504','t_free_m4d30_2504','t_low_m4d30_2504','t_low_f30_2504']
df_evalue = pd.merge(df_sample, df_bj[usecols], how='left', on='order_no')
df_evalue.info(show_counts=True)


# ### 5.3.2 效果对比

# In[153]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[154]:


model_score = ['y_prob_base_v6']
vars_score = ['t_low_f30_2504','t_low_m4d30_2504','t_free_f30_202504','t_free_m4d30_2504']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# #### 调用征信渠道

# In[155]:


tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue.info(show_counts=True)


# In[156]:


df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v6', 'apply_month')
df_ks_auc_month_pboc.head()


# In[157]:


df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v6', 'data_set')
df_ks_auc_set_pboc['渠道'] = '全渠道'
tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
df_ks_auc_set_pboc


# In[158]:


model_score = ['y_prob_base_v6']
vars_score = ['t_low_f30_2504','t_low_m4d30_2504','t_free_f30_202504','t_free_m4d30_2504']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2


# In[159]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_pboc_2_v2.to_excel(writer, sheet_name='df_ksauc_pboc_2_v2')
    df_ks_auc_month_pboc.to_excel(writer, sheet_name='df_ks_auc_month_pboc')
    df_ks_auc_set_pboc.to_excel(writer, sheet_name='df_ks_auc_set_pboc')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_v6_{timestamp}.xlsx')


# In[160]:


df_evalue.to_csv(result_path + r'提现全渠道高成本子分融合fpd30模型2410_2412_evalue.csv',index=False)


# # 6. 评分分布

# In[161]:


df_sample['apply_month'].value_counts()


# In[162]:


score = 'y_prob_base_v6'


# In[163]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[164]:


df_sample['score_bins'].head()


# In[165]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[166]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[167]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[168]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[169]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[ ]:







#==============================================================================
# File: 数据探索分析.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# # 原始数据表探索分析

# In[79]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# ## 不区分渠道，合并数据表

# In[3]:


# 获取167文件下的所有csv文件
file_dir = r'.\167'
file_name_167 = get_filename(file_dir)
file_name_167


# In[4]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_167):
    print('-----{}---{}------'.format(i, iterm))
    data = 'data_{}'.format(i)
    globals()[data] = pd.read_csv(r'.\167\{}'.format(iterm))   


# In[5]:


# 获取other文件下的所有csv文件
file_dir = r'.\other'
file_name_other = get_filename(file_dir)
file_name_other


# In[6]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_other):
    print('-----{}---{}------'.format(i, iterm))
    data1 = 'data_other_{}'.format(i)
    globals()[data1] = pd.read_csv(r'.\other\{}'.format(iterm))   


# In[52]:


# 合并数据
# df_auth = pd.concat([data_0, data_other_0], axis=0) # 授信数据
# del data_0, data_other_0,data_2, data_other_2,data_3, data_other_3,data_4, data_other_4,data_5
df_order = pd.concat([data_1, data_other_1], axis=0) # 提款数据
del data_1, data_other_1
# df_baihang_1 = pd.concat([data_2, data_other_2], axis=0) # 百行数据
# df_rong360_4 = pd.concat([data_3, data_other_3], axis=0) # 融360数据
# df_tengxun_1 = pd.concat([data_4, data_other_4], axis=0) # 腾讯数据
# df_xinyongsuanli_1 = data_5.copy()
# df_repay = pd.concat([data_6, data_other_5, data_other_6, data_other_7], axis=0) # 还款数据
# del data_6, data_other_5, data_other_6, data_other_7
gc.collect()


# In[87]:


print(df_auth.shape,df_order.shape,df_baihang_1.shape,df_rong360_4.shape,df_tengxun_1.shape,df_xinyongsuanli_1.shape,df_repay.shape)


# ## 提款表

# In[88]:


df_order_base = df_order.query("apply_date>='2022-05-01'")[['user_id','order_no','apply_date','order_status','loan_period','channel_id']]
df_order_base.info()
df_order_base.head()


# In[89]:


df_order_base['order_no'].nunique()


# In[90]:


df_order_base.shape


# ## 授信数据表

# In[88]:


# 授信数据
df_auth.info()


# In[89]:


df_auth.head()


# In[153]:


# 存储数据表和字段的质量
df_table = pd.DataFrame()
df_columns = pd.DataFrame()


# In[154]:


tmp1, tmp2 = data_view(df_auth, 'auth')

df_table = pd.concat([tmp1, df_table],axis=0)
df_columns = pd.concat([tmp2, df_columns],axis=0)


# In[156]:


df_table.to_excel(r'.\result\df_table.xlsx')
df_columns.to_excel(r'.\result\df_columns.xlsx')


# In[141]:


# 存储字段的分布情况
df_object = pd.DataFrame()
df_number = pd.DataFrame()
df_datetime = pd.DataFrame()


# In[142]:


# 离散型变量
for col in ['channel_id','channel_name','cert_type','auth_status','cust_type','close_reason']:
    tmp = object_explore(df_auth, col, 'auth')
    df_object = pd.concat([tmp, df_object],axis=0)
df_object.head()


# In[143]:


# 数值型变量
for col in ['auth_credit_amount']:
    tmp = numeric_explore(df_auth, col, 'auth')
    df_number = pd.concat([tmp, df_number],axis=0)
df_number.head()


# In[144]:


# 日期型变量
for col in ['apply_date', 'apply_time']:
    tmp = datetime_explore(df_auth, col, 'auth')
    df_datetime = pd.concat([tmp, df_datetime],axis=0)
df_datetime.head()


# In[145]:


# 存储数据
df_object.to_excel(r'.\result\df_object.xlsx')
df_number.to_excel(r'.\result\df_number.xlsx')
df_datetime.to_excel(r'.\result\df_datetime.xlsx')


# ## 三方数据匹配率

# ### 百行数据

# In[158]:


# 第三方数据
for i,j in zip([df_baihang_1, df_rong360_4, df_tengxun_1, df_xinyongsuanli_1],['baihang','rong360','tengxun','xinyongsuanli']):
    tmp1, tmp2 = data_view(i, j)
    df_table = pd.concat([tmp1, df_table],axis=0)
    df_columns = pd.concat([tmp2, df_columns],axis=0)


# In[159]:


df_table.to_excel(r'.\result\df_table.xlsx')
df_columns.to_excel(r'.\result\df_columns.xlsx')


# In[174]:


# 三方数据的覆盖率
third_part_recall = {}
recall = pd.DataFrame()

for i,j in zip([df_baihang_1, df_rong360_4, df_tengxun_1, df_xinyongsuanli_1],['baihang','rong360','tengxun','xinyongsuanli']):
    print('-----------------{}--------------'.format(j))
    tmp = df_auth[df_auth['order_no'].isin(list(i['order_no']))]
    third_part_recall[j] = str(tmp.shape[0]/df_auth.shape[0] * 100)[0:5] +'%'
    recall = pd.concat([recall, tmp['channel_id'].value_counts(dropna=False)], axis=0)
    recall = pd.concat([recall, tmp['auth_status'].value_counts(dropna=False)], axis=0)
    recall['table'] = j
    del tmp
    gc.collect()

third_part_recall


# In[175]:


# 查看三方数据覆盖率
third_part_recall = pd.DataFrame.from_dict(third_part_recall,orient='index')
third_part_recall.columns=['覆盖率']
third_part_recall.to_excel(r'.\result\三方数据覆盖率.xlsx')
third_part_recall


# In[177]:


recall.to_excel(r'.\result\不同维度的覆盖率.xlsx')


# # Y标签

# In[8]:


df_repay.info()
df_repay.head()


# In[9]:


# 格式转换
df_repay['lending_time'] = pd.to_datetime(df_repay['lending_time'], format='%Y-%m-%d')
df_repay['loan_time'] = pd.to_datetime(df_repay['loan_time'], format='%Y-%m-%d')
df_repay['pre_settle_time'] = pd.to_datetime(df_repay['pre_settle_time'], format='%Y-%m-%d')
df_repay['repay_date'] = pd.to_datetime(df_repay['repay_date'], format='%Y-%m-%d')
df_repay['repid_time'] = pd.to_datetime(df_repay['repid_time'], format='%Y-%m-%d')


# In[10]:


df_repay['order_status'].value_counts(dropna=False)


# In[11]:


df_repay['total_periods'].value_counts(dropna=False)


# In[14]:


df_repay['lending_time'].dt.strftime('%Y-%m').value_counts(dropna=False)


# In[15]:


df_repay['period'].value_counts(dropna=False)


# In[55]:


datetime.now()


# In[63]:


datetime(2022,5,1,0,0,0)


# In[56]:


df_repay_copy = df_repay.copy()


# In[59]:


df_repay_copy.shape


# In[64]:


df_repay = df_repay.query("lending_time>=datetime(2022,5,1,0,0,0)")
df_repay.shape


# In[65]:


df_repay['lending_time'].min()


# In[66]:


df_repay['is_dq'] = df_repay['repay_date'].apply(lambda x: 1 if x<=datetime.now() else 0)


# In[72]:


df_repay[df_repay['is_dq']==1][['period_settle_date','repay_date','lending_time','period','is_dq']].head(100)


# In[69]:


df_repay['period_settle_date'].fillna(datetime(2023,7,12),inplace=True)

df_repay['period_settle_date'].head()


# In[70]:


df_repay['period_settle_date'] = pd.to_datetime(df_repay['period_settle_date'],format='%Y-%m-%d')
df_repay['overdue_days'] = df_repay['period_settle_date'] - df_repay['repay_date']
df_repay['overdue_days'] = df_repay['overdue_days'].dt.days


# In[75]:


df_repay['Firs3ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['overdue_days'],df_repay['is_dq'])]
df_repay['Firs3ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['overdue_days'],df_repay['is_dq'])]
df_repay['Firs6ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['overdue_days'],df_repay['is_dq'])]
df_repay['Firs6ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['overdue_days'],df_repay['is_dq'])]


# In[77]:


df_repay[df_repay['is_dq']==0][['period_settle_date','repay_date','lending_time','period','is_dq','overdue_days','Firs3ever15']].head(100)


# In[91]:


# 借款金额、放款时间、借款利率
df_loan_amt = df_repay.groupby('order_no')['lending_time','loan_amount','loan_rate'].max()
df_loan_amt.head()


# In[92]:


df_loan_amt = df_loan_amt.reset_index()


# In[93]:


# 产生标签
df_Firs3 = df_repay.query("period<=3").groupby('order_no')['Firs3ever15','Firs3ever30'].max()
df_Firs3 = df_Firs3.reset_index()
df_Firs3.head()


# In[94]:


# 产生标签
df_Firs6 = df_repay.query("period<=6").groupby('order_no')['Firs6ever15','Firs6ever30'].max()
df_Firs6 = df_Firs6.reset_index()
df_Firs6.head()


# ## 形成大宽表

# In[95]:


df_kb = pd.merge(df_order_base,df_loan_amt,how='left',on='order_no')
df_kb = pd.merge(df_kb,df_Firs3,how='left',on='order_no')
df_kb = pd.merge(df_kb,df_Firs6,how='left',on='order_no')
df_kb.info(null_counts=True)
df_kb.head()


# In[96]:


df_kb['order_status'].value_counts(dropna=True)


# In[99]:


df_kb['apply_month'] = df_kb['apply_date'].str[0:7]
df_kb['apply_month'].head()


# In[108]:


xx0 = df_kb.query("order_status==6").groupby('channel_id')['order_no'].count()

xx1 = df_kb.query("order_status==6").groupby(by=['channel_id','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_kb.query("order_status==6").groupby(by=['channel_id','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_kb.query("order_status==6").groupby(by=['channel_id','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_kb.query("order_status==6").groupby(by=['channel_id','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_channel = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_channel


# In[109]:


xx0 = df_kb.query("order_status==6").groupby('apply_month')['order_no'].count()

xx1 = df_kb.query("order_status==6").groupby(by=['apply_month','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_kb.query("order_status==6").groupby(by=['apply_month','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_kb.query("order_status==6").groupby(by=['apply_month','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_kb.query("order_status==6").groupby(by=['apply_month','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_months = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_months


# In[111]:


df_channel.to_excel(r'.\result\df_channel.xlsx')
df_months.to_excel(r'.\result\df_months.xlsx')


# In[19]:


df_repay_3 = df_repay.query("period<=3")
df_repay_3.groupby('order_no')['Firs3ever15','Firs3ever30'].max()


# In[21]:


df_repay_3['period_settle_date'].fillna('2023-07-12',inplace=True)


# In[24]:


df_repay_3['period_settle_date'] = pd.to_datetime(df_repay_3['period_settle_date'],format='%Y-%m-%d')


# In[25]:


df_repay_3['overdue_days'] = df_repay_3['period_settle_date'] - df_repay_3['repay_date']
df_repay_3['overdue_days'] = df_repay_3['overdue_days'].dt.days


# In[26]:


df_repay_3[['overdue_days','overdue_day']].head()


# In[37]:


df_repay_3['is_fpd_15'] = df_repay_3['overdue_days'].apply(lambda x: 1 if x>15 else 0)
df_repay_3['is_fpd_15'].sum()


# In[38]:


df_repay_3['is_fpd_30'] = df_repay_3['overdue_days'].apply(lambda x: 1 if x>30 else 0)
df_repay_3['is_fpd_30'].sum()


# In[39]:


print(df_repay_3.groupby('order_no')['is_fpd_15'].max().value_counts(normalize=True))
print(df_repay_3.groupby('order_no')['is_fpd_30'].max().value_counts(normalize=True))


# In[40]:


df_period_3_fpd_15 = df_repay_3.groupby('user_id')['is_fpd_15'].max()
df_period_3_fpd_15 = df_period_3_fpd_15.reset_index()
df_period_3_fpd_15.head()


# In[41]:


df_period_3_fpd_30 = df_repay_3.groupby('user_id')['is_fpd_30'].max()
df_period_3_fpd_30 = df_period_3_fpd_30.reset_index()
df_period_3_fpd_30.head()


# In[42]:


print(pd.concat([df_period_3_fpd_15['is_fpd_15'].value_counts(normalize=True,dropna=False),df_period_3_fpd_15['is_fpd_15'].value_counts(dropna=False)],axis=1))
print(pd.concat([df_period_3_fpd_30['is_fpd_30'].value_counts(normalize=True,dropna=False),df_period_3_fpd_30['is_fpd_30'].value_counts(dropna=False)],axis=1))


# In[43]:


df_repay_6 = df_repay.query("lending_time>'2022-04-30' & period<=6")
df_repay_6.info(null_counts=True)


# In[44]:


df_repay_6['period_settle_date'].fillna('2023-07-12',inplace=True)
df_repay_6['period_settle_date'] = pd.to_datetime(df_repay_6['period_settle_date'],format='%Y-%m-%d')
df_repay_6['overdue_days'] = df_repay_6['period_settle_date'] - df_repay_6['repay_date']
df_repay_6['overdue_days'] = df_repay_6['overdue_days'].dt.days
df_repay_6[['overdue_days','overdue_day']].head()


# In[45]:


df_repay_6['is_fpd_15'] = df_repay_6['overdue_days'].apply(lambda x: 1 if x>15 else 0)
df_repay_6['is_fpd_15'].sum()


# In[47]:


df_repay_6['is_fpd_30'] = df_repay_6['overdue_days'].apply(lambda x: 1 if x>30 else 0)
df_repay_6['is_fpd_30'].sum()


# In[48]:


print(df_repay_6.groupby('order_no')['is_fpd_15'].max().value_counts(normalize=True))
print(df_repay_6.groupby('order_no')['is_fpd_30'].max().value_counts(normalize=True))


# In[49]:


df_period_6_fpd_15 = df_repay_6.groupby('user_id')['is_fpd_15'].max()
df_period_6_fpd_15 = df_period_6_fpd_15.reset_index()
df_period_6_fpd_15.head()


# In[50]:


df_period_6_fpd_30 = df_repay_6.groupby('user_id')['is_fpd_30'].max()
df_period_6_fpd_30 = df_period_6_fpd_30.reset_index()
df_period_6_fpd_30.head()


# In[51]:


print(pd.concat([df_period_6_fpd_15['is_fpd_15'].value_counts(normalize=True,dropna=False),df_period_6_fpd_15['is_fpd_15'].value_counts(dropna=False)],axis=1))
print(pd.concat([df_period_6_fpd_30['is_fpd_30'].value_counts(normalize=True,dropna=False),df_period_6_fpd_30['is_fpd_30'].value_counts(dropna=False)],axis=1))


# In[ ]:


df_period_6_fpd_30.to_csv(r'.\result\df_period_6_fpd_30.csv)




#==============================================================================
# File: 数据预处理.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 数据预处理

# 每个变量缺失率的计算
def missing_cal(df):
    """
    df :数据集
    
    return：每个变量的缺失率
    """
    missing_series = df.isnull().sum()/df.shape[0]
    missing_df = pd.DataFrame(missing_series).reset_index()
    missing_df = missing_df.rename(columns={'index':'col',
                                            0:'missing_pct'})
    missing_df = missing_df.sort_values('missing_pct',ascending=False).reset_index(drop=True)
    return missing_df

# 变量的缺失分布图
def plot_missing_var(df,plt_size=None):
    """
    df: 数据集
    plt_size :图纸的尺寸
    
    return: 缺失分布图（直方图形式)
    """
    missing_df = missing_cal(df)
    plt.figure(figsize=plt_size)
    plt.rcParams['font.sans-serif']=['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    x = missing_df['missing_pct']
    plt.hist(x=x,bins=np.arange(0,1.1,0.1),color='hotpink',ec='k',alpha=0.8)
    plt.ylabel('缺失值个数')
    plt.xlabel('缺失率')
    return plt.show()


# 单个样本的缺失分布
def plot_missing_user(df,plt_size=None):
    """
    df: 数据集
    plt_size: 图纸的尺寸
    
    return :缺失分布图（折线图形式）
    """
    missing_series = df.isnull().sum(axis=1)
    list_missing_num  = sorted(list(missing_series.values))
    plt.figure(figsize=plt_size)
    plt.rcParams['font.sans-serif']=['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    plt.plot(range(df.shape[0]),list_missing_num)
    plt.ylabel('缺失变量个数')
    plt.xlabel('sanples')
    return plt.show()


# 缺失值剔除（单个变量）
def missing_delete_var(df,threshold=None):
    """
    df:数据集
    threshold:缺失率删除的阈值
    
    return :删除缺失后的数据集
    """
    df2 = df.copy()
    missing_df = missing_cal(df)
    missing_col_num = missing_df[missing_df.missing_pct>=threshold].shape[0]
    missing_col = list(missing_df[missing_df.missing_pct>=threshold].col)
    df2 = df2.drop(missing_col,axis=1)
    print('缺失率超过{}的变量个数为{}'.format(threshold,missing_col_num))
    return df2


# 缺失值剔除（单个样本）
def missing_delete_user(df,threshold=None):
    """
    df:数据集
    threshold:缺失个数删除的阈值
    
    return :删除缺失后的数据集
    """
    df2 = df.copy()
    missing_series = df.isnull().sum(axis=1)
    missing_list = list(missing_series)
    missing_index_list = []
    for i,j in enumerate(missing_list):
        if j>=threshold:
            missing_index_list.append(i)
    df2 = df2[~(df2.index.isin(missing_index_list))]
    print('缺失变量个数在{}以上的用户数有{}个'.format(threshold,len(missing_index_list)))
    return df2


# 缺失值填充（类别型变量）
def fillna_cate_var(df,col_list,fill_type=None):
    """
    df:数据集
    col_list:变量list集合
    fill_type: 填充方式：众数/当做一个类别
    
    return :填充后的数据集
    """
    df2 = df.copy()
    for col in col_list:
        if fill_type=='class':
            df2[col] = df2[col].fillna('unknown')
        if fill_type=='mode':
            df2[col] = df2[col].fillna(df2[col].mode()[0])
    return df2


# 数值型变量的填充
# 针对缺失率在5%以下的变量用中位数填充
# 缺失率在5%--15%的变量用随机森林填充,可先对缺失率较低的变量先用中位数填充，在用没有缺失的样本来对变量作随机森林填充
# 缺失率超过15%的变量建议当做一个类别
def fillna_num_var(df,col_list,fill_type=None,filled_df=None):
    """
    df:数据集
    col_list:变量list集合
    fill_type:填充方式：中位数/随机森林/当做一个类别
    filled_df :已填充好的数据集，当填充方式为随机森林时 使用
    
    return:已填充好的数据集
    """
    df2 = df.copy()
    for col in col_list:
        if fill_type=='median':
            df2[col] = df2[col].fillna(df2[col].median())
        if fill_type=='class':
            df2[col] = df2[col].fillna(-999)
        if fill_type=='rf':
            rf_df = pd.concat([df2[col],filled_df],axis=1)
            known = rf_df[rf_df[col].notnull()]
            unknown = rf_df[rf_df[col].isnull()]
            x_train = known.drop([col],axis=1)
            y_train = known[col]
            x_pre = unknown.drop([col],axis=1)
            rf = RandomForestRegressor(random_state=0)
            rf.fit(x_train,y_train)
            y_pre = rf.predict(x_pre)
            df2.loc[df2[col].isnull(),col] = y_pre
    return df2


# 常变量/同值化处理
def const_delete(df,col_list,threshold=None):
    """
    df:数据集
    col_list:变量list集合
    threshold:同值化处理的阈值
    
    return :处理后的数据集
    """
    df2 = df.copy()
    const_col = []
    for col in col_list:
        const_pct = df2[col].value_counts().iloc[0]/df2[df2[col].notnull()].shape[0]
        if const_pct>=threshold:
            const_col.append(col)
    df2 = df2.drop(const_col,axis=1)
    print('常变量/同值化处理的变量个数为{}'.format(len(const_col)))
    return df2


# 分类型变量的降基处理
def descending_cate(df,col_list,threshold=None):
    """
    df: 数据集
    col_list:变量list集合
    threshold:降基处理的阈值
    
    return :处理后的数据集
    """
    df2 = df.copy()
    for col in col_list:
        value_series = df[col].value_counts()/df[df[col].notnull()].shape[0]
        small_value = []
        for value_name,value_pct in zip(value_series.index,value_series.values):
            if value_pct<=threshold:
                small_value.append(value_name)
        df2.loc[df2[col].isin(small_value),col]='other'
    return df2




#==============================================================================
# File: 模型建立.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
模型建立
@author: kantt
"""
#%%
import pandas as pd
import numpy as np
import toad
import os 
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
#%% 数据导入与处理
data_tmp1 = pd.read_csv(r'C:\Users\ruizhi\Desktop\kantt\1.base_data\valid_cust_first.csv') 

#列名筛选
col_list=data_tmp1.columns
drop_col=['Unnamed: 0.1','loan_amount_y','Unnamed: 0','order_no_y','channel_id_y','apply_date_y','apply_time','id_no_des_y','channel_id','apply_date_real']
data_tmp1.drop(columns=drop_col,inplace=True)

#剔除灰样本确认模型样本
data = data_tmp1[data_tmp1['Firs6ever30'].isin([0.,2.])]
#    Firs6ever30  user_id
# 0          0.0    86444
# 1          1.0    21734
# 2          2.0     8873

cal1=data.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()

print('数据大小：', data.shape) #(95317, 143)

data['target']=data['Firs6ever30']/2
#%%
#STEP1. - 数据集切分  训练集测试集
model = data[data.apply_month.isin(['2022-05','2022-06','2022-07','2022-08','2022-09'])]
oot = data[data.apply_month.isin(['2022-10','2022-11','2022-12'])]

allFeatures = list(model.columns.drop(['id_no_des_x','order_no_x','apply_date_x','order_status','loan_period','channel_id_x','loan_amount_x','lending_time','loan_rate',
'total_periods','Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30','apply_month','create_time',
'channel_name','cert_type','auth_status','auth_credit_amount','cust_type','close_reason','time_diff']))

X = model[allFeatures]
Y = model['target']

X_train, X_test, y_train, y_test = train_test_split(X ,Y, test_size=0.3, random_state=88, stratify=Y)

model.loc[X_train.index,'sample_set']='train'
model.loc[X_test.index,'sample_set']='test'

dt_train=model[model.sample_set=='train'] #34234
dt_test=model[model.sample_set=='test'] #14673

cal2=dt_train.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()
cal3=dt_test.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()
#%%
#STEP2. - 数据概况 --缺失率&分位数
var_cal_train=toad.detector.detect(dt_train[allFeatures])
var_cal_test=toad.detector.detect(dt_test[allFeatures])
var_cal_oot=toad.detector.detect(oot[allFeatures])
#数据概况 --iv
var_iv_train=toad.quality(dt_train[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_test=toad.quality(dt_test[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_oot=toad.quality(oot[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_train.sort_values('iv',ascending=False)
var_iv_test.sort_values('iv',ascending=False)
var_iv_oot.sort_values('iv',ascending=False)

train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.8, return_drop=True)
print(train_selected.shape)#(34234, 33)
col_target=train_selected.columns
col_target_key=['crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']
col_target_key_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_tengxun_b','model_score_01_x_tianchuang_b','model_score_01_xysl_1.0_b','model_score_01_xysl_3.0_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','model_score_01br_score_b','utl_b','model_score_01_rong360_b','model_score_01_fulin_b']
#%%

#STEP3. - 数据切分
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='chi', min_samples=None, n_bins=10, empty_separate=True) 
bins_result = c.export()
train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins['WEIGHT']=1

col_target_keep=['target','crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']

train_selected_adj=train_selected[col_target_keep]
train_selected_adj=train_selected[col_target_keep].fillna(-9999)
train_selected_adj=train_selected_adj.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
train_selected_adj['WEIGHT']=1
#%%
eps=0.0000001
def iv(data,columns):
    w=pd.DataFrame()
    for i in columns:
        a=pd.DataFrame()
        a['人数']=data['WEIGHT'].groupby(data[i]).sum()
        a['人数占比']=data['WEIGHT'].groupby(data[i]).sum()/data['WEIGHT'].sum()
        a['字段']=i
        a['分段']=data['WEIGHT'].groupby(data[i]).sum().index
        a['B数量']=data['target'].groupby(data[i]).sum()
        a['G数量']=a['人数']-a['B数量']
        a['bad_rate']=a['B数量']/a['人数']
        a['G占比']=a['G数量']/a['G数量'].sum()+eps
        a['B占比']=a['B数量']/a['B数量'].sum()+eps
        a['iv_tmp']=np.log(a['B占比']/a['G占比'])*(a['B占比']-a['G占比'])
        a['iv']=a['iv_tmp'].sum()
        a['woe']=np.log(a['G占比']/a['B占比'])
        w=w.append(a,ignore_index=True)
    return w
#%%
#手动分箱
# train_selected_adj['crd_loan_gap_b']=pd.qcut(train_selected_adj.crd_loan_gap,10,duplicates='drop')
# train_selected_adj['model_score_01_b']=pd.qcut(train_selected_adj.model_score_01,10,duplicates='drop')
# train_selected_adj['model_score_01_tengxun_b']=pd.qcut(train_selected_adj.model_score_01_tengxun,10,duplicates='drop')
# train_selected_adj['model_score_01_x_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_x_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_1.0_b']=pd.qcut(train_selected_adj['model_score_01_xysl_1.0'],10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_3.0_b']=pd.qcut(train_selected_adj['model_score_01_xysl_3.0'],10,duplicates='drop')
# train_selected_adj['model_score_01_y_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_y_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_zr_tdzx_b']=pd.qcut(train_selected_adj.model_score_01_zr_tdzx,10,duplicates='drop')
# train_selected_adj['model_score_01br_score_b']=pd.qcut(train_selected_adj.model_score_01br_score,10,duplicates='drop')
# train_selected_adj['model_score_01_rong360_b']=pd.qcut(train_selected_adj.model_score_01_rong360,10,duplicates='drop')
# train_selected_adj['model_score_01_fulin_b']=pd.qcut(train_selected_adj.model_score_01_fulin,10,duplicates='drop')
# train_selected_adj['utl_b']=pd.qcut(train_selected_adj.utl,10,duplicates='drop')

bin_train=pd.DataFrame()
bin_train=iv(train_selected_adj,col_target_key_adj)
#%%
train_selected_adj['crd_loan_gap_b']=pd.cut(train_selected_adj.crd_loan_gap,[-10000,0,7,np.inf])
train_selected_adj['model_score_01_b']=pd.cut(train_selected_adj.model_score_01,[-10000,0,690,720,740,750,850])
train_selected_adj['model_score_01_x_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_x_tianchuang,[-10000,0,575,645,850])
train_selected_adj['model_score_01_xysl_3_b']=pd.cut(train_selected_adj['model_score_01_xysl_3'],[-10000,0,580,600,620,645,670,700,np.inf])
train_selected_adj['model_score_01_y_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_y_tianchuang,[-10000,0,580,850])
train_selected_adj['model_score_01_zr_tdzx_b']=pd.cut(train_selected_adj.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
train_selected_adj['model_score_01br_score_b']=pd.cut(train_selected_adj.model_score_01br_score,[-10000,0,500,545,np.inf])
train_selected_adj['utl_b']=pd.cut(train_selected_adj.utl,[-10000,0.5,0.9,np.inf])
use_col_keep_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','model_score_01br_score_b','utl_b']

bin_train=iv(train_selected_adj,use_col_keep_adj)
#%%
adj_bins={'crd_loan_gap': [-10000,0,7,np.inf],
 'model_score_01': [-10000,0,690,720,740,750,850],
 'model_score_01_x_tianchuang': [-10000,0,575,645,850],
 'model_score_01_xysl_3.0': [-10000,0,580,600,620,645,670,700,np.inf],
 'model_score_01_y_tianchuang': [-10000,0,580,850],
 'model_score_01_zr_tdzx': [-10000,0,660,700,750,800,900],
 'model_score_01br_score': [-10000,0,500,545,np.inf],
 'utl':[-10000,0.5,0.9,np.inf]}
use_col_keep=['user_id','target','crd_loan_gap','model_score_01','model_score_01_x_tianchuang','model_score_01_xysl_3.0',
              'model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl']

# 更新分箱
c.update(adj_bins)
test_selected=dt_test[use_col_keep]
oot_selected=oot[use_col_keep]
test_selected=test_selected[use_col_keep].fillna(-9999)
oot_selected=oot_selected[use_col_keep].fillna(-9999)
test_selected=test_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
oot_selected=oot_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
#test_selected_bins = c.transform(test_selected, labels=True)
#oot_selected_bins = c.transform(oot_selected, labels=True)
test_selected['crd_loan_gap_b']=pd.cut(test_selected.crd_loan_gap,[-10000,0,7,np.inf])
test_selected['model_score_01_b']=pd.cut(test_selected.model_score_01,[-10000,0,690,720,740,750,850])
test_selected['model_score_01_x_tianchuang_b']=pd.cut(test_selected.model_score_01_x_tianchuang,[-10000,0,575,645,850])
test_selected['model_score_01_xysl_3_b']=pd.cut(test_selected['model_score_01_xysl_3'],[-10000,0,580,600,620,645,670,700,np.inf])
test_selected['model_score_01_y_tianchuang_b']=pd.cut(test_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
test_selected['model_score_01_zr_tdzx_b']=pd.cut(test_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
test_selected['model_score_01br_score_b']=pd.cut(test_selected.model_score_01br_score,[-10000,0,500,545,np.inf])
test_selected['utl_b']=pd.cut(test_selected.utl,[-10000,0.5,0.9,np.inf])

oot_selected['crd_loan_gap_b']=pd.cut(oot_selected.crd_loan_gap,[-10000,0,7,np.inf])
oot_selected['model_score_01_b']=pd.cut(oot_selected.model_score_01,[-10000,0,690,720,740,750,850])
oot_selected['model_score_01_x_tianchuang_b']=pd.cut(oot_selected.model_score_01_x_tianchuang,[-10000,0,575,645,850])
oot_selected['model_score_01_xysl_3_b']=pd.cut(oot_selected['model_score_01_xysl_3'],[-10000,0,580,600,620,645,670,700,np.inf])
oot_selected['model_score_01_y_tianchuang_b']=pd.cut(oot_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
oot_selected['model_score_01_zr_tdzx_b']=pd.cut(oot_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
oot_selected['model_score_01br_score_b']=pd.cut(oot_selected.model_score_01br_score,[-10000,0,500,545,np.inf])
oot_selected['utl_b']=pd.cut(oot_selected.utl,[-10000,0.5,0.9,np.inf])

test_selected['WEIGHT']=1
oot_selected['WEIGHT']=1
bin_test=pd.DataFrame()
bin_oot=pd.DataFrame()
bin_test=iv(test_selected,use_col_keep_adj)
bin_oot=iv(oot_selected,use_col_keep_adj)
#%%
#WOE转化
train_bins=train_selected_adj[['target','crd_loan_gap_b','model_score_01_b', 'model_score_01_x_tianchuang_b',
       'model_score_01_xysl_3_b', 'model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b', 'model_score_01br_score_b', 'utl_b']]
transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_bins), train_bins['target'], exclude=['target'])
#相关性检验
corr_train=train_woe.loc[:,use_col_keep_adj].corr()
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.formula.api as smf
import statsmodels.api as sm
#train_woe.rename(columns={"model_score_01_xysl_3.0":'model_score_01_xysl_3',"model_score_01_xysl_3.0_b":'model_score_01_xysl_3_b'},inplace=True)
#model_names_all=['model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01br_score_b','utl_b']
model_name1=['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_y_tianchuang_b','model_score_01_xysl_3_b','utl_b']

#model_names_shortest=['model_score_01_y_tianchuang_b','crd_loan_gap_b','model_score_01_b','model_score_01_xysl_3_b','utl_b']

formula="{} ~ {} +1".format('target','+'.join(model_name1))
formula

model_1=smf.glm(formula,train_woe[model_name1+['target']],family=sm.families.Binomial(sm.families.links.logit())).fit()

print(model_1.summary())
train_selected_adj['lr_prob']=model_1.predict(train_woe[model_name1])
print('train_KS:',toad.metrics.KS(train_selected_adj['lr_prob'],train_selected_adj['target']))
print('train_AUC:',toad.metrics.AUC(train_selected_adj['lr_prob'],train_selected_adj['target']))
#%%
# import matplotlib.pyplot as plt
# import matplotlib
# from sklearn import metrics
# from sklearn.metrics import roc_curve

# def plot_roc(y_label,y_pred):
#     tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred)
#     fig=plt.figure(figsize=(6,4))
#     ax=fig.add_subplot(1,1,1)
#     ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC)
#     ax.plot([0,1],[0,1],'r--')
#     ax.set_xlim(0,1)
#     ax.set_ylim(0,1)
#     ax.set_title('ROC')
#     ax.legend(loc='best')
#     return plt.show(ax)

# from sklearn.metrics import roc_auc_score,roc_curve
# y_pred=model_1.predict(train_woe[model_name1])

# #计算AUC值
# AUC=roc_auc_score(train_woe.target,y_pred)
# print('AUC:',AUC)

# #ROC曲线
# plot_roc(train_woe.target,y_pred)
# fpr,tpr,thresholds_train=roc_curve(train_woe.target,y_pred)
# KS=np.max(tpr-fpr)
# print('KS:',KS)
#%%    
#VIF检验
import warnings
warnings.filterwarnings('ignore')

vif=pd.DataFrame()
X = np.matrix(train_woe[model_name1])
vif['features']=model_name1
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
#%%
#模型验证
#PART1.测试集
test_selected1 = test_selected[list(train_woe.columns)]
test_woe = transer.transform(c.transform(test_selected1))
#PART2.验证集
oot_selected1 = oot_selected[list(train_woe.columns)]
oot_woe = transer.transform(c.transform(oot_selected1))


test_selected['lr_prob']=model_1.predict(test_woe[model_name1])
print('test_KS:',toad.metrics.KS(test_selected['lr_prob'],test_selected['target']))
print('test_AUC:',toad.metrics.AUC(test_selected['lr_prob'],test_selected['target']))
oot_selected['lr_prob']=model_1.predict(oot_woe[model_name1])
print('oot_KS:',toad.metrics.KS(oot_selected['lr_prob'],oot_selected['target']))
print('oot_AUC:',toad.metrics.AUC(oot_selected['lr_prob'],oot_selected['target']))
#%%

final_data = toad.selection.stepwise(train_woe[['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','utl_b','target']], target='target', estimator='ols', direction='both', criterion='aic', exclude=None)
final_oot = oot_woe[list(final_data.columns)]

cols = list(final_data.drop(['target'], axis=1).columns)
cols
print(toad.metrics.PSI(final_data[cols], final_oot[cols]))

# PSI
print(toad.metrics.PSI(train_selected_adj['lr_prob'], test_selected['lr_prob']))
#0.13370898870172565
print(toad.metrics.PSI(train_selected_adj['lr_prob'], oot_selected['lr_prob']))
#2.056865741841309
#%%
#XGB算法
train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.7, return_drop=True)
print(train_selected.shape)#(34234, 33)
train_selected = train_selected.drop(['model_score_01br_score'],axis=1)

test_selected=dt_test[train_selected.columns]
oot_selected=oot[train_selected.columns]
test_selected=test_selected.fillna(-9999)
oot_selected=oot_selected.fillna(-9999)
train_selected=train_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
test_selected=test_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
oot_selected=oot_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
#%%
xgb_model = xgb.XGBClassifier(learning_rate=0.05,
                              n_estimators = 200,
                              objective = "binary:logistic",
                              max_depth = 2,
                              n_jobs = -1,
                              min_child_weight = 3,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
                              )



# 对训练集进行预测
cols=['model_score_01','model_score_01_rong360','model_score_01_tengxun', 'model_score_01_xysl_1',
       'model_score_01_xysl_3', 'model_score_01_fulin','model_score_01_zr_tdzx', 'model_score_01_x_tianchuang',
       'model_score_01_y_tianchuang', 'crd_loan_gap', 'utl']
xgb_model.fit(train_selected[cols], train_selected['target'])
y_pred = xgb_model.predict_proba(train_selected[cols])[:,1]
fpr, tpr, thresholds = metrics.roc_curve(train_selected['target'], y_pred, pos_label=1)
roc_auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train KS: ', ks)
print('train AUC: ', roc_auc)
# train KS:  0.20861125536256614
# train AUC:  0.6434641313265247

# 对测试集进行预测
y_pred_test = xgb_model.predict_proba(test_selected[cols])[:,1]
fpr_test, tpr_test, thresholds_oot = metrics.roc_curve(test_selected['target'], y_pred_test, pos_label=1)
roc_auc_test = metrics.auc(fpr_test, tpr_test)
ks_test = max(tpr_test-fpr_test)
print('test KS: ', ks_test)
print('test AUC: ', roc_auc_test)
# oot KS:  0.14625199176125941
# oot AUC:  0.5794815487660571
y_pred_oot = xgb_model.predict_proba(oot_selected[cols])[:,1]
fpr_oot, tpr_oot, thresholds_oot = metrics.roc_curve(oot_selected['target'], y_pred_oot, pos_label=1)
roc_auc_oot = metrics.auc(fpr_oot, tpr_oot)
ks_oot = max(tpr_oot-fpr_oot)
print('oot KS: ', ks_oot)
print('oot AUC: ', roc_auc_oot)
# oot KS:  0.07385367336276216
# oot AUC:  0.5464396673897726
#%%
plt.plot(fpr, tpr, color='darkorange', lw=2, label='trian ROC curve (area = %0.2f)' % roc_auc)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='test ROC curve (area = %0.2f)' % roc_auc_oot)
plt.plot(fpr_oot, tpr_oot, color='red', lw=2, label='oot ROC curve (area = %0.2f)' % roc_auc_oot)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="best")
plt.show()
#%%
from sklearn.model_selection import GridSearchCV
param_test1 = {'max_depth':[2,3,4,5,6],'n_estimators':[100, 200, 300, 400, 500, 600]}

gsearch = GridSearchCV(
    estimator = xgb_model,
    param_grid=param_test1, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch.fit(train_selected[cols], train_selected['target'])

print('gsearch1.best_params_', gsearch.best_params_)
print('gsearch1.best_score_', gsearch.best_score_)
# gsearch1.best_params_ {'max_depth': 2, 'n_estimators': 200}
# gsearch1.best_score_ 0.599996281671739
#%%
xgb_model_2 = xgb.XGBClassifier(learning_rate=0.05,
                              n_estimators = 200,
                              objective = "binary:logistic",
                              max_depth = 2,
                              n_jobs = -1,
                              min_child_weight = 1,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
)

param_test2 = {'learning_rate':[i/20.0 for i in range(1,20)]}
gsearch2 = GridSearchCV(
    estimator = xgb_model_2,
    param_grid=param_test2, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch2.fit(train_selected[cols], train_selected['target'])

print('gsearch2.best_params_', gsearch2.best_params_)
print('gsearch2.best_score_', gsearch2.best_score_)
# gsearch2.best_params_ {'learning_rate': 0.05}
# gsearch2.best_score_ 0.599996281671739
#%%
xgb_model_3 = xgb.XGBClassifier(learning_rate=0.05,
                              n_estimators = 200,
                              objective = "binary:logistic",
                              max_depth = 2,
                              n_jobs = -1,
                              min_child_weight = 1,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
)

param_test3 = {'min_child_weight':[i for i in range(1,6,1)]}

gsearch3 = GridSearchCV(
    estimator = xgb_model_3,
    param_grid=param_test3, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch3.fit(train_selected[cols], train_selected['target'])

print('gsearch3.best_params_', gsearch3.best_params_)
print('gsearch3.best_score_', gsearch3.best_score_)

# gsearch3.best_params_ {'min_child_weight': 3}
# gsearch3.best_score_ 0.600126052707221
#%%



#==============================================================================
# File: 模型建立2.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
模型建立
@author: kantt
"""
#%%
import pandas as pd
import numpy as np
import toad
import os 
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
#%% 数据导入与处理
data_tmp1 = pd.read_csv(r'C:\Users\ruizhi\Desktop\kantt\1.base_data\valid_cust_first.csv') 

#列名筛选
col_list=data_tmp1.columns
drop_col=['Unnamed: 0.1','loan_amount_y','Unnamed: 0','order_no_y','channel_id_y','apply_date_y','apply_time','id_no_des_y','channel_id','apply_date_real']
data_tmp1.drop(columns=drop_col,inplace=True)

#剔除灰样本确认模型样本
data = data_tmp1[data_tmp1['Firs6ever30'].isin([0.,2.])]
#    channel_id_x  Firs6ever30  user_id
# 0           167          0.0    61704
# 1           167          2.0     5994
# 2           174          0.0    24740
# 3           174          2.0     2879

cal1=data.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()

print('数据大小：', data.shape) #(95317, 143)

data['target']=data['Firs6ever30']/2
#%%
#STEP1. - 数据集切分  训练集测试集
model = data
#[data.apply_month.isin(['2022-05','2022-06','2022-07','2022-08','2022-09'])]
#oot = data[data.apply_month.isin(['2022-10','2022-11','2022-12'])]

allFeatures = list(model.columns.drop(['id_no_des_x','order_no_x','apply_date_x','order_status','loan_period','channel_id_x','loan_amount_x','lending_time','loan_rate',
'total_periods','Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30','apply_month','create_time',
'channel_name','cert_type','auth_status','auth_credit_amount','cust_type','close_reason','time_diff']))

X = model[allFeatures]
Y = model['target']

X_train, X_test, y_train, y_test = train_test_split(X ,Y, test_size=0.3, random_state=88, stratify=Y)

model.loc[X_train.index,'sample_set']='train'
model.loc[X_test.index,'sample_set']='test'

dt_train=model[model.sample_set=='train'] #34234
dt_test=model[model.sample_set=='test'] #14673

cal2=dt_train.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()
cal3=dt_test.groupby(['apply_month','Firs6ever30'])['user_id'].count().reset_index()
#%%
#STEP2. - 数据概况 --缺失率&分位数
var_cal_train=toad.detector.detect(dt_train[allFeatures])
var_cal_test=toad.detector.detect(dt_test[allFeatures])
#数据概况 --iv
var_iv_train=toad.quality(dt_train[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_test=toad.quality(dt_test[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)

var_iv_train.sort_values('iv',ascending=False)
var_iv_test.sort_values('iv',ascending=False)

train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.7, return_drop=True)
print(train_selected.shape)#(34234, 33)
col_target=train_selected.columns
col_target_key=['crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']
col_target_key_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_tengxun_b','model_score_01_x_tianchuang_b','model_score_01_xysl_1.0_b','model_score_01_xysl_3.0_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','model_score_01br_score_b','utl_b','model_score_01_rong360_b','model_score_01_fulin_b']
#%%

#STEP3. - 数据核验
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='chi', min_samples=None, n_bins=10, empty_separate=True) 
bins_result = c.export()
train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins['WEIGHT']=1

col_target_keep=['target','crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']

train_selected_adj=train_selected[col_target_keep]
train_selected_adj=train_selected[col_target_keep].fillna(-9999)
train_selected_adj=train_selected_adj.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
train_selected_adj['WEIGHT']=1
#%%
eps=0.0000001
def iv(data,columns):
    w=pd.DataFrame()
    for i in columns:
        a=pd.DataFrame()
        a['人数']=data['WEIGHT'].groupby(data[i]).sum()
        a['人数占比']=data['WEIGHT'].groupby(data[i]).sum()/data['WEIGHT'].sum()
        a['字段']=i
        a['分段']=data['WEIGHT'].groupby(data[i]).sum().index
        a['B数量']=data['target'].groupby(data[i]).sum()
        a['G数量']=a['人数']-a['B数量']
        a['bad_rate']=a['B数量']/a['人数']
        a['G占比']=a['G数量']/a['G数量'].sum()+eps
        a['B占比']=a['B数量']/a['B数量'].sum()+eps
        a['iv_tmp']=np.log(a['B占比']/a['G占比'])*(a['B占比']-a['G占比'])
        a['iv']=a['iv_tmp'].sum()
        a['woe']=np.log(a['B占比']/a['G占比'])
        w=w.append(a,ignore_index=True)
    return w
#%%
#手动分箱
# train_selected_adj['crd_loan_gap_b']=pd.qcut(train_selected_adj.crd_loan_gap,10,duplicates='drop')
# train_selected_adj['model_score_01_b']=pd.qcut(train_selected_adj.model_score_01,10,duplicates='drop')
# train_selected_adj['model_score_01_tengxun_b']=pd.qcut(train_selected_adj.model_score_01_tengxun,10,duplicates='drop')
# train_selected_adj['model_score_01_x_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_x_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_1.0_b']=pd.qcut(train_selected_adj['model_score_01_xysl_1.0'],10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_3.0_b']=pd.qcut(train_selected_adj['model_score_01_xysl_3.0'],10,duplicates='drop')
# train_selected_adj['model_score_01_y_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_y_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_zr_tdzx_b']=pd.qcut(train_selected_adj.model_score_01_zr_tdzx,10,duplicates='drop')
# train_selected_adj['model_score_01br_score_b']=pd.qcut(train_selected_adj.model_score_01br_score,10,duplicates='drop')
# train_selected_adj['model_score_01_rong360_b']=pd.qcut(train_selected_adj.model_score_01_rong360,10,duplicates='drop')
# train_selected_adj['model_score_01_fulin_b']=pd.qcut(train_selected_adj.model_score_01_fulin,10,duplicates='drop')
# train_selected_adj['utl_b']=pd.qcut(train_selected_adj.utl,10,duplicates='drop')
# bin_train=iv(train_selected_adj,['model_score_01_tengxun_b'])
# bin_train=pd.DataFrame()
# bin_train=iv(train_selected_adj,col_target_key_adj)
#%%
train_selected_adj['crd_loan_gap_b']=pd.cut(train_selected_adj.crd_loan_gap,[-10000,0,7,np.inf])
train_selected_adj['model_score_01_b']=pd.cut(train_selected_adj.model_score_01,[-10000,0,690,720,740,750,850])
train_selected_adj['model_score_01_x_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_x_tianchuang,[-10000,0,575,645,850])
train_selected_adj['model_score_01_xysl_3_b']=pd.cut(train_selected_adj['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
train_selected_adj['model_score_01_y_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_y_tianchuang,[-10000,0,580,850])
train_selected_adj['model_score_01_zr_tdzx_b']=pd.cut(train_selected_adj.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
#train_selected_adj['model_score_01br_score_b']=pd.cut(train_selected_adj.model_score_01br_score,[-10000,0,500,545,np.inf])
train_selected_adj['utl_b']=pd.cut(train_selected_adj.utl,[-10000,0.5,0.9,np.inf])


use_col_keep_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','utl_b']

bin_train=iv(train_selected_adj,use_col_keep_adj)
#%%
adj_bins={'crd_loan_gap': [-10000,0,7,np.inf],
 'model_score_01': [-10000,0,690,720,740,750,850],
 'model_score_01_x_tianchuang': [-10000,0,575,645,850],
 'model_score_01_xysl_3.0': [-10000,0,580,600,620,645,670,700,np.inf],
 'model_score_01_y_tianchuang': [-10000,0,580,850],
 'model_score_01_zr_tdzx': [-10000,0,660,700,750,800,900],
# 'model_score_01br_score': [-10000,0,500,545,np.inf],
 'utl':[-10000,0.5,0.9,np.inf]}
use_col_keep=['user_id','target','crd_loan_gap','model_score_01','model_score_01_x_tianchuang','model_score_01_xysl_3.0',
              'model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl']

# 更新分箱
c.update(adj_bins)
test_selected=dt_test[use_col_keep]
test_selected=test_selected[use_col_keep].fillna(-9999)
test_selected=test_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})

#test_selected_bins = c.transform(test_selected, labels=True)
test_selected['crd_loan_gap_b']=pd.cut(test_selected.crd_loan_gap,[-10000,0,7,np.inf])
test_selected['model_score_01_b']=pd.cut(test_selected.model_score_01,[-10000,0,690,720,740,750,850])
test_selected['model_score_01_x_tianchuang_b']=pd.cut(test_selected.model_score_01_x_tianchuang,[-10000,0,575,645,850])
test_selected['model_score_01_xysl_3_b']=pd.cut(test_selected['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
test_selected['model_score_01_y_tianchuang_b']=pd.cut(test_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
test_selected['model_score_01_zr_tdzx_b']=pd.cut(test_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
#test_selected['model_score_01br_score_b']=pd.cut(test_selected.model_score_01br_score,[-10000,0,500,545,np.inf])
test_selected['utl_b']=pd.cut(test_selected.utl,[-10000,0.5,0.9,np.inf])

test_selected['WEIGHT']=1

bin_test=pd.DataFrame()

bin_test=iv(test_selected,use_col_keep_adj)

#%%
#WOE转化
train_bins=train_selected_adj[['target','crd_loan_gap_b','model_score_01_b', 'model_score_01_x_tianchuang_b',
       'model_score_01_xysl_3_b', 'model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b', 'utl_b']]
transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_bins), train_bins['target'], exclude=['target'])
#相关性检验
corr_train=train_woe.loc[:,use_col_keep_adj].corr()
#%% glm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.formula.api as smf
import statsmodels.api as sm
#train_woe.rename(columns={"model_score_01_xysl_3.0":'model_score_01_xysl_3',"model_score_01_xysl_3.0_b":'model_score_01_xysl_3_b'},inplace=True)
#model_names_all=['model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01br_score_b','utl_b']
model_name1=['crd_loan_gap_b','utl_b','model_score_01_zr_tdzx_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_y_tianchuang_b','model_score_01_xysl_3_b']

#model_names_shortest=['model_score_01_y_tianchuang_b','crd_loan_gap_b','model_score_01_b','model_score_01_xysl_3_b','utl_b']

formula="{} ~ {} +1".format('target','+'.join(model_name1))
formula

model_1=smf.glm(formula,train_woe[model_name1+['target']],family=sm.families.Binomial(sm.families.links.logit())).fit()

print(model_1.summary())
train_selected_adj['lr_prob']=model_1.predict(train_woe[model_name1])
print('train_KS:',toad.metrics.KS(train_selected_adj['lr_prob'],train_selected_adj['target']))
print('train_AUC:',toad.metrics.AUC(train_selected_adj['lr_prob'],train_selected_adj['target']))
# train_KS: 0.1766251686511271
# train_AUC: 0.6182322075804915
#%%
#VIF检验
import warnings
warnings.filterwarnings('ignore')

vif=pd.DataFrame()
X = np.matrix(train_woe[model_name1])
vif['features']=model_name1
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
#%%
#模型验证
#PART1.测试集
test_selected1 = test_selected[list(train_woe.columns)]
test_woe = transer.transform(c.transform(test_selected1))

test_selected['lr_prob']=model_1.predict(test_woe[model_name1])
print('test_KS:',toad.metrics.KS(test_selected['lr_prob'],test_selected['target']))
print('test_AUC:',toad.metrics.AUC(test_selected['lr_prob'],test_selected['target']))
# test_KS: 0.15758157287321917
# test_AUC: 0.6098124337703574
#%%
import matplotlib.pyplot as plt
import matplotlib
from sklearn import metrics
from sklearn.metrics import roc_curve

def plot_roc(y_label,y_pred):
    tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred)
    fig=plt.figure(figsize=(6,4))
    ax=fig.add_subplot(1,1,1)
    ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC)
    ax.plot([0,1],[0,1],'r--')
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    ax.set_title('ROC')
    ax.legend(loc='best')
    return plt.show(ax)

from sklearn.metrics import roc_auc_score,roc_curve
y_pred=model_1.predict(train_woe[model_name1])

#计算AUC值
AUC=roc_auc_score(train_woe.target,y_pred)
print('AUC:',AUC)

#ROC曲线
plot_roc(train_woe.target,y_pred)
fpr,tpr,thresholds_train=roc_curve(train_woe.target,y_pred)
KS=np.max(tpr-fpr)
print('KS:',KS)

#%% lr
final_data = toad.selection.stepwise(train_woe[['crd_loan_gap_b','utl_b','model_score_01_zr_tdzx_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_y_tianchuang_b','model_score_01_xysl_3_b','target']], target='target', estimator='ols', direction='both', criterion='aic', exclude=None)
final_test = test_woe[list(final_data.columns)]

cols = list(final_data.drop(['target'], axis=1).columns)
cols
print(toad.metrics.PSI(final_data[cols], final_test[cols]))
# crd_loan_gap_b                   0.000059
# utl_b                            0.000075
# model_score_01_zr_tdzx_b         0.000425
# model_score_01_b                 0.000122
# model_score_01_x_tianchuang_b    0.000129
# model_score_01_y_tianchuang_b    0.000070
# model_score_01_xysl_3_b          0.000121

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(final_data[cols], final_data['target'])

lr.coef_
#[[0.7069668 , 0.905133  , 0.46710861, 0.49909476, 0.623925  ,0.50154326, 0.84624351]]
# 预测训练/oot
pred_train = lr.predict_proba(final_data[cols])[:,1]
pred_test = lr.predict_proba(final_test[cols])[:,1]

# KS/AUC
from toad.metrics import KS,AUC

print('train KS: ', KS(pred_train, final_data['target']))
print('train AUC: ', AUC(pred_train, final_data['target']))
# train KS:  0.1769792245971497
# train AUC:  0.6182361029834929
print('-------------oot结果--------------------')
print('oot KS: ', KS(pred_test, final_test['target']))
print('oot AUC: ', AUC(pred_test, final_test['target']))
# oot KS:  0.15827445465362944
# oot AUC:  0.6097474969258205
# PSI
#print(toad.metrics.PSI(pred_train, pred_test))
#0.22801054821462285
#%%
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from sklearn.metrics import roc_curve
from matplotlib import pyplot as plt
fpr, tpr, thresholds = roc_curve( final_data['target'],pred_train)
roc_auc = auc(fpr, tpr)
ks = abs(fpr - tpr).max()

fpr_test, tpr_test, thresholds_test = roc_curve( final_test['target'],pred_test)
roc_auc_test = auc(fpr_test, tpr_test)
ks_test = abs(fpr_test - tpr_test).max()

print(roc_auc, ks, roc_auc_test, ks_test)

plt.plot(fpr, tpr, color='darkorange', lw=2, label='trian ROC curve (area = %0.2f)' % roc_auc)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='test ROC curve (area = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="best")
plt.show()

#%%#评分转换





#%%
#XGB算法

from matplotlib import pyplot as plt
import toad
import xgboost as xgb
from xgboost import plot_importance
from sklearn import metrics
from sklearn.model_selection import train_test_split
from datetime import datetime
import os 
import warnings
#%%
train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.7, return_drop=True)
print(train_selected.shape)#(34234, 33)
train_selected = train_selected.drop(['model_score_01br_score','value_015_bairong', 'value_021_bairong', 'value_024_bairong',
       'value_044_bairong', 'value_069_bairong', 'value_072_bairong',
       'value_078_bairong', 'value_084_bairong', 'value_086_bairong',
       'value_087_bairong', 'value_088_bairong', 'value_092_bairong',
       'value_093_bairong', 'value_094_bairong', 'value_097_bairong' ],axis=1)
train_selected=train_selected.fillna(-9999)

test_selected=dt_test[train_selected.columns]
test_selected=test_selected.fillna(-9999)

train_selected=train_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
test_selected=test_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
#%%
xgb_model = xgb.XGBClassifier(learning_rate=0.1,
                              n_estimators = 600,
                              objective = "binary:logistic",
                              max_depth = 2,
                              n_jobs = -1,
                              min_child_weight = 1,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
                              )

# 对训练集进行预测
cols=['model_score_01','model_score_01_rong360','model_score_01_tengxun', 'model_score_01_xysl_1',
        'model_score_01_xysl_3', 'model_score_01_fulin','model_score_01_zr_tdzx', 'model_score_01_x_tianchuang',
        'model_score_01_y_tianchuang', 'crd_loan_gap', 'utl']

xgb_model.fit(train_selected[cols], train_selected['target'])
y_pred = xgb_model.predict_proba(train_selected[cols])[:,1]
fpr, tpr, thresholds = metrics.roc_curve(train_selected['target'], y_pred, pos_label=1)
roc_auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train KS: ', ks)
print('train AUC: ', roc_auc)
# train KS:  0.22254443998938767
# train AUC:  0.6530843556171937
#%%
# 对测试集进行预测
y_pred_test = xgb_model.predict_proba(test_selected[cols])[:,1]
fpr_test, tpr_test, thresholds_oot = metrics.roc_curve(test_selected['target'], y_pred_test, pos_label=1)
roc_auc_test = metrics.auc(fpr_test, tpr_test)
ks_test = max(tpr_test-fpr_test)
print('test KS: ', ks_test)
print('test AUC: ', roc_auc_test)
# test KS:  0.1686553689980061
# test AUC:  0.6150864701513296
#%%
plot_importance(xgb_model,importance_type='gain')
xgb_model.get_booster().get_score(importance_type='gain')
#%%
plt.plot(fpr, tpr, color='darkorange', lw=2, label='trian ROC curve (area = %0.2f)' % roc_auc)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='test ROC curve (area = %0.2f)' % roc_auc_test)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="best")
plt.show()
#%%
from sklearn.model_selection import GridSearchCV
param_test1 = {'max_depth':[2,3,4,5,6],'n_estimators':[100, 200, 300, 400, 500, 600]}

gsearch = GridSearchCV(
    estimator = xgb_model,
    param_grid=param_test1, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch.fit(train_selected[cols], train_selected['target'])

print('gsearch1.best_params_', gsearch.best_params_)
print('gsearch1.best_score_', gsearch.best_score_)
# gsearch1.best_params_ {'max_depth': 4, 'n_estimators': 200}
# gsearch1.best_score_ 0.6189258712768686
#%%
xgb_model_2 = xgb.XGBClassifier(learning_rate=0.1,
                              n_estimators = 200,
                              objective = "binary:logistic",
                              max_depth = 4,
                              n_jobs = -1,
                              min_child_weight = 1,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
)

param_test2 = {'learning_rate':[i/20.0 for i in range(1,20)]}
gsearch2 = GridSearchCV(
    estimator = xgb_model_2,
    param_grid=param_test2, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch2.fit(train_selected[cols], train_selected['target'])

print('gsearch2.best_params_', gsearch2.best_params_)
print('gsearch2.best_score_', gsearch2.best_score_)
# gsearch2.best_params_ {'learning_rate': 0.1}
# gsearch2.best_score_ 0.617597702161911
#%%
xgb_model_3 = xgb.XGBClassifier(learning_rate=0.05,
                              n_estimators = 600,
                              objective = "binary:logistic",
                              max_depth = 2,
                              n_jobs = -1,
                              min_child_weight = 3,
                              subsample=1,
                              nthread = 1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_lambda = 300
)

param_test3 = {'min_child_weight':[i for i in range(1,6,1)]}

gsearch3 = GridSearchCV(
    estimator = xgb_model_3,
    param_grid=param_test3, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch3.fit(train_selected[cols], train_selected['target'])

print('gsearch3.best_params_', gsearch3.best_params_)
print('gsearch3.best_score_', gsearch3.best_score_)

# gsearch3.best_params_ {'min_child_weight': 1}
# gsearch3.best_score_ 0.6177236507430007
#%%
# 对训练集进行预测
pred_train = xgb_model.predict_proba(train_selected[cols])[:,1]
fpr, tpr, thresholds = metrics.roc_curve(train_selected['target'], pred_train, pos_label=1)
roc_auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train KS: ', ks)
print('train AUC: ', roc_auc)

# 对测试集进行预测
pred_test = xgb_model.predict_proba(test_selected[cols])[:,1]
fpr_test, tpr_test, thresholds_test = metrics.roc_curve(test_selected['target'], pred_test, pos_label=1)
roc_auc_test = metrics.auc(fpr_test, tpr_test)
ks_test = max(tpr_test-fpr_test)
print('oot KS: ', ks_test)
print('oot AUC: ', roc_auc_test)

# train KS:  0.22254443998938767
# train AUC:  0.6530843556171937
# oot KS:  0.1686553689980061
# oot AUC:  0.6150864701513296
#%%
train_selected=pd.concat([train_selected,dt_train[['channel_id_x']]],axis=1)
test_selected=pd.concat([test_selected,dt_test[['channel_id_x']]],axis=1)
#%%
train_selected_167 = train_selected.query("channel_id_x == 167")
oot_selected_167 = test_selected.query("channel_id_x == 167")

train_selected_174 = train_selected.query("channel_id_x == 174")
oot_selected_174 = test_selected.query("channel_id_x == 174")
# 167对训练集进行预测
pred_train_167 = xgb_model.predict_proba(train_selected_167[cols])[:,1]
fpr_167, tpr_167, thresholds_167 = metrics.roc_curve(train_selected_167['target'], pred_train_167, pos_label=1)
roc_auc_167 = metrics.auc(fpr_167, tpr_167)
ks_167 = max(tpr_167-fpr_167)
print('渠道167的train KS: ', ks_167)
print('渠道167的train AUC: ', roc_auc_167)

# 167对测试集进行预测
pred_oot_167 = xgb_model.predict_proba(oot_selected_167[cols])[:,1]
fpr_167_oot, tpr_167_oot, thresholds_oot_167 = metrics.roc_curve(oot_selected_167['target'], pred_oot_167, pos_label=1)
roc_auc_oot_167 = metrics.auc(fpr_167_oot, tpr_167_oot)
ks_oot_167 = max(tpr_167_oot-fpr_167_oot)
print('渠道167的oot KS: ', ks_oot_167)
print('渠道167的oot AUC: ', roc_auc_oot_167)
# 渠道167的train KS:  0.21872355347378353
# 渠道167的train AUC:  0.6504658914522002
# 渠道167的oot KS:  0.16664529540649697
# 渠道167的oot AUC:  0.6127123415962997

# 174对训练集进行预测
pred_train_174 = xgb_model.predict_proba(train_selected_174[cols])[:,1]
fpr_174, tpr_174, thresholds_174 = metrics.roc_curve(train_selected_174['target'], pred_train_174, pos_label=1)
roc_auc_174 = metrics.auc(fpr_174, tpr_174)
ks_174 = max(tpr_174-fpr_174)
print('渠道174的train KS: ', ks_174)
print('渠道174的train AUC: ', roc_auc_174)

# 174对测试集进行预测
pred_oot_174 = xgb_model.predict_proba(oot_selected_174[cols])[:,1]
fpr_174_oot, tpr_174_oot, thresholds_oot_174 = metrics.roc_curve(oot_selected_174['target'], pred_oot_174, pos_label=1)
roc_auc_oot_174 = metrics.auc(fpr_174_oot, tpr_174_oot)
ks_oot_174 = max(tpr_174_oot-fpr_174_oot)
print('渠道174的oot KS: ', ks_oot_174)
print('渠道174的oot AUC: ', roc_auc_oot_174)

# 渠道174的train KS:  0.22101607157543735
# 渠道174的train AUC:  0.6539843940970496
# 渠道174的oot KS:  0.17573411629090685
# 渠道174的oot AUC:  0.6144080536343404
#%%
# 业务效果
pred_data = toad.metrics.KS_bucket(pred_train, train_selected['target'], bucket=10, method='step')
pred_data_test = toad.metrics.KS_bucket(pred_test, test_selected['target'], bucket=10, method='step')

# 167业务效果
pred_data_167 = toad.metrics.KS_bucket(pred_train_167, train_selected_167['target'], bucket=10, method='step')
pred_data_oot_167 = toad.metrics.KS_bucket(pred_oot_167, oot_selected_167['target'], bucket=10, method='step')

# 174业务效果
pred_data_174 = toad.metrics.KS_bucket(pred_train_174, train_selected_174['target'], bucket=10, method='step')
pred_data_oot_174 = toad.metrics.KS_bucket(pred_oot_174, oot_selected_174['target'], bucket=10, method='step')
#%%
train_selected_167['prob'] = pred_train_167
oot_selected_167['prob'] = pred_oot_167

train_selected_174['prob'] = pred_train_174
oot_selected_174['prob'] = pred_oot_174

train_selected['prob'] = pred_train
test_selected['prob'] = pred_test

#%%
def Prob2Score(prob, basePoint, PDO):
    # 将概率转化成分数且为正整数
    y = np.log(prob / (1 - prob))
    y2 = basePoint + PDO / np.log(2) * (-y)
    score = y2.astype("int")
    return score
#%%

train_selected['score'] = train_selected['prob'].apply(lambda x:Prob2Score(x, 700, 60))
test_selected['score'] = test_selected['prob'].apply(lambda x:Prob2Score(x, 700, 60))
train_selected_174['score'] = train_selected_174['prob'].apply(lambda x:Prob2Score(x, 700, 60))
oot_selected_174['score'] = oot_selected_174['prob'].apply(lambda x:Prob2Score(x, 700, 60))
train_selected_167['score'] = train_selected_167['prob'].apply(lambda x:Prob2Score(x, 700, 60))
oot_selected_167['score'] = oot_selected_167['prob'].apply(lambda x:Prob2Score(x, 700, 60))
#%%
train_selected['score_band'] =pd.qcut(train_selected['score'],10)
test_selected['score_band'] =pd.cut(test_selected['score'],[-np.inf,850,867,880,892,903,914,927,941,962,np.inf])


train_selected_167['score_band'] =pd.cut(train_selected_167['score'],[-np.inf,850,867,880,892,903,914,927,941,962,np.inf])
oot_selected_167['score_band'] =pd.cut(oot_selected_167['score'],[-np.inf,850,867,880,892,903,914,927,941,962,np.inf])
train_selected_174['score_band'] =pd.cut(train_selected_174['score'],[-np.inf,850,867,880,892,903,914,927,941,962,np.inf])
oot_selected_174['score_band'] =pd.cut(oot_selected_174['score'],[-np.inf,850,867,880,892,903,914,927,941,962,np.inf])
#%%
cal_score1=train_selected_167.groupby('score_band')['target'].count().reset_index()
cal_score2=oot_selected_167.groupby('score_band')['target'].count().reset_index()

cal_score3=train_selected_167.groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()
cal_score4=oot_selected_167.groupby(['score_band','target'])['user_id'].count().unstack().reset_index()


cal_score1=train_selected_174.groupby('score_band')['target'].count().reset_index()
cal_score2=oot_selected_174.groupby('score_band')['target'].count().reset_index()

cal_score3=train_selected_174.groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()
cal_score4=oot_selected_174.groupby(['score_band','target'])['user_id'].count().unstack().reset_index()



cal_score1=train_selected.groupby('score_band')['target'].count().reset_index()
cal_score2=test_selected.groupby('score_band')['target'].count().reset_index()

cal_score3=train_selected.groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()
cal_score4=test_selected.groupby(['score_band','target'])['user_id'].count().unstack().reset_index()



#==============================================================================
# File: 模型评估.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 模型评估

# AUC 
def plot_roc(y_label,y_pred):
    """
    y_label:测试集的y
    y_pred:对测试集预测后的概率
    
    return:ROC曲线
    """
    tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred) 
    AUC = metrics.roc_auc_score(y_label,y_pred) 
    fig = plt.figure(figsize=(6,4))
    ax = fig.add_subplot(1,1,1)
    ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC) 
    ax.plot([0,1],[0,1],'r--')
    ax.set_ylim(0,1)
    ax.set_xlim(0,1)
    ax.set_title('ROC')
    ax.legend(loc='best')
    return plt.show(ax)


# KS 
def plot_model_ks(y_label,y_pred):
    """
    y_label:测试集的y
    y_pred:对测试集预测后的概率
    
    return:KS曲线
    """
    pred_list = list(y_pred) 
    label_list = list(y_label)
    total_bad = sum(label_list)
    total_good = len(label_list)-total_bad 
    items = sorted(zip(pred_list,label_list),key=lambda x:x[0]) 
    step = (max(pred_list)-min(pred_list))/200 
    
    pred_bin=[]
    good_rate=[] 
    bad_rate=[] 
    ks_list = [] 
    for i in range(1,201): 
        idx = min(pred_list)+i*step 
        pred_bin.append(idx) 
        label_bin = [x[1] for x in items if x[0]<idx] 
        bad_num = sum(label_bin)
        good_num = len(label_bin)-bad_num  
        goodrate = good_num/total_good 
        badrate = bad_num/total_bad
        ks = abs(goodrate-badrate) 
        good_rate.append(goodrate)
        bad_rate.append(badrate)
        ks_list.append(ks)
    
    fig = plt.figure(figsize=(8,6))
    ax = fig.add_subplot(1,1,1)
    ax.plot(pred_bin,good_rate,color='green',label='good_rate')
    ax.plot(pred_bin,bad_rate,color='red',label='bad_rate')
    ax.plot(pred_bin,ks_list,color='blue',label='good-bad')
    ax.set_title('KS:{:.3f}'.format(max(ks_list)))
    ax.legend(loc='best')
    return plt.show(ax)


# 交叉验证
def cross_verify(x,y,estimators,fold,scoring='roc_auc'):
    """
    x:自变量的数据集
    y:target的数据集
    estimators：验证的模型
    fold：交叉验证的策略
    scoring:评级指标，默认auc
    
    return:交叉验证的结果
    """
    cv_result = cross_val_score(estimator=estimators,X=x,y=y,cv=fold,n_jobs=-1,scoring=scoring)
    print('CV的最大AUC为:{}'.format(cv_result.max()))
    print('CV的最小AUC为:{}'.format(cv_result.min()))
    print('CV的平均AUC为:{}'.format(cv_result.mean()))
    plt.figure(figsize=(6,4))
    plt.title('交叉验证的评价指标分布图')
    plt.boxplot(cv_result,patch_artist=True,showmeans=True,
            boxprops={'color':'black','facecolor':'yellow'},
            meanprops={'marker':'D','markerfacecolor':'tomato'},
            flierprops={'marker':'o','markerfacecolor':'red','color':'black'},
            medianprops={'linestyle':'--','color':'orange'})
    return plt.show()


# 学习曲线
def plot_learning_curve(estimator,x,y,cv=None,train_size = np.linspace(0.1,1.0,5),plt_size =None):
    """
    estimator :画学习曲线的基模型
    x:自变量的数据集
    y:target的数据集
    cv:交叉验证的策略
    train_size:训练集划分的策略
    plt_size:画图尺寸
    
    return:学习曲线
    """
    from sklearn.model_selection import learning_curve
    train_sizes,train_scores,test_scores = learning_curve(estimator=estimator,
                                                          X=x,
                                                          y=y,
                                                          cv=cv,
                                                          n_jobs=-1,
                                                          train_sizes=train_size)
    train_scores_mean = np.mean(train_scores,axis=1)
    train_scores_std = np.std(train_scores,axis=1)
    test_scores_mean = np.mean(test_scores,axis=1)
    test_scores_std = np.std(test_scores,axis=1)
    plt.figure(figsize=plt_size)
    plt.xlabel('Training-example')
    plt.ylabel('score')
    plt.fill_between(train_sizes,train_scores_mean-train_scores_std,
                     train_scores_mean+train_scores_std,alpha=0.1,color='r')
    plt.fill_between(train_sizes,test_scores_mean-test_scores_std,
                     test_scores_mean+test_scores_std,alpha=0.1,color='g')
    plt.plot(train_sizes,train_scores_mean,'o-',color='r',label='Training-score')
    plt.plot(train_sizes,test_scores_mean,'o-',color='g',label='cross-val-score')
    plt.legend(loc='best')
    return plt.show()


# 混淆矩阵 /分类报告
def plot_matrix_report(y_label,y_pred): 
    """
    y_label:测试集的y
    y_pred:对测试集预测后的概率
    
    return:混淆矩阵
    """
    matrix_array = metrics.confusion_matrix(y_label,y_pred)
    plt.matshow(matrix_array, cmap=plt.cm.summer_r)
    plt.colorbar()

    for x in range(len(matrix_array)): 
        for y in range(len(matrix_array)):
            plt.annotate(matrix_array[x,y], xy =(x,y), ha='center',va='center')

    plt.xlabel('True label')
    plt.ylabel('Predict label')
    print(metrics.classification_report(y_label,y_pred))
    return plt.show()




#==============================================================================
# File: 离线特征变量工程化处理v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[7]:


import pandas as pd
import numpy as np
import time 
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc
from jinja2 import Template
import os

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'


# # 配置参数

# In[ ]:



# 配置参数
BATCH_SIZE = 1000
OUTPUT_DIR = "./generated_sql"
MAIN_SQL_FILE = "main_table.sql"  # 你可以把主表 SQL 写入文件中备用

os.makedirs(OUTPUT_DIR, exist_ok=True)


# # 生成sql脚本

# In[ ]:




# 示例变量清单格式：
# variable_name | table_name
variables_df = pd.read_excel("人行变量清单2506.xlsx")
variables_df.rename(columns={"Table": "table_name","var":"variable_name"}, inplace=True)
variables_df.info(show_counts=True)
variables_df.head()


# In[ ]:


# 分批处理
batches = [variables_df[i:i+BATCH_SIZE] for i in range(0, len(variables_df), BATCH_SIZE)]
print(len(batches))


# In[ ]:


# Jinja2 模板：特征变量部分
feature_template = Template("""
left join 
(
select t.*,
   ROW_NUMBER() OVER (PARTITION BY id_no_des ORDER BY dt DESC) AS rk 
from {{ full_table_name }} as t 
where dt <= date_sub('$[last_day(yyyy-MM-dd)]', 1) 
  and dt >= date_sub('$[last_day(yyyy-MM-dd)]', 100)
) as {{ alias_name }} on t.id_no_des = {{ alias_name }}.id_no_des and {{ alias_name }}.rk = 1
""")


# In[ ]:



# 主表模板占位符
MAIN_TEMPLATE = """
WITH main_base AS (
    -- 主表 SQL 放在这里
),
features AS (
    -- 所有 left join 的特征表放在这里
)
SELECT *
FROM main_base
LEFT JOIN features USING (id_no_des)
"""


# In[ ]:


# 读取主表 SQL（假设你已保存为文本文件）
with open(MAIN_SQL_FILE, 'r') as f:
    main_sql = f.read()


# In[ ]:


print(main_sql)


# In[ ]:


for idx, batch in enumerate(batches):
    print(f"\nProcessing Batch {idx + 1} / {len(batches)}")
    
    # 按 table_name 分组，得到 { 'tableA': ['var1', 'var2'], ... }
    grouped = batch.groupby('table_name')['variable_name'].apply(list).to_dict()
    
    feature_joins = []
    alias_counter = 1
    
    # 新增：记录每个表对应的别名和变量列表
    batch_aliases = {}

    for table_name, vars_list in grouped.items():
        alias_name = f"t{alias_counter}"
        alias_counter += 1
        
        # 记录当前表的别名和字段
        batch_aliases[table_name] = (alias_name, vars_list)
        
        # 填充模板，构造每个特征表的 SELECT 字段列表
        sql_part = feature_template.render(
            full_table_name=table_name,
            variables=vars_list,
            alias_name=alias_name
        )
        feature_joins.append(sql_part)
    
    # 合并所有 left join 子句
    all_features_sql = "\n".join(feature_joins)

    # 构造 SELECT 列表：t.*, t1.var1, t1.var2, t2.var101, ...
    feature_columns = [
        f"{alias}.{var}"
        for table_name, (alias, vars_list) in batch_aliases.items()
        for var in vars_list
    ]
    features_select_clause = ",\n        ".join(feature_columns)

    # 将主表和特征 join 合并
    final_sql = f"""
    WITH main_base AS (
        {main_sql}
    ),
    features_with_vars AS (
        SELECT 
            t.*,
            {features_select_clause}
        FROM main_base AS t
        {all_features_sql}
    )
    SELECT * FROM features_with_vars
    """
    
    # 保存到文件
    output_file = os.path.join(OUTPUT_DIR, f"batch_{idx + 1}.sql")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_sql)
        
    print(f"✅ 已生成 SQL 文件: {output_file}")


# # 处理数据

# In[2]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
import glob
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:



# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

def model_trian(df,target,varsname_base):
    ### 模型参数
    opt_params = {}
    opt_params['boosting'] = 'gbdt'
    opt_params['objective'] = 'binary'
    opt_params['metric'] = 'auc'
    opt_params['bagging_freq'] = 1
    opt_params['scale_pos_weight'] = 1 
    opt_params['seed'] = 1 
    opt_params['num_threads'] = -1 
    # 调参时设置成不用调参的参数
    opt_params['learning_rate'] = 0.1
    ## 正则参数，防止过拟合
    opt_params['bagging_fraction'] = 0.8628008772208227     
    opt_params['feature_fraction'] = 0.6177619614753441
    opt_params['lambda_l1'] = 0
    opt_params['lambda_l2'] = 300
    opt_params['early_stopping_rounds'] = 50

    # 调参后的参数需要变成整数型
    opt_params['num_leaves'] = 21
    opt_params['min_data_in_leaf'] = 103
    opt_params['max_depth'] = 2
    # 调参后的其他参
    opt_params['min_gain_to_split'] = 10
    
    df_sample = df[(df[target].notna())&(df[target]>=0)].reset_index(drop=True)
    df_sample[target] = df_sample[target].astype('int')

    # 确定数据集参数后，训练模型
    for i, col in enumerate(varsname_base):
        if df_sample[col].dtype=='object':
            df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')
            
    X_train, X_test, y_train, y_test = train_test_split(df_sample[varsname_base],
                                                        df_sample[target],
                                                        test_size=0.2, 
                                                        random_state=22, 
                                                        stratify=df_sample[target]
                                                       )

    train_set = lgb.Dataset(X_train, label=y_train)
    valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
    lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)
    df_importance = feature_importance(lgb_model) 
    df_importance = df_importance.reset_index()
    return df_importance


def load_sql_from_file(filepath):
    """读取 .sql 文件内容"""
    with open(filepath, 'r', encoding='utf-8') as f:
        sql_content = f.read()
    return sql_content




# In[4]:


# 定义一个函数：统一转换为字符串
def to_str(d):
    if isinstance(d, date):
        return d.strftime('%Y-%m-%d')
    elif pd.isna(d):
        return ''
    elif isinstance(d, str):
        return d
    else:
        return str(d)


# In[ ]:


non_feature_cols = ['order_no','user_id','id_no_des','channel_id','apply_date',
                    'target_fpd30','target_cpd30','target_mob4dpd30','target_mob6dpd30']
target = 'target_mob6dpd30'

input_dir='./generated_sql/'
output_file='all_features.csv'

# """
# 读取所有 SQL 文件，依次执行并合并结果
# """
sql_files = glob.glob(os.path.join(input_dir, "*.sql"))
all_data = []
for idx, file in enumerate(sql_files):
    print(f"\n🚀 正在处理第 {idx+1} / {len(sql_files)} 个 SQL 文件: {file}")

    try:
        sql = load_sql_from_file(file)
        df_sample_dict = {}

        # 计算今天的时间
        from datetime import datetime, timedelta, date
        this_day =datetime.strptime('2024-11-30', '%Y-%m-%d')
        end_day = datetime.strptime('2024-09-01', '%Y-%m-%d')

        while this_day >= end_day:
            run_day = this_day.strftime('%Y-%m-%d')
            # 替换日期变量（如果需要）
            final_sql = sql.replace("$[last_day(yyyy-MM-dd)]", f"{run_day}")
            print(f'=========================={run_day}=============================')
            df_sample_dict[run_day] = get_data(final_sql)
            this_day = this_day - timedelta(days=1)
        df = pd.concat(df_sample_dict.values(), ignore_index=True)

        if not df.empty:
            # 可选：删除重复字段（如 id_no_des 已存在于主表）
            cols_to_drop = [col for col in df.columns if df.columns.tolist().count(col) > 1]
            df = df.loc[:, ~df.columns.duplicated()]
            print(f"✅ 成功加载数据，行数: {len(df)}, 列数: {len(df.columns)}")
            varsname_base = [col for col in df.columns if col not in non_feature_cols]
            # 应用函数，转换为字符串
            df['apply_date'] = df['apply_date'].apply(to_str)
            df_importance = model_trian(df, target, varsname_base)
            all_data.append(df_importance)
            del df
            gc.collect()
        else:
            print(f"⚠️ 警告：{file} 返回空数据")

    except Exception as e:
        print(f"❌ 执行失败: {file}")
        print("错误详情:", str(e))
        continue

if all_data:
    print("\n📊 正在合并所有批次数据...")
    final_df = pd.concat(all_data, axis=0)

    # 保存最终结果
#         final_df.to_parquet(output_file)
    final_df.to_csv(output_file)


# In[8]:


final_df.info(show_counts=True)
final_df.head()


# In[14]:


sql_files


# In[15]:


for idx, file in enumerate(['./generated_sql/batch_2.sql','./generated_sql/batch_5.sql','./generated_sql/batch_6.sql']):
    print(f"\n🚀 正在处理第 {idx+1} / {len(sql_files)} 个 SQL 文件: {file}")

    try:
        sql = load_sql_from_file(file)
        df_sample_dict = {}

        # 计算今天的时间
        from datetime import datetime, timedelta, date
        this_day =datetime.strptime('2024-11-30', '%Y-%m-%d')
        end_day = datetime.strptime('2024-09-01', '%Y-%m-%d')

        while this_day >= end_day:
            run_day = this_day.strftime('%Y-%m-%d')
            # 替换日期变量（如果需要）
            final_sql = sql.replace("$[last_day(yyyy-MM-dd)]", f"{run_day}")
            print(f'=========================={run_day}=============================')
            df_sample_dict[run_day] = get_data(final_sql)
            this_day = this_day - timedelta(days=1)
        df = pd.concat(df_sample_dict.values(), ignore_index=True)

        if not df.empty:
            # 可选：删除重复字段（如 id_no_des 已存在于主表）
            cols_to_drop = [col for col in df.columns if df.columns.tolist().count(col) > 1]
            df = df.loc[:, ~df.columns.duplicated()]
            print(f"✅ 成功加载数据，行数: {len(df)}, 列数: {len(df.columns)}")
            varsname_base = [col for col in df.columns if col not in non_feature_cols]
            # 应用函数，转换为字符串
            df['apply_date'] = df['apply_date'].apply(to_str)
            df_importance = model_trian(df, target, varsname_base)
            all_data.append(df_importance)
            del df
            gc.collect()
        else:
            print(f"⚠️ 警告：{file} 返回空数据")

    except Exception as e:
        print(f"❌ 执行失败: {file}")
        print("错误详情:", str(e))
        continue

if all_data:
    print("\n📊 正在合并所有批次数据...")
    final_df = pd.concat(all_data, axis=0)

    # 保存最终结果
#         final_df.to_parquet(output_file)
    final_df.to_csv('all_features_v2.csv')


# In[16]:


final_df.info(show_counts=True)


# In[ ]:


final_data_v1 = final_data[['gain']]
final_data_v1.info(show_counts=True)
final_data_v1.head()


# In[ ]:


final_data_v2 = final_data_v1[final_data_v1>0]
final_data_v2.info(show_counts=True)


# In[ ]:


final_data_v2 = final_data_v2.dropna(how='all')
final_data_v2 =final_data_v2.reset_index()
final_data_v2.info(show_counts=True)


# In[ ]:


final_data_v2.to_csv('13500个离线人行变量特征重要性排序大于0.csv')




#==============================================================================
# File: 策略测算-规则模拟.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[73]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import xgboost as xgb
from xgboost import plot_importance
from sklearn import metrics
from sklearn.model_selection import train_test_split
from datetime import datetime
import os 
import warnings
warnings.filterwarnings("ignore")


# # 数据准备

# In[ ]:


# 全部授信申请客户
df1_auth = pd.read_csv(r'd:\liuyedao\model_result\model_data_20230728_bqc.csv')
df1_auth.info(show_counts=True)


# In[ ]:


# 去重数据
data = df1_auth.sort_values(by=['user_id','auth_status','apply_date_auth'], ascending=[True,True,False]).drop_duplicates(subset=['user_id'],keep='first')
print(data['user_id'].nunique(), data.shape)


# In[ ]:


index = list(data.query("auth_status==7 & Firs6ever30==Firs6ever30").index)
data.drop(index=index,axis=0,inplace=True)
data = data.reset_index(drop=True)
data.shape


# In[ ]:


# 剔除不必要的字段
data = data.drop(['apply_date_cash', 'lending_time', 'Firs3ever15', 'Firs3ever30', 'Firs6ever15', 'auth_credit_amount'],axis=1)


# In[ ]:


# 阿里申请反欺诈评分数据
pudao_15 = pd.read_pickle(r'D:\liuyedao\result\auth_pudao_15_20230725.pkl')
pudao_15.info()


# In[ ]:


# 补上阿里申请反欺诈评分
data = pd.merge(data, pudao_15[['order_no', 'model_score_01_pudao_15']], how='left', on='order_no')
data.shape


# In[184]:


# 补上百融多头的数据
df_bairong = pd.read_csv(r'd:\liuyedao\result\auth_bairong_1_20230801_策略.csv')
df_bairong.info(show_counts=True)


# In[237]:


# 补上百融多头数据
data_ = pd.merge(data[['order_no','auth_status']], df_bairong, how='left', on='order_no')
data_.shape


# In[186]:


data_.columns


# In[233]:


def cal_bins_pct(df, col, target='target'):
    total = df.groupby(col)['order_no'].count()
    tg_jj = df.groupby([col, 'auth_status'])['order_no'].count().unstack()
    base_df = pd.concat([total, tg_jj], axis=1)
    base_df.columns = ['授信申请','通过人数','拒绝人数']
    for x in base_df.columns:
        base_df['{}占比'.format(x)] = base_df[x]/base_df[x].sum()
    base_df['varsname'] = col
    return base_df


# In[223]:


def bins_fuc(df, col, bins=20):
    df_part1 = df[df[col].notnull()]
    df_part1[col] = pd.qcut(df_part1[col], bins, duplicates='drop')
    
    df_part2 = df[df[col].isnull()]
    df_part2[col] = pd.Categorical(['nan']*df_part2.shape[0])
    data_s = pd.concat([df_part1[col], df_part2[col]], axis=0)
    data_s = data_s.sort_index()
    
    return data_s


# In[229]:


col = data_.columns[6]
print(col)


# In[230]:


bins_fuc(data_, col).value_counts(dropna=False)


# In[238]:


result_bins = pd.DataFrame()
for col in data_.columns[5:]:
    data_[col] = bins_fuc(data_, col, bins=50)
    tmp = cal_bins_pct(data_, col, target='target')
    result_bins = pd.concat([tmp, result_bins])


# In[239]:


result_bins


# In[241]:


result_bins.to_excel(r"d:\liuyedao\mid_result\auth_策略模拟_单变量分析_百融.xlsx")


# In[ ]:


# 补上模型评分数据
model_score = pd.read_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_xgb_lr_score_v2.csv")
model_score.info()


# In[74]:


# 补上模型评分数据
model_score = pd.read_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_xgb_lr_score_v2_20230807.csv")
model_score.info()


# In[75]:


data = pd.merge(data, model_score[['order_no','score_xgb','score_lr']], how='left', on='order_no')
data.shape


# In[ ]:


data.info(show_counts=True)


# In[ ]:


data.rename(columns={'Firs6ever30':'target'},inplace=True)


# In[ ]:


data.to_csv(r'd:\liuyedao\result\auth_策略模拟_20230801_v1.csv',index=False)


# In[ ]:


data = data.drop(['model_score_01_pudao_15_y', 'apply_date_bairong', 'order_no_y_bairong', 'order_no_y_bairong'], axis=1)
data.rename(columns={'model_score_01_pudao_15_x':'model_score_01_pudao_15'}, inplace=True)
data.to_csv(r'd:\liuyedao\result\auth_策略模拟_20230801_v1.csv',index=False)


# In[ ]:


# 筛选通过的人数
df_approve = data[data['auth_status']==6]
df_approve.shape


# In[ ]:


df_approve = df_approve[df_approve['Firs6ever30'].isin([0.0, 2.0])]
df_approve.shape


# In[ ]:


df_approve['Firs6ever30'] = df_approve['Firs6ever30']/2
df_approve['Firs6ever30'].value_counts(dropna=False)


# In[ ]:


df_approve.rename(columns={'Firs6ever30':'target'},inplace=True)


# In[ ]:


df_approve.info(show_counts=True)


# In[ ]:





# In[ ]:


to_drop = list(data.columns)[0:7] + ['create_time_bairong']
cols_bairong = ['value_059_bairong','value_044_bairong','value_074_bairong','value_080_bairong','value_084_bairong','value_092_bairong','value_093_bairong','value_094_bairong','value_095_bairong','value_096_bairong','value_097_bairong','value_098_bairong']
vars_list = list(data.columns)[7:25] + ['model_score_01_pudao_15_x'] + cols_bairong
print(to_drop)
print(vars_list)


# # 单变量分析

# In[ ]:


# 第一次分箱
c = toad.transform.Combiner()
c.fit(df_approve.drop(to_drop,axis=1), y='target', method='dt', min_samples=1, n_bins=20, empty_separate=True) 
bins_result = c.export()


# In[ ]:


def regroup_bins(data_bins, col, target='target'):    
    total = data_bins.groupby(col)[target].count()
    bad = data_bins.groupby(col)[target].sum()
    regroup = pd.concat([total, bad],axis=1)
    regroup.columns = ['total', 'bad']
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_rate'] = regroup['bad']/regroup['total']
    regroup['total_pct'] = regroup['total']/regroup['total'].sum()
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
    regroup['good_pct'] = regroup['good']/regroup['good'].sum()
    regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
    regroup['goo_pct_cum'] = regroup['good_pct'].cumsum()
#     regroup['ks'] = regroup['bad_pct_cum'] - regroup['goo_pct_cum']
#     regroup['ks_max'] = regroup['ks'].max()
#     regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct']) * np.log(regroup['bad_pct']/regroup['good_pct'])
#     regroup['iv'] = regroup['iv_bins'].sum()
    regroup['varsname'] = col
    regroup['bins'] = regroup.index
    cols = ['varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'total_pct', 'bad_pct_cum', 'goo_pct_cum', 'bad_pct', 'good_pct']
    regroup = regroup[cols]
    
    return regroup


# In[ ]:


data_bins = c.transform(df_approve, labels=True)
data_bins.head()


# In[ ]:


df_result = pd.DataFrame()
for col in vars_list:
    print('------------变量：{}-----------'.format(col))
    tmp = regroup_bins(data_bins, col, target='target')
    df_result = pd.concat([df_result, tmp], axis=0)


# In[ ]:


df_result.head()


# In[ ]:


writer=pd.ExcelWriter(r"d:\liuyedao\mid_result\auth_策略模拟_单变量分析_v2_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
df_result.to_excel(writer,sheet_name='单变量分析')
writer.save()


# # 规则模拟

# In[2]:


data = pd.read_csv(r'd:\liuyedao\result\auth_策略模拟_20230801_v1.csv')
data.info(show_counts=True)


# In[ ]:





# In[286]:


# 读写阈值变量表
threshold = pd.read_excel(r'd:\liuyedao\mid_result\auth_策略模拟_规则变量及其阈值_20230809.xlsx',sheet_name='版本3_无')
threshold.info()
threshold.head()


# In[287]:


threshold_1 = threshold.query("规则结果=='小于阈值拒绝'")
threshold_2 = threshold.query("规则结果=='大于阈值拒绝'")


# In[288]:


threshold_1 = dict(zip(threshold_1['变量名'], threshold_1['阈值']))
threshold_2 = dict(zip(threshold_2['变量名'], threshold_2['阈值']))
print(threshold_1)
print(threshold_2)


# In[289]:


print(data.columns)


# In[290]:


# to_drop = ['model_score_01_x_tianchuang', 'model_score_01_zr_tongdun', 'model_score_01_baihang', 'model_score_01_xysl_3','value_098_bairong',
#           'create_time_bairong','model_score_01_pudao_8','model_score_01_moxingfen_14', 'model_score_01_xysl_1', 
#            'score_xgb_x', 'score_lr_x', 'score_xgb_y']
# to_drop = ['model_score_01_x_tianchuang', 'model_score_01_zr_tongdun', 'model_score_01_baihang', 'model_score_01_xysl_3','value_098_bairong',
#           'create_time_bairong','model_score_01_pudao_8','model_score_01_moxingfen_14', 'model_score_01_xysl_1', 
#            'score_xgb_x', 'score_lr_x', 'score_lr_y' ]
to_drop = ['model_score_01_x_tianchuang', 'model_score_01_zr_tongdun', 'model_score_01_baihang', 'model_score_01_xysl_3','value_098_bairong',
          'create_time_bairong','model_score_01_pudao_8','model_score_01_moxingfen_14', 'model_score_01_xysl_1', 
           'score_xgb_x', 'score_lr_x' ,'score_xgb_y', 'score_lr_y']
df = data.drop(to_drop, axis=1)
print(df.columns)


# In[273]:


# df.rename(columns={'score_lr_y':'score_lr'}, inplace=True)
# df.rename(columns={'score_xgb_y':'score_xgb'}, inplace=True)


# In[291]:


for col, thre in threshold_1.items():
    print(col)
    df[col] = df[col].apply(lambda x: 1 if pd.notnull(x) and x<thre else 0)

for col, thre in threshold_2.items():
    print(col)
    df[col] = df[col].apply(lambda x: 1 if pd.notnull(x) and x>thre else 0)


# In[292]:


anti_frau = threshold[threshold['规则类型']=='反欺诈']['变量名'].to_list()
multi_lend = threshold[threshold['规则类型']=='多头借贷']['变量名'].to_list()
credit_score = threshold[threshold['规则类型']=='综合信用']['变量名'].to_list()


# In[293]:


print(anti_frau)
print(multi_lend)
print(credit_score)


# In[294]:


df['is_anti_frau'] = df[anti_frau].max(axis=1)
df['is_multi_lend'] = df[multi_lend].max(axis=1)
df['is_credit_score'] = df[credit_score].max(axis=1)
df['all'] = df[['is_anti_frau','is_multi_lend','is_credit_score']].max(axis=1)


# In[295]:


df['node_sort'] = [*map(lambda x1, x2, x3: 1 if x1==1 else 2 if x2==1 else 3 if x3==1 else 0, df['is_anti_frau'], df['is_multi_lend'], df['is_credit_score'])]


# In[296]:


df[['is_anti_frau','is_multi_lend','is_credit_score','node_sort','all']].head()


# In[297]:


def cal_stratige_auth(df, col, target='target', channel=None):
    if channel:
        df = df.query("channel_id==@channel")
   
    total = df.groupby(col)['order_no'].count()
    tg_jj = df.groupby([col, 'auth_status'])['order_no'].count().unstack()
    lend =  df.groupby([col, target])['order_no'].count().unstack()
    base_df = pd.concat([total, tg_jj, lend], axis=1)
    base_df.columns = ['授信申请','通过人数','拒绝人数','好','灰','坏']
    
    return_result = pd.DataFrame()
    for i in range(1,len(base_df.index)):
        result = {}
        result['varsname'] = col
        result['index'] = base_df.index[i]
        result['授信申请'] = base_df['授信申请'].sum()
        result['命中授信申请人数'] = base_df.iloc[i]['授信申请']
        result['命中授信申请人数占比'] = str(result['命中授信申请人数']/result['授信申请']*100)[0:5] + '%'

        result['授信申请通过人数'] = base_df['通过人数'].sum()
        result['命中授信申请通过人数'] = base_df.iloc[i]['通过人数']
        result['命中授信申请通过人数占比'] = str(result['命中授信申请通过人数']/result['授信申请通过人数']*100)[0:5] + '%'

        result['授信申请拒绝人数'] = base_df['拒绝人数'].sum()
        result['命中授信申请拒绝人数'] = base_df.iloc[i]['拒绝人数']
        result['命中授信申请拒绝人数占比'] = str(result['命中授信申请拒绝人数']/result['授信申请拒绝人数']*100)[0:5] + '%'

        result['好客人数'] = base_df['好'].sum()
        result['命中好客人数'] = base_df.iloc[i]['好']
        result['命中好客人数占比'] = str(result['命中好客人数']/result['好客人数']*100)[0:5] + '%'

        result['坏客人数'] = base_df['坏'].sum()
        result['命中坏客人数'] = base_df.iloc[i]['坏']
        result['命中坏客人数占比'] = str(result['命中坏客人数']/result['坏客人数'] *100)[0:5] + '%'

        result['灰客人数'] = base_df['灰'].sum()
        result['命中灰客人数'] = base_df.iloc[i]['灰']
        result['命中灰客人数占比'] = str(result['命中灰客人数']/result['灰客人数'] *100)[0:5] + '%'

        result = pd.DataFrame(result,index=[col])
        return_result = pd.concat([return_result, result])
    
    return return_result


# In[298]:


cols = list(df.columns[df.columns.str.contains('model|value|is|score|node|all')])
print(cols)


# In[299]:


result = pd.DataFrame()

for col in threshold['变量名']:
    if col in cols: 
        print('----------{}-----------'.format(col))
        tmp = cal_stratige_auth(df, col, target='target', channel=None)
        result = pd.concat([result, tmp], axis=0)


# In[300]:


result_node = pd.DataFrame()
for col in ['is_anti_frau', 'is_multi_lend', 'is_credit_score', 'node_sort', 'all']:
        print('----------{}-----------'.format(col))
        tmp = cal_stratige_auth(df, col, target='target', channel=None)
        result_node = pd.concat([result_node, tmp], axis=0)


# In[301]:


from datetime import datetime 
writer=pd.ExcelWriter(r"d:\liuyedao\mid_result\auth_策略模拟_模拟结果_3_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
result_node.to_excel(writer,sheet_name='node')
result.to_excel(writer,sheet_name='rule')
writer.save()


# In[302]:


import gc 
gc.collect()




#==============================================================================
# File: 策略的数据分析-授信层.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[2]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# ## 不区分渠道，合并数据表

# In[5]:


# 获取167文件下的所有csv文件
file_dir = r'.\167'
file_name_167 = get_filename(file_dir)
file_name_167


# In[4]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_167):
    print('-----{}---{}------'.format(i, iterm))
    data = 'data_{}'.format(i)
    globals()[data] = pd.read_csv(r'.\167\{}'.format(iterm))   


# In[6]:


# 获取other文件下的所有csv文件
file_dir = r'.\other'
file_name_other = get_filename(file_dir)
file_name_other


# In[6]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_other):
    print('-----{}---{}------'.format(i, iterm))
    data1 = 'data_other_{}'.format(i)
    globals()[data1] = pd.read_csv(r'.\other\{}'.format(iterm))   


# ## 提款表

# In[7]:


df_order = pd.concat([data_1, data_other_1], axis=0) # 提款数据
df_order.info(null_counts=True)
df_order.head()


# In[9]:


print(df_order['order_no'].nunique(),df_order.shape)


# In[10]:


del data_1, data_other_1


# In[11]:


df_order = df_order.query("apply_date>='2022-05-01'")
# [['user_id','order_no','apply_date','order_status','loan_period','channel_id']]
print(df_order['order_no'].nunique(),df_order.shape)


# ## 授信数据表

# In[4]:


# 授信数据
df_auth = pd.concat([data_0, data_other_0], axis=0) # 授信数据
df_auth.info(null_counts=True)
df_auth.head()


# In[13]:


print(df_auth['order_no'].nunique(),df_auth.shape)


# In[14]:


del data_0, data_other_0


# In[15]:


df_auth = df_auth.query("apply_date>='2022-05-01'")
# [['user_id','order_no','apply_date','auth_status','auth_credit_amount','channel_id']]
print(df_auth['order_no'].nunique(),df_auth.shape)


# ## 还款计划表

# In[18]:


df_repay = pd.concat([data_6, data_other_5, data_other_6, data_other_7], axis=0) # 还款数据
df_repay.info(null_counts=True)
df_repay.head()


# In[19]:


print(df_repay['order_no'].nunique(),df_repay.shape)


# In[20]:


del data_6, data_other_5, data_other_6, data_other_7


# In[21]:


df_repay = df_repay.query("lending_time>='2022-05-01'")
print(df_repay['order_no'].nunique(),df_repay.shape)


# ### 获取借款金额、放款时间、借款利率

# In[23]:


# 借款金额、放款时间、借款利率/
df_loan_amt = df_repay.groupby('order_no')['lending_time','loan_amount','loan_rate','total_periods'].max()
df_loan_amt = df_loan_amt.reset_index()
df_loan_amt.info()
df_loan_amt.head()


# In[24]:


print(df_loan_amt['order_no'].nunique(),df_loan_amt.shape)


# ### Y标签

# In[50]:


# 截止目前，每期还款计划是否到期
df_repay['is_dq'] = df_repay['repay_date'].apply(lambda x: 1 if x<=datetime(2023,7,12) else 0)


# In[60]:


df_repay.info(null_counts=True)


# In[55]:


# 结清日期填充缺失值
# df_repay['period_settle_date'] = df_repay['period_settle_date_copy']
df_repay['period_settle_date'].fillna('2023-07-12',inplace=True)


# In[57]:


# 时间格式转换
df_repay['period_settle_date'] = pd.to_datetime(df_repay['period_settle_date'],format='%Y-%m-%d')
df_repay['repay_date'] = pd.to_datetime(df_repay['repay_date'], format='%Y-%m-%d')


# In[59]:


# 计算逾期天数
df_repay['yqts'] = df_repay['period_settle_date'] - df_repay['repay_date']
df_repay['yqts'] = df_repay['yqts'].dt.days
df_repay['yqts'].head()


# In[62]:


# 添加每期还款计划的标签
df_repay['Firs3ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs3ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs6ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs6ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['yqts'],df_repay['is_dq'])]


# In[47]:


df_Firs3 = df_repay.query("period<=3").groupby('order_no').agg({'Firs3ever15':'max','Firs3ever30':'max','is_dq':'min'})
df_Firs3.shape


# In[64]:


# 产生首三期的Y标签
df_Firs3 = df_repay.query("period<=3").groupby('order_no').agg({'Firs3ever15':'max','Firs3ever30':'max','is_dq':'min'})
df_Firs3 = df_Firs3.query("is_dq==1").reset_index() # 满足3期账龄要求
df_Firs3.head()
print(df_Firs3.shape)
print(df_Firs3['Firs3ever15'].value_counts())
print(df_Firs3['Firs3ever30'].value_counts())


# In[65]:


# 产生首六期的Y标签
df_Firs6 = df_repay.query("period<=6").groupby('order_no').agg({'Firs6ever15':'max','Firs6ever30':'max','is_dq':'min'})
df_Firs6 = df_Firs6.query("is_dq==1").reset_index() # 满足六期账龄要求
df_Firs6.head()
print(df_Firs6.shape)
print(df_Firs6['Firs6ever15'].value_counts())
print(df_Firs6['Firs6ever30'].value_counts())


# ## 形成订单层次的大宽表

# In[66]:


df_order_base = pd.merge(df_order, df_loan_amt,how='left',on='order_no')
df_order_base = pd.merge(df_order_base, df_Firs3, how='left', on='order_no')
df_order_base = pd.merge(df_order_base, df_Firs6, how='left', on='order_no')
df_order_base.info(null_counts=True)
df_order_base.head()


# In[67]:


cols = ['user_id','id_no_des','order_no','apply_date','order_status','loan_period','channel_id','loan_amount_x',
         'lending_time','loan_amount_y','loan_rate','total_periods',
         'Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30']
df_order_base = df_order_base[cols]
df_order_base.info()
df_order_base.head()


# In[68]:


df_order_base['apply_month'] = df_order_base['apply_date'].str[0:7]
df_order_base['apply_month'].head()


# In[69]:


xx0 = df_order_base.query("order_status==6").groupby('channel_id')['order_no'].count()

xx1 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_channel = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_channel


# In[70]:


xx0 = df_order_base.query("order_status==6").groupby('apply_month')['order_no'].count()

xx1 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_months = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_months


# In[71]:


df_channel.to_excel(r'.\result\df_channel.xlsx')
df_months.to_excel(r'.\result\df_months.xlsx')


# ### 保存数据

# In[73]:


df_order_base.to_pickle(r'.\result\df_order_base.pkl')
df_order.to_pickle(r'.\result\df_order.pkl')
df_auth.to_pickle(r'.\result\df_auth.pkl')
df_repay.to_pickle(r'.\result\df_repay.pkl')
df_loan_amt.to_pickle(r'.\result\df_loan_amt.pkl')
df_Firs3.to_pickle(r'.\result\df_Firs3.pkl')
df_Firs6.to_pickle(r'.\result\df_Firs6.pkl')


# In[1]:


df_order_base = pd.read_pickle(r'.\result\df_order_base.pkl')
df_order_base.info(null_counts=True)
df_order_base.head()


# ## 三方数据匹配率

# ### 读取授信数据

# In[3]:


df_auth_base = pd.read_pickle(r'D:\liuyedao\result\df_auth.pkl')
# df_auth_base = df_auth_base[['order_no','id_no_des','apply_date']]
# df_auth_base = df_auth_base[df_auth_base['auth_status']==6]
# df_auth_base = df_auth_base.reset_index(drop=True)
df_auth_base.info(show_counts=True)
df_auth_base.head()


# In[4]:


print(df_auth_base['user_id'].nunique(),df_auth_base['id_no_des'].nunique(), df_auth_base['order_no'].nunique(), df_auth_base.shape)


# In[5]:


df_auth_base['apply_date'].min()


# ### 百行数据

# In[5]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_baihang_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_baihang_1.csv') 
df_baihang = pd.concat([data_2, data_other_2], axis=0) # 百行数据
del data_2,data_other_2
gc.collect()
df_baihang.info(null_counts=True)
df_baihang.head()


# In[8]:


# print(df_baihang['id_no_des'].nunique(), df_baihang['order_no'].nunique(), df_baihang.shape)


# In[9]:


# df_baihang['return_massage'].value_counts()


# In[8]:


df_baihang = df_baihang[df_baihang['return_massage']=='请求成功']
print(df_baihang['order_no'].nunique(), df_baihang.shape)


# In[9]:


df_baihang = df_baihang[df_baihang['create_time']>='2022-05-01']
print(df_baihang['order_no'].nunique(), df_baihang.shape)


# In[10]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, df_baihang, cols_right, needcols=needcols, suffix='_baihang')
df1_new.info()
df1_new.head()


# In[16]:


df1_new.to_pickle(r'.\result\auth_baihang_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[11]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])


# In[12]:


# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)


# In[13]:


df_auth_base[df_auth_base['order_no'] == 'auth_118237273420230710233959']


# In[15]:


print(df1_new['order_no'].nunique(),df1_new.shape)


# In[37]:


# df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], df_baihang[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
# df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
# df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
# df1['days'] = df1['apply_date'] - df1['create_time']
# df1['days'] = df1['days'].dt.days
# df1.info(null_counts=True)
# df1.head(3)


# In[38]:


# # 添加标签
# def map_func(x, y):
#     if x==y:
#         return 1
#     elif pd.notnull(x) and pd.notnull(y):
#         return 0
#     else:
#         return -1

# df1['order_no_is_equal'] = df1[['order_no_x','order_no_y']].apply(lambda x:map_func(*x),axis=1)

# df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]


# In[39]:


# df1['order_no_is_equal'].value_counts(dropna=False)


# In[40]:


# df1_part1 = df1.query("order_no_is_equal==1")
# df1_part1.info(null_counts=True)
# print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[41]:


# df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[42]:


# df1_part2 = df1.query("order_no_is_equal==0")
# df1_part2 = df1_part2.query("days<=30 & days>=0")
# df1_part2.info(null_counts=True)
# print(df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[43]:


# df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print( df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[46]:


# # 合并数据
# df1_new = pd.concat([df1_part1, df1_part2], axis=0)
# df1_new.info(null_counts=True)
# print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[47]:


# df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[48]:


# df1_new.rename(columns={'order_no_x':'order_no'},inplace=True)


# In[49]:


# df1_new.info()


# ### 融360

# In[18]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_rong360_4.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_rong360_4.csv') 
df_rong360 = pd.concat([data_2, data_other_2], axis=0) 
del data_2,data_other_2
gc.collect()
df_rong360.info(null_counts=True)
df_rong360.head()
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[60]:


# df_rong360.groupby(['return_massage','value_001'])['order_no'].count().unstack()


# In[55]:


# df_rong360['return_massage'].value_counts()


# In[19]:


df_rong360 = df_rong360[df_rong360['return_massage'].isin(['请求成功','查询成功'])]
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[20]:


df_rong360 = df_rong360[df_rong360['create_time']>='2022-05-01']
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[25]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, df_rong360, cols_right, needcols=needcols, suffix='_rong360')
df1_new.info()
df1_new.head()


# In[26]:


df1_new.to_pickle(r'.\result\auth_rong360_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[27]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])


# In[28]:


# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)


# In[29]:


print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# In[61]:


# df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], df_rong360[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
# df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
# df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
# df1['days'] = df1['apply_date'] - df1['create_time']
# df1['days'] = df1['days'].dt.days
# df1.info(null_counts=True)
# df1.head(3)


# In[62]:


# df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
# df1['order_no_is_equal'].value_counts(dropna=False)


# In[63]:


# df1_part1 = df1.query("order_no_is_equal==1")
# print(df1_part1['order_no_x'].nunique(), df1_part1.shape)

# df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[64]:


# df1_part2 = df1.query("order_no_is_equal==0")
# df1_part2 = df1_part2.query("days<=30 & days>=0")
# print(df1_part2['order_no_x'].nunique(), df1_part2.shape)

# df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print( df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[66]:


# # 合并数据
# df1_new = pd.concat([df1_part1, df1_part2], axis=0)
# print( df1_new['order_no_x'].nunique(), df1_new.shape)

# df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
# print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[67]:


# usecols = ['order_no_x','order_no_y','create_time','model_score_01','order_no_is_equal']
# df1_new = df1_new[usecols]

# cols = []
# for col in usecols[1:]:
#     col = col+'_rong360'
#     cols.append(col)
# cols = ['order_no'] + cols
# print(cols)

# df1_new.columns = cols
# df1_new.info()


# ### 腾讯数据

# In[30]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_tengxun_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_tengxun_1.csv') 
df_tengxun = pd.concat([data_2, data_other_2], axis=0) # 百行数据
del data_2,data_other_2
gc.collect()
df_tengxun.info(null_counts=True)
df_tengxun.head()


# In[71]:


# print(df_tengxun['order_no'].nunique(), df_tengxun.shape)


# In[72]:


# df_tengxun.groupby(['return_massage','value_005'])['order_no'].count().unstack()


# In[73]:


# df_tengxun.groupby(['value_001','value_002'])['order_no'].count().unstack()


# In[34]:


df_tengxun['return_massage'].value_counts()


# In[36]:


df_tengxun['create_time'].min()


# In[39]:


del df_baihang,df_rong360


# In[40]:


gc.collect()


# In[41]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, df_tengxun, cols_right, needcols=needcols, suffix='_tengxun')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)


# In[42]:


df1_new.to_pickle(r'.\result\auth_tengxun_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[43]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])


# In[44]:


# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)
print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# ### 信用算力

# In[6]:


df_baihang = pd.read_pickle(r'.\result\auth_baihang_20230717.pkl')
df_rong360 = pd.read_pickle(r'.\result\auth_rong360_20230717.pkl')
df_tengxun = pd.read_pickle(r'.\result\auth_tengxun_20230717.pkl')


# In[7]:


df_auth_base = pd.merge(df_auth_base, df_baihang, how='left',on='order_no')
df_auth_base = pd.merge(df_auth_base, df_rong360, how='left',on='order_no')
df_auth_base = pd.merge(df_auth_base, df_tengxun, how='left',on='order_no')
df_auth_base.info()


# In[8]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_xinyongsuanli_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_xinyongsuanli_1.csv') 
df_xingyongsuanli = pd.concat([data_2, data_other_2], axis=0)
del data_2,data_other_2
gc.collect()
df_xingyongsuanli.info(null_counts=True)
df_xingyongsuanli.head()


# In[8]:


# print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[9]:


# df_xingyongsuanli.groupby(['return_massage','value_003'])['order_no'].count().unstack()


# In[9]:


df_xingyongsuanli = df_xingyongsuanli[df_xingyongsuanli['return_massage'].isin(['请求成功','处理成功'])]
print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[10]:


df_xingyongsuanli['create_time'].min()


# In[11]:


df_xingyongsuanli = df_xingyongsuanli[df_xingyongsuanli['create_time']>='2022-05-01']
print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[12]:


df_xingyongsuanli['value_002'].value_counts(dropna=False)


# In[14]:


df_xingyongsuanli[df_xingyongsuanli['value_002']==4.0]['order_no'].nunique()


# In[15]:


gc.collect()


# In[16]:


for name, group_df in df_xingyongsuanli.groupby(['value_002']):
    print(int(name))
    df1 = pd.merge(df_auth_base[['order_no','id_no_des','apply_date']], group_df[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
    df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
    df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
    df1['days'] = df1['apply_date'] - df1['create_time']
    df1['days'] = df1['days'].dt.days
    
    df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
    df1['order_no_is_equal'].value_counts(dropna=False)
    # 拆分数据
    df1_part1 = df1.query("order_no_is_equal==1")
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part2 = df1.query("order_no_is_equal==0")
    df1_part2 = df1_part2.query("days<=30 & days>=0")
    print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
    df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_part2['order_no_x'].nunique(), df1_part2.shape)
    
    # 合并数据
    df1_new = pd.concat([df1_part1, df1_part2], axis=0)
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    
    df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal','create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_new['order_no_x'].nunique(), df1_new.shape)

    usecols = ['order_no_x','order_no_y','create_time','order_no_is_equal','model_score_01']
    df1_new = df1_new[usecols]

    cols = []
    for col in usecols[1:]:
        col = col+'_xysl_' + str(int(name))
        cols.append(col)
    cols = ['order_no'] + cols
    print(cols)

    df1_new.columns = cols
    df1_new.info()
    
    data1 = 'df1_new_{}'.format(int(name))
    globals()[data1] = df1_new.copy()
    del df1, df1_part1, df1_new, df1_part2
    



# In[17]:


print(df1_new_1.shape,df1_new_2.shape,df1_new_3.shape,df1_new_4.shape)


# In[18]:


print(df1_new_1.shape[0]/df_auth_base.shape[0])
print(df1_new_2.shape[0]/df_auth_base.shape[0])
print(df1_new_3.shape[0]/df_auth_base.shape[0])
print(df1_new_4.shape[0]/df_auth_base.shape[0])


# In[20]:


# 关联base
for tmp_df in [df1_new_1,df1_new_2,df1_new_3,df1_new_4]:
    df_auth_base = pd.merge(df_auth_base, tmp_df, how='left',on='order_no')
    print(df_auth_base.shape)


# ### 保存数据

# In[22]:


df_auth_base.shape


# In[23]:


df_auth_base.to_pickle(r'.\result\df_auth_base_three_party_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# ### 计算上述四种数据的匹配率 

# In[24]:


df_auth_base.info()


# In[28]:


df_auth_base.notnull().sum()


# In[46]:


cols = ['order_no']+list(df_auth_base.columns[df_auth_base.columns.str.contains('order_no_y|model_score')])
cols


# In[47]:


channel = df_auth_base.groupby(by=['channel_id','auth_status'])[cols].count().unstack()
channel                                        


# In[48]:


df_auth_base['apply_month'] = df_auth_base['apply_date'].str[0:7]
year_month = df_auth_base.groupby(by=['apply_month','auth_status'])[cols].count().unstack()
# for col in list(year_month.columns)[1:]:
#     year_month[col+'_pct'] = year_month[col]/year_month['order_no']
year_month


# In[49]:


writer=pd.ExcelWriter(r".\result\三方数据匹配_授信_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
channel.to_excel(writer,sheet_name='渠道')
year_month.to_excel(writer,sheet_name='申请年月')
writer.save()


# ### 百融数据1

# In[3]:


df_auth_base_1 = pd.read_pickle(r'.\result\df_auth.pkl')


# In[28]:


df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20220501.csv')
# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20220701.csv')
# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20221001.csv')
# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_other_sub_bairong_5.csv')
# df_sub_bairong = pd.concat([data_0, data_1, data_2], axis=0)
# del data_0, data_1, data_2
# gc.collect()
# print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)
# df_sub_bairong.head()


# In[29]:


# df_sub_bairong.dtypes


# In[30]:


# df_sub_bairong['return_massage'].value_counts(dropna=False)


# In[31]:


# df_sub_bairong['model_score_01'].describe()


# In[32]:


# df_sub_bairong.isnull().sum()


# In[33]:


# df_sub_bairong['value_001'].head()


# In[34]:


# df_sub_bairong['value_002'].head()


# In[29]:


df_sub_bairong = df_sub_bairong[df_sub_bairong['return_massage']=='请求成功']
print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)

# df_sub_bairong = df_sub_bairong[df_sub_bairong['create_time']>='2022-05-01']
# print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[30]:


df_sub_bairong['create_time'].min()


# In[31]:


needcols = list(df_sub_bairong.columns[df_sub_bairong.columns.str.contains('value_')])[1:]
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base_1, cols_left, df_sub_bairong, cols_right, needcols=needcols, suffix='_sub_bairong')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)


# In[ ]:





# In[32]:


df1_new.to_csv(r'.\result\auth_167_sub_bairong_5_20220501.csv',index=False)


# In[33]:


del df_sub_bairong
gc.collect()


# In[34]:


# 合并数据
df_167_1 = pd.read_csv(r'.\result\auth_167_sub_bairong_5_20220501.csv')
df_167_2 = pd.read_csv(r'.\result\auth_167_sub_bairong_5_20220701.csv')
df_167_3 = pd.read_csv(r'.\result\auth_167_sub_bairong_5_20221001.csv')
df_other = pd.read_csv(r'.\result\auth_other_sub_bairong_5.csv')
df_sub_bairong = pd.concat([df_167_1,df_167_2,df_167_3,df_other],axis=0)
df_sub_bairong.info()
df_sub_bairong.head()


# In[37]:


# cols_left = ['order_no','id_no_des','apply_date']
# cols_right = ['order_no','id_no_des','create_time'] + list(df_sub_bairong.columns[df_sub_bairong.columns.str.contains('value_')])[1:]

# df1_new = process_data(df_order_base_1, cols_left, df_sub_bairong, cols_right)
# df1_new.info()
# df1_new.head()


# In[35]:


# 查看是否有重复数据
print(df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[36]:


df_sub_bairong['order_no'].value_counts()


# In[37]:


df_auth_base_1[df_auth_base_1['order_no']=='auth_115225524120220730142106']


# In[39]:


df_sub_bairong[df_sub_bairong['order_no']=='auth_115225524120220730142106']


# In[40]:


df_sub_bairong = df_sub_bairong.sort_values(by=['order_no','order_no_is_equal_sub_bairong','create_time_sub_bairong'], ascending=False)
df_sub_bairong = df_sub_bairong.drop_duplicates(subset=['order_no'],keep='first')


# In[41]:


# 查看是否有重复数据
print(df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[65]:


# float_cols = df_sub_bairong.select_dtypes(include=['float']).columns
# for col in float_cols:
#     df_sub_bairong[col].fillna(-9,inplace=True)
#     df_sub_bairong[col] = df_sub_bairong[col].apply(int)   


# In[42]:


df_sub_bairong.info()


# In[43]:


df_sub_bairong.to_pickle(r'.\result\auth_sub_bairong_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[45]:


# 覆盖率
print(df_sub_bairong.shape[0]/df_auth_base_1.shape[0])


# In[55]:


# 关联数据
recall = df_sub_bairong.notnull().sum()/df_auth_base_1.shape[0]
recall = pd.DataFrame(recall)
recall = recall.reset_index()
recall.columns = ['varsname','recall']
recall.to_excel(r'.\result\三方数据匹配_sub_bairong.xlsx')
# df_auth_base_1 = pd.merge(df_auth_base_1, df_sub_bairong, how='left',on='order_no')
# print(df_auth_base_1.shape)


# In[ ]:


writer=pd.ExcelWriter(r".\result\三方数据匹配_授信_sub_bairong_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
recall.to_excel(writer,sheet_name='sub_bairong')
writer.save()


# In[ ]:


df_auth_base_1.to_pickle(r'.\result\df_auth_base_sub_bairong_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# ### 百融数据2

# In[ ]:


# df_bairong_123 = pd.read_csv(r'D:\juzi\0712\dwd_beforeloan_third_combine_id_167_bairong_1_20221101.csv',usecols=['creat_time']) #完成
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201.csv',error_bad_lines=False)
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230101.csv') #完成
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230201.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230301.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20221101.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20230301.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20230501.csv')
# df_bairong = pd.concat([data_0, data_1, data_2], axis=0)
# del data_0, data_1, data_2
# gc.collect()
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)
df_bairong.info()
df_bairong.head()


# In[113]:


# df_bairong.dtypes


# In[115]:


# df_bairong['return_massage'].value_counts(dropna=False)


# In[116]:


# df_bairong['model_score_01'].describe()


# In[117]:


# df_bairong['value_001'].head()


# In[118]:


# df_bairong = df_bairong[df_bairong['return_massage']=='请求成功']
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)
# df_bairong = df_bairong[df_bairong['create_time']>='2022-05-01']
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)


# In[11]:


def process_data_bairong(df_left, cols_left, df_right, cols_right):
    df1 = pd.merge(df_left[cols_left], df_right[cols_right], how='inner', on='id_no_des')
    df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
    df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
    df1['days'] = df1['apply_date'] - df1['create_time']
    df1['days'] = df1['days'].dt.days
    df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
    
    # 拆分数据
    print('-----------拆分数据------------------')
    df1_part1 = df1.query("order_no_is_equal==1")
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part2 = df1.query("order_no_is_equal==0")
    df1_part2 = df1_part2.query("days<=30 & days>=0")
    print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
    df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_part2['order_no_x'].nunique(), df1_part2.shape)
    
    # 合并数据
    print('-----------合并数据------------------')
    df1_new = pd.concat([df1_part1, df1_part2], axis=0)
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal','create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    
    # 重新命名字段
    usecols = ['order_no_x','apply_date','order_no_y','create_time','order_no_is_equal'] + cols_right[3:]
    df1_new = df1_new[usecols]

    cols = []
    for col in usecols[1:]:
        col = col+'_bairong'
        cols.append(col)
    cols = ['order_no'] + cols
    df1_new.columns = cols
    
    return df1_new
  

def chunk_process_data(df_auth_base, date, path, engine='c', chunk_size=100000):
    df_chunks = pd.read_csv(r'D:\juzi\{}\{}'.format(date, path), engine=engine, chunksize=chunk_size)
    result_df = pd.DataFrame() 
    for i, chunk in enumerate(df_chunks):
        # df_bairong = df_bairong[df_bairong['return_massage']=='请求成功']
#         df_bairong = chunk[chunk['create_time']>='2022-05-01']
        print(chunk.shape)
        cols_left = ['order_no','id_no_des','apply_date']
        cols_right = ['order_no','id_no_des','create_time'] + list(chunk.columns[chunk.columns.str.contains('value_')])

        df1_new = process_data_bairong(df_auth_base, cols_left, chunk, cols_right)
        result_df = pd.concat([result_df, df1_new], axis=0)
        print('-----处理完分块数据：{}-------'.format(i))
        
        del chunk
    
    print('-----处理完数据：{}-----------------'.format(result_df.shape))
    result_df = result_df.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
    result_df = result_df.drop_duplicates(subset=['order_no'],keep='first')
    print('-----去重后数据：{}-----------------'.format(result_df.shape))
    
    return result_df


# In[22]:


# 获取167文件下的所有csv文件
file_dir = r'D:\juzi\0725'
file_name_167 = get_filename(file_dir)
file_name_167


# In[13]:


# 读取文件夹下的csv文件数据
for i, path in enumerate(file_name_167):
    if '_bairong_1' in path:
        print('-----{}---{}------'.format(i, path))
        df1_new = chunk_process_data(df_auth_base, '0725', path, engine='c', chunk_size=500000)
        df1_new.to_csv(r'd:\liuyedao\result\auth_{}.csv'.format(path[32:-4]),index=False)
        del df1_new
        gc.collect()
        print('------------------------完成数据存储--------------------------')
    else:
        print('-------非目标文件:{}-----------'.format(path))


# In[4]:


usecols1 = ['value_00' + str(i) + '_bairong' for i in range(1,10)]
usecols2 = ['value_0' + str(i) + '_bairong' for i in range(10,100) if i!=89]
usecols = ['order_no','apply_date_bairong','order_no_y_bairong','create_time_bairong','order_no_is_equal_bairong'] + usecols1 + usecols2
print(usecols)


# In[16]:


# 获取167文件下的所有csv文件
file_dir = r'D:\liuyedao\result'
file_name_167 = get_filename(file_dir)
print(file_name_167)


# In[20]:


# data_list = []
# for index, filename in enumerate(file_name_167):
#     if '_bairong_1' in filename:
#         var = 'data_{}'.format(index)
#         globals()[var] = pd.read_csv(r'd:\liuyedao\result\{}'.format(filename), usecols=usecols)  
#         data_list.append(globals()[var])
#     else:
#         print('------------------非目标文件------------------')
# df_bairong = pd.concat(data_list, axis=0)  
# df_bairong.info()


# In[5]:


df_167_01 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220501.csv', usecols=usecols)
df_167_02 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220515.csv', usecols=usecols)
df_167_03 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220601.csv', usecols=usecols)
df_167_04 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220701.csv', usecols=usecols)
df_167_05 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220801.csv', usecols=usecols)
df_167_06 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20220901.csv', usecols=usecols)
df_167_07 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20221001.csv', usecols=usecols)

df_167_1 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20221101.csv', usecols=usecols)
df_167_2 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20221201.csv', usecols=usecols)
df_167_3 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20230101.csv', usecols=usecols)
df_167_4 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20230201.csv', usecols=usecols)
df_167_5 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20230301.csv', usecols=usecols)
df_167_6 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20230501.csv', usecols=usecols)
df_167_7 = pd.read_csv(r'd:\liuyedao\result\auth_167_bairong_1_20230601.csv', usecols=usecols)

df_bairong_167 = pd.concat([df_167_01, df_167_02, df_167_03, df_167_04, df_167_05, df_167_06, df_167_07, df_167_1,df_167_2,df_167_3,df_167_4,df_167_5,df_167_6,df_167_7],axis=0)
df_bairong_167.info()
df_bairong_167.head()
print(df_bairong_167.shape, df_bairong_167['order_no'].nunique())


# In[6]:


df_174_01 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20220501.csv', usecols=usecols)
df_174_03 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20220601.csv', usecols=usecols)
df_174_04 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20220701.csv', usecols=usecols)
df_174_05 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20220801.csv', usecols=usecols)
df_174_06 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20220901.csv', usecols=usecols)
df_174_07 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20221001.csv', usecols=usecols)

df_174_1 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20221101.csv', usecols=usecols)
df_174_2 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20230301.csv', usecols=usecols)
df_174_3 = pd.read_csv(r'd:\liuyedao\result\auth_174_bairong_1_20230501.csv', usecols=usecols)

df_bairong_174 = pd.concat([df_174_01, df_174_03, df_174_04, df_174_05, df_174_06, df_174_07, df_174_1,df_174_2,df_174_3],axis=0)
df_bairong_174.info()
df_bairong_174.head()
# 查看是否有重复数据
print(df_bairong_174['order_no'].nunique(), df_bairong_174.shape)


# In[7]:


df_other_1 = pd.read_csv(r'd:\liuyedao\result\auth_80008_bairong_1_202306.csv', usecols=usecols)
df_other_2 = pd.read_csv(r'd:\liuyedao\result\auth_other_bairong_1_202205.csv', usecols=usecols)
df_other_3 = pd.read_csv(r'd:\liuyedao\result\auth_other_bairong_1_202211.csv', usecols=usecols)
df_other_4 = pd.read_csv(r'd:\liuyedao\result\auth_other_bairong_1_202301.csv', usecols=usecols)
df_other_5 = pd.read_csv(r'd:\liuyedao\result\auth_other_bairong_1_202303.csv', usecols=usecols)
df_other_6 = pd.read_csv(r'd:\liuyedao\result\auth_other_bairong_1_202305.csv', usecols=usecols)

df_bairong_other = pd.concat([df_other_1,df_other_2,df_other_3,df_other_4,df_other_5,df_other_6],axis=0)
df_bairong_other.info()
df_bairong_other.head()
# 查看是否有重复数据
print(df_bairong_other['order_no'].nunique(), df_bairong_other.shape)


# In[8]:


df_bairong = pd.concat([df_bairong_167,df_bairong_174,df_bairong_other],axis=0)
# df_bairong = pd.concat([df_bairong_167,df_bairong_174],axis=0)
df_bairong.info()
df_bairong.head()
# 查看是否有重复数据
print(df_bairong['order_no'].nunique(), df_bairong.shape)


# In[9]:


# 除掉重复数据
df_bairong = df_bairong.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
df_bairong = df_bairong.drop_duplicates(subset=['order_no'],keep='first')
# 查看是否有重复数据
print(df_bairong['order_no'].nunique(), df_bairong.shape)


# In[10]:


# df_bairong = df_bairong.reset_index(drop=True)
df_bairong.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('bairong_1', str(datetime.today())[:10].replace('-','')))


# In[32]:


del df_other_1,df_other_2,df_other_3,df_other_4,df_other_5,df_other_6
del df_174_01, df_174_03, df_174_04, df_174_05, df_174_06, df_174_07, df_174_1,df_174_2,df_174_3
del df_167_01, df_167_02, df_167_03, df_167_04, df_167_05, df_167_06, df_167_07, df_167_1,df_167_2,df_167_3,df_167_4,df_167_5,df_167_6,df_167_7

gc.collect()


# In[27]:


# 关联数据
recall = pd.DataFrame(df_bairong.notnull().sum())
recall = recall.reset_index()
recall.columns = ['varsname','notnull']
recall['recall'] = recall['notnull']/df_auth_base.shape[0]

recall.to_excel(r'd:\liuyedao\result\统计数据\auth_三方数据匹配_bairong.xlsx')


# In[11]:


# # 匹配关联
# df_auth_base = pd.merge(df_auth_base, df_bairong, how='left',on='order_no')
# df_auth_base.head(3)
# print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# ### 获取数据文件-匹配新增数据

# In[6]:


# 获取0717文件下的所有csv文件
file_dir = r'D:\juzi\0717'
file_name_0717 = get_filename(file_dir)
file_name_0717


# In[7]:


# 获取0719文件下的所有csv文件
file_dir = r'D:\juzi\0719'
file_name_0719 = get_filename(file_dir)
file_name_0719


# In[ ]:





# In[10]:


df_other_all = pd.read_csv(r'D:\juzi\0717\dwd_beforeloan_third_combine_id_other_all_all.csv')


# In[11]:


df_other_all.info()
df_other_all.head()


# In[12]:


df_other_all['ds'].value_counts()


# In[13]:


df_other_all['channel_id'].value_counts()


# ### 天创信用

# In[26]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0719):
    if 'tianchuang_1' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0719\{}'.format(iterm))
        data_list.append(globals()[data])


# In[27]:


print('列表长度：', len(data_list))
tianchuang_other = df_other_all.query("ds=='tianchuang_1'")
data_list.append(tianchuang_other)
print('列表长度：', len(data_list))


# In[29]:


tianchuang_other.info()


# In[28]:


tianchuang = pd.concat(data_list, axis=0)
tianchuang.info(null_counts=True)
tianchuang.head()


# In[30]:


tianchuang['return_massage'].value_counts(dropna=False)


# In[31]:


tianchuang['value_004'].value_counts(dropna=False)


# In[32]:


tianchuang['create_time'].min()


# In[33]:


tianchuang = tianchuang[tianchuang['return_massage']=='请求成功']
print(tianchuang['order_no'].nunique(), tianchuang.shape)

# tianchuang = tianchuang[tianchuang['create_time']>='2022-05-01']
# print(tianchuang['order_no'].nunique(), tianchuang.shape)


# In[34]:


tianchuang['value_001'].value_counts(dropna=False)


# In[35]:


tianchuang_part_q = tianchuang.query("value_001=='LBMQ150101'")
print(tianchuang_part_q['order_no'].nunique(), tianchuang_part_q.shape)

tianchuang_part_r = tianchuang.query("value_001=='LBMR150101'")
print(tianchuang_part_r['order_no'].nunique(), tianchuang_part_r.shape)


# In[36]:


tianchuang = pd.merge(tianchuang_part_q, tianchuang_part_r, how='outer',on=['order_no','id_no_des', 'channel_id','create_time'])
tianchuang.info(null_counts=True)


# In[39]:


needcols = ['model_score_01_x', 'model_score_01_y']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, tianchuang, cols_right, needcols=needcols, suffix='_tianchuang')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'D:\liuyedao\result\auth_{}_{}.pkl'.format('tianchuang_1', str(datetime.today())[:10].replace('-','')))


# In[41]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])

# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)
print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# ### 孚临

# In[42]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0717):
    if 'fulin_1' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0717\{}'.format(iterm))
        data_list.append(globals()[data])


# In[43]:


print('列表长度：', len(data_list))
fulin_other = df_other_all.query("ds=='fulin_1'")
print(fulin_other.shape)
data_list.append(fulin_other)
print('列表长度：', len(data_list))


# In[44]:


fulin = pd.concat(data_list, axis=0)
fulin.info(null_counts=True)
fulin.head()


# In[45]:


fulin['return_massage'].value_counts(dropna=False)


# In[46]:


fulin['return_code'].value_counts(dropna=False)


# In[47]:


fulin['create_time'].min()


# In[48]:


fulin = fulin[fulin['return_massage'].isin(['success','查询成功'])]
print(fulin['order_no'].nunique(), fulin.shape)

# fulin = fulin[fulin['create_time']>='2022-05-01']
# print(fulin['order_no'].nunique(), fulin.shape)


# In[49]:


fulin.groupby(['value_001','return_massage'])['order_no'].count().unstack()


# In[50]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, fulin, cols_right, needcols=needcols, suffix='_fulin')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)


# In[52]:


df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('fulin_1', str(datetime.today())[:10].replace('-','')))


# In[53]:


# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)
print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# ### 同盾智信分

# In[10]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0717):
    if 'tongdun_2' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0717\{}'.format(iterm))
        data_list.append(globals()[data])


# In[11]:


print('列表长度：', len(data_list))
tongdun_other = df_other_all.query("ds=='tongdun_2'")
print(tongdun_other.shape)
data_list.append(tongdun_other)
print('列表长度：', len(data_list))


# In[12]:


tongdun = pd.concat(data_list, axis=0)
tongdun.info(null_counts=True)
tongdun.head()


# In[13]:


tongdun['return_code'].value_counts(dropna=False)


# In[14]:


tongdun['value_005'].value_counts(dropna=False)


# In[15]:


tongdun = tongdun[tongdun['return_code']==1.0]
print(tongdun.shape)


# In[16]:


tongdun['create_time'].min()


# In[17]:


tongdun['value_002'].value_counts(dropna=False)


# In[ ]:


# tongdun['return_massage'].value_counts(dropna=False)
# tongdun = tongdun[tongdun['return_massage']=='请求成功']
# print(tongdun['order_no'].nunique(), tongdun.shape)

# tongdun = tongdun[tongdun['create_time']>='2022-05-01']
# print(tongdun['order_no'].nunique(), tongdun.shape)


# In[18]:


tongdun_part_1 = tongdun.query("value_002=='27984162d86f3baf'")
print(tongdun_part_1['order_no'].nunique(), tongdun_part_1.shape)

tongdun_part_2 = tongdun.query("value_002=='3a9de0313a0c65ee'")
print(tongdun_part_2['order_no'].nunique(), tongdun_part_2.shape)

tongdun = pd.merge(tongdun_part_1, tongdun_part_2, how='outer',on=['order_no','id_no_des','user_id','channel_id','create_time'])
tongdun.info(null_counts=True)


# In[19]:


tongdun.rename(columns={'model_score_01_x':'model_score_01_zr', 'model_score_01_y':'model_score_01_zx'},inplace=True)


# In[20]:


tongdun.columns


# In[22]:


needcols = ['model_score_01_zr','model_score_01_zx']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, tongdun, cols_right, needcols=needcols, suffix='_tongdun')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('tongdun_2', str(datetime.today())[:10].replace('-','')))


# ### 朴道-在网时长

# In[28]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0719):
    if 'pudao_6' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0719\{}'.format(iterm))
        data_list.append(globals()[data])


# In[29]:


print('列表长度：', len(data_list))
pudao_other = df_other_all.query("ds=='pudao_6'")
print(pudao_other.shape)
data_list.append(pudao_other)
print('列表长度：', len(data_list))


# In[30]:


pudao_6 = pd.concat(data_list,axis=0)
pudao_6.info(null_counts=True)
pudao_6.head()


# In[31]:


pudao_6['value_014'].value_counts(dropna=False)


# In[32]:


pudao_6['value_011'].value_counts(dropna=False)


# In[33]:


pudao_6['create_time'].min()


# In[34]:


pudao_6 = pudao_6[pudao_6['value_014']==0.0]
print(pudao_6['order_no'].nunique(), pudao_6.shape)


# In[35]:


needcols = ['value_012']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, pudao_6, cols_right, needcols=needcols, suffix='_pudao_6')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('pudao_6', str(datetime.today())[:10].replace('-','')))


# ### 朴道-在网状态 

# In[36]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0719):
    if 'pudao_7' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0719\{}'.format(iterm))
        data_list.append(globals()[data])


# In[37]:


print('列表长度：', len(data_list))
pudao_other = df_other_all.query("ds=='pudao_7'")
print(pudao_other.shape)
data_list.append(pudao_other)
print('列表长度：', len(data_list))


# In[38]:


pudao_7 = pd.concat(data_list,axis=0)
pudao_7.info(null_counts=True)
pudao_7.head()


# In[39]:


pudao_7['value_014'].value_counts(dropna=False)


# In[40]:


pudao_7['value_011'].value_counts(dropna=False)


# In[41]:


pudao_7['create_time'].min()


# In[42]:


pudao_7 = pudao_7[pudao_7['value_014']==0.0]


# In[ ]:





# In[43]:


needcols = ['value_012']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, pudao_7, cols_right, needcols=needcols, suffix='_pudao_7')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('pudao_7', str(datetime.today())[:10].replace('-','')))


# ### Fico分

# In[44]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0719):
    if 'ruizhi_6' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0719\{}'.format(iterm))
        data_list.append(globals()[data])


# In[45]:


print('列表长度：', len(data_list))
ruizhi_other = df_other_all.query("ds=='ruizhi_6'")
data_list.append(ruizhi_other)
print('列表长度：', len(data_list))


# In[46]:


ruizhi_6 = pd.concat(data_list,axis=0)
ruizhi_6.info(null_counts=True)
ruizhi_6.head()


# In[48]:


ruizhi_6['return_massage'].value_counts(dropna=False)


# In[49]:


ruizhi_6['create_time'].min()


# In[50]:


ruizhi_6 = ruizhi_6[ruizhi_6['return_massage']=='调用成功']
ruizhi_6.shape


# In[52]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, ruizhi_6, cols_right, needcols=needcols, suffix='_ruizhi_6')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('ruizhi_6', str(datetime.today())[:10].replace('-','')))


# ### 百融评分模型

# In[8]:


# 读取文件夹下的csv文件数据
data_list = []
for i, iterm in enumerate(file_name_0717):
    if 'moxingfen_14' in iterm:
        print('-----{}---{}------'.format(i, iterm))
        data = 'data_{}'.format(i)
        globals()[data] = pd.read_csv(r'D:\juzi\0717\{}'.format(iterm))
        data_list.append(globals()[data])



# In[14]:


print('列表长度：', len(data_list))
moxingfen_other = df_other_all.query("ds=='moxingfen_14'")
data_list.append(moxingfen_other)
print('列表长度：', len(data_list))


# In[15]:


moxingfen_other.shape


# In[16]:


moxingfen = pd.concat(data_list,axis=0)
moxingfen.info(null_counts=True)
moxingfen.head()


# In[17]:


moxingfen['create_time'].min()


# In[20]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, moxingfen, cols_right, needcols=needcols, suffix='_moxingfen_14')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)

df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('moxingfen_14', str(datetime.today())[:10].replace('-','')))


# In[22]:


gc.collect()


# ### 百行反欺诈

# In[30]:


xx = pd.read_csv(r'D:\juzi\0720\dwd_beforeloan_third_combine_id_all_baihang_5.csv',usecols=['create_time'])
xx.shape


# In[23]:


baihang_5 = pd.read_csv(r'D:\juzi\0720\dwd_beforeloan_third_combine_id_all_baihang_5.csv')
baihang_5.info(null_counts=True)
baihang_5.head()
print(baihang_5['id_no_des'].nunique(), baihang_5['order_no'].nunique(), baihang_5.shape)


# In[24]:


baihang_5.groupby(['return_massage','value_012'])['order_no'].count().unstack()


# In[25]:


baihang_5['return_massage'].value_counts(dropna=False)


# In[26]:


baihang_5['value_012'].value_counts(dropna=False)


# In[27]:


baihang_5['create_time'].min()


# In[28]:


baihang_5 = baihang_5[baihang_5['return_massage']=='查询成功']
print(baihang_5['id_no_des'].nunique(), baihang_5['order_no'].nunique(), baihang_5.shape)
# baihang_5 = baihang_5[baihang_5['create_time']>='2022-05-01']
# print(baihang_5['id_no_des'].nunique(), baihang_5['order_no'].nunique(), baihang_5.shape)


# In[32]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, baihang_5, cols_right, needcols=needcols, suffix='_baihang_5')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)
df1_new.to_pickle(r'd:\liuyedao\result\auth_{}_{}.pkl'.format('baihang_5',str(datetime.today())[:10].replace('-','')))



# ### 朴道友盟大额分

# In[8]:


pudao_8 = pd.read_csv(r'D:\juzi\0720\dwd_beforeloan_third_combine_id_all_pudao_8.csv')
pudao_8.info(null_counts=True)
pudao_8.head()
print(pudao_8['id_no_des'].nunique(), pudao_8['order_no'].nunique(), pudao_8.shape)


# In[9]:


pudao_8['channel_id'].value_counts()


# In[11]:


pudao_8.groupby(['return_massage','value_012'])['order_no'].count().unstack()


# In[12]:


pudao_8 = pudao_8[pudao_8['return_massage']=='OK']
print(pudao_8['id_no_des'].nunique(), pudao_8['order_no'].nunique(), pudao_8.shape)


# In[13]:


pudao_8['create_time'].min()
# pudao_8 = pudao_8[pudao_8['create_time']>='2022-05-01']
# print(pudao_8['id_no_des'].nunique(), pudao_8['order_no'].nunique(), pudao_8.shape)


# In[18]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, pudao_8, cols_right, needcols=needcols, suffix='_pudao_8')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)
df1_new.to_pickle(r'D:\liuyedao\result\auth_pudao_8_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[19]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])


# In[20]:


# 匹配关联
df_auth_base = pd.merge(df_auth_base, df1_new, how='left',on='order_no')
df_auth_base.head(3)
print( df_auth_base['order_no'].nunique(), df_auth_base.shape)


# # 阿里云反欺诈分

# In[10]:


pudao_15 = pd.read_csv(r'D:\juzi\0720\dwd_beforeloan_third_combine_id_pudao_15_baihang_5.csv')
print(pudao_15.shape)
gc.collect()
pudao_15.info(null_counts=True)
pudao_15.head()
print(pudao_15['id_no_des'].nunique(), pudao_15['order_no'].nunique(), pudao_15.shape)


# In[11]:


pudao_15['ds'].value_counts()


# In[12]:


pudao_15 = pudao_15.query("ds=='pudao_15'")
pudao_15.shape


# In[13]:


pudao_15.groupby(['return_massage','value_012'])['order_no'].count().unstack()


# In[14]:


pudao_15['create_time'].min()


# In[ ]:


# pudao_15['return_massage'].value_counts()
# pudao_15 = pudao_15[pudao_15['return_massage']=='请求成功']
# print(pudao_15['id_no_des'].nunique(), pudao_15['order_no'].nunique(), pudao_15.shape)
# pudao_15 = pudao_15[pudao_15['create_time']>='2022-05-01']
# print(pudao_15['id_no_des'].nunique(), pudao_15['order_no'].nunique(), pudao_15.shape)


# In[16]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

df1_new = process_data(df_auth_base, cols_left, pudao_15, cols_right, needcols=needcols, suffix='_pudao_15')
df1_new.info()
df1_new.head()
print( df1_new['order_no'].nunique(), df1_new.shape)
df1_new.to_pickle(r'D:\liuyedao\result\auth_pudao_15_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[17]:


# 覆盖率
print(df1_new.shape[0]/df_auth_base.shape[0])


# In[ ]:





# ### 三种数据的匹配率

# In[5]:


df_auth_base.info()


# In[7]:


# df_tongdun = pd.read_pickle(r'd:liuyedao\result\auth_tongdun_20230718.pkl')
# df_fulin = pd.read_pickle(r'.\result\auth_fulin_20230718.pkl')
# df_moxingfen = pd.read_pickle(r'.\result\auth_moxingfen_20230718.pkl')
data_list = []

df_baihang = pd.read_pickle(r'd:\liuyedao\result\auth_baihang_5_20230721.pkl')
data_list.append(df_baihang)

df_tianchuang = pd.read_pickle(r'd:\liuyedao\result\auth_tianchuang_1_20230720.pkl')
data_list.append(df_tianchuang)

df_tongdun = pd.read_pickle(r'd:\liuyedao\result\auth_tongdun_2_20230720.pkl')
data_list.append(df_tongdun)

df_fulin = pd.read_pickle(r'd:\liuyedao\result\auth_fulin_1_20230720.pkl')
data_list.append(df_fulin)

df_ruizhi = pd.read_pickle(r'd:\liuyedao\result\auth_ruizhi_6_20230720.pkl')
data_list.append(df_ruizhi)

df_moxingfen = pd.read_pickle(r'd:\liuyedao\result\auth_moxingfen_14_20230720.pkl')
data_list.append(df_moxingfen)

df_pudao_8 = pd.read_pickle(r'd:\liuyedao\result\auth_pudao_8_20230720.pkl')
data_list.append(df_pudao_8)

df_pudao_6 = pd.read_pickle(r'd:\liuyedao\result\auth_pudao_7_20230720.pkl')
data_list.append(df_pudao_6)

df_pudao_7 = pd.read_pickle(r'd:\liuyedao\result\auth_pudao_6_20230720.pkl')
data_list.append(df_pudao_7)


# In[8]:


# 匹配关联

# df_auth_base = pd.merge(df_auth_base, df_fulin, how='left',on='order_no')
# print(df_auth_base.shape)

# df_auth_base = pd.merge(df_auth_base, df_tongdun, how='left',on='order_no')
# print(df_auth_base.shape)

# df_auth_base = pd.merge(df_auth_base, df_moxingfen, how='left',on='order_no')
# print(df_auth_base.shape)
for dd in data_list:
    print(df_auth_base.shape, dd.shape)
    df_auth_base = pd.merge(df_auth_base, dd, how='left',on='order_no')
    print(df_auth_base.shape)


# In[9]:


df_auth_base.info()


# In[11]:


# 保存数据
df_auth_base.to_pickle(r'd:\liuyedao\result\auth_three_party_new_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[ ]:





# In[12]:


# 需要计算匹配率的字段
cols = ['order_no']+list(df_auth_base.columns[df_auth_base.columns.str.contains('order_no_y|model_score|value')])
cols


# In[13]:


# 按渠道统计各字段的匹配情况
channel = df_auth_base.groupby(by=['channel_id','auth_status'])[cols].count().unstack()
channel = channel.append(pd.DataFrame(channel.sum(axis=0)).T)
channel  


# In[14]:


# 通过与不通过的进行加总
for col in cols:
    channel.loc[:,(col, 0)] = channel.loc[:,(col, 6)] + channel.loc[:,(col, 7)]


# In[18]:


# 计算匹配率
channel_pct = channel.copy()
for col in cols:
    channel_pct.loc[:,(col, 0)] = channel_pct.loc[:,(col, 0)] / channel.loc[:,('order_no', 0)]
    channel_pct.loc[:,(col, 6)] = channel_pct.loc[:,(col, 6)] / channel.loc[:,('order_no', 6)]
    channel_pct.loc[:,(col, 7)] = channel_pct.loc[:,(col, 7)] / channel.loc[:,('order_no', 7)]
channel_pct


# In[46]:


df_auth_base['apply_month'] = df_auth_base['apply_date'].str[0:7]


# In[47]:


# 按申请年月统计匹配情况
year_month = df_auth_base.groupby(by=['apply_month','auth_status'])[cols].count().unstack()
year_month = year_month.append(pd.DataFrame(year_month.sum(axis=0)).T)
year_month


# In[48]:


# 通过与不通过的进行加总
for col in cols:
    year_month.loc[:,(col, 0)] = year_month.loc[:,(col, 6)] + year_month.loc[:,(col, 7)]


# In[49]:


# 计算匹配率
year_month_pct = year_month.copy()
for col in cols:
    year_month_pct.loc[:,(col, 0)] = year_month_pct.loc[:,(col, 0)] / year_month.loc[:,('order_no', 0)]
    year_month_pct.loc[:,(col, 6)] = year_month_pct.loc[:,(col, 6)] / year_month.loc[:,('order_no', 6)]
    year_month_pct.loc[:,(col, 7)] = year_month_pct.loc[:,(col, 7)] / year_month.loc[:,('order_no', 7)]
year_month_pct


# In[20]:


# 保存统计的数据
writer=pd.ExcelWriter(r"d:\liuyedao\result\三方数据匹配_授信_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
channel.to_excel(writer,sheet_name='渠道')
channel_pct.to_excel(writer,sheet_name='渠道比例')

# year_month.to_excel(writer,sheet_name='申请年月')
# year_month_pct.to_excel(writer,sheet_name='申请年月比例')

writer.save()




#==============================================================================
# File: 策略的数据分析.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# ## 不区分渠道，合并数据表

# In[3]:


# 获取167文件下的所有csv文件
file_dir = r'D:\juzi\0711\167'
file_name_167 = get_filename(file_dir)
file_name_167


# In[4]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_167):
    print('-----{}---{}------'.format(i, iterm))
    data = 'data_{}'.format(i)
    globals()[data] = pd.read_csv(r'.\167\{}'.format(iterm))   


# In[6]:


# 获取other文件下的所有csv文件
file_dir = r'.\other'
file_name_other = get_filename(file_dir)
file_name_other


# In[6]:


# 读取文件夹下的csv文件数据
for i, iterm in enumerate(file_name_other):
    print('-----{}---{}------'.format(i, iterm))
    data1 = 'data_other_{}'.format(i)
    globals()[data1] = pd.read_csv(r'.\other\{}'.format(iterm))   


# ## 提款表

# In[7]:


df_order = pd.concat([data_1, data_other_1], axis=0) # 提款数据
df_order.info(null_counts=True)
df_order.head()


# In[9]:


print(df_order['order_no'].nunique(),df_order.shape)


# In[10]:


del data_1, data_other_1


# In[11]:


df_order = df_order.query("apply_date>='2022-05-01'")
# [['user_id','order_no','apply_date','order_status','loan_period','channel_id']]
print(df_order['order_no'].nunique(),df_order.shape)


# ## 授信数据表

# In[12]:


# 授信数据
df_auth = pd.concat([data_0, data_other_0], axis=0) # 授信数据
df_auth.info(null_counts=True)
df_auth.head()


# In[13]:


print(df_auth['order_no'].nunique(),df_auth.shape)


# In[14]:


del data_0, data_other_0


# In[15]:


df_auth = df_auth.query("apply_date>='2022-05-01'")
# [['user_id','order_no','apply_date','auth_status','auth_credit_amount','channel_id']]
print(df_auth['order_no'].nunique(),df_auth.shape)


# ## 还款计划表

# In[18]:


df_repay = pd.concat([data_6, data_other_5, data_other_6, data_other_7], axis=0) # 还款数据
df_repay.info(null_counts=True)
df_repay.head()


# In[19]:


print(df_repay['order_no'].nunique(),df_repay.shape)


# In[20]:


del data_6, data_other_5, data_other_6, data_other_7


# In[21]:


df_repay = df_repay.query("lending_time>='2022-05-01'")
print(df_repay['order_no'].nunique(),df_repay.shape)


# ### 获取借款金额、放款时间、借款利率

# In[23]:


# 借款金额、放款时间、借款利率/
df_loan_amt = df_repay.groupby('order_no')['lending_time','loan_amount','loan_rate','total_periods'].max()
df_loan_amt = df_loan_amt.reset_index()
df_loan_amt.info()
df_loan_amt.head()


# In[24]:


print(df_loan_amt['order_no'].nunique(),df_loan_amt.shape)


# ### Y标签

# In[50]:


# 截止目前，每期还款计划是否到期
df_repay['is_dq'] = df_repay['repay_date'].apply(lambda x: 1 if x<=datetime(2023,7,12) else 0)


# In[60]:


df_repay.info(null_counts=True)


# In[55]:


# 结清日期填充缺失值
# df_repay['period_settle_date'] = df_repay['period_settle_date_copy']
df_repay['period_settle_date'].fillna('2023-07-12',inplace=True)


# In[57]:


# 时间格式转换
df_repay['period_settle_date'] = pd.to_datetime(df_repay['period_settle_date'],format='%Y-%m-%d')
df_repay['repay_date'] = pd.to_datetime(df_repay['repay_date'], format='%Y-%m-%d')


# In[59]:


# 计算逾期天数
df_repay['yqts'] = df_repay['period_settle_date'] - df_repay['repay_date']
df_repay['yqts'] = df_repay['yqts'].dt.days
df_repay['yqts'].head()


# In[62]:


# 添加每期还款计划的标签
df_repay['Firs3ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs3ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs6ever15'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=15 else 1,df_repay['yqts'],df_repay['is_dq'])]
df_repay['Firs6ever30'] = [*map(lambda t1,t2: -1 if t2==0 else 0 if t1<=0 else 2 if t1>=30 else 1,df_repay['yqts'],df_repay['is_dq'])]


# In[47]:


df_Firs3 = df_repay.query("period<=3").groupby('order_no').agg({'Firs3ever15':'max','Firs3ever30':'max','is_dq':'min'})
df_Firs3.shape


# In[64]:


# 产生首三期的Y标签
df_Firs3 = df_repay.query("period<=3").groupby('order_no').agg({'Firs3ever15':'max','Firs3ever30':'max','is_dq':'min'})
df_Firs3 = df_Firs3.query("is_dq==1").reset_index() # 满足3期账龄要求
df_Firs3.head()
print(df_Firs3.shape)
print(df_Firs3['Firs3ever15'].value_counts())
print(df_Firs3['Firs3ever30'].value_counts())


# In[65]:


# 产生首六期的Y标签
df_Firs6 = df_repay.query("period<=6").groupby('order_no').agg({'Firs6ever15':'max','Firs6ever30':'max','is_dq':'min'})
df_Firs6 = df_Firs6.query("is_dq==1").reset_index() # 满足六期账龄要求
df_Firs6.head()
print(df_Firs6.shape)
print(df_Firs6['Firs6ever15'].value_counts())
print(df_Firs6['Firs6ever30'].value_counts())


# ## 形成订单层次的大宽表

# In[66]:


df_order_base = pd.merge(df_order, df_loan_amt,how='left',on='order_no')
df_order_base = pd.merge(df_order_base, df_Firs3, how='left', on='order_no')
df_order_base = pd.merge(df_order_base, df_Firs6, how='left', on='order_no')
df_order_base.info(null_counts=True)
df_order_base.head()


# In[67]:


cols = ['user_id','id_no_des','order_no','apply_date','order_status','loan_period','channel_id','loan_amount_x',
         'lending_time','loan_amount_y','loan_rate','total_periods',
         'Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30']
df_order_base = df_order_base[cols]
df_order_base.info()
df_order_base.head()


# In[68]:


df_order_base['apply_month'] = df_order_base['apply_date'].str[0:7]
df_order_base['apply_month'].head()


# In[69]:


xx0 = df_order_base.query("order_status==6").groupby('channel_id')['order_no'].count()

xx1 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_channel = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_channel


# In[70]:


xx0 = df_order_base.query("order_status==6").groupby('apply_month')['order_no'].count()

xx1 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_order_base.query("order_status==6").groupby(by=['apply_month','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_months = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_months


# In[71]:


df_channel.to_excel(r'.\result\df_channel.xlsx')
df_months.to_excel(r'.\result\df_months.xlsx')


# ### 保存数据

# In[73]:


df_order_base.to_pickle(r'.\result\df_order_base.pkl')
df_order.to_pickle(r'.\result\df_order.pkl')
df_auth.to_pickle(r'.\result\df_auth.pkl')
df_repay.to_pickle(r'.\result\df_repay.pkl')
df_loan_amt.to_pickle(r'.\result\df_loan_amt.pkl')
df_Firs3.to_pickle(r'.\result\df_Firs3.pkl')
df_Firs6.to_pickle(r'.\result\df_Firs6.pkl')


# In[2]:


df_order_base = pd.read_pickle(r'.\result\df_order_base.pkl')
df_order_base.info(null_counts=True)
df_order_base.head()


# ## 三方数据匹配率

# ### 百行数据

# In[7]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_baihang_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_baihang_1.csv') 
df_baihang = pd.concat([data_2, data_other_2], axis=0) # 百行数据
del data_2,data_other_2
gc.collect()
df_baihang.info(null_counts=True)
df_baihang.head()


# In[8]:


print(df_baihang['id_no_des'].nunique(), df_baihang['order_no'].nunique(), df_baihang.shape)


# In[9]:


df_baihang['return_massage'].value_counts()


# In[10]:


df_baihang = df_baihang[df_baihang['return_massage']=='请求成功']
print(df_baihang['id_no_des'].nunique(), df_baihang['order_no'].nunique(), df_baihang.shape)


# In[13]:


df_baihang = df_baihang[df_baihang['create_time']>='2022-05-01']
print(df_baihang['id_no_des'].nunique(), df_baihang['order_no'].nunique(), df_baihang.shape)


# In[37]:


df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], df_baihang[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
df1['days'] = df1['apply_date'] - df1['create_time']
df1['days'] = df1['days'].dt.days
df1.info(null_counts=True)
df1.head(3)


# In[38]:


# # 添加标签
# def map_func(x, y):
#     if x==y:
#         return 1
#     elif pd.notnull(x) and pd.notnull(y):
#         return 0
#     else:
#         return -1

# df1['order_no_is_equal'] = df1[['order_no_x','order_no_y']].apply(lambda x:map_func(*x),axis=1)

df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]


# In[39]:


df1['order_no_is_equal'].value_counts(dropna=False)


# In[40]:


df1_part1 = df1.query("order_no_is_equal==1")
df1_part1.info(null_counts=True)
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[41]:


df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[42]:


df1_part2 = df1.query("order_no_is_equal==0")
df1_part2 = df1_part2.query("days<=30 & days>=0")
df1_part2.info(null_counts=True)
print(df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[43]:


df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[46]:


# 合并数据
df1_new = pd.concat([df1_part1, df1_part2], axis=0)
df1_new.info(null_counts=True)
print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[47]:


df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[48]:


df1_new.rename(columns={'order_no_x':'order_no'},inplace=True)


# In[49]:


df1_new.info()


# #### 订单基础关联百行数据

# In[50]:


df_order_base.info()


# In[52]:


usecols = ['order_no','order_no_y','create_time','model_score_01','order_no_is_equal']
df_order_base = pd.merge(df_order_base, df1_new[usecols], how='left',on='order_no')
df_order_base.info()
df_order_base.head()


# In[ ]:





# ### 融360

# In[54]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_rong360_4.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_rong360_4.csv') 
df_rong360 = pd.concat([data_2, data_other_2], axis=0) 
del data_2,data_other_2
gc.collect()
df_rong360.info(null_counts=True)
df_rong360.head()
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[60]:


df_rong360.groupby(['return_massage','value_001'])['order_no'].count().unstack()


# In[55]:


df_rong360['return_massage'].value_counts()


# In[56]:


df_rong360 = df_rong360[df_rong360['return_massage'].isin(['请求成功','查询成功'])]
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[57]:


df_rong360 = df_rong360[df_rong360['create_time']>='2022-05-01']
print(df_rong360['order_no'].nunique(), df_rong360.shape)


# In[61]:


df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], df_rong360[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
df1['days'] = df1['apply_date'] - df1['create_time']
df1['days'] = df1['days'].dt.days
df1.info(null_counts=True)
df1.head(3)


# In[62]:


df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
df1['order_no_is_equal'].value_counts(dropna=False)


# In[63]:


df1_part1 = df1.query("order_no_is_equal==1")
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)

df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[64]:


df1_part2 = df1.query("order_no_is_equal==0")
df1_part2 = df1_part2.query("days<=30 & days>=0")
print(df1_part2['order_no_x'].nunique(), df1_part2.shape)

df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[66]:


# 合并数据
df1_new = pd.concat([df1_part1, df1_part2], axis=0)
print( df1_new['order_no_x'].nunique(), df1_new.shape)

df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_new['order_no_x'].nunique(), df1_new.shape)


# In[67]:


usecols = ['order_no_x','order_no_y','create_time','model_score_01','order_no_is_equal']
df1_new = df1_new[usecols]

cols = []
for col in usecols[1:]:
    col = col+'_rong360'
    cols.append(col)
cols = ['order_no'] + cols
print(cols)

df1_new.columns = cols
df1_new.info()


# #### 关联融360的数据

# In[68]:


print(df1_new.shape[0]/df_order_base.shape[0])
df_order_base = pd.merge(df_order_base, df1_new, how='left',on='order_no')
df_order_base.info(null_counts=True)
df_order_base.head()


# ### 腾讯数据

# In[70]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_tengxun_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_tengxun_1.csv') 
df_tengxun = pd.concat([data_2, data_other_2], axis=0) # 百行数据
del data_2,data_other_2
gc.collect()
df_tengxun.info(null_counts=True)
df_tengxun.head()


# In[71]:


print(df_tengxun['order_no'].nunique(), df_tengxun.shape)


# In[72]:


df_tengxun.groupby(['return_massage','value_005'])['order_no'].count().unstack()


# In[73]:


df_tengxun.groupby(['value_001','value_002'])['order_no'].count().unstack()


# In[74]:


df_tengxun = df_tengxun[df_tengxun['return_massage']=='请求成功']
print(df_tengxun['id_no_des'].nunique(), df_tengxun['order_no'].nunique(), df_tengxun.shape)


# In[75]:


df_tengxun = df_tengxun[df_tengxun['create_time']>='2022-05-01']
print(df_tengxun['id_no_des'].nunique(), df_tengxun['order_no'].nunique(), df_tengxun.shape)


# In[76]:


df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], df_tengxun[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
df1['days'] = df1['apply_date'] - df1['create_time']
df1['days'] = df1['days'].dt.days
df1.info(null_counts=True)
df1.head(3)


# In[77]:


df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
df1['order_no_is_equal'].value_counts(dropna=False)


# In[78]:


df1_part1 = df1.query("order_no_is_equal==1")
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print(df1_part1['order_no_x'].nunique(), df1_part1.shape)


# In[79]:


df1_part2 = df1.query("order_no_is_equal==0")
df1_part2 = df1_part2.query("days<=30 & days>=0")
print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_part2['order_no_x'].nunique(), df1_part2.shape)


# In[82]:


# 合并数据
df1_new = pd.concat([df1_part1, df1_part2], axis=0)
df1_new.info(null_counts=True)
print( df1_new['order_no_x'].nunique(), df1_new.shape)
df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
print( df1_new['order_no_x'].nunique(), df1_new.shape)


usecols = ['order_no_x','order_no_y','create_time','model_score_01','order_no_is_equal']
df1_new = df1_new[usecols]

cols = []
for col in usecols[1:]:
    col = col+'_tengxun'
    cols.append(col)
cols = ['order_no'] + cols
print(cols)

df1_new.columns = cols
df1_new.info()


# #### 关联腾讯

# In[83]:


print(df1_new.shape[0]/df_order_base.shape[0])
df_order_base = pd.merge(df_order_base, df1_new, how='left',on='order_no')
df_order_base.info()
df_order_base.head()


# ### 信用算力

# In[3]:


df_order_base = pd.read_pickle(r'.\result\df_order_base_4_class_date.pkl')
df_order_base.info()


# In[6]:


df_order_base.drop(list(df_order_base.columns)[29:],axis=1,inplace=True)
df_order_base.shape


# In[7]:


data_2 = pd.read_csv(r'.\167\dwd_beforeloan_third_combine_id_167_xinyongsuanli_1.csv')
data_other_2 = pd.read_csv(r'.\other\dwd_beforeloan_third_combine_id_other_xinyongsuanli_1.csv') 
df_xingyongsuanli = pd.concat([data_2, data_other_2], axis=0)
del data_2,data_other_2
gc.collect()
df_xingyongsuanli.info(null_counts=True)
df_xingyongsuanli.head()


# In[8]:


print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[9]:


df_xingyongsuanli.groupby(['return_massage','value_003'])['order_no'].count().unstack()


# In[10]:


df_xingyongsuanli = df_xingyongsuanli[df_xingyongsuanli['return_massage'].isin(['请求成功','处理成功'])]
print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[11]:


df_xingyongsuanli = df_xingyongsuanli[df_xingyongsuanli['create_time']>='2022-05-01']
print(df_xingyongsuanli['id_no_des'].nunique(), df_xingyongsuanli['order_no'].nunique(), df_xingyongsuanli.shape)


# In[12]:


df_xingyongsuanli['value_002'].value_counts(dropna=False)


# In[14]:


df_xingyongsuanli[df_xingyongsuanli['value_002']==4.0]['order_no'].nunique()


# In[17]:


for name, group_df in df_xingyongsuanli.groupby(['value_002']):
    print(name)
    df1 = pd.merge(df_order_base[['order_no','id_no_des','apply_date']], group_df[['order_no','id_no_des','create_time','model_score_01']], how='inner', on='id_no_des')
    df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
    df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
    df1['days'] = df1['apply_date'] - df1['create_time']
    df1['days'] = df1['days'].dt.days
    df1.info(null_counts=True)
    df1.head(3)
    
    df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
    df1['order_no_is_equal'].value_counts(dropna=False)
    
    df1_part1 = df1.query("order_no_is_equal==1")
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part2 = df1.query("order_no_is_equal==0")
    df1_part2 = df1_part2.query("days<=30 & days>=0")
    print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
    df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_part2['order_no_x'].nunique(), df1_part2.shape)
    
    # 合并数据
    df1_new = pd.concat([df1_part1, df1_part2], axis=0)
    df1_new.info(null_counts=True)
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    
    df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_new['order_no_x'].nunique(), df1_new.shape)

    usecols = ['order_no_x','order_no_y','create_time','model_score_01','order_no_is_equal']
    df1_new = df1_new[usecols]

    cols = []
    for col in usecols[1:]:
        col = col+'_xysl_' + str(name)
        cols.append(col)
    cols = ['order_no'] + cols
    print(cols)

    df1_new.columns = cols
    df1_new.info()
    
    data1 = 'df1_new_{}'.format(name)
    globals()[data1] = df1_new.copy()
    del df1, df1_part1, df1_new, df1_part2
    



# In[26]:


df1_new_1 = globals()['df1_new_{}'.format(1.0)]
df1_new_2 = globals()['df1_new_{}'.format(2.0)]
df1_new_3 = globals()['df1_new_{}'.format(3.0)]
df1_new_4 = globals()['df1_new_{}'.format(4.0)]


# In[27]:


print(df1_new_1.shape,df1_new_2.shape,df1_new_3.shape,df1_new_4.shape)


# In[28]:


print(df1_new_1.shape[0]/df_order_base.shape[0], df1_new_2.shape[0]/df_order_base.shape[0], df1_new_3.shape[0]/df_order_base.shape[0], df1_new_4.shape[0]/df_order_base.shape[0])
for tmp_df in [df1_new_1,df1_new_2,df1_new_3,df1_new_4]:
    df_order_base = pd.merge(df_order_base, tmp_df, how='left',on='order_no')
    print(df_order_base.shape)


# #### 保存数据

# In[29]:


df_order_base.info()


# In[30]:


df_order_base.to_pickle(r'.\result\df_order_base_4_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[32]:


channel = df_order_base.groupby(by=['channel_id'])['order_no','order_no_y','order_no_y_rong360','order_no_y_tengxun','order_no_y_xysl_1.0','order_no_y_xysl_2.0','order_no_y_xysl_3.0','order_no_y_xysl_4.0'].count()
for col in list(channel.columns)[1:]:
    channel[col+'_pct'] = channel[col]/channel['order_no']
                                        
channel


# In[33]:


year_month = df_order_base.groupby(by=['apply_month'])['order_no','order_no_y','order_no_y_rong360','order_no_y_tengxun','order_no_y_xysl_1.0','order_no_y_xysl_2.0','order_no_y_xysl_3.0','order_no_y_xysl_4.0'].count()
for col in list(year_month.columns)[1:]:
    year_month[col+'_pct'] = year_month[col]/year_month['order_no']

year_month


# In[34]:


writer=pd.ExcelWriter(r".\result\三方数据匹配_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
channel.to_excel(writer,sheet_name='渠道')
year_month.to_excel(writer,sheet_name='申请年月')
writer.save()


# In[35]:


del year_month, channel,df1_new_1,df1_new_2,df1_new_3,df1_new_4,df_xingyongsuanli
gc.collect()


# ### 百融数据1

# In[3]:


df_order_base = pd.read_pickle(r'.\result\df_order_base_4_20230714.pkl')
df_order_base.info()


# In[68]:


df_order_base.columns[0:17]


# In[69]:


df_order_base_1 = df_order_base[list(df_order_base.columns[0:17])]
df_order_base_1.info()


# In[28]:


# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20220501.csv')
# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20220701.csv')
# df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_167_sub_bairong_5_20221001.csv')
df_sub_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_sub_id_other_sub_bairong_5.csv')
# df_sub_bairong = pd.concat([data_0, data_1, data_2], axis=0)
# del data_0, data_1, data_2
# gc.collect()
print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)
df_sub_bairong.head()


# In[29]:


df_sub_bairong.dtypes


# In[30]:


df_sub_bairong['return_massage'].value_counts(dropna=False)


# In[31]:


df_sub_bairong['model_score_01'].describe()


# In[32]:


df_sub_bairong.isnull().sum()


# In[33]:


df_sub_bairong['value_001'].head()


# In[34]:


df_sub_bairong['value_002'].head()


# In[35]:


df_sub_bairong = df_sub_bairong[df_sub_bairong['return_massage']=='请求成功']
print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)

df_sub_bairong = df_sub_bairong[df_sub_bairong['create_time']>='2022-05-01']
print(df_sub_bairong['id_no_des'].nunique(), df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[36]:


def process_data(df_left, cols_left, df_right, cols_right):
    df1 = pd.merge(df_left[cols_left], df_right[cols_right], how='inner', on='id_no_des')
    df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
    df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
    df1['days'] = df1['apply_date'] - df1['create_time']
    df1['days'] = df1['days'].dt.days
    # df1.info(null_counts=True)
    # df1.head(3)
    df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
    df1['order_no_is_equal'].value_counts(dropna=False)
    
    # 拆分数据
    print('-----------拆分数据------------------')
    df1_part1 = df1.query("order_no_is_equal==1")
    # df1_part1.info(null_counts=True)
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part2 = df1.query("order_no_is_equal==0")
    df1_part2 = df1_part2.query("days<=30 & days>=0")
    # df1_part2.info(null_counts=True)
    print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
    df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_part2['order_no_x'].nunique(), df1_part2.shape)
    
    # 合并数据
    print('-----------合并数据------------------')
    df1_new = pd.concat([df1_part1, df1_part2], axis=0)
    # df1_new.info(null_counts=True)
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal','create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    
    # 重新命名字段
    usecols = ['order_no_x','order_no_y','create_time','order_no_is_equal'] + cols_right[3:]
    df1_new = df1_new[usecols]

    cols = []
    for col in usecols[1:]:
        col = col+'_sub_bairong'
        cols.append(col)
    cols = ['order_no'] + cols
#     print(cols)

    df1_new.columns = cols
    # df1_new.info()
    
    return df1_new


# In[37]:


cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + list(df_sub_bairong.columns[df_sub_bairong.columns.str.contains('value_')])[1:]

df1_new = process_data(df_order_base_1, cols_left, df_sub_bairong, cols_right)
df1_new.info()
df1_new.head()


# In[38]:


df1_new.to_csv(r'.\result\other_sub_bairong_5.csv',index=False)


# In[39]:


del df_sub_bairong
gc.collect()


# In[40]:


# 合并数据
df_167_1 = pd.read_csv(r'.\result\167_sub_bairong_5_20220501.csv')
df_167_2 = pd.read_csv(r'.\result\167_sub_bairong_5_20220701.csv')
df_167_3 = pd.read_csv(r'.\result\167_sub_bairong_5_20221001.csv')
df_other = pd.read_csv(r'.\result\other_sub_bairong_5.csv')
df_sub_bairong = pd.concat([df_167_1,df_167_2,df_167_3,df_other],axis=0)
df_sub_bairong.info()
df_sub_bairong.head()


# In[41]:


# 查看是否有重复数据
print(df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[46]:


# df_order_base_1[df_order_base_1['order_no']=='98221026145204767528']


# In[47]:


# df_sub_bairong[df_sub_bairong['order_no']=='98221026145204767528']


# In[43]:


df_sub_bairong['order_no'].value_counts()


# In[48]:


df_sub_bairong = df_sub_bairong.sort_values(by=['order_no','order_no_is_equal_sub_bairong','create_time_sub_bairong'], ascending=False)
df_sub_bairong = df_sub_bairong.drop_duplicates(subset=['order_no'],keep='first')


# In[49]:


# 查看是否有重复数据
print(df_sub_bairong['order_no'].nunique(), df_sub_bairong.shape)


# In[65]:


float_cols = df_sub_bairong.select_dtypes(include=['float']).columns
for col in float_cols:
    df_sub_bairong[col].fillna(-9,inplace=True)
    df_sub_bairong[col] = df_sub_bairong[col].apply(int)   


# In[66]:


df_sub_bairong.info()


# In[67]:


df_sub_bairong.isnull().sum()


# In[61]:


vars = list(df_order_base.columns[df_order_base.columns.str.contains('_sub_bairong')])
df_order_base.drop(vars,axis=1,inplace=True)
df_order_base.info()


# In[62]:


print(df_sub_bairong.shape[0]/df_order_base.shape[0])
df_order_base = pd.merge(df_order_base, df_sub_bairong, how='left',on='order_no')
df_order_base.info()
print(df_order_base.shape)


# In[51]:


df_order_base.to_pickle(r'.\result\df_order_base_{}_{}.pkl'.format(5, str(datetime.today())[:10].replace('-','')))


# In[53]:


df_order_base.dtypes


# ### 百融数据2

# In[70]:


df_order_base_1 = df_order_base[list(df_order_base.columns[0:17])]
df_order_base_1.info()
df_order_base_1.to_pickle(r'.\result\df_order_base_1.pkl')


# In[71]:


del df_order_base
gc.collect()


# In[3]:


df_order_base_1 =pd.read_pickle(r'.\result\df_order_base_1.pkl')
df_order_base_1.info()


# In[5]:


# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221101.csv') #完成
df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201.csv',engine='python',error_bad_lines=False)
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230101.csv') #完成
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230201.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20230301.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20221101.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20230301.csv')
# df_bairong = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_174_bairong_1_20230501.csv')
# df_bairong = pd.concat([data_0, data_1, data_2], axis=0)
# del data_0, data_1, data_2
# gc.collect()
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)
df_bairong.info()
df_bairong.head()


# In[6]:


df_bairong.to_csv(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201_copy.csv',encoding='utf-8')


# In[7]:


df_bairong.dtypes


# In[8]:


df_bairong.isnull().sum()


# In[15]:


df_bairong['return_massage'].value_counts(dropna=False)


# In[16]:


df_bairong['model_score_01'].describe()


# In[17]:


df_bairong['value_001'].head()


# In[18]:


df_bairong['create_time'].min()


# In[118]:


# df_bairong = df_bairong[df_bairong['return_massage']=='请求成功']
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)
# df_bairong = df_bairong[df_bairong['create_time']>='2022-05-01']
# print(df_bairong['id_no_des'].nunique(), df_bairong['order_no'].nunique(), df_bairong.shape)


# In[19]:


def process_data(df_left, cols_left, df_right, cols_right):
    df1 = pd.merge(df_left[cols_left], df_right[cols_right], how='inner', on='id_no_des')
    df1['apply_date'] = pd.to_datetime(df1['apply_date'],format='%Y-%m-%d')
    df1['create_time'] = pd.to_datetime(df1['create_time'].str[0:10], format='%Y-%m-%d')
    df1['days'] = df1['apply_date'] - df1['create_time']
    df1['days'] = df1['days'].dt.days
    # df1.info(null_counts=True)
    # df1.head(3)
    df1['order_no_is_equal'] = [*map(lambda t1,t2: 1 if t2==t1 else 0,df1['order_no_x'],df1['order_no_y'])]
    df1['order_no_is_equal'].value_counts(dropna=False)
    
    # 拆分数据
    print('-----------拆分数据------------------')
    df1_part1 = df1.query("order_no_is_equal==1")
    # df1_part1.info(null_counts=True)
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    df1_part1 = df1_part1.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print(df1_part1['order_no_x'].nunique(), df1_part1.shape)
    
    df1_part2 = df1.query("order_no_is_equal==0")
    df1_part2 = df1_part2.query("days<=30 & days>=0")
    # df1_part2.info(null_counts=True)
    print(df1_part2['order_no_x'].nunique(), df1_part2.shape)
    df1_part2 = df1_part2.sort_values(by=['order_no_x', 'create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_part2['order_no_x'].nunique(), df1_part2.shape)
    
    # 合并数据
    print('-----------合并数据------------------')
    df1_new = pd.concat([df1_part1, df1_part2], axis=0)
    # df1_new.info(null_counts=True)
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    df1_new = df1_new.sort_values(by=['order_no_x','order_no_is_equal','create_time'], ascending=False).drop_duplicates(subset=['order_no_x'],keep='first')
    print( df1_new['order_no_x'].nunique(), df1_new.shape)
    
    # 重新命名字段
    usecols = ['order_no_x','order_no_y','create_time','order_no_is_equal'] + cols_right[3:]
    df1_new = df1_new[usecols]

    cols = []
    for col in usecols[1:]:
        col = col+'_bairong'
        cols.append(col)
    cols = ['order_no'] + cols
#     print(cols)

    df1_new.columns = cols
    # df1_new.info()
    
    return df1_new
  


# In[20]:


cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + list(df_bairong.columns[df_bairong.columns.str.contains('value_')])

df1_new = process_data(df_order_base_1, cols_left, df_bairong, cols_right)
df1_new.info()
df1_new.head()


# In[22]:


df1_new.to_csv(r'.\result\167_bairong_1_20221201.csv')


# In[47]:


def chunk_process_data(df_order_base_1, path, engine='c', chunk_size=100000):
    df_chunks = pd.read_csv(r'D:\share\dwd_beforeloan_third_combine_id_{}.csv'.format(path), engine=engine,chunksize=chunk_size)
    result_df = pd.DataFrame() 
    for i, chunk in enumerate(df_chunks):
        # df_bairong = df_bairong[df_bairong['return_massage']=='请求成功']
#         df_bairong = chunk[chunk['create_time']>='2022-05-01']
        print(chunk.shape)
        cols_left = ['order_no','id_no_des','apply_date']
        cols_right = ['order_no','id_no_des','create_time'] + list(chunk.columns[chunk.columns.str.contains('value_')])

        df1_new = process_data(df_order_base_1, cols_left, chunk, cols_right)
        result_df = pd.concat([result_df, df1_new], axis=0)
        print('-----处理分块数据：{}-------'.format(i))
        
        del chunk
    
    return result_df
    


# In[34]:


path = '174_bairong_1_20221101'
df1_new = chunk_process_data(df_order_base_1, path, engine='c', chunk_size=100000)


# In[35]:


df1_new.info()
df1_new.head()


# In[21]:


# 查看是否有重复数据
print(df1_new['order_no'].nunique(), df1_new.shape)


# In[19]:


# df_order_base_1[df_order_base_1['order_no']=='98230212162802942203']


# In[20]:


# df1_new[df1_new['order_no']=='98230212162802942203']


# In[37]:


df1_new = df1_new.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
df1_new = df1_new.drop_duplicates(subset=['order_no'],keep='first')


# In[38]:


# df1_new.to_csv(r'.\result\167_bairong_1_20221101.csv',index=False)
# df1_new.to_csv(r'.\result\167_bairong_1_20230101.csv',index=False)
df1_new.to_csv(r'.\result\{}.csv'.format(path),index=False)


# In[39]:


float_cols = df1_new.select_dtypes(include=['float']).columns
for col in float_cols:
    df1_new[col].fillna(-9,inplace=True)
    df1_new[col] = df1_new[col].astype('int8')  


# In[40]:


# df1_new.to_csv(r'.\result\167_bairong_1_20221101_2.csv',index=False)
# df1_new.to_csv(r'.\result\167_bairong_1_20230101_2.csv',index=False)
df1_new.to_csv(r'.\result\{}_2.csv'.format(path),index=False)


# In[41]:


path = '174_bairong_1_20230301'
df1_new = chunk_process_data(df_order_base_1, path, engine='c', chunk_size=100000)

# 查看是否有重复数据
print(df1_new['order_no'].nunique(), df1_new.shape)

df1_new = df1_new.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
df1_new = df1_new.drop_duplicates(subset=['order_no'],keep='first')

df1_new.to_csv(r'.\result\{}.csv'.format(path),index=False)

float_cols = df1_new.select_dtypes(include=['float']).columns
for col in float_cols:
    df1_new[col].fillna(-9,inplace=True)
    df1_new[col] = df1_new[col].astype('int8')  
df1_new.to_csv(r'.\result\{}_2.csv'.format(path),index=False)


# In[42]:


path = '174_bairong_1_20230501'
df1_new = chunk_process_data(df_order_base_1, path, engine='c', chunk_size=100000)

# 查看是否有重复数据
print(df1_new['order_no'].nunique(), df1_new.shape)

df1_new = df1_new.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
df1_new = df1_new.drop_duplicates(subset=['order_no'],keep='first')

df1_new.to_csv(r'.\result\{}.csv'.format(path),index=False)

float_cols = df1_new.select_dtypes(include=['float']).columns
for col in float_cols:
    df1_new[col].fillna(-9,inplace=True)
    df1_new[col] = df1_new[col].astype('int8')  
df1_new.to_csv(r'.\result\{}_2.csv'.format(path),index=False)


# In[53]:


import csv


# In[ ]:


# with open(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201.csv','r',encoding='utf-8') as inf,\
# open(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201_2.csv','w',encoding='utf-8') as outf:
#     csvreader = csv.DictReadder(inf, header)
#     fieldnames = csvreader.fieldnames
#     csvwriter = csv.DictWriter(outf, fieldnames)


# In[23]:


# test = pd.DataFrame()
# data = csv.reader(open(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201.csv','r',encoding='utf-8'))
# for d in data:
#     result = pd.DataFrame(d).T
#     test = pd.concat([test, result])
#     if test.shape[0]%10000==0:
#         print(test.shape[0]/10000)


# In[70]:


# test = pd.DataFrame()
# with open(r'D:\share\dwd_beforeloan_third_combine_id_167_bairong_1_20221201.csv','r',encoding='utf-8') as file:
#     reader = csv.reader(file)
#     columns = next(reader)
#     print(columns)
#     for row in reader:
#         print(row)
#         result = pd.DataFrame(row).T
# #         result.coulumns = columns
# #         print(result)
#         test = pd.concat([test, result])
#         if test.shape[0]%10000==0:
#             print(test.shape[0]/10000)


# In[24]:


# path = '167_bairong_1_20221201'
# df1_new = chunk_process_data(df_order_base_1, path, engine='c', chunk_size=20000)

# # 查看是否有重复数据
# print(df1_new['order_no'].nunique(), df1_new.shape)

# df1_new = df1_new.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
# df1_new = df1_new.drop_duplicates(subset=['order_no'],keep='first')

# df1_new.to_csv(r'.\result\{}.csv'.format(path),index=False)

# float_cols = df1_new.select_dtypes(include=['float']).columns
# for col in float_cols:
#     df1_new[col].fillna(-9,inplace=True)
#     df1_new[col] = df1_new[col].astype('int8')  
# df1_new.to_csv(r'.\result\{}_2.csv'.format(path),index=False)


# In[25]:


# 合并数据
df_167_1 = pd.read_csv(r'.\result\167_bairong_1_20221101.csv')
df_167_2 = pd.read_csv(r'.\result\167_bairong_1_20221201.csv')
df_167_3 = pd.read_csv(r'.\result\167_bairong_1_20230101.csv')
df_167_4 = pd.read_csv(r'.\result\167_bairong_1_20230201.csv')
df_167_5 = pd.read_csv(r'.\result\167_bairong_1_20230301.csv')

df_174_1 = pd.read_csv(r'.\result\174_bairong_1_20221101.csv')
df_174_2 = pd.read_csv(r'.\result\174_bairong_1_20230301.csv')
df_174_3 = pd.read_csv(r'.\result\174_bairong_1_20230501.csv')
df_bairong = pd.concat([df_167_1,df_167_2,df_167_3,df_167_4,df_174_1,df_174_2,df_174_3],axis=0)
df_bairong.info()
df_bairong.head()


# In[26]:


# 查看是否有重复数据
print(df_bairong['order_no'].nunique(), df_bairong.shape)


# In[27]:


df_bairong['order_no'].value_counts()


# In[28]:


df_order_base_1[df_order_base_1['order_no']=='98230113182731200479']


# In[29]:


df_bairong[df_bairong['order_no']=='98230113182731200479']


# In[31]:


# 除掉重复数据
df_bairong = df_bairong.sort_values(by=['order_no','order_no_is_equal_bairong','create_time_bairong'], ascending=False)
df_bairong = df_bairong.drop_duplicates(subset=['order_no'],keep='first')
# 查看是否有重复数据
print(df_bairong['order_no'].nunique(), df_bairong.shape)



# In[32]:


# 总体查得率
print(df_bairong.shape[0]/df_order_base_1.shape[0])


# In[33]:


# 关联
df_order_base_1 = pd.merge(df_order_base_1, df_bairong, how='left',on='order_no')
df_order_base_1.info()
print(df_order_base_1.shape)


# In[34]:


df_order_base_1.to_pickle(r'.\result\df_order_base_bairong_{}.pkl'.format(str(datetime.today())[:10].replace('-','')))


# In[36]:


df_order_base_1.head()


# In[42]:


cols = df_order_base_1.columns[df_order_base_1.columns.str.contains('_bairong')]


# In[43]:


xx1 = df_order_base_1.query("order_status==6").groupby(by=['channel_id'])[cols].count()
xx2 = df_order_base_1.query("order_status==6").groupby(by=['channel_id'])['order_no'].count()
xx = pd.merge(xx2, xx1, how='left',left_index=True,right_index=True)
xx


# In[44]:


xx.to_excel(r'.\result\借据层百融数据匹配率.xlsx')


# In[ ]:


# xx0 = df_order_base.query("order_status==6").groupby('channel_id')['order_no'].count()

# xx1 = df_order_base.query("order_status==6").groupby(by=['channel_id'])['order_no'].count().unstack()
# xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs3ever30']).count().unstack()
# xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

# xx3 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever15'])['order_no'].count().unstack()
# xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

# xx4 = df_order_base.query("order_status==6").groupby(by=['channel_id','Firs6ever30'])['order_no'].count().unstack()
# xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

# df_channel = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
# df_channel


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[125]:


del df_bairong
gc.collect()


# In[ ]:


# 合并数据
df_167_1 = pd.read_csv(r'.\result\167_sub_bairong_5_20220501.csv')
df_167_2 = pd.read_csv(r'.\result\167_sub_bairong_5_20220701.csv')
df_167_3 = pd.read_csv(r'.\result\167_sub_bairong_5_20221001.csv')
df_other = pd.read_csv(r'.\result\other_sub_bairong_5.csv')
df_bairong = pd.concat([df_167_1,df_167_2,df_167_3,df_other],axis=0)
df_bairong.info()
df_bairong.head()

# 查看是否有重复数据
print(df_bairong['order_no'].nunique(), df_bairong.shape)

df_bairong['order_no'].value_counts()

df_bairong = df_bairong.sort_values(by=['order_no','order_no_is_equal_sub_bairong','create_time_sub_bairong'], ascending=False)
df_bairong = df_bairong.drop_duplicates(subset=['order_no'],keep='first')


df_bairong = df_bairong.sort_values(by=['order_no','order_no_is_equal_sub_bairong','create_time_sub_bairong'], ascending=False)
df_bairong = df_bairong.drop_duplicates(subset=['order_no'],keep='first')
# df_order_base_1[df_order_base_1['order_no']=='98221026145204767528']
# df_bairong[df_bairong['order_no']=='98221026145204767528']

df_bairong['order_no'].value_counts()

df_bairong = df_bairong.sort_values(by=['order_no','order_no_is_equal_sub_bairong','create_time_sub_bairong'], ascending=False)
df_bairong = df_bairong.drop_duplicates(subset=['order_no'],keep='first')



    
vars = list(df_order_base.columns[df_order_base.columns.str.contains('_sub_bairong')])
df_order_base.drop(vars,axis=1,inplace=True)
df_order_base.info()

print(df_bairong.shape[0]/df_order_base.shape[0])
df_order_base = pd.merge(df_order_base, df_bairong, how='left',on='order_no')
df_order_base.info()
print(df_order_base.shape)

df_order_base.dtypes




#==============================================================================
# File: 策略的需求.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd


# In[26]:


df_order_base = pd.read_pickle(r'.\result\df_order_base.pkl')
df_order_base.info(null_counts=True)
df_order_base.head()


# In[ ]:


list(df_auth_base.query("apply_month>='202303' & Firs6ever30==Firs6ever30")['user_id'])


# In[27]:


df_order_base[df_order_base['user_id']==179765768]


# In[28]:


df_order_base['loan_period'].value_counts()


# In[29]:


df_order_base[df_order_base['user_id'].isin(list(df_auth_base.query("apply_month>='202303' & Firs6ever30==Firs6ever30")['user_id']))]['loan_period'].value_counts()


# In[ ]:


cols = ['user_id','id_no_des','order_no','apply_date','order_status','loan_period','channel_id','loan_amount_x',
         'lending_time','loan_amount_y','loan_rate','total_periods',
         'Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30']


# In[3]:


df_order_base['lending_time'] = pd.to_datetime(df_order_base['lending_time'] ,format='%Y-%m-%d')
df_order_base['lending_time'].head()


# In[4]:


df_order_base['apply_date'] = pd.to_datetime(df_order_base['apply_date'] ,format='%Y-%m-%d')
df_order_base['apply_date'].head()


# In[7]:


xx = df_order_base.query("order_status==6").groupby('user_id').agg({'apply_date':'min','lending_time':'min','Firs3ever15':'max','Firs3ever30':'max','Firs6ever15':'max','Firs6ever30':'max'})

xx.head()


# In[8]:


xx = xx.reset_index()
xx.head()


# In[11]:


xx.to_pickle(r'.\result\df_order_merge.pkl')


# ## 数据需求20230717

# In[3]:


df_auth = pd.read_pickle(r'D:\liuyedao\result\df_auth.pkl')
df_auth.info(null_counts=True)
df_auth.head()


# In[13]:


df_auth['apply_date'].min()


# In[28]:


credit_amout = df_auth.query("auth_status==6").groupby(by=['channel_id'])['auth_credit_amount'].agg(['max','min','mean'])
credit_amout


# In[9]:


df_auth.query("auth_status==6 & channel_id==777")['auth_credit_amount'].describe()


# In[5]:


df_auth.query("auth_status==6 & channel_id==777")['auth_credit_amount'].value_counts(dropna=False,sort=False)


# In[17]:


# 产品要素利率、期限、放款金额
df_order_base = pd.read_pickle(r'.\result\df_order_base.pkl')
df_order_base.info(null_counts=True)
df_order_base.head()


# In[18]:


df_order_base['apply_date'].min()


# In[19]:


product = df_order_base.query("order_status==6").groupby(by=['channel_id'])['loan_rate','total_periods','loan_amount_x'].agg(['max','min','mean'])
product


# In[32]:


# 风险表现
risk = df_order_base.query("order_status==6").groupby(by=['channel_id'])['order_no','Firs3ever15','Firs3ever30','Firs3ever15','Firs6ever30'].count()
risk


# In[38]:


df_order_base['Firs3ever15'].value_counts(dropna=False)


# In[33]:


risk.info()


# In[34]:


# 合并数据需求
df_demand_0717 = pd.concat([credit_amout, product, risk],axis=1)
df_demand_0717


# In[35]:


df_demand_0717 = df_demand_0717.reset_index()


# In[37]:


df_demand_0717.to_excel(r'.\result\df_demand_0717_part1.xlsx')


# #### 授信层风险表现

# In[11]:


df_auth_base = pd.read_pickle(r'.\result\df_auth.pkl')
df_auth_base.info()
df_auth_base.head()


# In[12]:


df_auth_base = df_auth_base.query("auth_status==6")
print(df_auth_base.shape)


# In[13]:


print(df_auth_base['user_id'].nunique())


# In[14]:


df_auth_base = df_auth_base.sort_values(['user_id','apply_time'],ascending=False).drop_duplicates(subset=['user_id'],keep='first')
print(df_auth_base.shape)


# In[15]:


print(df_auth_base['user_id'].nunique())


# In[16]:


df_order_merge = pd.read_pickle(r'.\result\df_order_merge.pkl')
df_order_merge.info()
df_order_merge.head()


# In[17]:


df_auth_base = pd.merge(df_auth_base, df_order_merge, how='left', on='user_id')
df_auth_base.info(null_counts=True)


# In[23]:


df_auth_base['apply_month'] = df_auth_base['apply_time'].str.replace('-','').str[0:6]
df_auth_base['apply_month'].head()


# In[25]:


df_auth_base.query("apply_month>='202303' & Firs6ever30==Firs6ever30").head(100)


# In[11]:


xx0 = df_auth_base.groupby('channel_id')['order_no'].count()

xx1 = df_auth_base.groupby(by=['channel_id','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_auth_base.groupby(by=['channel_id','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_auth_base.groupby(by=['channel_id','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_auth_base.groupby(by=['channel_id','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

df_channel = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
df_channel


# In[15]:


df_auth_base['apply_month'] = df_auth_base['apply_time'].str[0:7]
xx0 = df_auth_base.groupby('apply_month')['order_no'].count()

xx1 = df_auth_base.groupby(by=['apply_month','Firs3ever15'])['order_no'].count().unstack()
xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

xx2 = df_auth_base.groupby(by=['apply_month','Firs3ever30'])['order_no'].count().unstack()
xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

xx3 = df_auth_base.groupby(by=['apply_month','Firs6ever15'])['order_no'].count().unstack()
xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

xx4 = df_auth_base.groupby(by=['apply_month','Firs6ever30'])['order_no'].count().unstack()
xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

year_month = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
year_month


# In[18]:


from datetime import datetime
writer=pd.ExcelWriter(r".\result\授信_风险表现"+str(datetime.today())[:10].replace('-','')+'.xlsx')
df_channel.to_excel(writer,sheet_name='渠道')
year_month.to_excel(writer,sheet_name='申请年月')
writer.save()


# In[37]:


test = pd.DataFrame()
for channle,group_df in df_auth_base.groupby(by=['channel_id']):
    if channle not in [209,210,211,888]:
        print(channle)
        xx0 = group_df.groupby('apply_month')['order_no'].count()

        xx1 = group_df.groupby(by=['apply_month','Firs3ever15'])['order_no'].count().unstack()
        xx1.rename(columns={-1.0: 'Firs3ever15未到期', 0.0:'Firs3ever15好', 1.0:'Firs3ever15灰', 2.0:'Firs3ever15坏'}, inplace=True)

        xx2 = group_df.groupby(by=['apply_month','Firs3ever30'])['order_no'].count().unstack()
        xx2.rename(columns={-1.0: 'Firs3ever30未到期', 0.0:'Firs3ever30好', 1.0:'Firs3ever30灰', 2.0:'Firs3ever30坏'}, inplace=True)

        xx3 = group_df.groupby(by=['apply_month','Firs6ever15'])['order_no'].count().unstack()
        xx3.rename(columns={-1.0: 'Firs6ever15未到期', 0.0:'Firs6ever15好', 1.0:'Firs6ever15灰', 2.0:'Firs6ever15坏'}, inplace=True)

        xx4 = group_df.groupby(by=['apply_month','Firs6ever30'])['order_no'].count().unstack()
        xx4.rename(columns={-1.0: 'Firs6ever30未到期', 0.0:'Firs6ever30好', 1.0:'Firs6ever30灰', 2.0:'Firs6ever30坏'}, inplace=True)

        year_month = pd.concat([xx0,xx1,xx2,xx3,xx4],axis=1)
        year_month['channel_id'] = channle
        test = pd.concat([year_month,test],axis=0)


# In[39]:


test.to_excel(r'.\result\授信层风险表现_各渠道_各月份.xlsx')


# In[ ]:







#==============================================================================
# File: 行为数据读取和处理的方法1.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[12]:


df_sample_dict = {}


# In[13]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[14]:


print(len(table_name_list))


# In[15]:


table_name_list[:10]


# In[16]:


table_name_list = [
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m']


# In[19]:



for table_name in table_name_list:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no
    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and channel_id != 1
          and apply_date= '{run_day}'
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no') 
    print(tmp.shape)
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')
    
#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
    
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')
    
    df_sample_dict[table_name] = tmp
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[20]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_ = df_sample_.reset_index()
df_sample_.info(show_counts=True)
df_sample_.head()


# In[21]:


df_sample_.to_csv(result_path + r'01_behave_features_v2.csv', index=False)


# In[9]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['id_no_des'].nunique())


# In[10]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[12]:


varsname = df_sample_.columns.to_list()[21:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[13]:


print(result_path)


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col], errors='coerce')


# In[19]:


df_sample_.to_csv(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[ ]:





# In[20]:


# 设置数据存储
task_name = '06_提现全渠道无成本子分融合模型fpd30标签_2410_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[318]:


df_sample = df_sample_.query("fpd30>=0 & diff_days>30").reset_index(drop=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为数据读取和处理的方法2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[25]:


df_sample_dict = {}


# In[26]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[27]:


print(len(table_name_list))


# In[28]:


table_name_list[10:20]


# In[29]:


table_name_list = [
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1',
 'znzz_fintech_ads.dim_pub_user_fd_tal_vars']


# In[36]:


table_name_list[-3:]


# In[ ]:



for table_name in table_name_list[-3:]:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no
    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and channel_id != 1
          and apply_date= '{run_day}'
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no')  
    print(tmp.shape)
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')
    
#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')
    
    df_sample_dict[table_name] = tmp
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[32]:


table_name


# In[38]:


table_name


# In[39]:


# 计算今天的时间
today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-08-12', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 order_no
,t1.*
from 
    (
    select order_no,id_no_des,mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
    where dt = '2025-03-18'
      and channel_id != 1
      and apply_date= '{run_day}'
    ) as t
inner join
    (
    select * 
    from {table_name} as t 
    where dt=date_sub('{run_day}',1)
    ) as t1 on t.id_no_des=t1.id_no_des
;
'''
#         print(sql)
    print(f'=========================={run_day}=============================')
    df_sample_time_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)

tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
tmp = tmp.drop(columns=['id_no_des', 'dt'])
tmp = tmp.set_index('order_no')  
print(tmp.shape)
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')

#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')

df_sample_dict[table_name] = tmp

#     del tmp, df_sample_time_dict,tmp_selected,usecols
gc.collect()


# In[40]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_ = df_sample_.reset_index()
df_sample_.info(show_counts=True)
df_sample_.head()


# In[41]:


df_sample_.to_csv(result_path + r'02_behave_features_v2.csv', index=False)


# In[42]:


df_sample_dict.keys()


# In[43]:


len(df_sample_dict.keys())


# In[19]:


tmp_selected.shape


# In[21]:


tmp_selected,xx = toad.selection.select(tmp.query("mob4dpd30>=0"),
                                    target='mob4dpd30', 
                                    empty=0.90, iv=0.01, corr=0.85, 
                                    return_drop=True, exclude=None)


# In[22]:


xx


# In[24]:


xx['iv']


# In[12]:


varsname = df_sample_.columns.to_list()[21:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[13]:


print(result_path)


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col], errors='coerce')


# In[19]:


df_sample_.to_csv(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[ ]:





# In[20]:


# 设置数据存储
task_name = '06_提现全渠道无成本子分融合模型fpd30标签_2410_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[318]:


df_sample = df_sample_.query("fpd30>=0 & diff_days>30").reset_index(drop=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为数据读取和处理的方法3.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[12]:


df_sample_dict = {}


# In[13]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[14]:


print(len(table_name_list))


# In[15]:


tmp_list = table_name_list[20:30]
tmp_list 


# In[16]:


table_name_list = [
 'znzz_fintech_ads.dim_pub_user_fd_id_vars',
 'znzz_fintech_ads.dim_pub_user_fd_t1t_vars',
 'znzz_fintech_ads.dim_pub_user_fd_t1f_vars',
 'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars',
 'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars',
 'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7',
 'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1',
 'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2',
 'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3']


# In[17]:



for table_name in table_name_list:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no

    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and apply_date= '{run_day}'
          and channel_id != 1
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no')  
    print(f"------------{tmp.shape}------------")
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')

#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')
#     print(f"------------{tmp[usecols].shape}------------")
    
    df_sample_dict[table_name] = tmp
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[ ]:





# In[18]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_ = df_sample_.reset_index()
df_sample_.info(show_counts=True)
df_sample_.head()


# In[19]:


df_sample_.to_csv(result_path + r'03_behave_features_v2.csv', index=False)


# In[10]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[12]:


varsname = df_sample_.columns.to_list()[21:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[13]:


print(result_path)


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col], errors='coerce')


# In[19]:


df_sample_.to_csv(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[ ]:





# In[20]:


# 设置数据存储
task_name = '06_提现全渠道无成本子分融合模型fpd30标签_2410_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[318]:


df_sample = df_sample_.query("fpd30>=0 & diff_days>30").reset_index(drop=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为数据读取和处理的方法4.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[12]:


df_sample_dict = {}


# In[13]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[14]:


print(len(table_name_list))


# In[15]:


table_name_list[30:40]


# In[16]:


table_name_list = ['znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5',
 'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03',
 'znzz_fintech_ads.dim_pub_user_fd_addr_vars',
 'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars',
 'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01',
 'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02',
 'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03',
 'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06',
 'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od',
 'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk']


# In[17]:



for table_name in table_name_list:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no

    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and apply_date= '{run_day}'
          and channel_id != 1
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no')  
    print(f"------------{tmp.shape}------------")
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')
    
#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')
#     print(f"------------{tmp[usecols].shape}------------")
    df_sample_dict[table_name] = tmp
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[ ]:





# In[18]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_ = df_sample_.reset_index()
df_sample_.info(show_counts=True)
df_sample_.head()


# In[19]:


df_sample_.to_csv(result_path + r'04_behave_features_v2.csv', index=False)


# In[10]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[12]:


varsname = df_sample_.columns.to_list()[21:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[13]:


print(result_path)


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col], errors='coerce')


# In[19]:


df_sample_.to_csv(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[ ]:





# In[20]:


# 设置数据存储
task_name = '06_提现全渠道无成本子分融合模型fpd30标签_2410_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[318]:


df_sample = df_sample_.query("fpd30>=0 & diff_days>30").reset_index(drop=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为数据读取和处理的方法5.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[13]:


df_sample_dict = {}


# In[14]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[15]:


print(len(table_name_list))


# In[16]:


table_name_list[40:]


# In[17]:



table_name_list= ['znzz_fintech_ads.dim_pub_user_fd_adim_vars',
 'znzz_fintech_ads.llji_id_var_order_his_fd',
 'znzz_fintech_ads.llji_id_var_settle_his_fd',
 'znzz_fintech_ads.llji_id_var_overdue_his_fd',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add',
 'znzz_fintech_ads.llji_user_var_credit_uti_plus',
 'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars',
 'znzz_fintech_ads.dim_channel_capital_jk_vars',
 'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars',
 'znzz_fintech_ads.llji_id_var_repay_his_fd' ]


# In[18]:



for table_name in table_name_list:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no
    ,mob4dpd30
    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and apply_date= '{run_day}'
          and channel_id != 1
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no') 
    print(f"------------{tmp.shape}------------")
#     for col in tmp.columns:
#         if tmp[col].dtype=='object':
#             tmp[col] = pd.to_numeric(tmp[col], errors='coerce')
    
#     tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
#                                         target='mob4dpd30', 
#                                         empty=0.90, iv=0.01, corr=0.85, 
#                                         return_drop=False, exclude=None)
#     print(f"------------{tmp_selected.shape}------------")
#     usecols = tmp_selected.columns.to_list()
#     usecols.remove('mob4dpd30')
    
    df_sample_dict[table_name] = tmp
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[ ]:





# In[19]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_ = df_sample_.reset_index()
df_sample_.info(show_counts=True)
df_sample_.head()


# In[ ]:


df_sample_.drop(columns=['mob4dpd30'])


# In[21]:


df_sample_.drop(columns=['mob4dpd30']).to_csv(result_path + r'05_behave_features_v2+.csv', index=False)


# In[22]:


to_drop = [col for col in df_sample_.columns if f'{col}'.startswith('uti_')]
print(len(to_drop))


# In[23]:


to_drop.append('mob4dpd30')
to_drop.append('obs_date')


# In[24]:


df_sample_.drop(columns=to_drop,inplace=True)


# In[25]:


df_sample_.to_csv(result_path + r'05_behave_features_v3+.csv', index=False)


# In[26]:


df_sample_.shape


# In[10]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[12]:


varsname = df_sample_.columns.to_list()[21:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[13]:


print(result_path)


# In[16]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col], errors='coerce')


# In[19]:


df_sample_.to_csv(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + '提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[ ]:





# In[20]:


# 设置数据存储
task_name = '06_提现全渠道无成本子分融合模型fpd30标签_2410_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# In[318]:


df_sample = df_sample_.query("fpd30>=0 & diff_days>30").reset_index(drop=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为数据读取和处理的方法6.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[47]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   
    

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# # 0. 数据读取

# In[4]:


df_sample_dict = {}


# In[5]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his_mt'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.llji_id_var_repay_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[6]:


print(len(table_name_list))


# In[7]:


table_name_list = [
 'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
,'znzz_fintech_ads.llji_id_var_repay_his_fd'
                  ]


# In[8]:



for table_name in table_name_list:
    # 开始新表的数据读取
    print(f"*********开始新一轮表的数据读取，表名：{table_name}************")
    df_sample_time_dict = {}
    # 计算今天的时间
    today = datetime.now().strftime('%Y-%m-%d')
    print(today)
    
    this_day =datetime.strptime('2024-11-15', '%Y-%m-%d')
    end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')
    while this_day >= end_day:
        run_day = this_day.strftime('%Y-%m-%d')
        sql = f'''
    select 
     order_no
    ,mob4dpd30
    ,t1.*
    from 
        (
        select order_no,id_no_des,mob4dpd30
        from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
        where dt = '2025-03-18'
          and apply_date= '{run_day}'
          and channel_id != 1
        ) as t
    inner join
        (
        select * 
        from {table_name} as t 
        where dt=date_sub('{run_day}',1)
        ) as t1 on t.id_no_des=t1.id_no_des
    ;
    '''
#         print(sql)
        print(f'=========================={run_day}=============================')
        df_sample_time_dict[run_day] = get_data(sql)
        this_day = this_day - timedelta(days=1)
    
    tmp = pd.concat(df_sample_time_dict.values(), ignore_index=True)
    tmp = tmp.drop(columns=['id_no_des', 'dt'])
    tmp = tmp.set_index('order_no')   
    for col in tmp.columns:
        if tmp[col].dtype=='object':
            tmp[col] = pd.to_numeric(tmp[col], errors='coerce')
    
    tmp_selected = toad.selection.select(tmp.query("mob4dpd30>=0"),
                                        target='mob4dpd30', 
                                        empty=0.90, iv=0.01, corr=0.85, 
                                        return_drop=False, exclude=None)
    usecols = tmp_selected.columns.to_list()
    usecols.remove('mob4dpd30')
    
    df_sample_dict[table_name] = tmp[usecols]
    
#     del tmp, df_sample_time_dict,tmp_selected,usecols
    gc.collect()


# In[20]:


tmp.info()


# In[21]:


tmp_selected.info()


# In[23]:


tmp_selected,droped = toad.selection.select(tmp.query("mob4dpd30>=0"),
                                    target='mob4dpd30', 
                                    empty=0.90, iv=0.01, corr=0.85, 
                                    return_drop=True, exclude=None)


# In[24]:


for k, v in droped.items():
    print(k, ":", len(v))


# In[27]:


df_sample_ = pd.concat(df_sample_dict.values(),axis=1)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[28]:


df_sample_ = df_sample_.reset_index()
df_sample_.head()


# In[29]:


df_sample_.info()


# In[30]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[31]:


df_sample_.to_csv(result_path + r'06_behave_features.csv', index=False)


# In[32]:


sql = '''
select *
from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
where dt = '2025-03-18'
  and channel_id != 1
;
'''
df_target = get_data(sql)


# In[33]:


df_target.to_csv(result_path + r'00_model_target.csv', index=False)


# In[34]:


df_target.info(show_counts=True)


# In[37]:


df_target['apply_date'].max()


# In[38]:


df_target = df_target.query("apply_date>='2024-07-21' & apply_date<='2024-11-15'").reset_index(drop=True)
df_target.shape


# In[39]:


df_target['mob4dpd30'].value_counts()


# In[40]:


df_target = df_target.query("mob4dpd30>=0").reset_index(drop=True)
df_target.shape


# In[43]:


df_target['apply_month'] = df_target['apply_date'].str[0:7]
df_target['apply_month'].head()


# In[44]:


df_target['apply_month'].value_counts()


# In[45]:


df_target.loc[df_target.query("apply_date>='2024-07-21' & apply_date<='2024-09-30'").index, 'data_set']='1_train'
df_target.loc[df_target.query("apply_date>='2024-10-01' & apply_date<='2024-11-15'").index, 'data_set']='3_oot'
df_target['data_set'].head()


# In[46]:


df_target['data_set'].value_counts()


# In[48]:


df_target['channel_types'] = df_target['channel_id'].apply(channel_type)
df_target['channel_rates'] = df_target['channel_id'].apply(channel_rate)


# In[50]:


df_target['channel_rates'].value_counts()


# In[51]:


df_target['channel_types'].value_counts()


# In[53]:


df_target.shape


# In[52]:


df_target.to_csv(result_path + r'00_model_target.csv', index=False)


# In[42]:


df_sample = pd.merge(df_target, df_sample_ , how='left', on ='order_no')
df_sample.info(show_counts=True)


# In[319]:


df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-27'").index, 'data_set']='3_oot2'
df_sample['apply_month'] = df_sample['apply_date'].str[0:7]


# In[320]:


target = 'fpd30'


# In[321]:


df_sample.to_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv',index=False)
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[6]:


target = 'fpd30'
df_sample = pd.read_csv(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')
print(result_path + 'model_提现全渠道无成本子分融合模型fpd30标签2410_2411.csv')


# In[7]:


df_sample.info(show_counts=True)


# In[9]:


varsname = df_sample.columns[21:66].to_list()
print(len(varsname))


# # 1. 样本概况

# In[37]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[323]:


print(df_sample[target].value_counts())


# In[484]:


df_target_summary_month = get_target_summary(df_sample_30_tmp, target, 'apply_month')
print(df_target_summary_month)


# In[485]:


df_target_summary_set = get_target_summary(df_sample_30_tmp, target, 'data_set')
print(df_target_summary_set)


# In[326]:


task_name


# In[486]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[480]:


# 2.1 变量分布
df_explor = toad.detect(df_sample_30_tmp[varsname])
df_explor


# ## 2.1缺失值处理

# In[329]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[10]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan


# In[481]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample_30_tmp[varsname])
df_explor_v1


# In[482]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[483]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[5]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[335]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[336]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[337]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[338]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[341]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[342]:


df_iv.loc[to_drop_iv,:]


# In[343]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[344]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.80, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[345]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[346]:


df_iv.loc[to_drop2,:]


# In[347]:


to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[6]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[7]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[370]:


df_sample_30_tmp = df_sample.query("channel_id!=1").reset_index(drop=True)


# In[375]:


df_sample_30_tmp.info(show_counts=True)


# In[417]:


varsname_v2 = df_sample_30_tmp.columns.to_list()[21:66]


# In[418]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample_30_tmp[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[419]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[420]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample_30_tmp, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[422]:


new_bins_dict


# In[423]:


combiner.update(new_bins_dict)


# In[424]:


combiner.export()


# In[425]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[11]:


combiner = toad.transform.Combiner()
with open(result_path + '变量分箱字典_20250304124846.pkl', 'rb') as f:
    new_bins_dict = pickle.load(f)
    
combiner.load(new_bins_dict)


# In[426]:


df_sample_30_tmp['data_set'].value_counts()


# In[427]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample_30_tmp, df_sample_30_tmp.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[429]:


df_bins = combiner.transform(df_sample_30_tmp, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[430]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[ ]:





# In[12]:


df_bins = combiner.transform(df_sample, labels=True)


# In[431]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[432]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[433]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[363]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[364]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[365]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[366]:


to_drop3 = ['bh_alic002_1','bh_alic002_2','bh_alic002_3']
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[367]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[391]:


varsname_v3 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd'] + ['t_br_fpd', 't_beha3_fpd', 't_br_mob4', 't_br2_fpd', 'all_a_dz_derived_v1_fpd30_202502_g_p', 'xz_fpd']


# In[437]:


varsname_v3 = varsname_v2[:]
print(len(varsname_v3))


# ## 4.2 Y标签相关性删除

# In[435]:


target


# In[438]:


df_bins.shape
df_bins.head()


# In[403]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[ ]:



# 假设df是你的DataFrame，且已经对多个变量进行了分箱
# 存储处理后的DataFrame
df_sample_30_woe = df_bins.copy()


# In[439]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[440]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('0-1')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[411]:


df_sample_30_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[441]:


df_sample_woe['all_a_app_free_fpd30_202502_s'].unique()


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[369]:


df_sample_woe.head()


# In[442]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[443]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[444]:


df_corr_matrix.head()


# In[463]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix1 = df_sample_woe[varsname_v3].corr(method='kendall')
df_corr_matrix1.head()


# In[448]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[464]:


# 调用函数

df_high_corr1, to_drop41 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop41))


# In[449]:


df_high_corr


# In[465]:


df_high_corr1


# In[466]:


to_drop41


# In[447]:


print(to_drop4)


# In[375]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[456]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[457]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v3,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[458]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[459]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[460]:


to_drop5


# In[380]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[17]:


varsname_v5 = ['all_a_app_free_fpd30_202502_s', 'all_a_bhdj_fpd10_v1_p', 'all_a_br_derived_fpd30_202408_g_p', 'all_a_br_derived_v1_mob4dpd30_202502_st_p', 'all_a_br_derived_v2_fpd30_202411_g_p', 'all_a_dz_derived_v2_fpd30_202502_g_p', 'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard', 'hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard', 'hlv_d_holo_certno_variablecode_standard_bd003', 'hlv_d_holo_jk_certno_fpd1_score', 'hlv_d_holo_jk_certno_score_fpd30_v1', 'hlv_d_holo_jk_certno_score_fpd7_v1', 'hlv_d_holo_jk_certno_varcode_standard_bd0004', 'ypy_bhxz_a_fpd30_v1_prob_good', 'score_fpd0_v1', 'score_fpd6_v1', 'score_fpd10_v1', 'score_fpd10_v2', 'score_fpd30_v1', 'duxiaoman_6', 'hengpu_4', 'aliyun_5', 'baihang_28', 'pudao_34', 'feicuifen', 'pudao_20', 'pudao_68', 'ruizhi_6', 'hengpu_5', 'pudao_21', 'bh_alic002_4', 't_br2_mob4', 't_beha3_mob4', 'dz_fpd']
varsname_v5.remove('baihang_28')


# In[381]:


to_drop5


# In[462]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[556]:


df_corr_matrix.to_csv(r'df_corr_matrix.csv')


# In[18]:


df_iv_by_month = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_month')


# In[19]:


df_iv_by_month.info()
df_iv_by_month.head()


# In[20]:


df_iv_by_month.set_index('Unnamed: 0',inplace=True)
df_iv_by_month.head()


# In[44]:


df_iv_by_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_20250304133751.xlsx',sheet_name='df_iv_by_set')
df_iv_by_set.head()


# In[45]:


df_iv_by_set.set_index('Unnamed: 0',inplace=True)
df_iv_by_set.head()




#==============================================================================
# File: 行为评分卡特征变量开发v2.py
#==============================================================================

# observe_date：观察点日期

# ---------------------逾期类变量------------------
def his_max_ovdue_day(data, observe_date):
    """
    历史最大逾期天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len (data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date] 
        if len (data) >0 : 
            # 计算每一期的历史最大逾期天数
            ovdue_day = max(data[['repay_date', 'period_settle_date']].apply(
                lambda x:(observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days, axis=1).max(),0)
        else: 
            #未到还款日期-9998 
            ovdue_day = -9998 
    else : 
        # 无借据-9999 
        ovdue_day = -9999 
    
    return ovdue_day
    
def cur_ovdue_day(data, observe_date): 
    """
    当前逾期天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len (data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date] 
        if len (data) >0 : 
            # 计算每一期的当前逾期天数，取max
            ovdue_day = data[['repay_date', 'period_settle_date']].apply(
                lambda x:(observe_date-x[0]).days+1 if x[1]> observe_date else 0, axis=1).max()
        else: 
            #未到还款日期-9998 
            ovdue_day = -9998 
    else : 
        # 无借据-9999 
        ovdue_day = -9999 
    
    return ovdue_day
 
def cur_ovdue_period(data, observe_date):
    """
    当前逾期期数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期逾期状态，逾期取1，不逾期取0，求sum
            ovdue_period = data[['repay_date','period_settle_date']].apply(
            lambda x: 1 if x[1]>observe_date else 0 ,axis=1).sum()
        else :
            # 未到还款日期
            ovdue_period = -9998
    else :
        # 无借据 -9999
        ovdue_period = -9999
    
    return ovdue_period
            
def cur_ovdue_loan(data, observe_date):
    """
    当前逾期借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期逾期状态，逾期取order_no，不逾期取np.nan，求借据的唯一数
            ovdue_loan = data[['repay_date','period_settle_date','order_no']].apply(
            lambda x: x[2] if x[1]>observe_date else np.nan ,axis=1).nunique()
        else :
            # 未到还款日期
            ovdue_loan = -9998
    else :
        # 无借据 -9999
        ovdue_loan = -9999
    
    return ovdue_loan
            
def cur_ovdue_prin(data, observe_date):
    """
    当前逾期本金
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期逾期状态，逾期取principal(本期应还本金)，不逾期取0，求sum
            cur_ovdue_prin = data[['repay_date','period_settle_date','principal']].apply(
            lambda x: x[2] if x[1]>observe_date else 0 ,axis=1).sum()
        else :
            # 未到还款日期
            cur_ovdue_prin = -9998
    else :
        # 无借据 -9999
        cur_ovdue_prin = -9999
    
    return cur_ovdue_prin
           
def maxovdue_nearly_30days(data, observe_date, ndays=30):
    """
    近N天最大逾期天数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天的最大逾期天数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，求Max
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date - data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return max(ovdue_days_list)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 -9999
        return -9999
           
def maxovdue_nearly_60days(data, observe_date, ndays=60):
    """
    近N天最大逾期天数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天的最大逾期天数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，求Max
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date - data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return max(ovdue_days_list)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 -9999
        return -9999
           
def maxovdue_nearly_90days(data, observe_date, ndays=90):
    """
    近N天最大逾期天数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天的最大逾期天数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，求Max
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date - data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return max(ovdue_days_list)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 -9999
        return -9999
           
def maxovdue_nearly_180days(data, observe_date, ndays=180):
    """
    近N天最大逾期天数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天的最大逾期天数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，求Max
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date - data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return max(ovdue_days_list)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 -9999
        return -9999

           
def ovdue1_cnt_nearly_30days(data, observe_date, ndays=30, ovdue_days=1):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue1_cnt_nearly_90days(data, observe_date, ndays=90, ovdue_days=1):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue1_cnt_nearly_180days(data, observe_date, ndays=180, ovdue_days=1):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue1_cnt_his(data, observe_date, ndays=720, ovdue_days=1):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
            
def ovdue3_cnt_nearly_30days(data, observe_date, ndays=30, ovdue_days=3):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue3_cnt_nearly_90days(data, observe_date, ndays=90, ovdue_days=3):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue3_cnt_nearly_180days(data, observe_date, ndays=180, ovdue_days=3):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue3_cnt_his(data, observe_date, ndays=720, ovdue_days=3):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue7_cnt_nearly_30days(data, observe_date, ndays=30, ovdue_days=7):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue7_cnt_nearly_90days(data, observe_date, ndays=90, ovdue_days=7):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue7_cnt_nearly_180days(data, observe_date, ndays=180, ovdue_days=7):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue7_cnt_his(data, observe_date, ndays=720, ovdue_days=7):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue30_cnt_nearly_30days(data, observe_date, ndays=30, ovdue_days=30):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue30_cnt_nearly_90days(data, observe_date, ndays=90, ovdue_days=30):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue30_cnt_nearly_180days(data, observe_date, ndays=180, ovdue_days=30):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
           
def ovdue30_cnt_his(data, observe_date, ndays=720, ovdue_days=30):
    """
    近N天最大逾期1/3/7/30+天次数：ndays 30/60/90/180
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) > 0 : 
        # 计算近N天逾期1/3/7/30+天次数
        ovdue_days_list = [] # 统计每一期的近N天的逾期天数，判断逾期天数>=ovdue_days的次数
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            for i in range(len(data)):
                if (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]>observe_date):
                    #实还日期>应还日期 且 实还日期>观察点日期，则逾期天数=观察日期-应还日期+1
                    ovdue_days_list.append((observe_date-data['repay_date'].iloc[i]).days + 1)
                elif (data['period_settle_date'].iloc[i]>data['repay_date'].iloc[i]
                    and data['period_settle_date'].iloc[i]<=observe_date
                    and data['period_settle_date'].iloc[i]>observe_date + relativedelta(days=-ndays)):
                    #实还日期>应还日期 且 实还日期<=观察点日期 且实还日期>观察日期-N，则逾期天数=实还日期-应还日期
                    ovdue_days_list.append((data['period_settle_date'].iloc[i]-data['repay_date'].iloc[i]).days)
                else:
                    ovdue_days_list.append(0)
                
            return np.sum(np.array(ovdue_days_list)>ovdue_days)
        else :
            # 未到还款日期,默认值-9998
            return -9998
    else :
        # 无借据 默认值-9999
        return -9999
       
        
def first_ovdue_periods(data, observe_date):
    """
    首次逾期的期数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # 取历史逾期>=1最早的repay_date对应的period
                first_ovdue_period = data[data['ever_ovdue_day']>=1].sort_values(by=['repay_date','period'])['period'].iloc[0]
            else:
                # 无逾期，默认值-1
                first_ovdue_period = -1
        else :
            # 未到还款日期
            first_ovdue_period = -9998
    else :
        # 无借据 -9999
        first_ovdue_period = -9999
    
    return first_ovdue_period
  
  
def first_ovdue_days(data, observe_date):
    """
    首次逾期的天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # 取历史逾期>=1最早的repay_date对应的ever_ovdue_day
                first_ovdue_day = data[data['ever_ovdue_day']>=1].sort_values(by=['repay_date','period'])['ever_ovdue_day'].iloc[0]
            else:
                # 无逾期，默认值-1
                first_ovdue_day = -1
        else :
            # 未到还款日期
            first_ovdue_day = -9998
    else :
        # 无借据 -9999
        first_ovdue_day = -9999
    
    return first_ovdue_day
  
  
def first_ovdue1_days_to_cur(data, observe_date, ovdue_days=1):
    """
    最早一次逾期1+/30+距今天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # observe_date-取历史逾期>=ovdue_days最早的repay_date的天数差
                first_ovdue_day_fromnow = (observe_date-data[data['ever_ovdue_day']>=ovdue_days]['repay_date'].min()).days
            else:
                # 无逾期，默认值-1
                first_ovdue_day_fromnow = -1
        else :
            # 未到还款日期
            first_ovdue_day_fromnow = -9998
    else :
        # 无借据 -9999
        first_ovdue_day_fromnow = -9999
    
    return first_ovdue_day_fromnow
  
  
def last_ovdue1_days_to_cur(data, observe_date, ovdue_days=1):
    """
    最近一次逾期1+/30+距今天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # observe_date-取历史逾期>=ovdue_days最近的repay_date的天数差
                last_ovdue_day_fromnow = (observe_date-data[data['ever_ovdue_day']>=ovdue_days]['repay_date'].max()).days
            else:
                # 无逾期，默认值-1
                last_ovdue_day_fromnow = -1
        else :
            # 未到还款日期
            last_ovdue_day_fromnow = -9998
    else :
        # 无借据 -9999
        last_ovdue_day_fromnow = -9999
    
    return last_ovdue_day_fromnow
  
def first_ovdue30_days_to_cur(data, observe_date, ovdue_days=30):
    """
    最早一次逾期1+/30+距今天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # observe_date-取历史逾期>=ovdue_days最早的repay_date的天数差
                first_ovdue_day_fromnow = (observe_date-data[data['ever_ovdue_day']>=ovdue_days]['repay_date'].min()).days
            else:
                # 无逾期，默认值-1
                first_ovdue_day_fromnow = -1
        else :
            # 未到还款日期
            first_ovdue_day_fromnow = -9998
    else :
        # 无借据 -9999
        first_ovdue_day_fromnow = -9999
    
    return first_ovdue_day_fromnow
  
  
def last_ovdue30_days_to_cur(data, observe_date, ovdue_days=30):
    """
    最近一次逾期1+/30+距今天数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 剔除未到还款日期的还款计划
        data = data[data['repay_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            data['ever_ovdue_day'] = data[['repay_date','period_settle_date']].apply(
            lambda x: (observe_date-x[0]).days+1 if x[1]>observe_date else (x[1]-x[0]).days ,axis=1)
            
            if len(data[data['ever_ovdue_day']>=1])>=1:
                # observe_date-取历史逾期>=ovdue_days最近的repay_date的天数差
                last_ovdue_day_fromnow = (observe_date-data[data['ever_ovdue_day']>=ovdue_days]['repay_date'].max()).days
            else:
                # 无逾期，默认值-1
                last_ovdue_day_fromnow = -1
        else :
            # 未到还款日期
            last_ovdue_day_fromnow = -9998
    else :
        # 无借据 -9999
        last_ovdue_day_fromnow = -9999
    
    return last_ovdue_day_fromnow
 
    
# ---------------------还款行为类变量------------------

def normal_repay_cnt_nearly_60days(data, observe_date, ndays=60):
    """
    近N天正常还款次数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        cnt = len(data[(data['period_settle_date']<=data['repay_date'])&(data['period_settle_date']<=observe_date)
        &(data['period_settle_date']>observe_date+relativedelta(days=-ndays))])
        
        return cnt

    else :
        # 无借据 -9999
        return -9999

def normal_repay_cnt_history(data, observe_date, ndays=720):
    """
    近N天正常还款次数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        cnt = len(data[(data['period_settle_date']<=data['repay_date'])&(data['period_settle_date']<=observe_date)
        &(data['period_settle_date']>observe_date+relativedelta(days=-ndays))])
        
        return cnt

    else :
        # 无借据 -9999
        return -9999
  
def cur_balance_observe(data, observe_date):
    """
    观察日期对应当前剩余本金
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 总放款金额- 截止观察时间已还本金
        current_balance = (data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
                           - data[data['period_settle_date']<=observe_date]['already_repaid_principal'].sum())
        
    else :
        # 无借据 -9999
        current_balance = -9999
        
    return current_balance
    
 
def cur_prin_repay_pct(data, observe_date):
    """
    当前实际还款率
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 总放款金额- 截止观察时间已还本金
        already_repaid_principal =  data[data['period_settle_date']<=observe_date]['already_repaid_principal'].sum()
        loan_amount = data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
        cur_prin_repay_pct = already_repaid_principal/loan_amount
    else :
        # 无借据 -9999
        cur_prin_repay_pct = -9999
        
    return cur_prin_repay_pct
    

def cur_nopay_period(data, observe_date):
    """
    当前剩余期数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        settle_period = len(data[data['period_settle_date']<=observe_date])
        # 总期数
        res = data.groupby(by=['order_no','total_periods'])['period'].max().reset_index()
        all_period = res[['total_periods','period']].max(axis=1).sum()
        
        # 剩余期数
        norepay_period = all_period - settle_period
    else :
        # 无借据 -9999
        norepay_period =  -9999
        
    return norepay_period
    

def cur_nopay_period_pct(data, observe_date):
    """
    当前剩余期数占总放款期数的占比
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        settle_period = len(data[data['period_settle_date']<=observe_date])
        # 总期数
        res = data.groupby(by=['order_no','total_periods'])['total_periods'].max().reset_index()
        all_period = res[['total_periods','period']].max(axis=1).sum()
        
        # 剩余期数占比
        norepay_period_pct = (all_period - settle_period)/all_period
    else :
        # 无借据 -9999
        norepay_period_pct =  -9999
        
    return norepay_period_pct
    

def cur_nosettle_max_period(data, observe_date):
    """
    当前未结清借据最大期限
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 :
        
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 未结清
        if len(res[res['period_settle_date']>observe_date])>=1:
            cur_nosettle_max_period = res[res['period_settle_date']>observe_date][['total_periods','period']].max(axis=1).max(axis=0)
        else:
            # 无未结清, 默认值-1
            cur_nosettle_max_period = -1         
    else :
        # 无借据 -9999
        cur_nosettle_max_period =  -9999
        
    return cur_nosettle_max_period
    

def nearly_180days_advsettle_loan_pct(data, observe_date, ndays=180):
    """
    近N天内提前结清的借据数/总借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 :
        
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 实还日期<应还日期为提前结清，再增加近N天判断
        # 近N天提前结清借据数
        advsettle_loan1 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        advsettle_loan2 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        if advsettle_loan1==len(res):
            # 近N天前全部结清 默认值-1
            nearly_advsettle_loan_pct = -1
        else:
            nearly_advsettle_loan_pct = advsettle_loan2/len(res)
    else :
        # 无借据 -9999
        nearly_advsettle_loan_pct =  -9999
        
    return nearly_advsettle_loan_pct

def his_advsettle_loan_pct(data, observe_date, ndays=720):
    """
    近N天内提前结清的借据数/总借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 :
        
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 实还日期<应还日期为提前结清，再增加近N天判断
        # 近N天提前结清借据数
        advsettle_loan1 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        advsettle_loan2 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        if advsettle_loan1==len(res):
            # 近N天前全部结清 默认值-1
            nearly_advsettle_loan_pct = -1
        else:
            nearly_advsettle_loan_pct = advsettle_loan2/len(res)
    else :
        # 无借据 -9999
        nearly_advsettle_loan_pct =  -9999
        
    return nearly_advsettle_loan_pct


def nearly_180days_advsettle_loan(data, observe_date, ndays=180):
    """
    近N天内提前结清的借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 :
        # 提前结清
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 实还日期<应还日期为提前结清，再增加近N天判断
        # 近N天提前结清借据数
        advsettle_loan1 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        advsettle_loan2 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        if advsettle_loan1==len(res):
            # 近N天前全部结清 默认值-1
            nearly_advsettle_loan = -1
        else:
            nearly_advsettle_loan = advsettle_loan2
    else :
        # 无借据 -9999
        nearly_advsettle_loan =  -9999
        
    return nearly_advsettle_loan

def his_advsettle_loan(data, observe_date, ndays=720):
    """
    近N天内提前结清的借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 :
        # 提前结清
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 实还日期<应还日期为提前结清，再增加近N天判断
        # 近N天提前结清借据数
        advsettle_loan1 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']<=observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        advsettle_loan2 = len(set(
        list(res[(res['period_settle_date']<res['repay_date'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        
        +list(res[(res['total_periods']<res['period'])&(res['period_settle_date']<=observe_date)
        &(res['period_settle_date']>observe_date+relativedelta(days=-ndays))]['order_no'].unique())
        ))
        
        if advsettle_loan1==len(res):
            # 近N天前全部结清 默认值-1
            nearly_advsettle_loan = -1
        else:
            nearly_advsettle_loan = advsettle_loan2
    else :
        # 无借据 -9999
        nearly_advsettle_loan =  -9999
        
    return nearly_advsettle_loan
      

def payment_pct_t30(data, observe_date, ndays=30):
    """
    T-X实际还款率：分母为当前时点放款总金额
    """
    observe_date_new = observe_date + relativedelta(days=-ndays)
    
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 本期已还本金
        prin_payed = data[data['period_settle_date']<=observe_date_new]['already_repaid_principal'].sum()
        
        loan_amount = data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
        # 截止T-x观察时间已还本金/T总放款金额
        ever_prin_repay_pct = prin_payed/loan_amount
    else :
        # 无借据 -9999
        ever_prin_repay_pct = -9999
        
    return ever_prin_repay_pct
   
def last_repay_months_to_cur(data, observe_date):
    """
    最后一次还款距现在月份数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 选择已结清的的还款计划
        data = data[data['period_settle_date']<=observe_date]
        if len(data) > 0 : 
            # 计算每期的历史逾期天数
            max_period_settle_date = data['period_settle_date'].max()
            last_repay_months_fromnow = (12*(observe_date.year-max_period_settle_date.year)
                                            + observe_date.month-max_period_settle_date.month)
        else :
            # 无还款
            last_repay_months_fromnow = -1
    else :
        # 无借据 -9999
        last_repay_months_fromnow = -9999
    
    return last_repay_months_fromnow
  
    
def his_max_repay_bymonth(data, observe_date):
    """
    历史单月最大还款金额
    """
    data['settle_mth'] = data['period_settle_date'].apply(lambda x: int(str(x)[0:7].replace('-','')))
    
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    if len(data) >0 : 
        # 选择已结清的的还款计划
        data = data[data['period_settle_date']<=observe_date]
        if len(data) > 0 : 
            # 历史单月最大还款金额
            his_max_repay_bymonth = data.groupby(['settle_mth'])['already_repaid_principal'].sum().max()           
        else :
            # 无还款
            his_max_repay_bymonth = -1
    else :
        # 无借据 -9999
        his_max_repay_bymonth = -9999
    
    return his_max_repay_bymonth
  
      
def last_repay_bymonth(data, observe_date):
    """
    最后一次还款月还款金额
    """
    data['settle_mth'] = data['period_settle_date'].apply(lambda x: int(str(x)[0:7].replace('-','')))
    
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    if len(data) >0 : 

        # 选择已结清的的还款计划
        data = data[data['period_settle_date']<=observe_date]
        if len(data) > 0 :
            # 最近一次还款月的月还款金额
            last_repay_bymonth = data.groupby(['settle_mth'])['already_repaid_principal'].sum().sort_index().iloc[-1]        
        else :
            # 无还款
            last_repay_bymonth = -1
    else :
        # 无借据 -9999
        last_repay_bymonth = -9999
    
    return last_repay_bymonth
   
def payment75pct_t30(data, observe_date, ndays=30):
    """
    T-x实际还款率：分母为T-x时点放款总金额
    """
    observe_date_new = observe_date + relativedelta(days=-ndays)
    
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date_new]

    if len(data) >0 : 

        if data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()>0.1:
            # 已还本金
            prin_payed = data[data['period_settle_date']<=observe_date_new]['already_repaid_principal'].sum()
            
            loan_amount = data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
            # 截止T-x观察时间已还本金/T-x总放款金额
            ever_prin_repay_pct2 = prin_payed/loan_amount
            if ever_prin_repay_pct2>0.75:
                ever_prin_repay_pct2 = 1
            else:
                ever_prin_repay_pct2 = 0
        else:
            # 分母为0 默认值为-1
            ever_prin_repay_pct2 = -1
    else :
        # 无借据 -9999
        ever_prin_repay_pct2 = -9999
        
    return ever_prin_repay_pct2

    
def bal60_bal0_ratio(data, observe_date, ndays=30):
    """
    T-x贷款余额/T-0贷款余额
    """
    observe_date_new = observe_date + relativedelta(days=-ndays)
    
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        # 截止T-x观察时间贷款余额/T-0贷款余额
        balx = (data[data['lending_time']<=observe_date_new][['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
                -data[data['period_settle_date']<=observe_date_new]['already_repaid_principal'].sum())
        
        bal0 = (data[data['lending_time']<=observe_date][['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
                -data[data['period_settle_date']<=observe_date]['already_repaid_principal'].sum())
        
        if bal0>0.1:
            cur_bal_repay_pct = balx/bal0
        else:
            # 分母为0 默认值为-1
            cur_bal_repay_pct = -1
    else :
        # 无借据 -9999
        cur_bal_repay_pct = -9999
        
    return cur_bal_repay_pct
    
# ----------------借款行为--------------      
def his_loan_cnt(data, observe_date):
    """
    历史成功借据数
    """   
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        return data['order_no'].nunique()
    else :
        # 无借据 -9999
        return -9999

def first_loan_months_fromnow(data, observe_date):
    """
    首笔借据距今月份数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 首笔借据距今月份数
        max_lending_time = data['lending_time'].min()
        first_loan_months_fromnow = (12*(observe_date.year-max_lending_time.year)
                                    + (observe_date.month-max_lending_time.month))
    else :
        # 无借据 -9999
        first_loan_months_fromnow = -9999
    
    return first_loan_months_fromnow
  
def all_settle_months_fromnow(data, observe_date):
    """
    客户结清距今月份数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 最后一期还款数据
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        
        if len(res[res['period_settle_date']<=observe_date])==len(res):
            # 判断是否所有借据结清
            last_period_settle_date = res[res['period_settle_date']<=observe_date]['period_settle_date'].max()
            all_settle_months_fromnow = (12*(observe_date.year - last_period_settle_date.year)
                                        +(observe_date.month - last_period_settle_date.month))
        else:
            # 客户未结清默认值-1
            all_settle_months_fromnow = -1
    else :
        # 无借据 -9999
        all_settle_months_fromnow = -9999
    
    return all_settle_months_fromnow
    
def his_loan_period_avg(data, observe_date):
    """
    历史成功借据平均期限
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]
    
    if len(data) >0 : 
        # 按照实际最大期限求avg
        loan_period = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        loan_period = loan_period[['total_periods','period']].max(axis=1).to_list()
        
        his_loan_period_avg = sum(loan_period)/len(loan_period)
    else :
        # 无借据 -9999
        his_loan_period_avg = -9999
    
    return his_loan_period_avg
    
def his_loan_amount_sum(data, observe_date):
    """
    历史成功借据金额之和
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        # 历史成功借据金额之和
        his_loan_amount_sum = data[['order_no','loan_amount']].drop_duplicates()['loan_amount'].sum()
    else :
        # 无借据 -9999
        his_loan_amount_sum = -9999
    
    return his_loan_amount_sum
     
def cur_order_nosettle_cnt(data, observe_date):
    """
    在贷借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        # 最后一期还款数据
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 在贷借据数 = 所有借据数 - 结清借据数
        cur_order_nosettle_cnt =len(res) - len(res[res['period_settle_date']<=observe_date])
    else :
        # 无借据 -9999
        cur_order_nosettle_cnt = -9999
    
    return cur_order_nosettle_cnt
     
def his_loan_settle_cnt(data, observe_date):
    """
    历史结清借据数
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        # 最后一期还款数据
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 结清借据数
        his_loan_settle_cnt = len(res[res['period_settle_date']<=observe_date])
    else :
        # 无借据 -9999
        his_loan_settle_cnt = -9999
    
    return his_loan_settle_cnt
     
def his_loan_settle_amt(data, observe_date):
    """
    历史结清借据本金
    """
    # 剔除观察时间点后的借据
    data = data[data['lending_time']<=observe_date]

    if len(data) >0 : 
        # 最后一期还款数据
        res = data.sort_values(by=['order_no','total_periods','period']).drop_duplicates(['order_no'],keep='last')
        # 结清借据金额
        his_loan_settle_amt = res[res['period_settle_date']<=observe_date]['loan_amount'].sum()
    else :
        # 无借据 -9999
        his_loan_settle_amt = -9999
    
    return his_loan_settle_amt
 
# -------------授信-------------    
def observe_credit_amt(data, observe_date, ndays=0):
    """
    最新授信额度
    """
    # 剔除观察时间点后的授信申请信息
    data = data[data['apply_date']<=observe_date+relativedelta(days=-ndays)]

    if len(data) >0 : 
        # 剔除授信失败的数据
        data = data[data['auth_status']==6]
        if len(data)>0:
            observe_credit_amt = data.sort_values(by=['apply_date'])['auth_credit_amount'].iloc[-1]
        else:
            # 没有通过授信 默认值-1
            observe_credit_amt = -1
    else :
        # 无授信 -9999
        observe_credit_amt = -9999
    
    return observe_credit_amt
    
def his_max_credit_amt(data, observe_date):
    """
    历史最高授信额度
    """
    # # 剔除观察时间点后的授信申请信息
    data = data[data['apply_date']<=observe_date]

    if len(data) >0 : 
        # 剔除授信失败的数据
        data = data[data['auth_status']==6]
        if len(data)>0:
            his_max_credit_amt = data['auth_credit_amount'].max()
        else:
            # 没有通过授信 默认值-1
            his_max_credit_amt = -1
    else :  
        # 无授信 -9999
        his_max_credit_amt = -9999
    
    return his_max_credit_amt



#==============================================================================
# File: 评分卡实现和评估.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 评分卡实现

# 评分卡刻度 
def cal_scale(score,odds,PDO,model):
    """
    odds：设定的坏好比
    score:在这个odds下的分数
    PDO: 好坏翻倍比
    model:逻辑回归模型
    
    return :A,B,base_score
    """
    B = 20/(np.log(odds)-np.log(2*odds))
    A = score-B*np.log(odds)
    base_score = A+B*model.intercept_[0]
    print('B: {:.2f}'.format(B))
    print('A: {:.2f}'.format(A))
    print('基础分为：{:.2f}'.format(base_score))
    return A,B,base_score


# 变量得分表
def score_df_concat(woe_df,model,B):
    """
    woe_df: woe结果表
    model:逻辑回归模型
    
    return:变量得分结果表
    """
    coe = list(model.coef_[0])
    columns = list(woe_df.col.unique())
    scores=[]
    for c,col in zip(coe,columns):
        score=[]
        for w in list(woe_df[woe_df.col==col].woe):
            s = round(c*w*B,0)
            score.append(s)
        scores.extend(score)
    woe_df['score'] = scores
    score_df = woe_df.copy()
    return score_df


# 分数转换 
def score_transform(df,target,df_score):
    """
    df:数据集
    target:目标变量的字段名
    df_score:得分结果表
    
    return:得分转化之后的数据集
    """
    df2 = df.copy()
    for col in df2.drop([target],axis=1).columns:
        x = df2[col]
        bin_map = df_score[df_score.col==col]
        bin_res = np.array([0]*x.shape[0],dtype=float)
        for i in bin_map.index:
            lower = bin_map['min_bin'][i]
            upper = bin_map['max_bin'][i]
            if lower == upper:
                x1 = x[np.where(x == lower)[0]]
            else:
                x1 = x[np.where((x>=lower)&(x<=upper))[0]]
            mask = np.in1d(x,x1)
            bin_res[mask] = bin_map['score'][i]
        bin_res = pd.Series(bin_res,index=x.index)
        bin_res.name = x.name
        df2[col] = bin_res
    return df2


# 得分的KS 
def plot_score_ks(df,score_col,target):
    """
    df:数据集
    target:目标变量的字段名
    score_col:最终得分的字段名
    """
    total_bad = df[target].sum()
    total_good = df[target].count()-total_bad
    score_list = list(df[score_col])
    target_list = list(df[target])
    items = sorted(zip(score_list,target_list),key=lambda x:x[0]) 
    step = (max(score_list)-min(score_list))/200 
    
    score_bin=[] 
    good_rate=[] 
    bad_rate=[] 
    ks_list = [] 
    for i in range(1,201):
        idx = min(score_list)+i*step 
        score_bin.append(idx) 
        target_bin = [x[1] for x in items if x[0]<idx]  
        bad_num = sum(target_bin)
        good_num = len(target_bin)-bad_num 
        goodrate = good_num/total_good 
        badrate = bad_num/total_bad
        ks = abs(goodrate-badrate) 
        good_rate.append(goodrate)
        bad_rate.append(badrate)
        ks_list.append(ks)
        
    fig = plt.figure(figsize=(8,6))
    ax = fig.add_subplot(1,1,1)
    ax.plot(score_bin,good_rate,color='green',label='good_rate')
    ax.plot(score_bin,bad_rate,color='red',label='bad_rate')
    ax.plot(score_bin,ks_list,color='blue',label='good-bad')
    ax.set_title('KS:{:.3f}'.format(max(ks_list)))
    ax.legend(loc='best')
    return plt.show(ax)


# PR曲线
def plot_PR(df,score_col,target,plt_size=None):
    """
    df:得分的数据集
    score_col:分数的字段名
    target:目标变量的字段名
    plt_size:绘图尺寸
    
    return: PR曲线
    """
    total_bad = df[target].sum()
    score_list = list(df[score_col])
    target_list = list(df[target])
    score_unique_list = sorted(set(list(df[score_col])))
    items = sorted(zip(score_list,target_list),key=lambda x:x[0]) 

    precison_list = []
    tpr_list = []
    for score in score_unique_list:
        target_bin = [x[1] for x in items if x[0]<=score]  
        bad_num = sum(target_bin)
        total_num = len(target_bin)
        precison = bad_num/total_num
        tpr = bad_num/total_bad
        precison_list.append(precison)
        tpr_list.append(tpr)
    
    plt.figure(figsize=plt_size)
    plt.title('PR曲线')
    plt.xlabel('查全率')
    plt.ylabel('精确率')
    plt.plot(tpr_list,precison_list,color='tomato',label='PR曲线')
    plt.legend(loc='best')
    return plt.show()


# 得分分布图
def plot_score_hist(df,target,score_col,plt_size=None,cutoff=None):
    """
    df:数据集
    target:目标变量的字段名
    score_col:最终得分的字段名
    plt_size:图纸尺寸
    cutoff :划分拒绝/通过的点
    
    return :好坏用户的得分分布图
    """    
    plt.figure(figsize=plt_size)
    x1 = df[df[target]==1][score_col]
    x2 = df[df[target]==0][score_col]
    sns.kdeplot(x1,shade=True,label='坏用户',color='hotpink')
    sns.kdeplot(x2,shade=True,label='好用户',color ='seagreen')
    plt.axvline(x=cutoff)
    plt.legend()
    return plt.show()




# 得分明细表 
def score_info(df,score_col,target,x=None,y=None,step=None):
    """
    df:数据集
    target:目标变量的字段名
    score_col:最终得分的字段名
    x:最小区间的左值
    y:最大区间的右值
    step:区间的分数间隔
    
    return :得分明细表
    """
    df['score_bin'] = pd.cut(df[score_col],bins=np.arange(x,y,step),right=True)
    total = df[target].count()
    bad = df[target].sum()
    good = total - bad
    
    group = df.groupby('score_bin')
    score_info_df = pd.DataFrame()
    score_info_df['用户数'] = group[target].count()
    score_info_df['坏用户'] = group[target].sum()
    score_info_df['好用户'] = score_info_df['用户数']-score_info_df['坏用户']
    score_info_df['违约占比'] = score_info_df['坏用户']/score_info_df['用户数']
    score_info_df['累计用户'] = score_info_df['用户数'].cumsum()
    score_info_df['坏用户累计'] = score_info_df['坏用户'].cumsum()
    score_info_df['好用户累计'] = score_info_df['好用户'].cumsum()
    score_info_df['坏用户累计占比'] = score_info_df['坏用户累计']/bad 
    score_info_df['好用户累计占比'] = score_info_df['好用户累计']/good
    score_info_df['累计用户占比'] = score_info_df['累计用户']/total 
    score_info_df['累计违约占比'] = score_info_df['坏用户累计']/score_info_df['累计用户']
    score_info_df = score_info_df.reset_index()
    return score_info_df


# 绘制提升图和洛伦兹曲线
def plot_lifting(df,score_col,target,bins=10,plt_size=None):
    """
    df:数据集，包含最终的得分
    score_col:最终分数的字段名
    target:目标变量名
    bins:分数划分成的等份数
    plt_size:绘图尺寸
    
    return:提升图和洛伦兹曲线
    """
    score_list = list(df[score_col])
    label_list = list(df[target])
    items = sorted(zip(score_list,label_list),key = lambda x:x[0])
    step = round(df.shape[0]/bins,0)
    bad = df[target].sum()
    all_badrate = float(1/bins)
    all_badrate_list = [all_badrate]*bins
    all_badrate_cum = list(np.cumsum(all_badrate_list))
    all_badrate_cum.insert(0,0)
    
    score_bin_list=[]
    bad_rate_list = []
    for i in range(0,bins,1):
        index_a = int(i*step)
        index_b = int((i+1)*step)
        score = [x[0] for x in items[index_a:index_b]]
        tup1 = (min(score),)
        tup2 = (max(score),)
        score_bin = tup1+tup2
        score_bin_list.append(score_bin)
        label_bin = [x[1] for x in items[index_a:index_b]]
        bin_bad = sum(label_bin)
        bin_bad_rate = bin_bad/bad
        bad_rate_list.append(bin_bad_rate)
    bad_rate_cumsum = list(np.cumsum(bad_rate_list))
    bad_rate_cumsum.insert(0,0)
    
    plt.figure(figsize=plt_size)
    x = score_bin_list
    y1 = bad_rate_list
    y2 = all_badrate_list
    y3 = bad_rate_cumsum
    y4 = all_badrate_cum
    plt.subplot(1,2,1)
    plt.title('提升图')
    plt.xticks(np.arange(bins)+0.15,x,rotation=90)
    bar_width= 0.3
    plt.bar(np.arange(bins),y1,width=bar_width,color='hotpink',label='score_card')
    plt.bar(np.arange(bins)+bar_width,y2,width=bar_width,color='seagreen',label='random')
    plt.legend(loc='best')
    plt.subplot(1,2,2)
    plt.title('洛伦兹曲线图')
    plt.plot(y3,color='hotpink',label='score_card')
    plt.plot(y4,color='seagreen',label='random')
    plt.xticks(np.arange(bins+1),rotation=0)
    plt.legend(loc='best')
    return plt.show()


# 设定cutoff点，衡量有效性
def rule_verify(df,col_score,target,cutoff):
    """
    df:数据集
    target:目标变量的字段名
    col_score:最终得分的字段名    
    cutoff :划分拒绝/通过的点
    
    return :混淆矩阵
    """
    df['result'] = df.apply(lambda x:30 if x[col_score]<=cutoff else 10,axis=1)
    TP = df[(df['result']==30)&(df[target]==1)].shape[0] 
    FN = df[(df['result']==30)&(df[target]==0)].shape[0] 
    bad = df[df[target]==1].shape[0] 
    good = df[df[target]==0].shape[0] 
    refuse = df[df['result']==30].shape[0] 
    passed = df[df['result']==10].shape[0] 
    
    acc = round(TP/refuse,3) 
    tpr = round(TP/bad,3) 
    fpr = round(FN/good,3) 
    pass_rate = round(refuse/df.shape[0],3) 
    matrix_df = pd.pivot_table(df,index='result',columns=target,aggfunc={col_score:pd.Series.count},values=col_score) 
    
    print('精确率:{}'.format(acc))
    print('查全率:{}'.format(tpr))
    print('误伤率:{}'.format(fpr))
    print('规则拒绝率:{}'.format(pass_rate))
    return matrix_df




#==============================================================================
# File: 评分卡监控.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 绘制变量的得分占比偏移图
def plot_var_shift(df,day_col,score_col,plt_size=None):
    """
    df:变量在一段时间内，每个区间上的得分
    day_col:时间的字段名（天）
    score_col:得分的字段名
    plt_size: 绘图尺寸
    
    return:变量区间得分的偏移图
    """
    day_list = sorted(set(list(df[day_col]))) 
    score_list = sorted(set(list(df[score_col])))
    # 计算每天各个区间得分的占比
    prop_day_list = []
    for day in day_list:
        prop_list = []
        for score in score_list:
            prop = df[(df[day_col]==day)&(df[score_col]==score)].shape[0]/df[df[day_col]==day].shape[0]
            prop_list.append(prop)
        prop_day_list.append(prop_list)
    
    # 将得分占比的转化为画图的格式
    sub_list = []
    for p in prop_day_list:
        p_cumsum = list(np.cumsum(p))
        p_cumsum = p_cumsum[:-1]
        p_cumsum.insert(0,0)
        bar1_list = [1]*int(len(p_cumsum))
        sub = [bar1_list[i]-p_cumsum[i] for i in range(len(p_cumsum))]
        sub_list.append(sub)
    array = np.array(sub_list)
    
    stack_prop_list = [] # 面积图的y值
    bar_prop_list = [] # 堆积柱状图的y
    for i in range(len(score_list)):
        bar_prop = array[:,i]
        bar_prop_list.append(bar_prop)
        stack_prop = []
        for j in bar_prop:
            a = j
            b = j
            stack_prop.append(a)
            stack_prop.append(b)
        stack_prop_list.append(stack_prop)
    
    # 画图的x坐标轴
    x_bar = list(range(1,len(day_list)*2,2)) # 堆积柱状图的x值
    x_stack = []    # 面积图的x值
    for i in x_bar:
        c = i-0.5
        d = i+0.5
        x_stack.append(c)
        x_stack.append(d)
    
    # 绘图
    fig = plt.figure(figsize=plt_size)
    ax1 = fig.add_subplot(1,1,1)
    # 先清除x轴的刻度
    ax1.xaxis.set_major_formatter(plt.FuncFormatter(''.format)) 
    ax1.set_xticks(range(1,len(day_list)*2,2))
    # 将y轴的刻度设置为百分比形式
    def to_percent(temp, position):
        return '%1.0f'%(100*temp) + '%'
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(to_percent))
    # 自定义x轴刻度标签
    for a,b in zip(x_bar,day_list): 
        ax1.text(a,-0.08,b,ha='center',va='bottom')
    # 绘制面积图和堆积柱状图
    for i,s in zip(range(len(day_list)),score_list):
        ax1.stackplot(x_stack,stack_prop_list[i],alpha=0.25)
        ax1.bar(x_bar,bar_prop_list[i],width=1,label='得分:{}'.format(s))
        # 添加y轴刻度虚线
        ax1.grid(True, 'major', 'y', ls='--', lw=.5, c='black', alpha=.3)
        ax1.legend(loc='best')
    plt.show()

    
# 计算评分的PSI
def score_psi(df1,df2,id_col,score_col,x,y,step=None):
    """
    df1:建模样本的得分,包含用户id,得分
    df2:上线样本的得分，包含用户id，得分
    id_col:用户id字段名
    score_col:得分的字段名
    x:划分得分区间的left值
    y:划分得分区间的right值
    step:步长
    
    return: 得分psi表
    """
    df1['score_bin'] = pd.cut(df1[score_col],bins=np.arange(x,y,step))
    model_score_group = df1.groupby('score_bin',as_index=False)[id_col].count().                           assign(pct=lambda x:x[id_col]/x[id_col].sum()).                           rename(columns={id_col:'建模样本户数',
                                           'pct':'建模户数占比'})
    df2['score_bin'] = pd.cut(df2[score_col],bins=np.arange(x,y,step))
    online_score_group = df2.groupby('score_bin',as_index=False)[id_col].count().                           assign(pct=lambda x:x[id_col]/x[id_col].sum()).                           rename(columns={id_col:'线上样本户数',
                                           'pct':'线上户数占比'})
    score_compare = pd.merge(model_score_group,online_score_group,on='score_bin',how='inner')
    score_compare['占比差异'] = score_compare['线上户数占比'] - score_compare['建模户数占比']
    score_compare['占比权重'] = np.log(score_compare['线上户数占比']/score_compare['建模户数占比'])
    score_compare['Index']= score_compare['占比差异']*score_compare['占比权重']
    score_compare['PSI'] = score_compare['Index'].sum()
    return score_compare


# 评分比较分布图
def plot_score_compare(df,plt_size=None):
    fig = plt.figure(figsize=plt_size)
    x = df.score_bin
    y1 = df.建模户数占比
    y2 = df.线上户数占比
    width=0.3
    plt.title('评分分布对比图')
    plt.xlabel('得分区间')
    plt.ylabel('用户占比')
    plt.xticks(np.arange(len(x))+0.15,x)
    plt.bar(np.arange(len(y1)),y1,width=width,color='seagreen',label='建模样本')
    plt.bar(np.arange(len(y2))+width,y2,width=width,color='hotpink',label='上线样本')
    plt.legend()
    return plt.show() 


# 变量稳定度分析
def var_stable(score_result,df,var,id_col,score_col,bins):
    """
    score_result:评分卡的score明细表，包含区间，用户数，用户占比,得分
    var：分析的变量名
    df:上线样本变量的得分，包含用户id,变量的value，变量的score
    id_col:df的用户id字段名
    score_col:df的得分字段名
    bins:变量划分的区间
    
    return :变量的稳定性分析表
    """
    model_var_group = score_result.loc[score_result.col==var,                      ['bin','total','totalrate','score']].reset_index(drop=True).                      rename(columns={'total':'建模用户数',
                                      'totalrate':'建模用户占比',
                                      'score':'得分'})
    df['bin'] = pd.cut(df[score_col],bins=bins)
    online_var_group = df.groupby('bin',as_index=False)[id_col].count()                         .assign(pct=lambda x:x[id_col]/x[id_col].sum())                         .rename(columns={id_col:'线上用户数',
                                          'pct':'线上用户占比'})
    var_stable_df = pd.merge(model_var_group,online_var_group,on='bin',how='inner')
    var_stable_df = var_stable_df.iloc[:,[0,3,1,2,4,5]]
    var_stable_df['得分'] = var_stable_df['得分'].astype('int64')
    var_stable_df['建模样本权重'] = np.abs(var_stable_df['得分']*var_stable_df['建模用户占比'])
    var_stable_df['线上样本权重'] = np.abs(var_stable_df['得分']*var_stable_df['线上用户占比'])
    var_stable_df['权重差距'] = var_stable_df['线上样本权重'] - var_stable_df['建模样本权重']
    return var_stable_df




#==============================================================================
# File: 身份证md5授信实时m4d30融合模型_2501_2502-Copy1.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# 设置数据存储
task_name = '身份证号md5授信实时融合模型'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./身份证号md5授信实时融合模型'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. 数据读取

# In[4]:


print(result_path)


# In[125]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# In[163]:


sql="""

    select 
     t3.*
    ,t2.*
    ,umeng_sdk_score 
    ,tianchuang_score 
    ,fico_model 
    ,haina_model
from 
    (
    select *
    from znzz_fintech_ads.lxl_model_auth_tags
    where dt>='2025-01-01' 
      and dt<='2025-03-09'
      and channel_id !=1 
    ) as t3 
    inner join     
    (
        SELECT
            order_no,
        MAX(CASE WHEN variable_code = 'id5_off_m3d30_2507' THEN good_score END) AS id5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2507' THEN good_score END) AS id5_off_m4d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509' THEN good_score END) AS id5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509v2' THEN good_score END) AS id5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'md5_off_m3d30_2507' THEN good_score END) AS md5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509' THEN good_score END) AS md5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509v2' THEN good_score END) AS md5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'M1B0071' THEN good_score END) AS M1B0071,
        MAX(CASE WHEN variable_code = 'M1B0072' THEN good_score END) AS M1B0072,
        MAX(CASE WHEN variable_code = 'M1B0073' THEN good_score END) AS M1B0073,
        MAX(CASE WHEN variable_code = 'M1B0074' THEN good_score END) AS M1B0074,
        MAX(CASE WHEN variable_code = 'M1B0075' THEN good_score END) AS M1B0075
        FROM znzz_fintech_ads.apply_model01_scores_off
        WHERE apply_time >= '2025-01-01'
          AND apply_time <= '2025-03-09'
        GROUP BY order_no
    ) as t2 on t2.order_no=t3.order_no

    left join 
    (
      select
     order_no
    ,max(case when ds='umeng_sdk_v2' then cast(value as double) else null end) as umeng_sdk_score 
    ,max(case when ds='lxl_tianchuang' then cast(value as double) else null end) as tianchuang_score
    ,max(case when ds='ypy_third_data_fico_model' then cast(value as double) else null end) as fico_model
    ,max(case when ds='ypy_third_data_haina_model' then cast(value as double) else null end) as haina_model
      from znzz_fintech_ads.fkmodel_union_model_combine_fd as t
      where dt>='' 
        and ds in ('ypy_third_data_fico_model','umeng_sdk_v2','lxl_tianchuang','ypy_third_data_haina_model') 
        and apply_date<='2025-03-09'
        and apply_date>='2025-01-01'
    group by order_no
    ) as t1  on t1.order_no=t3.order_no
    
    ; 


"""


# In[164]:


df_tmp = get_data(sql)


# In[165]:


df_tmp.info(show_counts=True)
df_tmp.head()


# In[166]:


for col in fico_id5.feature_name():
    df_tmp[col]=pd.to_numeric(df_tmp[col])


# In[167]:


fico_id5= load_model_from_pkl('./身份证号md5授信实时融合模型/身份证号md5授信实时融合模型_v1_20250917184021.pkl')
print(fico_id5.feature_name())
df_tmp['id5_off_fico_m4d30_2509'] = fico_id5.predict(df_tmp[fico_id5.feature_name()], num_iteration=fico_id5.best_iteration)
df_tmp['id5_off_fico_m4d30_2509'].head()


# In[168]:


df_tmp['apply_month']=df_tmp['apply_date'].str[0:7]


# In[169]:


df_tmp['target_ar'].value_counts()


# In[170]:


df_tmp['channel_types'] = df_tmp['channel_id'].apply(channel_type)
df_tmp['channel_rates'] = df_tmp['channel_id'].apply(channel_rate)


# In[173]:


score_list = ['id5_off_fico_m4d30_2509', 'id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2']
print(len(score_list))
print(score_list)

target_list = ['target_ar']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_tmp.loc[df_tmp[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)
df_ksauc_all_v2
# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

# df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
# df_ksauc_all1


# In[174]:


df_ksauc_all_v2.to_excel('新融合模型fico_分客群.xlsx')


# In[ ]:


df_sample_dict = {}


# In[ ]:


from datetime import timedelta


# In[175]:


sql="""

    select 
     t3.order_no as order_no1
    ,t3.id_no_des
    ,t3.user_id
    ,t3.channel_id
    ,t3.apply_date
    ,t3.target_cpd30
    ,t3.target_mob3dpd30
    ,t3.target_mob4dpd30
    ,t2.*
    ,umeng_sdk_score 
    ,tianchuang_score 
    ,fico_model 
    ,haina_model
    ,customer_tags
    ,umeng_flag
from 
    (
        select t1.order_no,t2.user_id, t2.id_no_des, t2.channel_id,t1.create_time,t1.apply_date,
        target_cpd30,target_mob3dpd30,target_mob4dpd30,
        ROW_NUMBER() OVER (PARTITION BY t1.order_no ORDER BY t1.create_time DESC) AS rk
        from znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t1 
        inner join znzz_fintech_ads.dm_f_zzj_test_user_target as t2
        on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des
        where t1.dt='2025-09-18' 
          and t1.auth_status = 6
          and t1.apply_date>='2025-01-01'
          and t1.apply_date<='2025-04-18'
          and t1.channel_id != 1
          and t2.dt='2025-09-18' 
    ) as t3 
    inner join     
    (
        SELECT
            order_no,
        MAX(CASE WHEN variable_code = 'id5_off_m3d30_2507' THEN good_score END) AS id5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2507' THEN good_score END) AS id5_off_m4d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509' THEN good_score END) AS id5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509v2' THEN good_score END) AS id5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'md5_off_m3d30_2507' THEN good_score END) AS md5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509' THEN good_score END) AS md5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509v2' THEN good_score END) AS md5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'M1B0070' THEN good_score END) AS M1B0070,
        MAX(CASE WHEN variable_code = 'M1B0071' THEN good_score END) AS M1B0071,
        MAX(CASE WHEN variable_code = 'M1B0072' THEN good_score END) AS M1B0072,
        MAX(CASE WHEN variable_code = 'M1B0073' THEN good_score END) AS M1B0073,
        MAX(CASE WHEN variable_code = 'M1B0074' THEN good_score END) AS M1B0074,
        MAX(CASE WHEN variable_code = 'M1B0075' THEN good_score END) AS M1B0075,
        MAX(CASE WHEN variable_code = 'M1B0077' THEN good_score END) AS M1B0077
        FROM znzz_fintech_ads.apply_model01_scores_off
        WHERE apply_time >= '2025-01-01'
          AND apply_time <= '2025-04-18'
        GROUP BY order_no
    ) as t2 on t2.order_no=t3.order_no and t3.rk=1
    inner join 
    (
    select *
    from znzz_fintech_ads.lxl_model_auth_tags
    where dt>='2025-01-01' 
      and dt<='2025-04-18'
      and channel_id !=1 
      and auth_status=6
    ) as t4 on t3.order_no=t4.order_no
    left join 
    (
      select
     order_no
    ,max(case when ds='umeng_sdk_v2' then cast(value as double) else null end) as umeng_sdk_score 
    ,max(case when ds='lxl_tianchuang' then cast(value as double) else null end) as tianchuang_score
    ,max(case when ds='ypy_third_data_fico_model' then cast(value as double) else null end) as fico_model
    ,max(case when ds='ypy_third_data_haina_model' then cast(value as double) else null end) as haina_model
      from znzz_fintech_ads.fkmodel_union_model_combine_fd as t
      where dt>='' 
        and ds in ('ypy_third_data_fico_model','umeng_sdk_v2','lxl_tianchuang','ypy_third_data_haina_model') 
        and apply_date<='2025-04-18'
        and apply_date>='2025-01-01'
    group by order_no
    ) as t1  on t1.order_no=t3.order_no
    
    ; 


"""


# In[176]:


df_sample_ = get_data(sql)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[280]:


varsname = df_sample_.columns.to_list()[9:-2]
print(len(varsname))
print(varsname)


# In[187]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[188]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[189]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[190]:


varsname = df_sample_.columns.to_list()[9:-6] + ['fico_model']


# In[281]:


df_sample = df_sample_.query("target_mob4dpd30>=0 ")
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[192]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[193]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-28'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-04-18'").index, 'data_set']='3_oot2'


# In[194]:


df_sample.to_csv('前筛实时模型250919.csv',index=False)


# In[279]:


df_sample_.to_csv('前筛实时模型250919_ys.csv',index=False)


# # 1. 样本概况

# In[195]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[196]:


print(df_sample[target].value_counts())


# In[197]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[198]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[199]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[200]:


# # 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[201]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[202]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[203]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[204]:


varsname


# In[205]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='quantile', n_bins=10)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[ ]:





# In[206]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[207]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[208]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[209]:


target


# In[210]:


varsname_v2 = varsname[:]


# In[211]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[212]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[213]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[214]:


new_bins_dict


# In[215]:


combiner.load(new_bins_dict)


# In[216]:


combiner.export()


# In[217]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[218]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[219]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[220]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[221]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[222]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[223]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[224]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[225]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'友盟联合建模_v6_20250717140214.bin')  # 自动识别 .txt/.bin/.json


# ## 5.1 数据预处理

# In[226]:


df_sample[target].value_counts()


# In[227]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[228]:


df_sample[modeltrian_target].value_counts()


# In[229]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[230]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[231]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[232]:


df_sample['channel_types'].value_counts()


# In[233]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[234]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[252]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.07
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 80


# In[ ]:


lgb_params = {
        'objective': 'binary',          # 二分类问题
        'metric': 'auc',                # 评估指标
        'boosting_type': 'gbdt',        # 提升类型：梯度提升决策树
        'num_leaves':8,               # 树的最大叶子数 2^max_depth
        'min_data_in_leaf': 800,
        'min_gain_to_split': 80,
        'min_sum_hessian_in_leaf':0.13, 
        'learning_rate': 0.07,           # 学习率 0.03
        'feature_fraction': 0.7,
        'bagging_fraction': 0.7,
        'bagging_freq': 1,
        'max_depth': 3,   # 4
        # 'subsample': 0.8,               # 样本采样比例
        'colsample_bytree': 0.8,        # 特征采样比例
        'reg_alpha': 5,               # L1正则化
        'reg_lambda': 7,              # L2正则化
        'verbose': -1,                  # 控制日志输出
        'random_state': 42,             # 随机种子
        'num_threads': 40,
        'seed': 0,
        'scale_pos_weight':1,
    }


# In[253]:


print("最优参数opt_params: ", opt_params)


# In[254]:


varsname


# In[268]:


varsname_base = ['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0072',
 'm1b0073',
 'm1b0074',
 'm1b0075',
 'm1b0077',
 'fico_model']


# In[269]:


print(len(varsname_base))
print(varsname_base)


# In[270]:


# 确定数据集参数后，训练模型
X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
print(df_sample.groupby(['data_set'])['order_no'].count())
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
print(df_sample.groupby(['data_set'])['order_no'].count())


# In[271]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[272]:


# 优化后评估模型效果
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[273]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[274]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[276]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[277]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[265]:


df_sample["customer_tags"].value_counts()


# In[266]:


df_sample['fico数据是否缺失']=df_sample['fico_model'].apply(lambda x: '1_不缺失' if pd.notna(x) else '2_有缺失' )
df_sample['fico数据是否缺失'].value_counts()


# In[278]:


# 按 flag 分组计算
df_ks_auc_set_all = (
    df_sample[df_sample['fico_model'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v1', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all


# In[250]:


fico_model0918= load_model_from_pkl('./身份证号md5授信实时融合模型/身份证号md5授信实时融合模型_v1_20250917184021.pkl')
print(fico_model0918.feature_name())
df_sample['y_pred_v2'] = fico_model0918.predict(df_sample[fico_model0918.feature_name()], num_iteration=fico_model0918.best_iteration)
df_sample['y_pred_v2'].head()


# In[251]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample[df_sample['fico_model'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v2', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[65]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 参数优化

# In[ ]:


def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=3, max_evals=50):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数""" 
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': 0,
                'lambda_l2': 300,
                'min_gain_to_split':10,
                'early_stopping_rounds': 30,
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[ ]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[ ]:


# 确定数据集参数后，训练模型
X_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]


# In[ ]:


# 2 定义搜索空间
spaces = {
          # general parameters
          "learning_rate":hp.uniform('learning_rate', 0.05, 0.1),
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",15,31,1),
          'max_depth': hp.quniform('max_depth', 4, 6, 1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",50,150,10),
          "feature_fraction":hp.uniform("feature_fraction",0.7,0.9),
          "bagging_fraction":hp.uniform("bagging_fraction",0.7,0.9)
          }


# In[ ]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=30)
print("best_params: ", best_params)


# In[ ]:


bst_params = {
        #general parameters
        'objective': 'binary',
        'boosting': 'gbdt',
        'metric': 'auc',
        'learning_rate': best_params['learning_rate'], # 学习率,超参
        #tuning parameters
        'num_leaves': int(best_params['num_leaves']), # 叶子节点数, 超参
        'min_data_in_leaf': int(best_params['min_data_in_leaf']), # 一个叶子上数据的最小数量, 超参
        'max_depth': int(best_params['max_depth']), # 树的深度
        'bagging_freq': 1,
        'bagging_fraction': best_params['bagging_fraction'], # 数据抽样, 超参
        'feature_fraction': best_params['feature_fraction'], # 特征抽样, 超参
        'lambda_l1': 0, # l1 正则化
        'lambda_l2': 300, # l2 正则化
        'min_gain_to_split':10, # 切分最小增益
        'early_stopping_rounds': 30,
        'scale_pos_weight': 1,
        'seed': 1
        }
print("最优参数bst_params: ", bst_params)


# In[ ]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("最优参数opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# 优化后评估模型效果
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[ ]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['fico数据是否缺失','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='分客群')  
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[66]:


df_sample.info(show_counts=True)


# In[67]:


df_zh = pd.read_csv('score.csv')
df_zh.info(show_counts=True)
df_zh.head()


# ### 5.3.1数据处理

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[68]:


df_evalue = pd.merge(df_sample, df_zh, how='inner', on='order_no')
df_evalue.info(show_counts=True)
df_evalue.head()


# In[69]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[70]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[71]:


filepath = '/home/liaoxilin/联合建模/友盟sdk&百行多头/'
umeng_model= load_model_from_pkl(filepath + 'result_友盟联合分融合模型/友盟联合分融合模型_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[72]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_友盟Fico/友盟Fico离线融合_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[73]:


fico_model_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型/fico联合分融合模型_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[74]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v2/fico联合分融合模型v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl


# In[75]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[76]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[77]:


# 方法2：使用向量化操作（更高效）
a_has_data = df_evalue['fico_model'].notna()  # 等价于 ~df['col_a'].isna()
# b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score'].notna()

df_evalue['友盟fico是否缺失'] = np.where(
    a_has_data  & c_has_data,  # 都有数据（都不是NaN）
    '1_都不缺失',
    np.where(
        ~a_has_data  & ~c_has_data,  # 都无数据（都是NaN）
        '2_都有缺失',
        None  # 其他情况（一个有数据一个无数据）
    )
)


# In[78]:


df_evalue['友盟fico是否缺失'].value_counts(dropna=False)


# ### 5.3.2 效果对比

# In[79]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[80]:


print(df_evalue.columns.to_list())


# In[81]:


df_evalue.info(show_counts=True)


# In[82]:


com_list = ['y_pred_v1','score','fico_score','tianchuang_score','id5_off_cpd30_2508',
            'id5_off_fico_cpd30_2508',
            'id5_off_fico_v3_1_cpd30_2508',
            'id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507',
            'md5_off_m4d30_2509v2']


# In[94]:


com_list = ['y_pred_v1','score','fico_score','tianchuang_score',
            'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507',
            'md5_off_m4d30_2509v2']


# In[83]:


map_dict = {'3_oot2':'20250301-20250415','2_test':'20250101-20250228','1_train':'20250101-20250228'}


# In[ ]:


# df_evalue['channel_types'] = df_evalue['channel_id'].apply(channel_type)
# df_evalue['channel_rates'] = df_evalue['channel_id'].apply(channel_rate)


# In[ ]:


# df_evalue.loc[df_evalue.query("apply_date>='2025-01-01' & apply_date<='2025-02-28'").index, 'data_set']='1_train'
# df_evalue.loc[df_evalue.query("apply_date>='2025-03-01' & apply_date<='2025-04-05'").index, 'data_set']='3_oot2'


# In[ ]:


# X_train_ = df_evalue.query("data_set not in ('3_oot1','3_oot2')")[varsname]
# y_train_ = df_evalue.query("data_set not in ('3_oot1','3_oot2')")['target_mob4dpd30_1']
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_evalue.loc[X_train.index, 'data_set']='1_train'
# df_evalue.loc[X_test.index, 'data_set']='2_test'


# In[95]:


score_list = com_list
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
tmp_df_evalue.shape


# In[89]:





# In[84]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[85]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2.insert(0, 'time_windowns', value=df_ksauc_all2['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all2


# In[86]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[87]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all4.insert(1, 'time_windowns', value=df_ksauc_all4['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all4


# In[88]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','fico数据是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','fico数据是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','fico数据是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("fico数据是否缺失=='1_不缺失' & flag=='1_新客' & channel=='金科渠道'").reset_index(drop=True)


# # 6. 评分分布

# In[ ]:





# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


score = 'y_pred_v3'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[ ]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[ ]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix_month)


# In[ ]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[ ]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[ ]:


df_evalue.to_csv(result_path + 'fico离线融合_report.csv',index=False)
print(result_path + 'fico离线融合_report.csv')


# # 7.模型部署和回溯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## 字段名称
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## 本地文件，注意：只写文件名即可，参数是 list 类型
        csv_filename_list=['天创模型分数v2.csv'],
        ## 本地文件路径，注意：需要本地的绝对路径
        input_path='/data/home/liaoxilin/联合建模/友盟sdk&百行多头/',                    
        ## 上传的数据库
        database='znzz_fintech_ads',        
        ## 上传的表名
        table_name='lxl_model_',
        # 分区字段
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # 自定义分隔符
        delimiter='|'
       ) 




#==============================================================================
# File: 身份证md5授信实时m4d30融合模型_2501_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# 设置数据存储
task_name = '身份证号md5授信实时融合模型'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./身份证号md5授信实时融合模型'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. 数据读取

# In[4]:


print(result_path)


# In[125]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# In[163]:


sql="""

    select 
     t3.*
    ,t2.*
    ,umeng_sdk_score 
    ,tianchuang_score 
    ,fico_model 
    ,haina_model
from 
    (
    select *
    from znzz_fintech_ads.lxl_model_auth_tags
    where dt>='2025-01-01' 
      and dt<='2025-03-09'
      and channel_id !=1 
    ) as t3 
    inner join     
    (
        SELECT
            order_no,
        MAX(CASE WHEN variable_code = 'id5_off_m3d30_2507' THEN good_score END) AS id5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2507' THEN good_score END) AS id5_off_m4d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509' THEN good_score END) AS id5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509v2' THEN good_score END) AS id5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'md5_off_m3d30_2507' THEN good_score END) AS md5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509' THEN good_score END) AS md5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509v2' THEN good_score END) AS md5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'M1B0071' THEN good_score END) AS M1B0071,
        MAX(CASE WHEN variable_code = 'M1B0072' THEN good_score END) AS M1B0072,
        MAX(CASE WHEN variable_code = 'M1B0073' THEN good_score END) AS M1B0073,
        MAX(CASE WHEN variable_code = 'M1B0074' THEN good_score END) AS M1B0074,
        MAX(CASE WHEN variable_code = 'M1B0075' THEN good_score END) AS M1B0075
        FROM znzz_fintech_ads.apply_model01_scores_off
        WHERE apply_time >= '2025-01-01'
          AND apply_time <= '2025-03-09'
        GROUP BY order_no
    ) as t2 on t2.order_no=t3.order_no

    left join 
    (
      select
     order_no
    ,max(case when ds='umeng_sdk_v2' then cast(value as double) else null end) as umeng_sdk_score 
    ,max(case when ds='lxl_tianchuang' then cast(value as double) else null end) as tianchuang_score
    ,max(case when ds='ypy_third_data_fico_model' then cast(value as double) else null end) as fico_model
    ,max(case when ds='ypy_third_data_haina_model' then cast(value as double) else null end) as haina_model
      from znzz_fintech_ads.fkmodel_union_model_combine_fd as t
      where dt>='' 
        and ds in ('ypy_third_data_fico_model','umeng_sdk_v2','lxl_tianchuang','ypy_third_data_haina_model') 
        and apply_date<='2025-03-09'
        and apply_date>='2025-01-01'
    group by order_no
    ) as t1  on t1.order_no=t3.order_no
    
    ; 


"""


# In[164]:


df_tmp = get_data(sql)


# In[165]:


df_tmp.info(show_counts=True)
df_tmp.head()


# In[166]:


for col in fico_id5.feature_name():
    df_tmp[col]=pd.to_numeric(df_tmp[col])


# In[167]:


fico_id5= load_model_from_pkl('./身份证号md5授信实时融合模型/身份证号md5授信实时融合模型_v1_20250917184021.pkl')
print(fico_id5.feature_name())
df_tmp['id5_off_fico_m4d30_2509'] = fico_id5.predict(df_tmp[fico_id5.feature_name()], num_iteration=fico_id5.best_iteration)
df_tmp['id5_off_fico_m4d30_2509'].head()


# In[168]:


df_tmp['apply_month']=df_tmp['apply_date'].str[0:7]


# In[169]:


df_tmp['target_ar'].value_counts()


# In[170]:


df_tmp['channel_types'] = df_tmp['channel_id'].apply(channel_type)
df_tmp['channel_rates'] = df_tmp['channel_id'].apply(channel_rate)


# In[173]:


score_list = ['id5_off_fico_m4d30_2509', 'id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2']
print(len(score_list))
print(score_list)

target_list = ['target_ar']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_tmp.loc[df_tmp[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)
df_ksauc_all_v2
# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

# df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
# df_ksauc_all1


# In[174]:


df_ksauc_all_v2.to_excel('新融合模型fico_分客群.xlsx')


# In[ ]:


df_sample_dict = {}


# In[ ]:


from datetime import timedelta


# In[175]:


sql="""

    select 
     t3.order_no as order_no1
    ,t3.id_no_des
    ,t3.user_id
    ,t3.channel_id
    ,t3.apply_date
    ,t3.target_cpd30
    ,t3.target_mob3dpd30
    ,t3.target_mob4dpd30
    ,t2.*
    ,umeng_sdk_score 
    ,tianchuang_score 
    ,fico_model 
    ,haina_model
    ,customer_tags
    ,umeng_flag
from 
    (
        select t1.order_no,t2.user_id, t2.id_no_des, t2.channel_id,t1.create_time,t1.apply_date,
        target_cpd30,target_mob3dpd30,target_mob4dpd30,
        ROW_NUMBER() OVER (PARTITION BY t1.order_no ORDER BY t1.create_time DESC) AS rk
        from znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t1 
        inner join znzz_fintech_ads.dm_f_zzj_test_user_target as t2
        on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des
        where t1.dt='2025-09-18' 
          and t1.auth_status = 6
          and t1.apply_date>='2025-01-01'
          and t1.apply_date<='2025-04-18'
          and t1.channel_id != 1
          and t2.dt='2025-09-18' 
    ) as t3 
    inner join     
    (
        SELECT
            order_no,
        MAX(CASE WHEN variable_code = 'id5_off_m3d30_2507' THEN good_score END) AS id5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2507' THEN good_score END) AS id5_off_m4d30_2507,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509' THEN good_score END) AS id5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'id5_off_m4d30_2509v2' THEN good_score END) AS id5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'md5_off_m3d30_2507' THEN good_score END) AS md5_off_m3d30_2507,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509' THEN good_score END) AS md5_off_m4d30_2509,
        MAX(CASE WHEN variable_code = 'md5_off_m4d30_2509v2' THEN good_score END) AS md5_off_m4d30_2509v2,
        MAX(CASE WHEN variable_code = 'M1B0070' THEN good_score END) AS M1B0070,
        MAX(CASE WHEN variable_code = 'M1B0071' THEN good_score END) AS M1B0071,
        MAX(CASE WHEN variable_code = 'M1B0072' THEN good_score END) AS M1B0072,
        MAX(CASE WHEN variable_code = 'M1B0073' THEN good_score END) AS M1B0073,
        MAX(CASE WHEN variable_code = 'M1B0074' THEN good_score END) AS M1B0074,
        MAX(CASE WHEN variable_code = 'M1B0075' THEN good_score END) AS M1B0075,
        MAX(CASE WHEN variable_code = 'M1B0077' THEN good_score END) AS M1B0077
        FROM znzz_fintech_ads.apply_model01_scores_off
        WHERE apply_time >= '2025-01-01'
          AND apply_time <= '2025-04-18'
        GROUP BY order_no
    ) as t2 on t2.order_no=t3.order_no and t3.rk=1
    inner join 
    (
    select *
    from znzz_fintech_ads.lxl_model_auth_tags
    where dt>='2025-01-01' 
      and dt<='2025-04-18'
      and channel_id !=1 
      and auth_status=6
    ) as t4 on t3.order_no=t4.order_no
    left join 
    (
      select
     order_no
    ,max(case when ds='umeng_sdk_v2' then cast(value as double) else null end) as umeng_sdk_score 
    ,max(case when ds='lxl_tianchuang' then cast(value as double) else null end) as tianchuang_score
    ,max(case when ds='ypy_third_data_fico_model' then cast(value as double) else null end) as fico_model
    ,max(case when ds='ypy_third_data_haina_model' then cast(value as double) else null end) as haina_model
      from znzz_fintech_ads.fkmodel_union_model_combine_fd as t
      where dt>='' 
        and ds in ('ypy_third_data_fico_model','umeng_sdk_v2','lxl_tianchuang','ypy_third_data_haina_model') 
        and apply_date<='2025-04-18'
        and apply_date>='2025-01-01'
    group by order_no
    ) as t1  on t1.order_no=t3.order_no
    
    ; 


"""


# In[176]:


df_sample_ = get_data(sql)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[280]:


varsname = df_sample_.columns.to_list()[9:-2]
print(len(varsname))
print(varsname)


# In[187]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[188]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[189]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[190]:


varsname = df_sample_.columns.to_list()[9:-6] + ['fico_model']


# In[281]:


df_sample = df_sample_.query("target_mob4dpd30>=0 ")
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[192]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[193]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-28'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-04-18'").index, 'data_set']='3_oot2'


# In[194]:


df_sample.to_csv('前筛实时模型250919.csv',index=False)


# In[279]:


df_sample_.to_csv('前筛实时模型250919_ys.csv',index=False)


# # 1. 样本概况

# In[195]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[196]:


print(df_sample[target].value_counts())


# In[197]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[198]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[199]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[200]:


# # 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[201]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[202]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[203]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[204]:


varsname


# In[205]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='quantile', n_bins=10)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[ ]:





# In[206]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[207]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[208]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[209]:


target


# In[210]:


varsname_v2 = varsname[:]


# In[211]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[212]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[213]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[214]:


new_bins_dict


# In[215]:


combiner.load(new_bins_dict)


# In[216]:


combiner.export()


# In[217]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[218]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[219]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[220]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[221]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[222]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[223]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[224]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[225]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'友盟联合建模_v6_20250717140214.bin')  # 自动识别 .txt/.bin/.json


# ## 5.1 数据预处理

# In[226]:


df_sample[target].value_counts()


# In[227]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[228]:


df_sample[modeltrian_target].value_counts()


# In[229]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[230]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[231]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[232]:


df_sample['channel_types'].value_counts()


# In[233]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[234]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[252]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.07
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 80


# In[ ]:


lgb_params = {
        'objective': 'binary',          # 二分类问题
        'metric': 'auc',                # 评估指标
        'boosting_type': 'gbdt',        # 提升类型：梯度提升决策树
        'num_leaves':8,               # 树的最大叶子数 2^max_depth
        'min_data_in_leaf': 800,
        'min_gain_to_split': 80,
        'min_sum_hessian_in_leaf':0.13, 
        'learning_rate': 0.07,           # 学习率 0.03
        'feature_fraction': 0.7,
        'bagging_fraction': 0.7,
        'bagging_freq': 1,
        'max_depth': 3,   # 4
        # 'subsample': 0.8,               # 样本采样比例
        'colsample_bytree': 0.8,        # 特征采样比例
        'reg_alpha': 5,               # L1正则化
        'reg_lambda': 7,              # L2正则化
        'verbose': -1,                  # 控制日志输出
        'random_state': 42,             # 随机种子
        'num_threads': 40,
        'seed': 0,
        'scale_pos_weight':1,
    }


# In[253]:


print("最优参数opt_params: ", opt_params)


# In[254]:


varsname


# In[268]:


varsname_base = ['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0072',
 'm1b0073',
 'm1b0074',
 'm1b0075',
 'm1b0077',
 'fico_model']


# In[269]:


print(len(varsname_base))
print(varsname_base)


# In[270]:


# 确定数据集参数后，训练模型
X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
print(df_sample.groupby(['data_set'])['order_no'].count())
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
print(df_sample.groupby(['data_set'])['order_no'].count())


# In[271]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[272]:


# 优化后评估模型效果
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[273]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[274]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[276]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[277]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[265]:


df_sample["customer_tags"].value_counts()


# In[266]:


df_sample['fico数据是否缺失']=df_sample['fico_model'].apply(lambda x: '1_不缺失' if pd.notna(x) else '2_有缺失' )
df_sample['fico数据是否缺失'].value_counts()


# In[278]:


# 按 flag 分组计算
df_ks_auc_set_all = (
    df_sample[df_sample['fico_model'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v1', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all


# In[250]:


fico_model0918= load_model_from_pkl('./身份证号md5授信实时融合模型/身份证号md5授信实时融合模型_v1_20250917184021.pkl')
print(fico_model0918.feature_name())
df_sample['y_pred_v2'] = fico_model0918.predict(df_sample[fico_model0918.feature_name()], num_iteration=fico_model0918.best_iteration)
df_sample['y_pred_v2'].head()


# In[251]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample[df_sample['fico_model'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v2', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[65]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 参数优化

# In[ ]:


def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=3, max_evals=50):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数""" 
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': 0,
                'lambda_l2': 300,
                'min_gain_to_split':10,
                'early_stopping_rounds': 30,
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[ ]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[ ]:


# 确定数据集参数后，训练模型
X_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]


# In[ ]:


# 2 定义搜索空间
spaces = {
          # general parameters
          "learning_rate":hp.uniform('learning_rate', 0.05, 0.1),
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",15,31,1),
          'max_depth': hp.quniform('max_depth', 4, 6, 1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",50,150,10),
          "feature_fraction":hp.uniform("feature_fraction",0.7,0.9),
          "bagging_fraction":hp.uniform("bagging_fraction",0.7,0.9)
          }


# In[ ]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=30)
print("best_params: ", best_params)


# In[ ]:


bst_params = {
        #general parameters
        'objective': 'binary',
        'boosting': 'gbdt',
        'metric': 'auc',
        'learning_rate': best_params['learning_rate'], # 学习率,超参
        #tuning parameters
        'num_leaves': int(best_params['num_leaves']), # 叶子节点数, 超参
        'min_data_in_leaf': int(best_params['min_data_in_leaf']), # 一个叶子上数据的最小数量, 超参
        'max_depth': int(best_params['max_depth']), # 树的深度
        'bagging_freq': 1,
        'bagging_fraction': best_params['bagging_fraction'], # 数据抽样, 超参
        'feature_fraction': best_params['feature_fraction'], # 特征抽样, 超参
        'lambda_l1': 0, # l1 正则化
        'lambda_l2': 300, # l2 正则化
        'min_gain_to_split':10, # 切分最小增益
        'early_stopping_rounds': 30,
        'scale_pos_weight': 1,
        'seed': 1
        }
print("最优参数bst_params: ", bst_params)


# In[ ]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("最优参数opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# 优化后评估模型效果
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[ ]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['fico数据是否缺失','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='分客群')  
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[66]:


df_sample.info(show_counts=True)


# In[67]:


df_zh = pd.read_csv('score.csv')
df_zh.info(show_counts=True)
df_zh.head()


# ### 5.3.1数据处理

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[68]:


df_evalue = pd.merge(df_sample, df_zh, how='inner', on='order_no')
df_evalue.info(show_counts=True)
df_evalue.head()


# In[69]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[70]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[71]:


filepath = '/home/liaoxilin/联合建模/友盟sdk&百行多头/'
umeng_model= load_model_from_pkl(filepath + 'result_友盟联合分融合模型/友盟联合分融合模型_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[72]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_友盟Fico/友盟Fico离线融合_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[73]:


fico_model_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型/fico联合分融合模型_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[74]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v2/fico联合分融合模型v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl


# In[75]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[76]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[77]:


# 方法2：使用向量化操作（更高效）
a_has_data = df_evalue['fico_model'].notna()  # 等价于 ~df['col_a'].isna()
# b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score'].notna()

df_evalue['友盟fico是否缺失'] = np.where(
    a_has_data  & c_has_data,  # 都有数据（都不是NaN）
    '1_都不缺失',
    np.where(
        ~a_has_data  & ~c_has_data,  # 都无数据（都是NaN）
        '2_都有缺失',
        None  # 其他情况（一个有数据一个无数据）
    )
)


# In[78]:


df_evalue['友盟fico是否缺失'].value_counts(dropna=False)


# ### 5.3.2 效果对比

# In[79]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[80]:


print(df_evalue.columns.to_list())


# In[81]:


df_evalue.info(show_counts=True)


# In[82]:


com_list = ['y_pred_v1','score','fico_score','tianchuang_score','id5_off_cpd30_2508',
            'id5_off_fico_cpd30_2508',
            'id5_off_fico_v3_1_cpd30_2508',
            'id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507',
            'md5_off_m4d30_2509v2']


# In[94]:


com_list = ['y_pred_v1','score','fico_score','tianchuang_score',
            'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507',
            'md5_off_m4d30_2509v2']


# In[83]:


map_dict = {'3_oot2':'20250301-20250415','2_test':'20250101-20250228','1_train':'20250101-20250228'}


# In[ ]:


# df_evalue['channel_types'] = df_evalue['channel_id'].apply(channel_type)
# df_evalue['channel_rates'] = df_evalue['channel_id'].apply(channel_rate)


# In[ ]:


# df_evalue.loc[df_evalue.query("apply_date>='2025-01-01' & apply_date<='2025-02-28'").index, 'data_set']='1_train'
# df_evalue.loc[df_evalue.query("apply_date>='2025-03-01' & apply_date<='2025-04-05'").index, 'data_set']='3_oot2'


# In[ ]:


# X_train_ = df_evalue.query("data_set not in ('3_oot1','3_oot2')")[varsname]
# y_train_ = df_evalue.query("data_set not in ('3_oot1','3_oot2')")['target_mob4dpd30_1']
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_evalue.loc[X_train.index, 'data_set']='1_train'
# df_evalue.loc[X_test.index, 'data_set']='2_test'


# In[95]:


score_list = com_list
tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
tmp_df_evalue.shape


# In[89]:





# In[84]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[85]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2.insert(0, 'time_windowns', value=df_ksauc_all2['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all2


# In[86]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[87]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all4.insert(1, 'time_windowns', value=df_ksauc_all4['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all4


# In[88]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','fico数据是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','fico数据是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','fico数据是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("fico数据是否缺失=='1_不缺失' & flag=='1_新客' & channel=='金科渠道'").reset_index(drop=True)


# # 6. 评分分布

# In[ ]:





# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


score = 'y_pred_v3'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[ ]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[ ]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix_month)


# In[ ]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[ ]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[ ]:


df_evalue.to_csv(result_path + 'fico离线融合_report.csv',index=False)
print(result_path + 'fico离线融合_report.csv')


# # 7.模型部署和回溯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## 字段名称
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## 本地文件，注意：只写文件名即可，参数是 list 类型
        csv_filename_list=['天创模型分数v2.csv'],
        ## 本地文件路径，注意：需要本地的绝对路径
        input_path='/data/home/liaoxilin/联合建模/友盟sdk&百行多头/',                    
        ## 上传的数据库
        database='znzz_fintech_ads',        
        ## 上传的表名
        table_name='lxl_model_',
        # 分区字段
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # 自定义分隔符
        delimiter='|'
       ) 




#==============================================================================
# File: 转转-三方数据评分卡建模-LR-final.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[928]:


import pandas as pd
import numpy as np
import toad
import os 
from datetime import datetime
from sklearn.model_selection import train_test_split
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[966]:


os.getcwd()


# In[1573]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# # 1.读取数据集

# In[3]:


data_target = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob_9_or_6.csv') 
data_vars = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_total_three_part_data.csv') 
print('数据大小：', data_target.shape)
print('数据大小：', data_vars.shape)


# In[4]:


data_target.info()


# In[5]:


data_vars.info()


# In[6]:


data_vars.columns[0:9]


# In[7]:


data = pd.merge(data_target, data_vars, how='inner', on='user_id')
data.info()
print(data.shape)


# In[ ]:





# In[8]:


print(data.user_id.nunique(), data.order_no.nunique())


# In[9]:


data['auth_status'].value_counts(dropna=False)


# In[10]:


data.query("auth_status==7")


# In[11]:


# 去重
data = data.sort_values(['user_id', 'apply_time'], ascending=False).drop_duplicates(subset=['user_id'],keep='first')


# In[12]:


print(data.shape, data.user_id.nunique(), data.order_no.nunique())


# In[13]:


data_copy = data.copy()


# ## 1.1标签处理

# In[14]:


data['target_mob9'].value_counts(dropna=False)


# In[16]:


data = data[data['target_mob9'].isin([0.0, 1.0])]
data.info()


# In[17]:


tmp = data.groupby(['lending_month'])['target_mob9'].agg({'count','sum','mean'})
tmp = pd.concat([tmp, pd.DataFrame(tmp.sum(axis=0),columns=['总计']).T], axis=0)
tmp.loc['总计', 'mean'] = tmp.loc['总计', 'sum'] /tmp.loc['总计', 'count']
tmp


# In[18]:


data = data.reset_index(drop=True)
data.info()


# In[19]:


data_copy_copy = data.copy()


# In[931]:


data = data_copy_copy.copy()
data['target'] = data['target_mob9']


# In[932]:


data.columns[0:17]


# ## 1.2拆分数据

# In[933]:


to_drop = list(data.columns)[0:17]
print(to_drop)


# In[934]:


data = data.reset_index(drop=True)
data.info()


# In[935]:


df_model = data.drop(to_drop, axis=1)


# In[936]:


df_model.info()


# In[958]:


df_var = df_model.drop(['target'], axis=1)
df_target = df_model['target']

X = np.array(df_var)
y = np.array(df_target)


# In[959]:


X_train, X_test, y_train, y_test = train_test_split(df_var ,df_target, test_size=0.3, random_state=22, stratify=y)


# In[942]:


df_model.loc[X_train.index,'sample_set'] = 'train'
df_model.loc[X_test.index,'sample_set'] = 'valid'


# In[943]:


train = pd.merge(X_train, y_train, how='inner', left_index=True, right_index=True)
print(train.shape, X_train.shape, y_train.shape)


# In[944]:


valid = pd.merge(X_test, y_test, how='inner', left_index=True, right_index=True)
print(valid.shape, X_test.shape, y_test.shape)


# In[945]:


print(train['target'].value_counts())
print(valid['target'].value_counts())


# # 2.数据探索分析

# In[946]:


train.info()
train.shape


# In[947]:


valid.info()
valid.shape


# In[948]:


df_model.info()
df_model.shape


# In[949]:


df_model.select_dtypes(include='object').columns


# In[950]:


to_drop = list(df_model.select_dtypes(include='object').columns)


# In[405]:


df_explore = toad.detect(df_model.drop(to_drop,axis=1))
df_explore['total'] = 'all'


# In[406]:


df_explore.shape


# In[407]:


for name, tmp_df in df_model.groupby(['sample_set']):
    print(name)
    tmp_explore = toad.detect(tmp_df.drop(to_drop,axis=1))
    tmp_explore['type_{}'.format(name)] = name
    df_explore = pd.merge(df_explore, tmp_explore, how='left', left_index=True, right_index=True,suffixes=['',f'_{name}'])
    
# train_df_explore = toad.detect(train.drop(to_drop,axis=1))
# oot_df_explore = toad.detect(oot.drop(to_drop,axis=1))
# train_df_iv = toad.quality(train.drop(to_drop,axis=1),'target',iv_only=True)
# oot_df_iv = toad.quality(oot.drop(to_drop,axis=1),'target',iv_only=True)


# In[260]:


df_explore.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_model_数据探索性分析_final.xlsx")


# # 3.特征筛选

# In[1177]:


train = df_model.query("sample_set=='train'").drop('sample_set', axis=1)
valid = df_model.query("sample_set=='valid'").drop('sample_set', axis=1)
train_selected, dropped = toad.selection.select(train, target='target', empty=0.90, iv=0.02, corr=0.70, return_drop=True, exclude=None)


# In[1178]:


train_selected.shape


# In[1179]:


for i in dropped.keys():
    print("变量筛选维度：{}， 共剔除变量{}".format(i, len(dropped[i])))


# In[1180]:


dropped_vars = pd.Series(dropped)


# In[1181]:


df_iv = toad.quality(train_selected,'target',iv_only=True)
df_iv.head(3)


# In[1182]:


df_iv['iv'].describe()


# In[1183]:


df_iv[df_iv['iv']>=0.5].index


# In[415]:


import xgboost as xgb
from xgboost import plot_importance

xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 100
)

xgb_model.fit(train_selected.drop('target',axis=1), train_selected['target'])
feature_importances_xgb = xgb_model.get_booster().get_score(importance_type='gain')
feature_importances_xgb = pd.Series(feature_importances_xgb)


# In[419]:


feature_importances_xgb.describe()


# In[427]:


feature_importances_xgb = feature_importances_xgb.sort_values(ascending=False)


# In[420]:


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(train_selected.drop('target',axis=1).fillna(-9999), train_selected['target'])


# In[421]:


feature_importances = pd.Series(rf.feature_importances_, index=train_selected.drop('target',axis=1).columns)
feature_importances = feature_importances.sort_values(ascending=False)
feature_importances.shape


# In[422]:


feature_importances.describe()


# In[426]:


jj_feature_importances = [x for x in feature_importances_xgb.index if x in feature_importances.head(100).index] 
len(jj_feature_importances)


# In[617]:


train_selected_copy = train_selected.copy()


# In[833]:


train_selected = train_selected_copy.copy()


# In[429]:


feature_importances_xgb.head()


# In[728]:


train_selected = train_selected[list(feature_importances_xgb.index) +['target']]
train_selected.shape


# # 4. 变量分箱

# In[1184]:


# 第一次分箱
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='chi', min_samples=0.01, n_bins=20, empty_separate=True) 
bins_result = c.export()


# In[1185]:


train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins.head(2)


# In[1186]:


valid_selected = valid[train_selected.columns]
valid_selected_bins = c.transform(valid_selected, labels=True)
valid_selected_bins.head(2)


# In[1187]:


df_result = pd.DataFrame()
for col in train_selected_bins.columns[:-1]:
#     print('------------变量：{}-----------'.format(col))
    tmp = regroup(train_selected_bins, col, target='target')
    df_result = pd.concat([df_result, tmp], axis=0)


# In[1188]:


df_result_valid = pd.DataFrame()
for col in valid_selected_bins.columns[:-1]:
#     print('------------变量：{}-----------'.format(col))
    tmp = regroup(valid_selected_bins, col, target='target')
    df_result_valid = pd.concat([df_result_valid, tmp], axis=0)


# In[1189]:


tmp = pd.merge(df_result, df_result_valid, how='left', on=['bins', 'varsname'], suffixes=['_train', '_valid'])


# In[1190]:


tmp.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mondel_变量分箱_卡方_20_final.xlsx',index=False)


# In[1191]:


# 自动调整分箱
adj_bins = {}
for col in list(bins_result.keys()):
    tmp = np.isnan(bins_result[col])
    cutbins = [bins_result[col][x] for x in range(len(tmp)) if not tmp[x]]
    if len(cutbins)>0:
        cutbins = [float('-inf')] + cutbins + [float('inf')]
        cutoffpoints = ContinueVarBins(train_selected, col, flag='target', cutbins=cutbins)
        if train_selected[col].min()==cutoffpoints[0]:
            adj_bins[col] = [-0.5] + cutoffpoints[1:]
        else:
            adj_bins[col] = [-0.5] + cutoffpoints


# In[1192]:


# 调整分箱:空值单独一箱
train_selected_copy =train_selected.copy()
train_selected.fillna(-9999,inplace=True)

# adj_bins = {}
# for col in list(bins_result.keys()):
#     tmp = np.isnan(bins_result[col])
#     cutbins = [bins_result[col][x] for x in range(len(tmp)) if not tmp[x]]
#     if len(cutbins)>0:
#         cutbins = [float('-inf')] + cutbins + [float('inf')]
#         cutoffpoints = ContinueVarBins(train_selected, col, flag='target', cutbins=cutbins)
#         adj_bins[col] = cutoffpoints
# 更新分箱
c.update(adj_bins)


# In[ ]:


# c.export()
# c.load(dict)
# c.transform(dataframe, labels=False)


# ### 观察分箱并调整

# In[124]:


from toad.plot import bin_plot
from toad.plot import badrate_plot

# to_drop = ['target','type']
data_new = pd.concat([train_selected, oot_selected],axis=0)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[0]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[1]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[2]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[3]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[4]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[5]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[6]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[7]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[8]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[9]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[10]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# # from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# # badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[11]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# # from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# # badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 调整分箱
# adj_bins = {'value_097_bairong': [-0.5, 3.0, 4,0, 5.0, 6.0, 9.0]}
# c.update(adj_bins)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[12]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[14]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[15]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[16]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# # 5.WOE转换

# In[1193]:


transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_selected), train_selected['target'], exclude=['target'])


# In[1555]:


import pickle
with open('zz_model_combiner_mob9.pkl', 'wb') as f1:
    pickle.dump(c, f1)
    
with open('zz_model_transer_mob9.pkl', 'wb') as f2:
    pickle.dump(transer, f2)


# In[1194]:


train_woe.shape


# In[1195]:


train_woe, dropped = toad.selection.select(train_woe, target='target', empty=0.9, iv=0.02, corr=0.70, return_drop=True, exclude=None)
print(train_woe.shape)


# In[1198]:


df_regroup = pd.DataFrame()
# train_selected_bins = c.transform(train_selected)
bins_data = c.transform(train_selected)[list(train_woe.columns)]

for col in bins_data.columns[:-1]:
    tmp = CalWoeIv(bins_data, col, target='target')
    df_regroup = pd.concat([df_regroup, tmp], axis=0)


# In[1386]:


df_regroup.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\df_regroup.xlsx') 


# In[1199]:


to_drop1 = list(set(df_regroup.query("iv>=0.50")['varsname']))
to_drop2 = list(set(df_regroup.query("iv<=0.02")['varsname']))
print(len(to_drop1), len(to_drop2))


# In[1200]:


to_drop1


# In[1201]:


train_woe.drop(to_drop1+to_drop2, axis=1, inplace=True)
train_woe.shape


# ## Corr

# In[1202]:


df_iv = df_regroup.drop_duplicates(subset=['varsname'],keep='first').set_index('varsname')
df_iv.head()


# In[1203]:


to_drop_corr = CorrSelect(train_woe, df_iv, exclude_list=['target'], threshold=0.7)
print(len(to_drop_corr))


# # 6.逐步回归

# In[1050]:


# train_woe.drop(to_drop_corr,axis=1,inplace=True)
# train_woe.shape


# In[1204]:


train_woe.shape


# In[1205]:


train_woe.info()


# In[1206]:


# 'value_019_pudao_4','value_021_pudao_4',
to_drop = ['model_score_01_2_bairong_8','model_score_01_hangliezhi_1','model_score_01_fulin_1']
train_woe.drop(to_drop, axis=1, inplace=True)


# In[1207]:


final_data = toad.selection.stepwise(train_woe, target='target', estimator='ols', direction='both', criterion='aic', exclude=None)
final_data.shape


# In[1208]:


final_data.info()


# In[1209]:


final_data.info()


# In[1210]:


set(final_data.columns) - set(feature_importances[feature_importances>0.01].index)


# In[1211]:


set(final_data.columns) - set(feature_importances_xgb.index)


# In[1212]:


final_valid = transer.transform(c.transform(valid[list(final_data.columns)]))
final_valid.shape


# In[1213]:


cols = list(final_data.drop(['target'], axis=1).columns)
print(cols)


# In[1214]:


print(toad.metrics.PSI(final_data[cols], final_valid[cols]))


# # 7.建模和评估

# In[1239]:


def lr(X_train,X_test,y_train,y_test):
    from sklearn.linear_model import LogisticRegression
    from scipy import stats
    from toad.metrics import KS,AUC 
    # 模型训练和p_values查看 只能是线性回归
    clf = LogisticRegression(C=1e8).fit(X_train, y_train)
    params = np.append(clf.intercept_,clf.coef_)
    
    new_X_train = pd.DataFrame({"Constant":np.ones(len(X_train))}).join(X_train.reset_index(drop=True))
    predictions = clf.predict(X_train)
    MSE = (sum((y_train-predictions)**2))/(len(new_X_train)-len(new_X_train.columns))

    var_b = MSE*(np.linalg.inv(np.dot(new_X_train.T,new_X_train)).diagonal())
    sd_b = np.sqrt(var_b)
    ts_b = params/ sd_b

    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(new_X_train)-1))) for i in ts_b]

    sd_b = np.round(sd_b,3)
    ts_b = np.round(ts_b,3)
    p_values = np.round(p_values,3)
    params = np.round(params,4)

    myDF3 = pd.DataFrame()
    myDF3["Vars"],myDF3["Coefficients"],myDF3["Standard Errors"],myDF3["t values"],myDF3["P values"] = [new_X_train.columns,params,sd_b,ts_b,p_values]
    print(myDF3)
    
    # 预测值
    pred_train = clf.predict_proba(X_train)[:,1]
    pred_test = clf.predict_proba(X_test)[:,1]
    
    # 训练集KS/AUC
    print('-------------训练集结果--------------------')
    print('train AUC: ', AUC(pred_train, y_train))
    print('train KS: ', KS(pred_train, y_train))
    
    # 测试集KS/AUC
    print('-------------测试集结果--------------------')
    print('test AUC: ', AUC(pred_test, y_test))
    print('test KS: ', KS(pred_test, y_test))
    
    print('-------------------------分割线--------------------------')
    # 模型评估
    train_AUC = roc_auc_score(y_train, pred_train)
    train_fpr, train_tpr, train_thresholds = roc_curve(y_train,pred_train)
    train_KS = max(train_tpr - train_fpr)
    print('TRAIN AUC: {}'.format(train_AUC))
    print('TRAIN KS: {}'.format(train_KS))
    
    test_AUC = roc_auc_score(y_test, pred_test)
    test_fpr, test_tpr, test_thresholds = roc_curve(y_test,pred_test)
    test_KS = max(test_tpr - test_fpr)
    print('Test AUC: {}'.format(test_AUC))
    print('Test KS: {}'.format(test_KS))
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(train_fpr,train_tpr,color='darkorange',lw=3,label='Train ROC curve (Area = %0.2f)'%train_AUC)
    plt.plot(test_fpr,test_tpr,color='navy',lw=3,label='Test ROC curve (Area = %0.2f)'%test_AUC)
    plt.plot([0,1],[0,1],color='gray',lw=1,linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title('Train&Test ROC curve',fontsize=25)
    plt.legend(loc='lower right',fontsize=20)

    return (myDF3, params, clf)


# In[1387]:


cols


# In[1388]:


tmp_corr = final_data[cols].corr()


# In[1678]:


cols


# In[1679]:


tmp_vif = sklearn_vif(cols, final_data[cols])
tmp_vif


# In[1680]:


from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
X = np.matrix(final_data[cols])
vif['features']=cols
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
vif


# In[1389]:


# 保存统计的数据
writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mobdel_mob9_vif_corr.xlsx')
tmp_corr.to_excel(writer,sheet_name='corr')
vif.to_excel(writer,sheet_name='vif')
writer.save()


# In[1230]:


# 单次训练
def LR_model(X_train,X_test,y_train,y_test):
    clf = sm.Logit(y_train, sm.add_constant(X_train)).fit()
    print(clf.summary())
    # 模型评估
    train_y_pred = clf.predict(sm.add_constant(X_train))
    train_AUC = roc_auc_score(y_train, train_y_pred)
    train_fpr, train_tpr, train_thresholds = roc_curve(y_train,train_y_pred)
    train_KS = max(train_tpr - train_fpr)
    print('TRAIN AUC: {}'.format(train_AUC))
    print('TRAIN KS: {}'.format(train_KS))
    
    test_y_pred = clf.predict(sm.add_constant(X_test))
    test_AUC = roc_auc_score(y_test,test_y_pred)
    test_fpr, test_tpr, test_thresholds = roc_curve(y_test,test_y_pred)
    test_KS = max(test_tpr - test_fpr)
    print('Test AUC: {}'.format(test_AUC))
    print('Test KS: {}'.format(test_KS))
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(train_fpr,train_tpr,color='darkorange',lw=3,label='Train ROC curve (Area = %0.2f)'%train_AUC)
    plt.plot(test_fpr,test_tpr,color='navy',lw=3,label='Test ROC curve (Area = %0.2f)'%test_AUC)
    plt.plot([0,1],[0,1],color='gray',lw=1,linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title('Train&Test ROC curve',fontsize=25)
    plt.legend(loc='lower right',fontsize=20)
    
    return clf


# In[1557]:


cols


# In[1558]:


cols.remove('apply_loan_result_json_als_m12_id_pdl_orgnum_bairong_1')


# In[1559]:


cols


# In[1711]:


LR_model(final_data[cols], final_valid[cols], final_data['target'], final_valid['target'])


# In[1560]:


clf = LR_model(final_data[cols], final_valid[cols], final_data['target'], final_valid['target'])


# In[1554]:


import pickle
with open('zz_model_sm_mob9.pkl', 'wb') as f1:
    pickle.dump(clf, f1)
    
with open('zz_model_sklearn_mob9.pkl', 'wb') as f2:
    pickle.dump(lr_clr, f2)


# In[1561]:


lr_result, lr_param, lr_clr = lr(final_data[cols], final_valid[cols], final_data['target'], final_valid['target'])


# In[1124]:


# cols.remove('model_score_01_aliyun_2')
# cols.remove('model_score_01_2_bairong_8')
# cols.remove('apply_loan_result_json_als_m12_id_nbank_cf_orgnum_bairong_1')
# cols.remove('model_score_01_tengxun_1')
# cols.remove('model_score_01_baihang_1')


# In[1245]:


df_regroup_final = pd.DataFrame()
train_selected_bins = c.transform(train_selected,labels=True)[list(final_data.columns)]

for col in train_selected_bins.columns[:-1]:
    tmp = CalWoeIv(train_selected_bins, col, target='target')
    df_regroup_final = pd.concat([df_regroup_final, tmp], axis=0)


# In[1246]:


df_regroup_final.head()


# In[1225]:


df_regroup_final.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mondel_变量分箱_卡_20_final.xlsx',index=False)


# # 9.转换评分

# In[1562]:


card = toad.ScoreCard(combiner=c,
                      transer = transer,
                      base_odds=3,
                      base_score=700,
                      pdo=60,
                      rate=2,
                      C=1e8
                     )


# In[1563]:


card.fit(final_data[cols], final_data['target'])


# In[1370]:


import pickle
with open('zz_card_mob9.pkl', 'wb') as f:
    pickle.dump(card, f)


# In[1484]:


card.coef_


# In[1696]:


card.coef_


# In[1697]:


card.features_


# In[1698]:


card1 = card.export(to_frame=True).round(0)
card1


# In[1250]:


card1 = card.export(to_frame=True)
card1


# In[1257]:


df_regroup_final['new_bins'] = df_regroup_final['bins'].str.split('[').apply(lambda x:'['+x[1])
df_regroup_final.head()


# In[1258]:


final_card = pd.merge(df_regroup_final,card1,how='inner',left_on=['varsname', 'new_bins'], right_on=['name','value'])
final_card.head()


# In[1259]:


final_card.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\card_final.xlsx')


# # 10.分数分布

# In[1564]:


train['score'] = card.predict(train[cols].fillna(-9999)).round(0)
valid['score'] = card.predict(valid[cols].fillna(-9999)).round(0)


# In[1565]:


train[cols+['score']].info()


# In[1566]:


print(train['score'].min(), train['score'].max())
print(valid['score'].min(), valid['score'].max())


# In[1708]:


# 分割点
train['q_bins'] = pd.qcut(train['score'], 10, duplicates='drop')
train['s_bins'] = pd.cut(train['score'], 10, right=True, include_lowest=False)
cutbins_q = [float('-inf')] + list(train.groupby(['q_bins'])['score'].max())
cutbins_s = [float('-inf')] + list(train.groupby(['s_bins'])['score'].max())
print(cutbins_q)
print(cutbins_s)


# In[1709]:


# cutbins_s = [float('-inf')] + list(np.floor(np.linspace(train['score'].quantile(0.001),train['score'].max(),10)))
cutbins_s = cutbins_s[0:1]+cutbins_s[3:]
print(cutbins_s)


# In[1508]:


# 分割点
train['q_bins'] = pd.qcut(train['score'], 10, duplicates='drop')
train['s_bins'] = pd.cut(train['score'], 10, right=True, include_lowest=False)
cutbins_q = [float('-inf')] + list(train.groupby(['q_bins'])['score'].max())
cutbins_s = [float('-inf')] + list(train.groupby(['s_bins'])['score'].max())
print(cutbins_q)
print(cutbins_s)


# In[1710]:


def score_distribute(data, col, target='target'):    
    total = data.groupby(col)[target].count()
    bad = data.groupby(col)[target].sum()
    regroup = pd.concat([total, bad],axis=1)
    regroup.columns = ['total', 'bad']
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_rate'] = regroup['bad']/regroup['total']
    regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
    regroup['total_pct'] = regroup['total']/regroup['total'].sum()
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
    regroup['good_pct'] = regroup['good']/regroup['good'].sum()
    regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
    regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
    regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
    regroup['ks'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
    regroup['varsname'] = col
    regroup['bins'] = regroup.index
    regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
    usecols = ['varsname','bins','bad','good','total','ks', 'bad_rate','bad_rate_cum','lift_cum','total_pct_cum'
            ,'total_pct', 'bad_pct','good_pct','bad_pct_cum','good_pct_cum']
    return_regroup = regroup[usecols]
    return_regroup = return_regroup.reset_index(drop=True)

    return return_regroup


# In[1715]:


# 训练集分数分布
train['q_bins'] = pd.cut(train['score'], cutbins_q, right=True, include_lowest=False)
train['s_bins'] = pd.cut(train['score'], cutbins_s, right=True, include_lowest=False)
train_q_bins = score_distribute(train, 'q_bins', target='target')
train_s_bins = score_distribute(train, 's_bins', target='target')
train_q_bins.reset_index(drop=True,inplace=True)
train_s_bins.reset_index(drop=True,inplace=True)


# In[1716]:


# 测试集分数分布
valid['q_bins'] = pd.cut(valid['score'], cutbins_q, right=True)
valid['s_bins'] = pd.cut(valid['score'], cutbins_s, right=True)
valid_q_bins = score_distribute(valid, 'q_bins', target='target')
valid_s_bins = score_distribute(valid, 's_bins', target='target')
valid_q_bins.reset_index(drop=True,inplace=True)
valid_s_bins.reset_index(drop=True,inplace=True)


# In[1717]:


writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_model_分数分布_v7_mob9.xlsx')
train_q_bins.to_excel(writer,sheet_name='train_等频')
train_s_bins.to_excel(writer,sheet_name='train_等宽')
valid_q_bins.to_excel(writer,sheet_name='valid_等频')
valid_s_bins.to_excel(writer,sheet_name='valid_等宽')
writer.save()


# In[1625]:


def cal_psi(exp, act):
    psi = []
    for i in range(len(exp)):
        psi_i = (act[i] - exp[i])*np.log(act[i]/exp[i])
        psi.append(psi_i)
    return sum(psi)


# In[1662]:


print(cal_psi(train_s_bins['total_pct'], valid_s_bins['total_pct'])) 


# In[1684]:


print(cal_psi(train_s_bins['total_pct'], valid_s_bins['total_pct'])) 


# In[1685]:


train_s_bins


# In[1682]:


train_s_bins['total_pct']


# In[1683]:


valid_s_bins['total_pct']


# # 11.全部授信申请客户

# In[1345]:


# 全部授信申请客户
df1_auth = pd.merge(data_vars, data_target, how='left', on='user_id')
df1_auth.info()
print(data.shape)


# In[1346]:


print(df1_auth['user_id'].nunique(), df1_auth.shape)


# In[1347]:


df2_auth = df1_auth.sort_values(['user_id','apply_time'], ascending=False).drop_duplicates(subset=['user_id'],keep='first')


# In[1348]:


df2_auth.columns[0:17]


# In[1349]:


index = list(data.query("auth_status==7 & target_mob9==target_mob9").index)
index


# In[1307]:


# index = list(data.query("auth_status==7 & target_mob9==target_mob9").index)
# data.drop(index=index,axis=0,inplace=True)
# data.shape


# In[1664]:


cols


# In[1665]:


usecols_=list(data.columns[0:17])+cols
print(usecols_)


# In[1666]:


df3_auth = df2_auth[usecols_]
df3_auth.info(show_counts=True)


# In[1667]:


for col in cols:
    df3_auth[col].fillna(-9999, inplace=True)
df3_auth.info(show_counts=True)


# In[1668]:


card.features_


# In[1669]:


df3_auth['score'] = card.predict(df3_auth[cols]).round(0)
df3_auth.shape


# In[1670]:


df3_auth['score'].head()


# In[1719]:


# cutbins_q = [float('-inf')] + list(train.groupby(['q_bins'])['score'].max())
# cutbins_s = [float('-inf')] + list(train.groupby(['s_bins'])['score'].max())
print( cutbins_s)
print(cutbins_q)


# In[1720]:


df3_auth['bins_step'] = pd.cut(df3_auth['score'], cutbins_s, right=True)
df3_auth['bins_quantile'] = pd.cut(df3_auth['score'], cutbins_q, right=True)


# In[1674]:


def cal_auth(df_auth, bins='bins', target='target'):
    total_lend = df_auth.groupby(bins)['order_no',target].count()
    tg_jj = df_auth.groupby([bins, 'auth_status'])['order_no'].count().unstack()
    lend =  df_auth.groupby([bins, target])['order_no'].count().unstack()
    result = pd.concat([total_lend, tg_jj, lend], axis=1)
    result.columns = ['授信申请','放款人数','通过人数','拒绝人数','未到期','好','灰','坏']

    result_sum = pd.DataFrame(result.sum(axis=0),columns=['total']).T

    for col in result.columns:
        result['{}占比'.format(col)] = result[col]/result[col].sum()
        result['{}累计占比'.format(col)] = result['{}占比'.format(col)].cumsum()

    result['坏客率'] = result['坏']/(result['好']+result['坏'])
    result['通过率'] = result['通过人数']/result['授信申请']
    result = pd.concat([result, result_sum],axis=0)

    cols = ['授信申请','授信申请占比','授信申请累计占比','通过人数','通过率','通过人数占比','通过人数累计占比','拒绝人数','拒绝人数占比','拒绝人数累计占比',
            '放款人数','放款人数占比','放款人数累计占比','好','好占比','好累计占比',
            '坏','坏占比','坏累计占比','坏客率','灰','灰占比','灰累计占比','未到期','未到期占比','未到期累计占比']
    result = result[cols]
    
    return result


# In[1721]:


result_auth_step = cal_auth(df3_auth, bins='bins_step', target='target_mob9')
result_auth_quantile = cal_auth(df3_auth, bins='bins_quantile', target='target_mob9')


# In[1676]:


result_auth_step.head(2)


# In[1722]:


result_auth_step.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_model_分数分布_授信申请_mob9_s_v4.xlsx")
result_auth_quantile.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_model_分数分布_授信申请_mob9_q_v3.xlsx")


# In[1365]:


df3_auth.drop(list(df3_auth.columns)[1:9],axis=1).to_csv(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_建模_评分卡_mob9_score_20230825.csv",index=False)


# # oot验证

# In[1456]:


oot_data = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob3.csv')


# In[1457]:


oot_data.info()


# In[1458]:


oot_data['target'].value_counts()


# In[1459]:


data.info()


# In[1465]:


data['target_mob9'].value_counts()


# In[1466]:


oot_data_new = oot_data[~oot_data['user_id'].isin(data['user_id'])]
oot_data_new.shape


# In[1467]:


oot_data_new['target'].value_counts()


# In[1498]:


tmp = oot_data_new.groupby(['lending_month','target'])['user_id'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[1469]:


cols = ['model_score_01_baihang_1',
 'model_score_01_tengxun_1',
 'model_score_01_3_xinyongsuanli_1',
 'model_score_01_aliyun_2',
 'apply_loan_result_json_als_m12_id_pdl_orgnum_bairong_1',
 'apply_loan_result_json_als_m6_id_oth_orgnum_bairong_1']


# In[1686]:


cols


# In[1687]:


oot = pd.merge(oot_data_new, data_vars, how='inner', on='user_id')[cols+['target']]
oot.shape


# In[1689]:


oot.info()


# In[1520]:


oot_copy = oot.copy()


# In[1688]:


oot['target'].value_counts()


# In[1521]:


oot['target'].value_counts()


# In[1690]:


oot = oot[oot['target'].isin([0.0, 1.0])]
oot = oot.reset_index(drop=True)
oot.shape


# In[1691]:


oot.shape


# In[1692]:


oot['target'].value_counts()


# In[1528]:


oot['target'].mean()


# In[1693]:


oot.info()


# In[1695]:


clf.params


# In[1694]:


oot_mob3_15 = transer.transform(c.transform(oot.fillna(-9999)))
oot_mob3_15['prob'] = clf.predict(sm.add_constant(oot_mob3_15[cols]))

from toad.metrics import KS,AUC
print('train AUC: ', AUC(oot_mob3_15['prob'], oot_mob3_15['target']))
print('train KS: ', KS(oot_mob3_15['prob'], oot_mob3_15['target']))


# In[1714]:


oot_AUC = roc_auc_score(oot_mob3_15['target'],oot_mob3_15['prob'])
oot_fpr, oot_tpr, oot_thresholds = roc_curve(oot_mob3_15['target'],oot_mob3_15['prob'])
oot_KS = max(oot_tpr - oot_fpr)
print('Test AUC: {}'.format(oot_AUC))
print('Test KS: {}'.format(oot_KS))
fig = plt.figure(figsize=(10,10))
plt.plot(oot_fpr,oot_tpr,color='darkorange',lw=3,label='Train ROC curve (Area = %0.2f)'%oot_AUC)
plt.plot([0,1],[0,1],color='gray',lw=1,linestyle='--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('False Positive Rate',fontsize=16)
plt.ylabel('True Positive Rate',fontsize=16)
plt.title('Train&Test ROC curve',fontsize=25)
plt.legend(loc='lower right',fontsize=20)


# In[1699]:


cols


# In[1700]:


oot_mob3_15['prob'] = lr_clr.predict_proba(oot_mob3_15[cols])[:,1]

from toad.metrics import KS,AUC
print('train AUC: ', AUC(oot_mob3_15['prob'], oot_mob3_15['target']))
print('train KS: ', KS(oot_mob3_15['prob'], oot_mob3_15['target']))


# In[1701]:


oot['score'] = card.predict(oot[cols].fillna(-9999)).round(0)


# In[1723]:


oot['q_bins'] = pd.cut(oot['score'], cutbins_q, right=True, include_lowest=False)
oot['s_bins'] = pd.cut(oot['score'], cutbins_s, right=True, include_lowest=False)
oot_q_bins = score_distribute(oot, 'q_bins', target='target')
oot_s_bins = score_distribute(oot, 's_bins', target='target')
oot_q_bins.reset_index(drop=True,inplace=True)
oot_s_bins.reset_index(drop=True,inplace=True)


# In[1706]:


oot_s_bins


# In[1543]:


oot_s_bins


# In[1552]:


def cal_psi(exp, act):
    psi = []
    for i in range(len(exp)):
        psi_i = (act[i] - exp[i])*np.log(act[i]/exp[i])
        psi.append(psi_i)
    return sum(psi)

print(cal_psi(train_s_bins['total_pct'], oot_s_bins['total_pct'])) 


# In[1724]:


writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_model_分数分布_v6_mob9_oot.xlsx')
oot_q_bins.to_excel(writer,sheet_name='oot_等频')
oot_s_bins.to_excel(writer,sheet_name='oot_等宽')
writer.save()


# In[60]:


df3_auth = pd.read_csv(r'd:\liuyedao\model_result\oot_data_20230815.csv')
df3_auth.info()


# In[8]:


def cal_score_tc(x):
    score_list = []
    if x<555.5:
        score=188
    elif x>=555.5 and x<599.5:
        score=192
    elif x>=555.5 and x<599.5:
        score=192
    elif x>=599.5:
        score=212
    else:
        score=187
    return score
        
def cal_score_bh(x):      
    if x<699.5:
        score=182
    elif x>=699.5 and x<719.5:
        score=188
    elif x>=719.5 and x<734.5:
        score=193
    elif x>=734.5 and x<756.5:
        score=204
    elif x>=756.5 and x<774.5:
        score=211
    elif x>=774.5:
        score=218
    else:
        score=199
    return score

def cal_score_xysl(x):  
    if x<569.5:
        score=153
    elif x>=569.5 and x<612.5:
        score=171
    elif x>=612.5 and x<635.5:
        score=185
    elif x>=635.5 and x<660.5:
        score=196
    elif x>=660.5 and x<680.5:
        score=206
    elif x>=680.5 and x<699.5:
        score=217
    elif x>=699.5 and x<723.5:
        score=234
    elif x>=723.5:
        score=273
    else:
        score=159
    
    return score
        


# In[9]:


usecols = ['order_no','user_id','id_no_des','channel_id','apply_date_auth','apply_time','auth_status',
           'model_score_01_x_tianchuang','model_score_01_xysl_3','model_score_01_baihang','Firs6ever30','score_lr']

df3_auth['model_score_01_x_tianchuang_score'] = df3_auth['model_score_01_x_tianchuang'].apply(lambda x: cal_score_tc(x))
df3_auth['model_score_01_baihang_score'] = df3_auth['model_score_01_baihang'].apply(lambda x: cal_score_bh(x))
df3_auth['model_score_01_xysl_3_score'] = df3_auth['model_score_01_xysl_3'].apply(lambda x: cal_score_xysl(x))
df3_auth['score_lr'] = df3_auth[['model_score_01_x_tianchuang_score', 'model_score_01_baihang_score','model_score_01_xysl_3_score']].sum(axis=1)

df4_auth = df3_auth[usecols]


# In[11]:


df4_auth.info()
df4_auth.to_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_LR_score_20230815_oot.csv",index=False)


# In[12]:


df4_auth.info(show_counts=True)


# In[62]:


df3_auth['apply_month'] = df3_auth['apply_date_auth'].str[0:7]
df3_auth['apply_month'].value_counts()


# In[63]:


df3_auth['Firs6ever30'].value_counts()


# In[64]:


list(lr.feature_names_in_)


# In[66]:


auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()


# In[68]:


# 对训练集进行预测
auth_data_mob6 = df3_auth[df3_auth['Firs6ever30'].isin([0.0, 2.0])][list(lr.feature_names_in_) + ['Firs6ever30','apply_month','order_no']]
# auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()
auth_data_mob6 = auth_data_mob6[auth_data_mob6['apply_month']=='2023-01']
auth_data_mob6['target'] = auth_data_mob6['Firs6ever30'] /2.0
# auth_data_mob6.info()
auth_data_mob6 = transer.transform(c.transform(auth_data_mob6.fillna(-999)))
auth_data_mob6['prob'] = lr.predict_proba(auth_data_mob6[list(lr.feature_names_in_)])[:,1]

from toad.metrics import KS,AUC
print('train AUC: ', AUC(auth_data_mob6['prob'], auth_data_mob6['target']))
print('train KS: ', KS(auth_data_mob6['prob'], auth_data_mob6['target']))


# In[74]:


# 对训练集进行预测
auth_data_mob3 = df3_auth[df3_auth['Firs3ever30'].isin([0.0, 2.0])][list(lr.feature_names_in_) + ['Firs3ever30','apply_month','order_no']]
# auth_data_mob3.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()
auth_data_mob3 = auth_data_mob3[auth_data_mob3['apply_month']!='2023-05']
auth_data_mob3['target'] = auth_data_mob3['Firs3ever30'] /2.0
# auth_data_mob3.info()
auth_data_mob3 = transer.transform(c.transform(auth_data_mob3.fillna(-999)))
auth_data_mob3['prob'] = lr.predict_proba(auth_data_mob3[list(lr.feature_names_in_)])[:,1]



# In[ ]:


from toad.metrics import KS,AUC
print('train AUC: ', AUC(auth_data_mob3['prob'], auth_data_mob3['target']))
print('train KS: ', KS(auth_data_mob3['prob'], auth_data_mob3['target']))


# In[76]:


auth_data_mob3.info()


# In[75]:


auth_data_mob3.groupby(['apply_month', 'Firs3ever30'])['order_no'].count().unstack()


# In[78]:


for i in list(auth_data_mob3['apply_month'].unique()):
    print(i)
    tmp = auth_data_mob3[auth_data_mob3['apply_month']==i]
    print('train AUC: ', AUC(tmp['prob'], tmp['target']))
    print('train KS: ', KS(tmp['prob'], tmp['target']))
    print('-----------------------------------')




#==============================================================================
# File: 转转-三方数据评分卡建模-LR-mob6.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import toad
import os 
from datetime import datetime
from sklearn.model_selection import train_test_split
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


os.getcwd()


# In[3]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# # 1.读取数据集

# In[4]:


data_target = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob_9_or_6.csv') 
data_vars = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_total_three_part_data.csv') 
print('数据大小：', data_target.shape)
print('数据大小：', data_vars.shape)


# In[5]:


data_target.info()


# In[6]:


data_vars.info()


# In[6]:


data_vars.columns[0:9]


# In[7]:


data = pd.merge(data_vars, data_target, how='left', on='user_id')
data.info()
print(data.shape)


# In[8]:


print(data.user_id.nunique(), data.order_no.nunique())


# In[9]:


data['auth_status'].value_counts(dropna=False)


# In[10]:


data.query("auth_status==7")


# In[11]:


# 去重
# data = data.sort_values(['user_id', 'apply_time'], ascending=False).drop_duplicates(subset=['user_id'],keep='first')


# In[12]:


print(data.shape, data.user_id.nunique(), data.order_no.nunique())


# In[13]:


# data_copy = data.copy()


# ## 1.1标签处理

# In[14]:


data['target_mob6'].value_counts(dropna=False)


# In[17]:


del data_vars, data_target
gc.collect()
data_model = data[data['target_mob6'].isin([0.0, 1.0])]
data_model.info()


# In[19]:


tmp = data_model.groupby(['lending_month'])['target_mob6'].agg({'count','sum','mean'})
tmp = pd.concat([tmp, pd.DataFrame(tmp.sum(axis=0),columns=['总计']).T], axis=0)
tmp.loc['总计', 'mean'] = tmp.loc['总计', 'sum'] /tmp.loc['总计', 'count']
tmp


# In[20]:


data_model = data_model.reset_index(drop=True)
data_model.info()


# In[21]:


data_model = data_model.copy()


# In[22]:


data_model['target'] = data_model['target_mob6']


# ## 1.2拆分数据

# In[24]:


to_drop = ['user_id', 'target_mob6', 'target_mob9', 'ever_overdue_days_mob6', 'ovd_status_ever_mob6', 'ever_overdue_days_mob9', 'ovd_status_ever_mob9', 'lending_time', 'lending_month', 'order_no', 'id_no_des', 'channel_id', 'auth_status', 'apply_date', 'apply_time', 'apply_month', 'auth_credit_amount']
print(to_drop)


# In[25]:


df_model = data_model.drop(to_drop, axis=1)


# In[26]:


df_model.info()


# In[27]:


df_var = df_model.drop(['target'], axis=1)
df_target = df_model['target']

X = np.array(df_var)
y = np.array(df_target)


# In[28]:


X_train, X_test, y_train, y_test = train_test_split(df_var ,df_target, test_size=0.3, random_state=22, stratify=y)


# In[29]:


df_model.loc[X_train.index,'sample_set'] = 'train'
df_model.loc[X_test.index,'sample_set'] = 'valid'


# In[30]:


train = pd.merge(X_train, y_train, how='inner', left_index=True, right_index=True)
print(train.shape, X_train.shape, y_train.shape)


# In[31]:


valid = pd.merge(X_test, y_test, how='inner', left_index=True, right_index=True)
print(valid.shape, X_test.shape, y_test.shape)


# In[32]:


print(train['target'].value_counts())
print(valid['target'].value_counts())


# # 2.数据探索分析

# In[33]:


train.info()
train.shape


# In[34]:


valid.info()
valid.shape


# In[35]:


df_model.info()
df_model.shape


# In[36]:


df_model.select_dtypes(include='object').columns


# In[37]:


to_drop = list(df_model.select_dtypes(include='object').columns)


# In[38]:


df_explore = toad.detect(df_model.drop(to_drop,axis=1))
df_explore['total'] = 'all'


# In[39]:


df_explore.shape


# In[40]:


for name, tmp_df in df_model.groupby(['sample_set']):
    print(name)
    tmp_explore = toad.detect(tmp_df.drop(to_drop,axis=1))
    tmp_explore['type_{}'.format(name)] = name
    df_explore = pd.merge(df_explore, tmp_explore, how='left', left_index=True, right_index=True,suffixes=['',f'_{name}'])
    
# train_df_explore = toad.detect(train.drop(to_drop,axis=1))
# oot_df_explore = toad.detect(oot.drop(to_drop,axis=1))
# train_df_iv = toad.quality(train.drop(to_drop,axis=1),'target',iv_only=True)
# oot_df_iv = toad.quality(oot.drop(to_drop,axis=1),'target',iv_only=True)


# In[41]:


df_explore.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_mob6_model_数据探索性分析_final.xlsx")


# # 3.特征筛选

# In[42]:


# train = df_model.query("sample_set=='train'").drop('sample_set', axis=1)
# valid = df_model.query("sample_set=='valid'").drop('sample_set', axis=1)
train_selected, dropped = toad.selection.select(train, target='target', empty=0.90, iv=0.02, corr=0.70, return_drop=True, exclude=None)


# In[43]:


train_selected.shape


# In[44]:


for i in dropped.keys():
    print("变量筛选维度：{}， 共剔除变量{}".format(i, len(dropped[i])))


# In[45]:


dropped_vars = pd.Series(dropped)


# In[46]:


df_iv = toad.quality(train_selected,'target',iv_only=True)
df_iv.head(3)


# In[47]:


df_iv['iv'].describe()


# In[48]:


df_iv[df_iv['iv']>=0.5].index


# In[415]:


import xgboost as xgb
from xgboost import plot_importance

xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 100
)

xgb_model.fit(train_selected.drop('target',axis=1), train_selected['target'])
feature_importances_xgb = xgb_model.get_booster().get_score(importance_type='gain')
feature_importances_xgb = pd.Series(feature_importances_xgb)


# In[419]:


feature_importances_xgb.describe()


# In[427]:


feature_importances_xgb = feature_importances_xgb.sort_values(ascending=False)


# In[420]:


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(train_selected.drop('target',axis=1).fillna(-9999), train_selected['target'])


# In[421]:


feature_importances = pd.Series(rf.feature_importances_, index=train_selected.drop('target',axis=1).columns)
feature_importances = feature_importances.sort_values(ascending=False)
feature_importances.shape


# In[422]:


feature_importances.describe()


# In[426]:


jj_feature_importances = [x for x in feature_importances_xgb.index if x in feature_importances.head(100).index] 
len(jj_feature_importances)


# In[617]:


train_selected_copy = train_selected.copy()


# In[833]:


train_selected = train_selected_copy.copy()


# In[429]:


feature_importances_xgb.head()


# In[728]:


train_selected = train_selected[list(feature_importances_xgb.index) +['target']]
train_selected.shape


# # 4. 变量分箱

# In[50]:


# 第一次分箱
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='dt', min_samples=0.01, n_bins=20, empty_separate=True) 
bins_result = c.export()


# In[51]:


train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins.head(2)


# In[52]:


valid_selected = valid[train_selected.columns]
valid_selected_bins = c.transform(valid_selected, labels=True)
valid_selected_bins.head(2)


# In[53]:


df_result = pd.DataFrame()
for col in train_selected_bins.columns[:-1]:
#     print('------------变量：{}-----------'.format(col))
    tmp = regroup(train_selected_bins, col, target='target')
    df_result = pd.concat([df_result, tmp], axis=0)


# In[54]:


df_result_valid = pd.DataFrame()
for col in valid_selected_bins.columns[:-1]:
#     print('------------变量：{}-----------'.format(col))
    tmp = regroup(valid_selected_bins, col, target='target')
    df_result_valid = pd.concat([df_result_valid, tmp], axis=0)


# In[55]:


tmp = pd.merge(df_result, df_result_valid, how='left', on=['bins', 'varsname'], suffixes=['_train', '_valid'])


# In[56]:


tmp.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mob6_mondel_变量分箱_卡方_20_final.xlsx',index=False)


# In[57]:


# 自动调整分箱
adj_bins = {}
for col in list(bins_result.keys()):
    tmp = np.isnan(bins_result[col])
    cutbins = [bins_result[col][x] for x in range(len(tmp)) if not tmp[x]]
    if len(cutbins)>0:
        cutbins = [float('-inf')] + cutbins + [float('inf')]
        cutoffpoints = ContinueVarBins(train_selected, col, flag='target', cutbins=cutbins)
        if train_selected[col].min()==cutoffpoints[0]:
            adj_bins[col] = [-0.5] + cutoffpoints[1:]
        else:
            adj_bins[col] = [-0.5] + cutoffpoints


# In[58]:


# 调整分箱:空值单独一箱
train_selected_copy =train_selected.copy()
train_selected.fillna(-9999,inplace=True)

# adj_bins = {}
# for col in list(bins_result.keys()):
#     tmp = np.isnan(bins_result[col])
#     cutbins = [bins_result[col][x] for x in range(len(tmp)) if not tmp[x]]
#     if len(cutbins)>0:
#         cutbins = [float('-inf')] + cutbins + [float('inf')]
#         cutoffpoints = ContinueVarBins(train_selected, col, flag='target', cutbins=cutbins)
#         adj_bins[col] = cutoffpoints
# 更新分箱
c.update(adj_bins)


# In[ ]:


# c.export()
# c.load(dict)
# c.transform(dataframe, labels=False)


# ### 观察分箱并调整

# In[124]:


from toad.plot import bin_plot
from toad.plot import badrate_plot

# to_drop = ['target','type']
data_new = pd.concat([train_selected, oot_selected],axis=0)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[0]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[1]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[2]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[3]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[4]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[5]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[6]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[7]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[8]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# 静态观察
col = train_selected.drop(to_drop,axis=1).columns[9]
print('========{}：变量的分箱图========='.format(col))
bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
bin_plot(c.transform(oot_selected[[col, 'target']], labels=True), x=col, target='target')
# 查看时间内分箱稳定性
badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[10]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# # from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# # badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[11]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# # from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# # badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# badrate_plot(c.transform(data_new[[col, 'target','type']], labels=True), x='type', target='target', by=col)


# In[ ]:


# # 调整分箱
# adj_bins = {'value_097_bairong': [-0.5, 3.0, 4,0, 5.0, 6.0, 9.0]}
# c.update(adj_bins)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[12]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[14]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[15]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# In[ ]:


# # 静态观察
# col = train_selected.drop(to_drop,axis=1).columns[16]
# print('========{}：变量的分箱图========='.format(col))
# bin_plot(c.transform(train_selected[[col, 'target']], labels=True), x=col, target='target')
# # bin_plot(c.transform(oot[[col, 'target']], labels=True), x=col, target='target')

# from toad.plot import badrate_plot
# # 看时间内的分箱稳定性
# badrate_plot(c.transform(train_selected[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)
# # badrate_plot(c.transform(oot[[col, 'target','year_month']], labels=True), x='year_month', target='target', by=col)


# # 5.WOE转换

# In[137]:


transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_selected), train_selected['target'], exclude=['target'])


# In[118]:


train_woe.shape


# In[138]:


train_woe, dropped = toad.selection.select(train_woe, target='target', empty=0.9, iv=0.01, corr=0.70, return_drop=True, exclude=None)
print(train_woe.shape)


# In[139]:


dropped


# In[96]:


df_regroup = pd.DataFrame()
# train_selected_bins = c.transform(train_selected)
bins_data = c.transform(train_selected, labels=True)[list(train_woe.columns)]

for col in bins_data.columns[:-1]:
    tmp = CalWoeIv(bins_data, col, target='target')
    df_regroup = pd.concat([df_regroup, tmp], axis=0)


# In[140]:


to_drop1 = list(set(df_regroup.query("iv>=0.30")['varsname']))
# to_drop2 = list(set(df_regroup.query("iv<=0.02")['varsname']))
print(len(to_drop1), len(to_drop2))


# In[141]:


train_woe.drop(to_drop1, axis=1, inplace=True)
train_woe.shape


# In[99]:


df_regroup.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mob6_mondel_变量分箱_woe.xlsx',index=False)


# ## Corr

# In[142]:


df_iv = df_regroup.drop_duplicates(subset=['varsname'],keep='first').set_index('varsname')
df_iv.head()


# In[143]:


to_drop_corr = CorrSelect(train_woe, df_iv, exclude_list=['target'], threshold=0.7)
print(len(to_drop_corr))


# # 6.逐步回归

# In[1050]:


# train_woe.drop(to_drop_corr,axis=1,inplace=True)
# train_woe.shape


# In[102]:


train_woe.shape


# In[144]:


train_woe.info()


# In[120]:


# 'value_019_pudao_4','value_021_pudao_4','model_score_01_2_bairong_8','model_score_01_hangliezhi_1','model_score_01_fulin_1'
to_drop = ['model_score_01_rong360_4','model_score_01_1_bileizhen_1','value_013_pudao_4', 'value_017_pudao_4','model_score_01_2_bairong_8']                                    
train_woe.drop(to_drop, axis=1, inplace=True)


# In[ ]:


pdi_d7_cell_nbank_nloan_allnum_pudao_3                         -0.0641      0.176     -0.364      0.716      -0.409       0.281
value_017_pudao_4                                               -0.6856      0.190     -3.610      0.000      -1.058      -0.313
model_score_01_2_bairong_8                                      -0.8888  


# In[145]:


final_data = toad.selection.stepwise(train_woe, target='target', estimator='ols', direction='both', criterion='aic', exclude=None)
final_data.shape


# In[146]:


final_data.info()


# In[1210]:


set(final_data.columns) - set(feature_importances[feature_importances>0.01].index)


# In[1211]:


set(final_data.columns) - set(feature_importances_xgb.index)


# In[147]:


final_valid = transer.transform(c.transform(valid[list(final_data.columns)]))
final_valid.shape


# In[148]:


cols = list(final_data.drop(['target'], axis=1).columns)
print(cols)


# In[149]:


print(toad.metrics.PSI(final_data[cols], final_valid[cols]))


# # 7.建模和评估

# In[111]:


from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
def lr(X_train,X_test,y_train,y_test):
    from sklearn.linear_model import LogisticRegression
    from scipy import stats
    from toad.metrics import KS,AUC 
    # 模型训练和p_values查看 只能是线性回归
    clf = LogisticRegression(C=1e8).fit(X_train, y_train)
    params = np.append(clf.intercept_,clf.coef_)
    
    new_X_train = pd.DataFrame({"Constant":np.ones(len(X_train))}).join(X_train.reset_index(drop=True))
    predictions = clf.predict(X_train)
    MSE = (sum((y_train-predictions)**2))/(len(new_X_train)-len(new_X_train.columns))

    var_b = MSE*(np.linalg.inv(np.dot(new_X_train.T,new_X_train)).diagonal())
    sd_b = np.sqrt(var_b)
    ts_b = params/ sd_b

    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(new_X_train)-1))) for i in ts_b]

    sd_b = np.round(sd_b,3)
    ts_b = np.round(ts_b,3)
    p_values = np.round(p_values,3)
    params = np.round(params,4)

    myDF3 = pd.DataFrame()
    myDF3["Vars"],myDF3["Coefficients"],myDF3["Standard Errors"],myDF3["t values"],myDF3["P values"] = [new_X_train.columns,params,sd_b,ts_b,p_values]
    print(myDF3)
    
    # 预测值
    pred_train = clf.predict_proba(X_train)[:,1]
    pred_test = clf.predict_proba(X_test)[:,1]
    
    # 训练集KS/AUC
    print('-------------训练集结果--------------------')
    print('train AUC: ', AUC(pred_train, y_train))
    print('train KS: ', KS(pred_train, y_train))
    
    # 测试集KS/AUC
    print('-------------测试集结果--------------------')
    print('test AUC: ', AUC(pred_test, y_test))
    print('test KS: ', KS(pred_test, y_test))
    
    print('-------------------------分割线--------------------------')
    # 模型评估
    train_AUC = roc_auc_score(y_train, pred_train)
    train_fpr, train_tpr, train_thresholds = roc_curve(y_train,pred_train)
    train_KS = max(train_tpr - train_fpr)
    print('TRAIN AUC: {}'.format(train_AUC))
    print('TRAIN KS: {}'.format(train_KS))
    
    test_AUC = roc_auc_score(y_test, pred_test)
    test_fpr, test_tpr, test_thresholds = roc_curve(y_test,pred_test)
    test_KS = max(test_tpr - test_fpr)
    print('Test AUC: {}'.format(test_AUC))
    print('Test KS: {}'.format(test_KS))
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(train_fpr,train_tpr,color='darkorange',lw=3,label='Train ROC curve (Area = %0.2f)'%train_AUC)
    plt.plot(test_fpr,test_tpr,color='navy',lw=3,label='Test ROC curve (Area = %0.2f)'%test_AUC)
    plt.plot([0,1],[0,1],color='gray',lw=1,linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title('Train&Test ROC curve',fontsize=25)
    plt.legend(loc='lower right',fontsize=20)

    return (myDF3, params, clf)


# In[150]:


tmp_corr = final_data[cols].corr()
tmp_corr


# In[151]:


tmp_vif = sklearn_vif(cols, final_data[cols])
tmp_vif


# In[172]:


# 保存统计的数据
writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mobdel_mob6_vif_corr.xlsx')
tmp_corr.to_excel(writer,sheet_name='总体')
tmp_vif.to_excel(writer,sheet_name='通过')
writer.save()


# In[152]:


from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
X = np.matrix(final_data[cols])
vif['features']=cols
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
vif


# In[115]:


import matplotlib.pyplot as plt
# 单次训练
def LR_model(X_train,X_test,y_train,y_test):
    import statsmodels.api as sm
    clf = sm.Logit(y_train, sm.add_constant(X_train)).fit()
    print(clf.summary())
    # 模型评估
    train_y_pred = clf.predict(sm.add_constant(X_train))
    train_AUC = roc_auc_score(y_train, train_y_pred)
    train_fpr, train_tpr, train_thresholds = roc_curve(y_train,train_y_pred)
    train_KS = max(train_tpr - train_fpr)
    print('TRAIN AUC: {}'.format(train_AUC))
    print('TRAIN KS: {}'.format(train_KS))
    
    test_y_pred = clf.predict(sm.add_constant(X_test))
    test_AUC = roc_auc_score(y_test,test_y_pred)
    test_fpr, test_tpr, test_thresholds = roc_curve(y_test,test_y_pred)
    test_KS = max(test_tpr - test_fpr)
    print('Test AUC: {}'.format(test_AUC))
    print('Test KS: {}'.format(test_KS))
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(train_fpr,train_tpr,color='darkorange',lw=3,label='Train ROC curve (Area = %0.2f)'%train_AUC)
    plt.plot(test_fpr,test_tpr,color='navy',lw=3,label='Test ROC curve (Area = %0.2f)'%test_AUC)
    plt.plot([0,1],[0,1],color='gray',lw=1,linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title('Train&Test ROC curve',fontsize=25)
    plt.legend(loc='lower right',fontsize=20)
    
    return clf


# In[166]:


cols.remove('model_score_01_1_bileizhen_1')


# In[167]:


clf = LR_model(final_data[cols], final_valid[cols], final_data['target'], final_valid['target'])


# In[160]:


lr_result, lr_param, lr_clr = lr(final_data[cols], final_valid[cols], final_data['target'], final_valid['target'])


# In[1124]:


# cols.remove('model_score_01_aliyun_2')
# cols.remove('model_score_01_2_bairong_8')
# cols.remove('apply_loan_result_json_als_m12_id_nbank_cf_orgnum_bairong_1')
# cols.remove('model_score_01_tengxun_1')
# cols.remove('model_score_01_baihang_1')


# In[168]:


df_regroup_final = pd.DataFrame()
train_selected_bins = c.transform(train_selected,labels=True)[list(final_data.columns)]

for col in cols:
    tmp = CalWoeIv(train_selected_bins, col, target='target')
    df_regroup_final = pd.concat([df_regroup_final, tmp], axis=0)


# In[169]:


df_regroup_final.reset_index(drop=True)


# In[170]:


df_regroup_final.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_mondel_变量分箱_20_final_mob6.xlsx',index=False)


# # 9.转换评分

# In[175]:


card = toad.ScoreCard(combiner=c,
                      transer = transer,
                      base_odds=5,
                      base_score=700,
                      pdo=60,
                      rate=2,
                      C=1e8
                     )


# In[174]:


(1-final_data['target'].mean())/final_data['target'].mean()


# In[176]:


card.fit(final_data[cols], final_data['target'])


# In[177]:


import pickle
with open('zz_card_mob6.pkl', 'wb') as f:
    pickle.dump(card, f)


# In[178]:


card.coef_


# In[179]:


card1 = card.export(to_frame=True)
card1


# In[180]:


df_regroup_final['new_bins'] = df_regroup_final['bins'].str.split('[').apply(lambda x:'['+x[1])
df_regroup_final.head()


# In[181]:


final_card = pd.merge(df_regroup_final,card1,how='inner',left_on=['varsname', 'new_bins'], right_on=['name','value'])
final_card.head()


# In[182]:


final_card.to_excel(r'D:\liuyedao\转转渠道\result\model_mid_result\card_final_mob6.xlsx')


# # 10.分数分布

# In[196]:


def regroup(data_bins, col, target='target'):    
    total = data_bins.groupby(col)[target].count()
    bad = data_bins.groupby(col)[target].sum()
    regroup = pd.concat([total, bad],axis=1)
    regroup.columns = ['total', 'bad']
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_rate'] = regroup['bad']/regroup['total']
    regroup['total_pct'] = regroup['total']/regroup['total'].sum()
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
    regroup['good_pct'] = regroup['good']/regroup['good'].sum()
    regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
    regroup['goo_pct_cum'] = regroup['good_pct'].cumsum()
    regroup['ks_bins'] = regroup['bad_pct_cum'] - regroup['goo_pct_cum']
    regroup['ks'] = regroup['ks_bins'].max()
    regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct']) * np.log(regroup['bad_pct']/regroup['good_pct'])
    regroup['iv'] = regroup['iv_bins'].sum()
    regroup['varsname'] = col
    regroup['bins'] = regroup.index
    regroup['lift'] = regroup['bad_rate']/data_bins[target].mean()
    usecols = ['varsname','bins','iv','ks','bad','good','total','bad_rate','total_pct','lift'
            ,'bad_pct','good_pct','bad_pct_cum','goo_pct_cum','ks_bins','iv_bins']
    return_regroup = regroup[usecols]

    return return_regroup


# In[183]:


train['score'] = card.predict(train[cols]).round(0)
valid['score'] = card.predict(valid[cols]).round(0)


# In[184]:


print(train['score'].min(), train['score'].max())
print(valid['score'].min(), valid['score'].max())


# In[195]:


pred_data = toad.metrics.KS_bucket(train['score'], train['target'], bucket=10, method='quantile')
pred_data = toad.metrics.KS_bucket(train['score'], train['target'], bucket=10, method='step')
pred_data.insert(1, 'left_bins', pred_data['max'].shift(1).fillna(float('-inf')))
pred_data.insert(0,'bins', pred_data[['left_bins','max']].apply(lambda x: '(' + str(x.left_bins) + ', ' + str(x['max']) + ']', axis=1))
pred_data


# In[245]:


# 分割点
train['q_bins'] = pd.qcut(train['score'], 10, duplicates='drop')
train['s_bins'] = pd.cut(train['score'], 10, right=True, include_lowest=False)
cutbins_q = [float('-inf')] + list(train.groupby(['q_bins'])['score'].max())
cutbins_s = [float('-inf')] + list(train.groupby(['s_bins'])['score'].max())
print(cutbins_q)
print(cutbins_s)


# In[246]:


# 训练集分数分布
train['q_bins'] = pd.cut(train['score'], cutbins_q, right=True, include_lowest=False)
train['s_bins'] = pd.cut(train['score'], cutbins_s, right=True, include_lowest=False)
train_q_bins = regroup(train, 'q_bins', target='target')
train_s_bins = regroup(train, 's_bins', target='target')
train_q_bins.reset_index(drop=True,inplace=True)
train_s_bins.reset_index(drop=True,inplace=True)


# In[249]:


# 测试集分数分布
valid['q_bins'] = pd.cut(valid['score'], cutbins_q, right=True)
valid['s_bins'] = pd.cut(valid['score'], cutbins_s, right=True)
valid_q_bins = regroup(valid, 'q_bins', target='target')
valid_s_bins = regroup(valid, 's_bins', target='target')
valid_q_bins.reset_index(drop=True,inplace=True)
valid_s_bins.reset_index(drop=True,inplace=True)


# In[250]:


writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\model_mid_result\auth_model_分数分布_v3_mob6.xlsx')
train_q_bins.to_excel(writer,sheet_name='train_等频')
train_s_bins.to_excel(writer,sheet_name='train_等宽')
valid_q_bins.to_excel(writer,sheet_name='valid_等频')
valid_s_bins.to_excel(writer,sheet_name='valid_等宽')
writer.save()


# In[252]:


def cal_psi(exp, act):
    psi = []
    for i in range(len(exp)):
        psi_i = (act[i] - exp[i])*np.log(act[i]/exp[i])
        psi.append(psi_i)
    return sum(psi)


# In[253]:


print(cal_psi(train_s_bins['total_pct'], valid_s_bins['total_pct'])) 


# # 11.全部授信申请客户

# In[207]:


data.shape


# In[212]:


cols


# In[224]:


# 全部授信申请客户
df1_auth = data[cols]
df1_auth[['user_id','apply_date','auth_status','target']] = data[['user_id','apply_date','auth_status','target_mob6']]
df1_auth.info()


# In[225]:


print(df1_auth['user_id'].nunique(), df1_auth.shape)


# In[226]:


df2_auth = df1_auth.sort_values(['user_id','apply_date'], ascending=False).drop_duplicates(subset=['user_id'],keep='first')


# In[227]:


df2_auth.columns[0:17]


# In[229]:


index = list(df2_auth.query("auth_status==7 & target==target").index)
index


# In[1307]:


# index = list(data.query("auth_status==7 & target_mob9==target_mob9").index)
# data.drop(index=index,axis=0,inplace=True)
# data.shape


# In[1351]:


# usecols_=list(data.columns[0:17])+cols
# print(usecols_)


# In[230]:


df3_auth = df2_auth.copy()
df3_auth.info(show_counts=True)


# In[231]:


for col in cols:
    df3_auth[col].fillna(-9999, inplace=True)
df3_auth.info(show_counts=True)


# In[232]:


df3_auth['score'] = card.predict(df3_auth).round(0)
df3_auth.shape


# In[233]:


df3_auth['score'].head()


# In[234]:


cutbins_q = [float('-inf')] + list(train.groupby(['q_bins'])['score'].max())
cutbins_s = [float('-inf')] + list(train.groupby(['s_bins'])['score'].max())


# In[242]:


cutbins_s


# In[243]:


cutbins_q


# In[235]:


df3_auth['bins_step'] = pd.cut(df3_auth['score'], cutbins_s, right=True)
df3_auth['bins_quantile'] = pd.cut(df3_auth['score'], cutbins_q, duplicates='drop')


# In[238]:


def cal_auth(df_auth, bins='bins', target='target'):
    total_lend = df_auth.groupby(bins)['user_id',target].count()
    tg_jj = df_auth.groupby([bins, 'auth_status'])['user_id'].count().unstack()
    lend =  df_auth.groupby([bins, target])['user_id'].count().unstack()
    result = pd.concat([total_lend, tg_jj, lend], axis=1)
    result.columns = ['授信申请','放款人数','通过人数','拒绝人数','未到期','好','灰','坏']

    result_sum = pd.DataFrame(result.sum(axis=0),columns=['total']).T

    for col in result.columns:
        result['{}占比'.format(col)] = result[col]/result[col].sum()
        result['{}累计占比'.format(col)] = result['{}占比'.format(col)].cumsum()

    result['坏客率'] = result['坏']/(result['好']+result['坏'])
    result['通过率'] = result['通过人数']/result['授信申请']
    result = pd.concat([result, result_sum],axis=0)

    cols = ['授信申请','授信申请占比','授信申请累计占比','通过人数','通过率','通过人数占比','通过人数累计占比','拒绝人数','拒绝人数占比','拒绝人数累计占比',
            '放款人数','放款人数占比','放款人数累计占比','好','好占比','好累计占比',
            '坏','坏占比','坏累计占比','坏客率','灰','灰占比','灰累计占比','未到期','未到期占比','未到期累计占比']
    result = result[cols]
    
    return result


# In[239]:


result_auth_step = cal_auth(df3_auth, bins='bins_step', target='target')
result_auth_quantile = cal_auth(df3_auth, bins='bins_quantile', target='target')


# In[240]:


result_auth_step.head(2)


# In[241]:


result_auth_step.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_mob6_model_分数分布_授信申请_v3.xlsx")
result_auth_quantile.to_excel(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_mob6_model_分数分布_授信申请_v4.xlsx")


# In[1365]:


df3_auth.drop(list(df3_auth.columns)[1:9],axis=1).to_csv(r"D:\liuyedao\转转渠道\result\model_mid_result\auth_建模_评分卡_mob9_score_20230825.csv",index=False)


# # oot验证

# In[60]:


df3_auth = pd.read_csv(r'd:\liuyedao\model_result\oot_data_20230815.csv')
df3_auth.info()


# In[8]:


def cal_score_tc(x):
    score_list = []
    if x<555.5:
        score=188
    elif x>=555.5 and x<599.5:
        score=192
    elif x>=555.5 and x<599.5:
        score=192
    elif x>=599.5:
        score=212
    else:
        score=187
    return score
        
def cal_score_bh(x):      
    if x<699.5:
        score=182
    elif x>=699.5 and x<719.5:
        score=188
    elif x>=719.5 and x<734.5:
        score=193
    elif x>=734.5 and x<756.5:
        score=204
    elif x>=756.5 and x<774.5:
        score=211
    elif x>=774.5:
        score=218
    else:
        score=199
    return score

def cal_score_xysl(x):  
    if x<569.5:
        score=153
    elif x>=569.5 and x<612.5:
        score=171
    elif x>=612.5 and x<635.5:
        score=185
    elif x>=635.5 and x<660.5:
        score=196
    elif x>=660.5 and x<680.5:
        score=206
    elif x>=680.5 and x<699.5:
        score=217
    elif x>=699.5 and x<723.5:
        score=234
    elif x>=723.5:
        score=273
    else:
        score=159
    
    return score
        


# In[9]:


usecols = ['order_no','user_id','id_no_des','channel_id','apply_date_auth','apply_time','auth_status',
           'model_score_01_x_tianchuang','model_score_01_xysl_3','model_score_01_baihang','Firs6ever30','score_lr']

df3_auth['model_score_01_x_tianchuang_score'] = df3_auth['model_score_01_x_tianchuang'].apply(lambda x: cal_score_tc(x))
df3_auth['model_score_01_baihang_score'] = df3_auth['model_score_01_baihang'].apply(lambda x: cal_score_bh(x))
df3_auth['model_score_01_xysl_3_score'] = df3_auth['model_score_01_xysl_3'].apply(lambda x: cal_score_xysl(x))
df3_auth['score_lr'] = df3_auth[['model_score_01_x_tianchuang_score', 'model_score_01_baihang_score','model_score_01_xysl_3_score']].sum(axis=1)

df4_auth = df3_auth[usecols]


# In[11]:


df4_auth.info()
df4_auth.to_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_LR_score_20230815_oot.csv",index=False)


# In[12]:


df4_auth.info(show_counts=True)


# In[62]:


df3_auth['apply_month'] = df3_auth['apply_date_auth'].str[0:7]
df3_auth['apply_month'].value_counts()


# In[63]:


df3_auth['Firs6ever30'].value_counts()


# In[64]:


list(lr.feature_names_in_)


# In[66]:


auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()


# In[68]:


# 对训练集进行预测
auth_data_mob6 = df3_auth[df3_auth['Firs6ever30'].isin([0.0, 2.0])][list(lr.feature_names_in_) + ['Firs6ever30','apply_month','order_no']]
# auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()
auth_data_mob6 = auth_data_mob6[auth_data_mob6['apply_month']=='2023-01']
auth_data_mob6['target'] = auth_data_mob6['Firs6ever30'] /2.0
# auth_data_mob6.info()
auth_data_mob6 = transer.transform(c.transform(auth_data_mob6.fillna(-999)))
auth_data_mob6['prob'] = lr.predict_proba(auth_data_mob6[list(lr.feature_names_in_)])[:,1]

from toad.metrics import KS,AUC
print('train AUC: ', AUC(auth_data_mob6['prob'], auth_data_mob6['target']))
print('train KS: ', KS(auth_data_mob6['prob'], auth_data_mob6['target']))


# In[74]:


# 对训练集进行预测
auth_data_mob3 = df3_auth[df3_auth['Firs3ever30'].isin([0.0, 2.0])][list(lr.feature_names_in_) + ['Firs3ever30','apply_month','order_no']]
# auth_data_mob3.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()
auth_data_mob3 = auth_data_mob3[auth_data_mob3['apply_month']!='2023-05']
auth_data_mob3['target'] = auth_data_mob3['Firs3ever30'] /2.0
# auth_data_mob3.info()
auth_data_mob3 = transer.transform(c.transform(auth_data_mob3.fillna(-999)))
auth_data_mob3['prob'] = lr.predict_proba(auth_data_mob3[list(lr.feature_names_in_)])[:,1]



# In[ ]:


from toad.metrics import KS,AUC
print('train AUC: ', AUC(auth_data_mob3['prob'], auth_data_mob3['target']))
print('train KS: ', KS(auth_data_mob3['prob'], auth_data_mob3['target']))


# In[76]:


auth_data_mob3.info()


# In[75]:


auth_data_mob3.groupby(['apply_month', 'Firs3ever30'])['order_no'].count().unstack()


# In[78]:


for i in list(auth_data_mob3['apply_month'].unique()):
    print(i)
    tmp = auth_data_mob3[auth_data_mob3['apply_month']==i]
    print('train AUC: ', AUC(tmp['prob'], tmp['target']))
    print('train KS: ', KS(tmp['prob'], tmp['target']))
    print('-----------------------------------')




#==============================================================================
# File: 转转-三方数据评分卡建模-xgboost.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import xgboost as xgb
from xgboost import plot_importance
from sklearn import metrics
from sklearn.model_selection import train_test_split
from datetime import datetime
import os 
import warnings
warnings.filterwarnings("ignore")


# In[2]:


os.getcwd()


# In[4]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# # 1.读取数据集

# In[5]:


data_target = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob_9_or_6.csv') 
data_vars = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_total_three_part_data.csv') 
print('数据大小：', data_target.shape)
print('数据大小：', data_vars.shape)


# In[9]:


data_vars_subset = data_vars[data_vars['user_id'].isin(data_target['user_id'])]


# In[11]:


raw_data = pd.merge(data_target, data_vars_subset, how='inner', on='user_id')
print(raw_data.shape)


# In[15]:


# 去重
data = raw_data.sort_values(['user_id', 'apply_time'], ascending=False).drop_duplicates(subset=['user_id'],keep='first')


# In[16]:


index = list(data.query("auth_status==7 & target_mob9==target_mob9").index)
data.drop(index=index,axis=0,inplace=True)
data.reset_index(drop=True, inplace=True)
data.shape


# In[17]:


data['auth_status'].value_counts()


# In[18]:


data['channel_id'].value_counts()


# In[19]:


data_copy = data.copy()


# In[20]:


data['target_mob9'].value_counts(dropna=False)


# In[25]:


data['target'] = data['target_mob9']


# ## 拆分数据集

# In[26]:


to_drop = list(data.columns)[0:17]
print(to_drop)


# In[27]:


data = data.drop(to_drop, axis=1)
data = data[data['target'].isin([0.0, 1.0])]
data.reset_index(drop=True,inplace=True)
data.shape


# In[28]:


df_var = data.drop(['target'], axis=1)
df_target = data['target']

X = np.array(df_var)
y = np.array(df_target)


# In[29]:


X_train, X_test, y_train, y_test = train_test_split(df_var ,df_target, test_size=0.3, random_state=22, stratify=y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)


# In[30]:


train = pd.merge(X_train, y_train, how='inner', left_index=True, right_index=True)
print(train.shape)


# In[31]:


oot = pd.merge(X_test, y_test, how='inner', left_index=True, right_index=True)
print(oot.shape)


# In[45]:


train.to_csv(r'd:\liuyedao\model_result\{}_model_data_channel_xgb_train_{}.csv'.format('auth', str(datetime.today())[:10].replace('-','')))
oot.to_csv(r'd:\liuyedao\model_result\{}_model_data_channel_xgb_oot_{}.csv'.format('auth', str(datetime.today())[:10].replace('-','')))


# In[3]:


train = pd.read_csv(r'd:\liuyedao\model_result\auth_model_data_channel_xgb_train_20230728.csv')
oot = pd.read_csv(r'd:\liuyedao\model_result\auth_model_data_channel_xgb_oot_20230728.csv')


# # 2.数据探索分析

# In[47]:


train_df_explore = toad.detect(train.drop(to_drop,axis=1))
oot_df_explore = toad.detect(oot.drop(to_drop,axis=1))

train_df_iv = toad.quality(train.drop(to_drop,axis=1),'target',iv_only=True)
oot_df_iv = toad.quality(oot.drop(to_drop,axis=1),'target',iv_only=True)

writer=pd.ExcelWriter(r"d:\liuyedao\mid_result\auth_建模_数据探索性分析_xgb_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
train_df_explore.to_excel(writer,sheet_name='train_df_explore')
oot_df_explore.to_excel(writer,sheet_name='oot_df_explore')
train_df_iv.to_excel(writer,sheet_name='train_df_iv')
oot_df_iv.to_excel(writer,sheet_name='oot_df_iv')
writer.save()


# # 3.特征筛选

# In[33]:


train_selected, dropped = toad.selection.select(train, target='target', empty=0.9, iv=0.01, corr=0.60, return_drop=True, exclude=None)
print(train_selected.shape)


# In[34]:


for i in dropped.keys():
    print("变量筛选维度：{}， 共剔除变量{}".format(i, len(dropped[i])))


# In[35]:


oot_selected =oot[train_selected.columns]
oot_selected.info()


# # 4.模型训练和评估

# In[37]:


cols = list(train_selected.columns[:-1])


# In[38]:


xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 1,
                              gamma=0,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 100
)

# 对训练集训练模型
# xgb_model.fit(train_selected[cols], train_selected['target'], early_stopping_rounds=1,eval_metric='auc',
#               eval_set=[(oot_selected[cols], oot_selected['target'])])

xgb_model.fit(train_selected[cols], train_selected['target'])


# In[39]:


plot_importance(xgb_model, importance_type='gain')


# In[89]:


xgb_model.fit(train_selected[cols], train_selected['target'])


# In[90]:


# 对训练集进行预测
y_pred = xgb_model.predict_proba(train_selected[cols])[:,1]
fpr, tpr, thresholds = metrics.roc_curve(train_selected['target'], y_pred, pos_label=1)
roc_auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train KS: ', ks)
print('train AUC: ', roc_auc)

# 对测试集进行预测
y_pred_oot = xgb_model.predict_proba(oot_selected[cols])[:,1]
fpr_oot, tpr_oot, thresholds_oot = metrics.roc_curve(oot_selected['target'], y_pred_oot, pos_label=1)
roc_auc_oot = metrics.auc(fpr_oot, tpr_oot)
ks_oot = max(tpr_oot-fpr_oot)
print('oot KS: ', ks_oot)
print('oot AUC: ', roc_auc_oot)


# In[42]:


plt.plot(fpr, tpr, color='darkorange', lw=2, label='trian ROC curve (area = %0.2f)' % roc_auc)
plt.plot(fpr_oot, tpr_oot, color='blue', lw=2, label='test ROC curve (area = %0.2f)' % roc_auc_oot)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="best")
plt.show()


# In[50]:


from bayes_opt import BayesianOptimization
from sklearn.model_selection import KFold, cross_validate, cross_val_score
import time


# In[78]:


def xgb_cv(X, y, learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, reg_alpha, reg_lambda):
    xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=learning_rate,
                              n_estimators = int(n_estimators),
                              max_depth = int(max_depth),
                              min_child_weight = int(min_child_weight),
                              gamma=gamma,
                              subsample=subsample,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = int(reg_alpha),
                              reg_lambda = int(reg_lambda))   
    cv = KFold(n_splits=5, shuffle=True, random_state=11)
    valid_loss = cross_validate(xgb_model,X,y,scoring='roc_auc',cv=cv,n_jobs=-1,error_score='raise')
    return np.mean(valid_loss['test_score'])

def bayes_opt_xgb(X, y):
    def xgb_cross_valid(learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, reg_alpha, reg_lambda):
        return xgb_cv(X, y, learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, reg_alpha, reg_lambda)
    optimizer = BayesianOptimization(xgb_cross_valid,
                                {
                                    'max_depth':(3, 10),
                                    'min_child_weight':(1, 20),
                                    'n_estimators':(50, 500),
                                    'learning_rate':(0, 1),
                                    'subsample':(0.7, 1.0),
                                    'gamma':(0, 1),
                                    'reg_alpha':(0, 300),
                                    'reg_lambda':(0, 300)
                                })
    
    start_time = time.time()
    optimizer.maximize(init_points=8, n_iter=5)
    end_time = time.time()
    print("花费时间：", end_time-start_time)
    opt_best = optimizer.max
    print("final result:" ,opt_best)
    
    return opt_best


# In[79]:


# X_train, X_test, y_train, y_test
opt_best = bayes_opt_xgb(X_train, y_train)


# In[85]:


opt_best['params']['gamma']


# In[87]:


xgb_model = xgb.XGBClassifier(booster='gbtree',
                          learning_rate=opt_best['params']['learning_rate'],
                          n_estimators = int(opt_best['params']['n_estimators']),
                          max_depth = int(opt_best['params']['max_depth']),
                          min_child_weight = int(opt_best['params']['min_child_weight']),
                          gamma=opt_best['params']['gamma'],
                          subsample=opt_best['params']['subsample'],
                          objective = "binary:logistic",
                          nthread = 1,
                          n_jobs = -1,
                          random_state = 1,
                          scale_pos_weight = 1,
                          reg_alpha = int(opt_best['params']['reg_alpha']),
                          reg_lambda = int(opt_best['params']['reg_lambda'])) 


# In[91]:


oot_data = pd.read_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob3.csv')


# In[92]:


oot_data_new = oot_data[~oot_data['user_id'].isin(data['user_id'])]
oot_data_new.shape


# In[ ]:





# In[210]:


from sklearn.model_selection import GridSearchCV


# In[211]:


param_test1 = {'max_depth':[3,4,5,6,7,8], 'min_child_weight':[i for i in range(1,6,1)]}

gsearch = GridSearchCV(
    estimator = xgb_model,
    param_grid=param_test1, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch.fit(train_selected[cols], train_selected['target'])

print('gsearch1.best_params_', gsearch.best_params_)
print('gsearch1.best_score_', gsearch.best_score_)


# In[212]:


xgb_model_2 = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 500,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 1
)

param_test2 = {'learning_rate':[i/20.0 for i in range(1,20)]}
gsearch2 = GridSearchCV(
    estimator = xgb_model_2,
    param_grid=param_test2, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch2.fit(train_selected[cols], train_selected['target'])

print('gsearch2.best_params_', gsearch2.best_params_)
print('gsearch2.best_score_', gsearch2.best_score_)


# In[214]:


xgb_model_3 = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 500,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 1
)
param_test3 = {'n_estimators':[100, 200, 300, 400, 500, 600]}

gsearch3 = GridSearchCV(
    estimator = xgb_model_3,
    param_grid=param_test3, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch3.fit(train_selected[cols], train_selected['target'])

print('gsearch3.best_params_', gsearch3.best_params_)
print('gsearch3.best_score_', gsearch3.best_score_)


# In[215]:


xgb_model_4 = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 1
)
param_test4 = {'gamma':[i/10.0 for i in range(10)]}

gsearch4 = GridSearchCV(
    estimator = xgb_model_4,
    param_grid=param_test4, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch4.fit(train_selected[cols], train_selected['target'])

print('gsearch4.best_params_', gsearch4.best_params_)
print('gsearch4.best_score_', gsearch4.best_score_)


# In[216]:


xgb_model_5 = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 1
)
param_test5 = {'reg_alpha':[0, 0.0001, 0.001, 0.1, 1, 100]}

gsearch5 = GridSearchCV(
    estimator = xgb_model_5,
    param_grid=param_test5, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch5.fit(train_selected[cols], train_selected['target'])

print('gsearch5.best_params_', gsearch5.best_params_)
print('gsearch5.best_score_', gsearch5.best_score_)


# In[217]:


xgb_model_6 = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 1
)
param_test6 = {'reg_lambda':[0, 0.001, 0.1, 1, 100]}

gsearch6 = GridSearchCV(
    estimator = xgb_model_6,
    param_grid=param_test6, 
    scoring='roc_auc', 
    n_jobs=-1, 
    cv=5)
gsearch6.fit(train_selected[cols], train_selected['target'])

print('gsearch6.best_params_', gsearch6.best_params_)
print('gsearch6.best_score_', gsearch6.best_score_)


# In[218]:


xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 100
)

# 对训练集训练模型
xgb_model.fit(train_selected[cols], train_selected['target'])


# In[15]:


# 保存模型
import pickle
# save
# pickle.dump(xgb_model, open(r"D:\liuyedao\model_result\auth_xgb_model.pkl", "wb"))


# In[16]:


# load
xgb_model_pkl = pickle.load(open(r"D:\liuyedao\model_result\auth_xgb_model.pkl", "rb"))


# In[17]:


plot_importance(xgb_model_pkl, importance_type='gain')


# In[7]:


from sklearn2pmml.pipeline import PMMLPipeline
from sklearn2pmml import sklearn2pmml


# In[8]:


cols


# In[9]:


xgb_model = xgb.XGBClassifier(booster='gbtree',
                              learning_rate=0.05,
                              n_estimators = 200,
                              max_depth = 3,
                              min_child_weight = 2,
                              gamma=0.7,
                              subsample=1,
                              colsample_bytree=1,
                              objective = "binary:logistic",
                              nthread = 1,
                              n_jobs = -1,
                              random_state = 1,
                              scale_pos_weight = 1,
                              reg_alpha = 0,
                              reg_lambda = 100
)


# In[10]:


pipline_xgb_model = PMMLPipeline([("classfier",xgb_model)])
pipline_xgb_model.fit(train_selected[cols], train_selected['target'])


# In[13]:


sklearn2pmml(pipline_xgb_model, r'D:\liuyedao\model_result\xgb_model_20230810.pmml', with_repr=True)


# In[219]:


# 对训练集进行预测
pred_train = xgb_model.predict_proba(train_selected[cols])[:,1]
fpr, tpr, thresholds = metrics.roc_curve(train_selected['target'], pred_train, pos_label=1)
roc_auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train KS: ', ks)
print('train AUC: ', roc_auc)

# 对测试集进行预测
pred_oot = xgb_model.predict_proba(oot_selected[cols])[:,1]
fpr_oot, tpr_oot, thresholds_oot = metrics.roc_curve(oot_selected['target'], pred_oot, pos_label=1)
roc_auc_oot = metrics.auc(fpr_oot, tpr_oot)
ks_oot = max(tpr_oot-fpr_oot)
print('oot KS: ', ks_oot)
print('oot AUC: ', roc_auc_oot)


# In[249]:


xgb_model.get_booster().get_score(importance_type='gain')


# In[250]:


plot_importance(xgb_model, importance_type='gain')


# In[220]:


train_selected_167 = train_selected.query("channel_id == 167")
oot_selected_167 = oot_selected.query("channel_id == 167")


train_selected_174 = train_selected.query("channel_id == 174")
oot_selected_174 = oot_selected.query("channel_id == 174")


# In[221]:


# 167对训练集进行预测
pred_train_167 = xgb_model.predict_proba(train_selected_167[cols])[:,1]
fpr_167, tpr_167, thresholds_167 = metrics.roc_curve(train_selected_167['target'], pred_train_167, pos_label=1)
roc_auc_167 = metrics.auc(fpr_167, tpr_167)
ks_167 = max(tpr_167-fpr_167)
print('渠道167的train KS: ', ks_167)
print('渠道167的train AUC: ', roc_auc_167)

# 167对测试集进行预测
pred_oot_167 = xgb_model.predict_proba(oot_selected_167[cols])[:,1]
fpr_167_oot, tpr_167_oot, thresholds_oot_167 = metrics.roc_curve(oot_selected_167['target'], pred_oot_167, pos_label=1)
roc_auc_oot_167 = metrics.auc(fpr_167_oot, tpr_167_oot)
ks_oot_167 = max(tpr_167_oot-fpr_167_oot)
print('渠道167的oot KS: ', ks_oot_167)
print('渠道167的oot AUC: ', roc_auc_oot_167)


# 174对训练集进行预测
pred_train_174 = xgb_model.predict_proba(train_selected_174[cols])[:,1]
fpr_174, tpr_174, thresholds_174 = metrics.roc_curve(train_selected_174['target'], pred_train_174, pos_label=1)
roc_auc_174 = metrics.auc(fpr_174, tpr_174)
ks_174 = max(tpr_174-fpr_174)
print('渠道174的train KS: ', ks_174)
print('渠道174的train AUC: ', roc_auc_174)

# 174对测试集进行预测
pred_oot_174 = xgb_model.predict_proba(oot_selected_174[cols])[:,1]
fpr_174_oot, tpr_174_oot, thresholds_oot_174 = metrics.roc_curve(oot_selected_174['target'], pred_oot_174, pos_label=1)
roc_auc_oot_174 = metrics.auc(fpr_174_oot, tpr_174_oot)
ks_oot_174 = max(tpr_174_oot-fpr_174_oot)
print('渠道174的oot KS: ', ks_oot_174)
print('渠道174的oot AUC: ', roc_auc_oot_174)


# In[222]:


# 业务效果
pred_data = toad.metrics.KS_bucket(pred_train, train_selected['target'], bucket=10, method='step')
pred_data


# In[223]:


train_selected_167['prob'] = pred_train_167
oot_selected_167['prob'] = pred_oot_167

train_selected_174['prob'] = pred_train_174
oot_selected_174['prob'] = pred_oot_174

train_selected['prob'] = pred_train
oot_selected['prob'] = pred_oot


# In[224]:


def Prob2Score(prob, base_odds=35, base_score=700, pdo=60, rate=2) :
    # 将概率转化成分数且为正整数
    y = np.log((1 - prob) / prob)
    factor = pdo/np.log(rate)
    offset = base_score - factor * np.log(base_odds)
    score = offset +  factor * (y)
    
    return score


# In[225]:


train_selected['score'] = train_selected['prob'].apply(lambda x:Prob2Score(x))
oot_selected['score'] = oot_selected['prob'].apply(lambda x:Prob2Score(x))

train_selected_167['score'] = train_selected_167['prob'].apply(lambda x:Prob2Score(x))
oot_selected_167['score'] = oot_selected_167['prob'].apply(lambda x:Prob2Score(x))

train_selected_174['score'] = train_selected_174['prob'].apply(lambda x:Prob2Score(x))
oot_selected_174['score'] = oot_selected_174['prob'].apply(lambda x:Prob2Score(x))


# In[226]:


[round(Prob2Score(x),0) for x in pred_data['min']]


# In[232]:


cut_bins = [float('-inf'), 529.0, 542.0, 553.0, 564.0, 577.0, 594.0, 611.0, 633.0, 660.0, float('inf')]

train_selected['bins'] = pd.cut(train_selected['score'], bins=cut_bins, include_lowest=True, right=False)
oot_selected['bins'] = pd.cut(oot_selected['score'], bins=cut_bins, include_lowest=True, right=False)

train_selected_167['bins'] = pd.cut(train_selected_167['score'], bins=cut_bins, include_lowest=True, right=False)
oot_selected_167['bins'] = pd.cut(oot_selected_167['score'], bins=cut_bins, include_lowest=True, right=False)

train_selected_174['bins'] = pd.cut(train_selected_174['score'], bins=cut_bins, include_lowest=True, right=False)
oot_selected_174['bins'] = pd.cut(oot_selected_174['score'], bins=cut_bins, include_lowest=True, right=False)


# In[233]:


def regroup(data, col, target='target'):
    total = data.groupby(col)[target].count()
    bad = data.groupby(col)[target].sum()
    regroup = pd.concat([total, bad],axis=1)
    regroup.columns = ['total', 'bad']
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_rate'] = regroup['bad']/regroup['total']
    regroup['total_pct'] = regroup['total']/regroup['total'].sum()
    regroup['varsname'] = col
    regroup['bins'] = regroup.index
    cols = ['varsname','bins','bad','good','total','bad_rate','total_pct']
    regroup = regroup[cols]
    return regroup


# In[234]:


train_selected_regroup = regroup(train_selected, 'bins')
oot_selected_regroup = regroup(oot_selected, 'bins')
train_selected_167_regroup = regroup(train_selected_167, 'bins')
oot_selected_167_regroup = regroup(oot_selected_167, 'bins')
train_selected_174_regroup = regroup(train_selected_174, 'bins')
oot_selected_174_regroup = regroup(oot_selected_174, 'bins')


# In[235]:


writer=pd.ExcelWriter(r"d:\liuyedao\mid_result\auth_建模_评分卡_业务效果_3_xgb_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
train_selected_regroup.to_excel(writer,sheet_name='train')
oot_selected_regroup.to_excel(writer,sheet_name='test')
train_selected_167_regroup.to_excel(writer,sheet_name='train_167')
oot_selected_167_regroup.to_excel(writer,sheet_name='test_167')
train_selected_174_regroup.to_excel(writer,sheet_name='train_174')
oot_selected_174_regroup.to_excel(writer,sheet_name='test_174')
writer.save()


# In[247]:


def cal_psi(exp, act):
    psi = []
    for i in range(len(exp)):
        psi_i = (act[i] - exp[i])*np.log(act[i]/exp[i])
        psi.append(psi_i)
    return sum(psi)


# In[248]:


print(cal_psi(train_selected_regroup['total_pct'], oot_selected_regroup['total_pct'])) 
print(cal_psi(train_selected_167_regroup['total_pct'], oot_selected_167_regroup['total_pct'])) 
print(cal_psi(train_selected_174_regroup['total_pct'], oot_selected_174_regroup['total_pct'])) 


# In[236]:


def cal_auth(df_auth, channel=None):
    if channel:
        df_auth = df_auth.query("channel_id==@channel")
        
    total_lend = df_auth.groupby('bins')['order_no','Firs6ever30'].count()
    tg_jj = df_auth.groupby(['bins', 'auth_status'])['order_no'].count().unstack()
    lend =  df_auth.groupby(['bins', 'Firs6ever30'])['order_no'].count().unstack()
    result = pd.concat([total_lend, tg_jj, lend], axis=1)
    result.columns = ['授信申请','放款人数','通过人数','拒绝人数','好','灰','坏']
    
    result_sum = pd.DataFrame(result.sum(axis=0),columns=['total']).T
    
    for col in result.columns:
        result['{}占比'.format(col)] = result[col]/result[col].sum()
        result['{}累计占比'.format(col)] = result['{}占比'.format(col)].cumsum()
        
    result['坏客率'] = result['坏']/(result['好']+result['坏'])
    result['通过率'] = result['通过人数']/result['授信申请']
    result = pd.concat([result, result_sum],axis=0)
    
    cols = ['授信申请','授信申请占比','授信申请累计占比','通过人数','通过率','拒绝人数','拒绝人数占比','拒绝人数累计占比',
            '通过人数占比','通过人数累计占比','放款人数','放款人数占比','放款人数累计占比','好','好占比','好累计占比',
            '坏','坏占比','坏累计占比','坏客率','灰','灰占比','灰累计占比']
    result = result[cols]
    
    return result


# In[238]:


auth_data = data_copy[list(train_selected.columns[:-4])+['Firs6ever30']]


# In[239]:


auth_data.columns


# In[240]:


cols = list(auth_data.columns[7:-1])
print(cols)


# In[241]:


cut_bins = [float('-inf'), 529.0, 542.0, 553.0, 564.0, 577.0, 594.0, 611.0, 633.0, 660.0, float('inf')]

auth_data['prob'] = xgb_model.predict_proba(auth_data[cols])[:,1]
auth_data['score'] = auth_data['prob'].apply(lambda x:Prob2Score(x))
auth_data['bins'] = pd.cut(auth_data['score'], bins=cut_bins, include_lowest=True, right=False)


# In[242]:


result_auth = cal_auth(auth_data, channel=None)
result_auth_167 = cal_auth(auth_data, channel=167)
result_auth_174 = cal_auth(auth_data, channel=174)


# In[243]:


print(toad.metrics.PSI(pred_train, pred_oot))
print(toad.metrics.PSI(pred_train_167, pred_oot_167))
print(toad.metrics.PSI(pred_train_174, pred_oot_174))


# In[244]:


writer=pd.ExcelWriter(r"d:\liuyedao\mid_result\auth_建模_评分卡_业务效果_授信申请_xgb_{}_v{}.xlsx".format(str(datetime.today())[:10].replace('-',''),2))
result_auth.to_excel(writer,sheet_name='all')
result_auth_167.to_excel(writer,sheet_name='167')
result_auth_174.to_excel(writer,sheet_name='174')
writer.save()


# In[245]:


auth_data.info(show_counts=True)


# In[246]:


auth_data.to_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_xgb_score_20230807.csv",index=False)


# # oot数据

# In[4]:


from pypmml import Model


# In[35]:


auth_data = pd.read_csv(r'd:\liuyedao\model_result\oot_data_20230815.csv')
auth_data.info()


# In[36]:


import pickle
# load
xgb_model_pkl = pickle.load(open(r"D:\liuyedao\model_result\auth_xgb_model.pkl", "rb"))
xgb_model_pkl.get_booster().get_score(importance_type='gain')


# In[37]:


model_cols = list(xgb_model_pkl.get_booster().get_score(importance_type='gain').keys())
model_cols


# In[12]:


def Prob2Score(prob, base_odds=35, base_score=700, pdo=60, rate=2) :
    # 将概率转化成分数且为正整数
    y = np.log((1 - prob) / prob)
    factor = pdo/np.log(rate)
    offset = base_score - factor * np.log(base_odds)
    score = offset +  factor * (y)
    
    return score


auth_data['score_xgb'] = auth_data['prob'].apply(lambda x:Prob2Score(x))


# In[22]:


usecols = ['order_no','user_id','id_no_des','channel_id','apply_date_auth','apply_time','auth_status'] + cols + ['Firs6ever30', 'score_xgb']
usecols


# In[17]:


auth_data = auth_data[usecols]
auth_data.to_csv(r"d:\liuyedao\mid_result\auth_建模_评分卡_xgb_score_20230815_oot.csv",index=False)


# In[18]:


auth_data.info(show_counts=True)


# In[38]:


auth_data['apply_month'] = auth_data['apply_date_auth'].str[0:7]
auth_data['apply_month'].value_counts()


# In[26]:


auth_data['Firs6ever30'].value_counts()


# In[50]:


# 对训练集进行预测
auth_data_mob6 = auth_data[auth_data['Firs6ever30'].isin([0.0, 2.0])][model_cols + ['Firs6ever30','apply_month','order_no']]
auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()


# In[51]:


auth_data_mob6 = auth_data_mob6[auth_data_mob6['apply_month']=='2023-01']
auth_data_mob6['target'] = auth_data_mob6['Firs6ever30'] /2.0
auth_data_mob6.info()


# In[52]:


auth_data_mob6.groupby(['apply_month', 'Firs6ever30'])['order_no'].count().unstack()


# In[53]:


model_cols


# In[54]:


auth_data_mob6['prob'] = xgb_model_pkl.predict_proba(auth_data_mob6[model_cols])[:,1]

fpr, tpr, thresholds = metrics.roc_curve(auth_data_mob6['target'], auth_data_mob6['prob'], pos_label=1)
auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train AUC: ', auc)
print('train KS: ', ks)


# In[55]:


auth_data_mob3 = auth_data[auth_data['Firs3ever30'].isin([0.0, 2.0])][model_cols + ['Firs3ever30','apply_month','order_no']]
auth_data_mob3.groupby(['apply_month', 'Firs3ever30'])['order_no'].count().unstack()


# In[56]:


auth_data_mob3 = auth_data_mob3[auth_data_mob3['apply_month']!='2023-05']
auth_data_mob3['target'] = auth_data_mob3['Firs3ever30'] /2.0
auth_data_mob3.info()


# In[57]:


auth_data_mob3.groupby(['apply_month', 'Firs3ever30'])['order_no'].count().unstack()


# In[58]:


# 对训练集进行预测
auth_data_mob3['prob'] = xgb_model_pkl.predict_proba(auth_data_mob3[model_cols])[:,1]

fpr, tpr, thresholds = metrics.roc_curve(auth_data_mob3['target'], auth_data_mob3['prob'], pos_label=1)
auc = metrics.auc(fpr, tpr)
ks = max(tpr-fpr)
print('train AUC: ', auc)
print('train KS: ', ks)


# In[59]:


for i in list(auth_data_mob3['apply_month'].unique()):
    print(i)
    tmp = auth_data_mob3[auth_data_mob3['apply_month']==i]
    tmp['prob'] = xgb_model_pkl.predict_proba(tmp[model_cols])[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(tmp['target'], tmp['prob'], pos_label=1)
    auc = metrics.auc(fpr, tpr)
    ks = max(tpr-fpr)
    print('train AUC: ', auc)
    print('train KS: ', ks)
    print('-----------------------------------')




#==============================================================================
# File: 转转渠道_数据分析-授信层.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[65]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc
import os

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[66]:


path = r'D:\liuyedao\转转渠道'
os.chdir(path)


# In[67]:


os.getcwd()


# # 1.基础数据表

# In[4]:


# 授信表
df_auth = pd.read_csv('dwd_beforeloan_auth_examine_fd_80005.csv')


# In[135]:


df_auth.head()


# In[5]:


print(df_auth.shape, df_auth.apply_date.min(), df_auth.apply_date.max())


# In[7]:


# 订单表
df_order = pd.read_csv('dwd_beforeloan_order_examine_fd_80005.csv')
print(df_order.shape, df_order.apply_date.min(), df_order.apply_date.max())


# In[82]:


# 还款计划表
df_repay_plan = pd.read_csv('dwd_cap_repay_plan_fd_80005.csv')
print(df_repay_plan.shape, df_repay_plan.lending_time.min(), df_repay_plan.lending_time.max())


# In[83]:


#数据去重
# 授信表
# df_auth = df_auth.sort_values(['user_id','apply_date'],ascending=False).drop_duplicates(subset=['user_id'],keep='first')
# df_auth_drop  = df_auth[['user_id','id_no_des', 'auth_status','apply_date','auth_credit_amount']]

# 还款计划表
df_repay_plan = df_repay_plan.drop_duplicates(subset=['order_no'],keep='first')
df_repay_plan_drop = df_repay_plan[['order_no','user_id','id_no_des', 'lending_time','loan_amount','loan_rate','total_periods']]


# In[190]:


tmp = df_repay_plan_drop.copy()
tmp['lend_month'] = tmp['lending_time'].str[0:7]
tmp.groupby(['lend_month', 'loan_rate'])['order_no'].count().unstack()


# In[9]:


df_auth_order = pd.merge(df_auth_drop, df_order[['user_id','order_no','id_no_des','order_status','apply_date']], how='inner', on='user_id', suffixes=['_auth', '_order'])
df_auth_order.info()


# In[10]:


df_auth_order_repay = pd.merge(df_auth_order, df_repay_plan_drop, how='inner', on='order_no',suffixes=['_auth','_repay'])
print(df_auth_order.shape, df_auth_drop.shape, df_repay_plan_drop.shape, df_auth_order_repay.shape)


# In[11]:


df_auth_order_repay.info()


# In[25]:


df_auth_order_repay.query("apply_date_auth<=apply_date_order").shape


# In[ ]:


df_auth_order_repay['credit_limit_use'] = df_auth_order_repay['loan_amount']/df_auth_order_repay['auth_credit_amount']
df_auth_order_repay['credit_use_days'] = (pd.to_datetime(df_auth_order_repay['apply_date_order'],format='%Y-%m-%d') - pd.to_datetime(df_auth_order_repay['apply_date_auth'],format='%Y-%m-%d')).dt.days 


# In[ ]:


df_auth_order_repay_first = df_auth_order_repay.sort_values(['user_id_order','apply_date_order']).drop_duplicates(subset=['user_id_order'],keep='first')


# In[ ]:


df_auth_order_repay['credit_limit_use'].describe()


# In[ ]:


df_auth_order_repay['credit_use_days'].describe()


# In[ ]:


df_auth_order_repay_first['credit_limit_use_bins'] = pd.cut(df_auth_order_repay_first['credit_limit_use'], bins=[-1,0,0.1,0.2,0.3,0.5,0.8,1.0])
df_auth_order_repay_first['credit_use_days_bins'] = pd.cut(df_auth_order_repay_first['credit_use_days'], bins=[-1,0, 7,15,30,60,90,120,150,180,999])


# In[ ]:


df_auth_order_repay_first['credit_limit_use_bins'].value_counts(dropna=False)


# In[ ]:


df_auth_order_repay_first['credit_use_days_bins'].value_counts(dropna=False)


# # 1.授信表

# In[ ]:


df_auth['tacticskq_name'].value_counts(dropna=False)


# In[ ]:


df_auth['apply_month'] = df_auth['apply_date'].str[0:7]


# In[ ]:


df_auth['apply_date'].str[0:7].value_counts(sort=False)


# In[ ]:


pd.concat([df_auth['auth_status'].value_counts(dropna=False),df_auth['auth_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


df_auth.groupby(['apply_month', 'auth_status'])['order_no'].count().unstack()


# In[ ]:


pd.concat([df1['education'].value_counts(dropna=False),df1['education'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


df_auth_num = df_auth.groupby(['id_no_des'])['order_no'].count().reset_index()
df_auth_num['bins'] = pd.cut(df_auth_num['order_no'], bins=[0,1,2,3,4,5,10,20, 30,99999])
df_auth_num['bins'].value_counts(dropna=False)


# In[ ]:


pd.merge(df1, df2, how='inner', on='user_id').shape


# In[ ]:


df1[df1['auth_status']==6]['auth_credit_amount'].describe()


# # 2.提现表

# In[68]:


# 订单表
df_order = pd.read_csv('dwd_beforeloan_order_examine_fd_80005.csv')
print(df_order.shape, df_order.apply_date.min(), df_order.apply_date.max())


# In[93]:


tmp = df_repay_plan_drop.groupby('user_id')['order_no'].count()
tmp.value_counts()


# In[94]:


tmp_df = pd.DataFrame(tmp).query("order_no==1").reset_index()
tmp_df.shape


# In[95]:


tmp_df.head()


# In[96]:


tmp_df['user_id'].nunique()


# In[97]:


tmp = pd.merge(tmp_df, df_order, how='inner', on='user_id')
tmp_1 = tmp.groupby('user_id')['order_no_y'].count()
tmp_1.value_counts()


# In[99]:


tmp_1_df = pd.DataFrame(tmp_1).query("order_no_y>1").reset_index()
tmp_1_df.shape


# In[125]:


tmp_2_df = pd.DataFrame(tmp_1).query("order_no_y==1").reset_index()


# In[126]:


tmp_3_df = pd.merge(tmp_2_df, df_order, how='inner', on='user_id')
tmp_3_df.shape


# In[129]:


tmp_3_df['apply_month'] = tmp_3_df['apply_date'].str[0:7]
tmp_3_df['apply_month'].value_counts(sort=False)


# In[139]:


# print(tmp_3_df.shape)
# tmp_3_df = pd.merge(tmp_3_df, df_auth[['user_id','auth_credit_amount']], how='inner', on='user_id')
# print(tmp_3_df.shape)
tmp_3_df['credit_limit_use'] = tmp_3_df['loan_amount']/tmp_3_df['auth_credit_amount_x']
tmp_3_df['credit_limit_use_bins'] = pd.cut(tmp_3_df['credit_limit_use'], bins=[-1,0,0.1,0.2,0.3,0.5,0.8,0.9,1.0])
tmp_3_df['credit_limit_use_bins'].value_counts(dropna=False)


# In[136]:


df_auth.columns


# In[100]:


tmp_1_df.head()


# In[102]:


tmp_2 = pd.merge(tmp_1_df, df_order.query("order_status==7"), how='inner', on='user_id')
tmp_2.shape


# In[103]:


tmp_2['user_id'].nunique()


# In[108]:


tmpxx = pd.pivot_table(df_order, values='order_no', index='user_id', columns='order_status',aggfunc='count',fill_value=0)[[6,7]]


# In[116]:


tmpxx.head()


# In[111]:


tmpxx = tmpxx.reset_index()


# In[115]:


tmpxx.columns = ['user_id', '通过', '拒绝']


# In[117]:


tmpxx_2 = tmpxx.query("通过>0 & 拒绝>0")
tmpxx_2.shape


# In[122]:


tmpxx_3 = pd.merge(tmpxx_2, tmp_df, how='inner', on='user_id')
tmpxx_3.shape


# In[124]:


tmpxx_3.head(100)


# In[106]:


help(pd.pivot_table)


# In[ ]:


pd.concat([df2['cust_type'].value_counts(dropna=False),df2['cust_type'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


pd.concat([df2['tacticskq_name'].value_counts(dropna=False),df2['tacticskq_name'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


pd.concat([df_order['order_status'].value_counts(dropna=False),df_order['order_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


df_order['apply_date'].str[0:7].value_counts(sort=False)


# In[ ]:


df_order['apply_month'] = df_order['apply_date'].str[0:7]


# In[ ]:


df_order.groupby(['apply_month', 'order_status'])['order_no'].count().unstack()


# In[ ]:


df2[df2['order_status']==6]['loan_amount'].describe()


# In[ ]:


df_order['order_status'].value_counts(dropna=False)


# # 3.还款计划表

# In[ ]:


xx = df3[['order_no', 'loan_rate','total_periods']].drop_duplicates(subset=['order_no'], keep='first')
xx.groupby(['loan_rate', 'total_periods'])['order_no'].count().unstack()

# pd.concat([df3['loan_rate'].value_counts(dropna=False),df3['loan_rate'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


pd.concat([df3['order_status'].value_counts(dropna=False),df3['order_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


pd.concat([df3['total_periods'].value_counts(dropna=False),df3['total_periods'].value_counts(dropna=False,normalize=True)],axis=1)


# In[ ]:


xx = df3[['order_no', 'lending_time']].drop_duplicates(subset=['order_no'], keep='first')
xx['lending_time'] = df3['lending_time'].str[0:7]
xx.groupby(['lending_time_1'])['order_no'].count().unstack()


# In[ ]:


df3['repay_date'].str[0:7].value_counts(sort=False)


# In[ ]:


pd.merge(df2, df3, how='inner', on='order_no').shape


# In[ ]:


df3['loan_rate'].describe()


# In[ ]:


df3['period_settle_date'].str[0:7].value_counts(dropna=True)


# In[ ]:


xx = df3[['order_no', 'lending_time','total_periods']].drop_duplicates(subset=['order_no'], keep='first')
xx['lending_time_1'] = df3['lending_time'].str[0:7]
xx.groupby(['lending_time_1', 'total_periods'])['order_no'].count().unstack()


# # 4. 标签定义

# In[140]:


df_repay = df_base.copy()


# In[141]:


df_repay.shape


# In[142]:


usecols = ['order_no','user_id','lending_time', 'loan_amount', 'loan_rate', 'total_periods', 'mob','mob_date']
df_repay = df_repay.groupby(usecols).agg({'ever_overdue_days':'max',
                                          'ovd_status_ever':'max',
                                         'curr_overdue_days':'max',
                                         'ovd_status_curr':'max'})
df_repay = df_repay.reset_index()


# In[143]:


def defin_target(t1, t2, mob):
    if t1>=2:
        target = 1
    elif t1==0 and t2==mob:
        target = 0
    elif t1==1 and t2==mob:
        target = 0.5
    elif t2<mob:
        target = -1
    else:
        target = np.nan
    return target


# In[20]:


df_flag = df_repay.query("mob<=9")

usecols = ['order_no','user_id','lending_time', 'loan_amount', 'loan_rate', 'total_periods']
df_flag = df_flag.groupby(usecols).agg({'ever_overdue_days':'max',
                                          'ovd_status_ever':'max',
                                         'curr_overdue_days':'max',
                                         'ovd_status_curr':'max',
                                        'mob':'max'
                                       })
df_flag = df_flag.reset_index()
df_flag['target'] = [*map(lambda t1,t2: defin_target(t1, t2, 9), df_flag['ovd_status_ever'],df_flag['mob'])]


# In[21]:


df_flag.info()
df_flag.head()


# In[23]:


df_flag['target'].value_counts()


# In[24]:


df_flag['lending_month'] = df_flag['lending_time'].apply(lambda x:str(x)[0:7])


# In[46]:


tmp = df_flag.groupby(['lending_month','target'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[32]:


df_flag_mob6 = df_repay.query("mob<=6")

usecols = ['order_no','user_id','lending_time', 'loan_amount', 'loan_rate', 'total_periods']
df_flag_mob6 = df_flag_mob6.groupby(usecols).agg({'ever_overdue_days':'max',
                                                  'ovd_status_ever':'max',
                                                 'curr_overdue_days':'max',
                                                 'ovd_status_curr':'max',
                                                'mob':'max'
                                               })
df_flag_mob6 = df_flag_mob6.reset_index()
df_flag_mob6['target'] = [*map(lambda t1,t2: defin_target(t1, t2, 6), df_flag_mob6['ovd_status_ever'],df_flag_mob6['mob'])]
df_flag_mob6['lending_month'] = df_flag_mob6['lending_time'].apply(lambda x:str(x)[0:7])


# In[33]:


df_flag_mob6.info()


# In[34]:


df_flag_mob6['target'].value_counts()


# In[35]:


tmp = df_flag_mob6.groupby(['lending_month','target'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[42]:


needcols = ['order_no','user_id','lending_time', 'loan_amount', 'loan_rate', 'total_periods']
df_target = pd.merge(df_flag, df_flag_mob6, how='outer', on=needcols, suffixes=['_mob9', '_mob6'])
df_target.info()
df_target.head()


# In[44]:


df_target['target_mob6'].value_counts()


# In[45]:


df_target['target_mob9'].value_counts()


# In[154]:


def defin_target_mob3(t1, t2, mob):
    if t1>15:
        target = 1
    elif t1==0 and t2==mob:
        target = 0
    elif t1>0 and t1<=15 and t2==mob:
        target = 0.5
    elif t2<mob:
        target = -1
    else:
        target = np.nan
    return target


# In[155]:


df_flag_mob3 = df_repay.query("mob<=3")

usecols = ['order_no','user_id','lending_time', 'loan_amount', 'loan_rate', 'total_periods']
df_flag_mob3 = df_flag_mob3.groupby(usecols).agg({'ever_overdue_days':'max',
                                          'ovd_status_ever':'max',
                                         'curr_overdue_days':'max',
                                         'ovd_status_curr':'max',
                                        'mob':'max'
                                       })
df_flag_mob3 = df_flag_mob3.reset_index()
df_flag_mob3['target'] = [*map(lambda t1,t2: defin_target_mob3(t1, t2, 3), df_flag_mob3['ever_overdue_days'],df_flag_mob3['mob'])]


# In[156]:


df_auth_target_mob3 = df_flag_mob3.groupby(['user_id']).agg({
                                                 'ever_overdue_days':'max',
                                                 'ovd_status_ever':'max',
                                                'target':'max',
                                                'lending_time':'min'
                                               })
df_auth_target_mob3['lending_month'] = df_auth_target_mob3['lending_time'].apply(lambda x:str(x)[0:7])
df_auth_target_mob3 = df_auth_target_mob3.reset_index()


# In[157]:


tmp = df_auth_target_mob3.groupby(['lending_month','target'])['user_id'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[158]:


df_auth_target_mob3.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob3.csv',index=False)


# In[64]:


df_target.to_csv(r'D:\liuyedao\转转渠道\mid_result\order_target_mob_9_or_6.csv',index=False)


# In[57]:


df_auth_target = df_target.groupby(['user_id']).agg({'target_mob6':'max',
                                                  'target_mob9':'max',
                                                 'ever_overdue_days_mob6':'max',
                                                 'ovd_status_ever_mob6':'max',
                                                 'ever_overdue_days_mob9':'max',
                                                 'ovd_status_ever_mob9':'max',
                                                'lending_time':'min'
                                               })


# In[58]:


df_auth_target['lending_month'] = df_auth_target['lending_time'].apply(lambda x:str(x)[0:7])
df_auth_target = df_auth_target.reset_index()


# In[59]:


df_auth_target.info()


# In[61]:


tmp = df_auth_target.groupby(['lending_month','target_mob6'])['user_id'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[62]:


tmp = df_auth_target.groupby(['lending_month','target_mob9'])['user_id'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[63]:


df_auth_target.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_target_mob_9_or_6.csv',index=False)


# # 5.基础数据

# In[8]:


from datetime import datetime
from dateutil.relativedelta import relativedelta
import calendar

def calc_curr_overdue_days(x):
    if x['repay_date'] >= x['mob_date']:
        due_days = 0
    elif x['is_settle_period'] == 0 and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] >= x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] < x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = 0
    else:
        due_days = 0 
    return due_days

def calc_ever_overdue_days(x):
    if x['repay_date'] >= x['mob_date']:
        due_days = 0
    elif x['is_settle_period'] == 0 and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] >= x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] < x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['period_settle_date'] - x['repay_date']).days
    else:
        due_days = 0 
    return due_days


def calculate_month_difference(start_date, end_date):
#     start_date = datetime.strptime(start_date, "%Y-%m-%d")
#     end_date = datetime.strptime(end_date, "%Y-%m-%d")

    # 取得两个月末日期
    start_month_end = (start_date + relativedelta(day=31)).replace(day=1)
    end_month_end = (end_date + relativedelta(day=31)).replace(day=1)

    # 计算月份数
    month_difference = (end_month_end.year - start_month_end.year) * 12 + (end_month_end.month - start_month_end.month)

    return month_difference


def get_month_end_dates(year, month_start, month_end):
    month_end_dates = []
    
    for month in range(month_start, month_end + 1):
        _, last_day = calendar.monthrange(year, month)
        month_end_date = datetime(year, month, last_day)
        month_end_dates.append(month_end_date)
    
    return month_end_dates



def calc_ovedue_status(x):
    if x<=0:
        status=0
    elif x>0 and x<=30:
        status=1
    elif x>30 and x<=60:
        status=2
    elif x>60 and x<=90:
        status=3
    elif x>90 and x<=120:
        status=4
    elif x>120:
        status=5
    else:
        status=np.nan
    return status


# In[9]:


cols = ['order_no','user_id','id_no_des','lending_time','loan_amount','loan_rate','total_periods','period', 'repay_date','period_settle_date','repid_time',
        'is_settle_period','overdue_day','actual_overdue_days','principal','already_repaid_principal','repay_amount','interest',
        'already_repaid_interest','repay_plan_id']

df_base = pd.read_csv('dwd_cap_repay_plan_fd_80005.csv')[cols]
df_base = df_base.sort_values(['order_no', 'period']).reset_index(drop=True)
print(df_base.shape)


# In[10]:


time1 = get_month_end_dates(2022, 6, 12)
time2 = get_month_end_dates(2023, 1, 8)


# In[11]:


mob_month_end = pd.DataFrame({'mob_date':[date.strftime("%Y-%m-%d") for date in (time1+time2)]})
mob_month_end['keys'] = 1  
mob_month_end


# In[12]:


df_base['keys']=1
df_base = df_base.merge(mob_month_end, on='keys').drop('keys',axis=1)
df_base.shape


# In[13]:


# df_base['mob_month'] = df_base['mob_date'].str[0:7]
# df_base['lending_month'] = df_base['lending_time'].str[0:7]

df_base['repay_date'] = pd.to_datetime(df_base['repay_date'],format='%Y-%m-%d')
df_base['period_settle_date'] = pd.to_datetime(df_base['period_settle_date'],format='%Y-%m-%d')
df_base['lending_time'] = pd.to_datetime(df_base['lending_time'].str[0:10],format='%Y-%m-%d')
df_base['repid_time'] = pd.to_datetime(df_base['repid_time'].str[0:10],format='%Y-%m-%d')
df_base['mob_date'] = pd.to_datetime(df_base['mob_date'],format='%Y-%m-%d')

df_base[['lending_time','repay_date','period_settle_date','mob_date']].head(2)


# In[14]:


df_base = df_base.query("mob_date>=lending_time & mob_date< '2023-08-01' ")
df_base = df_base.reset_index(drop=True)
df_base.shape


# In[15]:


# 计算账龄
df_base['mob'] = [*map(lambda t1,t2: calculate_month_difference(t1, t2), df_base['lending_time'], df_base['mob_date'])]
# 计算逾期天数
df_base['ever_overdue_days'] = df_base.apply(calc_ever_overdue_days, axis=1)
df_base['curr_overdue_days'] = df_base.apply(calc_curr_overdue_days, axis=1)


# In[16]:


#计算到期应还本金
df_base['due_prin_amt'] = df_base.apply(lambda x: x['principal'] if x['repay_date'] <= x['mob_date'] else 0, axis=1)

#计算到期已还本金
df_base['act_prin_amt'] = df_base.apply(lambda x: x['already_repaid_principal'] if x['repay_date'] <= x['mob_date'] and x['repid_time'] <= x['mob_date'] else 0, axis=1)


#计算逾期状态
df_base['ovd_status_ever'] = df_base['ever_overdue_days'].apply(lambda x: calc_ovedue_status(x))
df_base['ovd_status_curr'] = df_base['curr_overdue_days'].apply(lambda x: calc_ovedue_status(x))


# In[ ]:





# # 账龄分析

# In[181]:


df_vintage = df_base.copy()


# In[182]:


#标记逾期31天以上标识
df_vintage['ovd_flag_ever_30'] = df_vintage.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['ever_overdue_days'] > 30 else 0, axis=1)
df_vintage['ovd_flag_ever_15'] = df_vintage.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['ever_overdue_days'] > 15 else 0, axis=1)

df_vintage = df_vintage.groupby(['order_no','user_id','lending_time', 'loan_rate', 'loan_amount','mob','mob_date']).agg({
                                                                                                     'ovd_flag_ever_30':'max',
                                                                                                     'ovd_flag_ever_15':'max'})
df_vintage = df_vintage.reset_index()
df_vintage.head()


# In[183]:


df_vintage_24 = df_vintage.query("loan_rate<0.025")
df_vintage_36 = df_vintage.query("loan_rate>0.025")


# In[184]:


def vintage_analysis(df1):
    df1 = df1.groupby(['user_id','mob']).agg({'lending_time':'min',
                                             'ovd_flag_ever_30':'max',
                                             'ovd_flag_ever_15':'max'})
    df1 = df1.reset_index()
    df1['lending_month'] = df1['lending_time'].apply(lambda x:str(x)[0:7])
    tmp = df1.groupby(['lending_month', 'mob']).agg({'user_id':'count',
                                                     'ovd_flag_ever_30':'sum',
                                                     'ovd_flag_ever_15':'sum'})
    tmp = tmp.reset_index()
    tmp_total = pd.pivot_table(tmp, values='user_id', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
    tmp_30 = pd.pivot_table(tmp, values='ovd_flag_ever_30', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
    tmp_15 = pd.pivot_table(tmp, values='ovd_flag_ever_15', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)

    tmp_30_pct = tmp_30/tmp_total
    tmp_15_pct = tmp_15/tmp_total
    
    return tmp_total,tmp_30,tmp_15,tmp_30_pct,tmp_15_pct


# In[185]:


tmp_total,tmp_30,tmp_15,tmp_30_pct,tmp_15_pct = vintage_analysis(df_vintage)
tmp_total_24,tmp_30_24,tmp_15_24,tmp_30_pct_24,tmp_15_pct_24 = vintage_analysis(df_vintage_24)
tmp_total_36,tmp_30_36,tmp_15_36,tmp_30_pct_36,tmp_15_pct_36 = vintage_analysis(df_vintage_36)


# In[188]:


writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\账龄分析_授信_"+str(datetime.today())[:10].replace('-','')+'.xlsx')

tmp_30_pct.to_excel(writer,sheet_name='tmp_30_pct')
tmp_15_pct.to_excel(writer,sheet_name='tmp_15_pct')

tmp_30_pct_24.to_excel(writer,sheet_name='tmp_30_pct_24')
tmp_15_pct_24.to_excel(writer,sheet_name='tmp_15_pct_24')

tmp_30_pct_36.to_excel(writer,sheet_name='tmp_30_pct_36')
tmp_15_pct_36.to_excel(writer,sheet_name='tmp_15_pct_36')

writer.save()


# In[189]:


tmp_total_24


# In[191]:


tmp_15_24


# # 6.滚动率分析

# In[87]:


df_roll = df_base.groupby(['order_no','user_id','lending_time', 'total_periods', 'loan_amount','loan_rate','mob_date']).agg({'ever_overdue_days':'max',
                                                                                                              'ovd_status_ever':'max',
                                                                                                             'curr_overdue_days':'max',
                                                                                                             'ovd_status_curr':'max'})
df_roll = df_roll.reset_index()
print(df_roll.shape)
df_roll = df_roll[df_roll['order_no'].isin(df_auth_order_repay['order_no'])]
df_roll = df_roll.reset_index(drop=True)
print(df_roll.shape)


# In[88]:


df_roll_24 = df_roll.query("loan_rate<0.025")
df_roll_36 = df_roll.query("loan_rate>0.025")


# In[89]:


print(df_roll_24.shape, df_roll_36.shape)


# In[90]:


mob_date_month = get_month_end_dates(2022, 8, 12)+get_month_end_dates(2023, 1, 7)
mob_date_month


# In[91]:


def roll_rate_analysis(df_roll, mob_date_month, is_auth=1):
    roll_curr = pd.DataFrame(columns=[0,1,2,3,4,5,'All','mob_date_month'])
    roll_curr_rate = pd.DataFrame(columns=[0,1,2,3,4,5,'All','mob_date_month'])
    for i in range(len(mob_date_month)-1):
        date1 = mob_date_month[i]
        date2 = mob_date_month[i+1]
        df_roll_part1 = df_roll.query("lending_time<= @date1 & mob_date<= @date1")
        df_roll_part2 = df_roll.query("lending_time<= @date1 & mob_date<= @date2 & mob_date> @date1")
        
        if is_auth==1:
            df_roll_part1 = df_roll_part1.groupby(['user_id']).agg({'curr_overdue_days':'max','ovd_status_curr':'max'})
            df_roll_part1 = df_roll_part1.reset_index()
            df_roll_part2 = df_roll_part2.groupby(['user_id']).agg({'curr_overdue_days':'max','ovd_status_curr':'max'})
            df_roll_part2 = df_roll_part2.reset_index()
            kyes = 'user_id'
        else:
            kyes = 'order_no'
            
        df_roll_v1 = pd.merge(df_roll_part1, df_roll_part2, how='left',on=kyes)

        tmp_curr = pd.pivot_table(df_roll_v1, values=kyes,index=['ovd_status_curr_x'], columns=['ovd_status_curr_y'],aggfunc='count',margins=True)
        tmp_curr_rate = tmp_curr.copy()
        for k in list(tmp_curr.columns):
            tmp_curr_rate[k] = tmp_curr[k]/tmp_curr["All"]
        for j in tmp_curr.index:
            if j==0:
                tmp_curr_rate.loc[j, 'to_bad_rate'] = tmp_curr_rate.loc[j, list(tmp_curr.columns)[1:-1]].sum()
                tmp_curr_rate.loc[j, 'status'] = "M0-M0+"
            elif j==1:
                tmp_curr_rate.loc[j, 'to_bad_rate'] = tmp_curr_rate.loc[j, list(tmp_curr.columns)[2:-1]].sum()
                tmp_curr_rate.loc[j, 'status'] = "M1-M1+"
            elif j==2:
                tmp_curr_rate.loc[j, 'to_bad_rate'] = tmp_curr_rate.loc[j, list(tmp_curr.columns)[3:-1]].sum()
                tmp_curr_rate.loc[j, 'status'] = "M2-M2+"
            elif j==3:
                tmp_curr_rate.loc[j, 'to_bad_rate'] = tmp_curr_rate.loc[j, list(tmp_curr.columns)[4:-1]].sum()
                tmp_curr_rate.loc[j, 'status'] = "M3-M3+"
            elif j==4:
                tmp_curr_rate.loc[j, 'to_bad_rate'] = tmp_curr_rate.loc[j, list(tmp_curr.columns)[5:-1]].sum()
                tmp_curr_rate.loc[j, 'status'] = "M4-M4+"
            else:
                pass
        tmp_curr['mob_date_month'] = mob_date_month[i]
        tmp_curr_rate['mob_date_month'] = mob_date_month[i]

        roll_curr = pd.concat([roll_curr, tmp_curr])
        roll_curr_rate = pd.concat([roll_curr_rate, tmp_curr_rate])
        
    return roll_curr, roll_curr_rate
  


# In[93]:


roll_curr_total, roll_curr_rate_total = roll_rate_analysis(df_roll, mob_date_month)
roll_curr_24, roll_curr_rate_24 = roll_rate_analysis(df_roll_24, mob_date_month)
roll_curr_36, roll_curr_rate_36 = roll_rate_analysis(df_roll_36, mob_date_month)


# In[94]:


writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\滚动率分析_授信_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
roll_curr_total.to_excel(writer,sheet_name='roll_curr_total')
roll_curr_rate_total.to_excel(writer,sheet_name='roll_curr_rate_total')

roll_curr_24.to_excel(writer,sheet_name='roll_curr_24')
roll_curr_rate_24.to_excel(writer,sheet_name='roll_curr_rate_24')

roll_curr_36.to_excel(writer,sheet_name='roll_curr_36')
roll_curr_rate_36.to_excel(writer,sheet_name='roll_curr_rate_36')
writer.save()  


# # 迁徙率分析

# In[140]:


df_qx = df_base.query("mob_date< '2023-08-01'")
df_qx.shape


# In[141]:


# 汇总聚合到借据层
df_qx = df_qx.groupby(['order_no','user_id', 'lending_time', 'loan_rate', 'loan_amount','mob_date']).agg({'ovd_status_curr':'max',
                                                                                                    'curr_overdue_days':'max',
                                                                                                    'ever_overdue_days':'max',
                                                                                                    'ovd_status_ever':'max',
                                                                                                    'act_prin_amt':'sum'})
df_qx = df_qx.reset_index()
print(df_qx.shape)
df_qx.head()


# In[142]:


df_qx['loan_bal'] = df_qx['loan_amount'] - df_qx['act_prin_amt'] #剩余本金


# In[143]:


df_qx_24 = df_qx.query("loan_rate<0.025")
df_qx_36 = df_qx.query("loan_rate>0.025")


# In[144]:


def cal_qxl(df, ovd_status='ovd_status_curr', is_auth=1):
    if is_auth==1:
        # 汇总聚合到用户层
        df_qx = df.groupby(['user_id','mob_date']).agg({'ovd_status_curr':'max',
                                                            'curr_overdue_days':'max',
                                                            'ever_overdue_days':'max',
                                                            'ovd_status_ever':'max',
                                                            'act_prin_amt':'sum',
                                                           'loan_amount':'sum',
                                                           'loan_bal':'sum'
                                                          })
        df_qx = df_qx.reset_index()
        values= 'user_id'
    else:
        values= 'order_no'
    df_qx_tmp = df_qx.groupby(['mob_date', ovd_status]).agg({'user_id':'count','loan_bal':'sum'})
    df_qx_tmp = df_qx_tmp.reset_index() 
    tmp_curr_cnt = pd.pivot_table(df_qx_tmp, values=values,index=['mob_date'], columns=[ovd_status],aggfunc='sum',margins=True)
    tmp_curr_cnt_rate = pd.DataFrame(columns=['M0-M1','M1-M2','M2-M3','M3-M4','M4-M5'])
    for i in range(1,len(tmp_curr_cnt.index)-2):
        tmp_curr_cnt_rate.loc[tmp_curr_cnt.index[i],'M0-M1'] = tmp_curr_cnt.loc[tmp_curr_cnt.index[i+1], 1]/tmp_curr_cnt.loc[tmp_curr_cnt.index[i], 0]
        tmp_curr_cnt_rate.loc[tmp_curr_cnt.index[i],'M1-M2'] = tmp_curr_cnt.loc[tmp_curr_cnt.index[i+1], 2]/tmp_curr_cnt.loc[tmp_curr_cnt.index[i], 1]
        tmp_curr_cnt_rate.loc[tmp_curr_cnt.index[i],'M2-M3'] = tmp_curr_cnt.loc[tmp_curr_cnt.index[i+1], 3]/tmp_curr_cnt.loc[tmp_curr_cnt.index[i], 2]
        tmp_curr_cnt_rate.loc[tmp_curr_cnt.index[i],'M3-M4'] = tmp_curr_cnt.loc[tmp_curr_cnt.index[i+1], 4]/tmp_curr_cnt.loc[tmp_curr_cnt.index[i], 3]
        tmp_curr_cnt_rate.loc[tmp_curr_cnt.index[i],'M4-M5'] = tmp_curr_cnt.loc[tmp_curr_cnt.index[i+1], 5]/tmp_curr_cnt.loc[tmp_curr_cnt.index[i], 4]
        
    tmp_curr_amt = pd.pivot_table(df_qx_tmp, values='loan_bal',index=['mob_date'], columns=[ovd_status],aggfunc='sum',margins=True)
    tmp_curr_amt_rate = pd.DataFrame(columns=['M0-M1','M1-M2','M2-M3','M3-M4','M4-M5'])
    for i in range(1,len(tmp_curr_amt.index)-2):
        tmp_curr_amt_rate.loc[tmp_curr_amt.index[i],'M0-M1'] = tmp_curr_amt.loc[tmp_curr_amt.index[i+1], 1]/tmp_curr_amt.loc[tmp_curr_amt.index[i], 0]
        tmp_curr_amt_rate.loc[tmp_curr_amt.index[i],'M1-M2'] = tmp_curr_amt.loc[tmp_curr_amt.index[i+1], 2]/tmp_curr_amt.loc[tmp_curr_amt.index[i], 1]
        tmp_curr_amt_rate.loc[tmp_curr_amt.index[i],'M2-M3'] = tmp_curr_amt.loc[tmp_curr_amt.index[i+1], 3]/tmp_curr_amt.loc[tmp_curr_amt.index[i], 2]
        tmp_curr_amt_rate.loc[tmp_curr_amt.index[i],'M3-M4'] = tmp_curr_amt.loc[tmp_curr_amt.index[i+1], 4]/tmp_curr_amt.loc[tmp_curr_amt.index[i], 3]
        tmp_curr_amt_rate.loc[tmp_curr_amt.index[i],'M4-M5'] = tmp_curr_amt.loc[tmp_curr_amt.index[i+1], 5]/tmp_curr_amt.loc[tmp_curr_amt.index[i], 4]    
    
    return tmp_curr_cnt,tmp_curr_cnt_rate,tmp_curr_amt,tmp_curr_amt_rate


# In[145]:


tmp_cnt,tmp_cnt_rate,tmp_amt,tmp_amt_rate = cal_qxl(df_qx, ovd_status='ovd_status_curr')


# In[146]:


tmp_cnt_24,tmp_cnt_rate_24,tmp_amt_24,tmp_amt_rate_24 = cal_qxl(df_qx_24, ovd_status='ovd_status_curr')
tmp_cnt_36,tmp_cnt_rate_36,tmp_amt_36,tmp_amt_rate_36 = cal_qxl(df_qx_36, ovd_status='ovd_status_curr')


# In[148]:


writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\迁徙率分析_授信_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
tmp_cnt.to_excel(writer,sheet_name='tmp_cnt')
tmp_cnt_rate.to_excel(writer,sheet_name='tmp_cnt_rate')
tmp_amt.to_excel(writer,sheet_name='tmp_amt')
tmp_amt_rate.to_excel(writer,sheet_name='tmp_amt_rate')

tmp_cnt_24.to_excel(writer,sheet_name='tmp_cnt_24')
tmp_cnt_rate_24.to_excel(writer,sheet_name='tmp_cnt_rate_24')
tmp_amt_24.to_excel(writer,sheet_name='tmp_amt_24')
tmp_amt_rate_24.to_excel(writer,sheet_name='tmp_amt_rate_24')

tmp_cnt_36.to_excel(writer,sheet_name='tmp_cnt_36')
tmp_cnt_rate_36.to_excel(writer,sheet_name='tmp_cnt_rate_36')
tmp_amt_36.to_excel(writer,sheet_name='tmp_amt_36')
tmp_amt_rate_36.to_excel(writer,sheet_name='tmp_amt_rate_36')
writer.save()




#==============================================================================
# File: 转转渠道_数据分析.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[116]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc
import os

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


path = r'D:\liuyedao\转转渠道'
os.chdir(path)


# In[3]:


os.getcwd()


# In[696]:


df_auth = pd.read_csv('dwd_beforeloan_auth_examine_fd_80005.csv')


# In[697]:


df_auth.info(show_counts=True)


# In[698]:


print(df_auth.columns)


# In[699]:


print(df_auth.apply_date.min(), df_auth.apply_date.max())


# In[700]:


df_order = pd.read_csv('dwd_beforeloan_order_examine_fd_80005.csv')
df_order.shape


# In[701]:


df_repay_plan = pd.read_csv('dwd_cap_repay_plan_fd_80005.csv')
df_repay_plan.shape


# In[10]:


import toad
df_explore1 = toad.detect(df1)
df_explore2 = toad.detect(df2)
df_explore3 = toad.detect(df3)


# In[11]:


from datetime import datetime
writer=pd.ExcelWriter(r"d:\liuyedao\转转渠道\result\zz渠道_数据探索分析_{}_v{}.xlsx".format(str(datetime.today())[:10].replace('-',''),1))
df_explore1.to_excel(writer,sheet_name='auth')
df_explore2.to_excel(writer,sheet_name='order')
df_explore3.to_excel(writer,sheet_name='repay')
writer.save()


# In[702]:


print(df_order.apply_date.min(), df_order.apply_date.max())


# In[703]:


print(df_repay_plan.create_date.min(), df_repay_plan.create_date.max())


# In[715]:


df_repay_plan_drop = df_repay_plan[['order_no','user_id','lending_time','loan_amount']].drop_duplicates(subset=['order_no'],keep='first')


# In[718]:


df_order_num = df_repay_plan_drop.groupby(['user_id'])['order_no'].count().reset_index()
df_order_num['bins'] = pd.cut(df_order_num['order_no'], bins=[0,1,2,3,4,5,6,7,8,9,10,20, 99999])
df_order_num['bins'].value_counts(dropna=False)


# In[728]:


df_auth_drop = df_auth.sort_values(['user_id','apply_date'],ascending=False).drop_duplicates(subset=['user_id'],keep='first')[['user_id','id_no_des', 'auth_status','apply_date','auth_credit_amount']]
df_auth_order = pd.merge(df_order[['user_id','order_no', 'order_status','apply_date']], df_auth_drop, how='inner', on='user_id')
df_auth_order_repay = pd.merge(df_repay_plan_drop, df_auth_order, how='inner', on='order_no')
print(df_auth_order.shape, df_auth_drop.shape, df_repay_plan_drop.shape, df_auth_order_repay.shape)


# In[729]:


df_auth_order_repay.info()


# In[730]:


df_auth_order_repay.rename(columns={'user_id_x':'user_id_repay','user_id_y':'user_id_order', 'apply_date_x':'apply_date_order','apply_date_y':'apply_date_auth'},inplace=True)


# In[731]:


df_auth_order_repay.info()


# In[732]:


df_auth_order_repay.head()


# In[733]:


df_auth_order_repay['credit_limit_use'] = df_auth_order_repay['loan_amount']/df_auth_order_repay['auth_credit_amount']
df_auth_order_repay['credit_use_days'] = (pd.to_datetime(df_auth_order_repay['apply_date_order'],format='%Y-%m-%d') - pd.to_datetime(df_auth_order_repay['apply_date_auth'],format='%Y-%m-%d')).dt.days 


# In[735]:


df_auth_order_repay_first = df_auth_order_repay.sort_values(['user_id_order','apply_date_order']).drop_duplicates(subset=['user_id_order'],keep='first')


# In[736]:


df_auth_order_repay['credit_limit_use'].describe()


# In[737]:


df_auth_order_repay['credit_use_days'].describe()


# In[749]:


df_auth_order_repay_first['credit_limit_use_bins'] = pd.cut(df_auth_order_repay_first['credit_limit_use'], bins=[-1,0,0.1,0.2,0.3,0.5,0.8,1.0])
df_auth_order_repay_first['credit_use_days_bins'] = pd.cut(df_auth_order_repay_first['credit_use_days'], bins=[-1,0, 7,15,30,60,90,120,150,180,999])


# In[750]:


df_auth_order_repay_first['credit_limit_use_bins'].value_counts(dropna=False)


# In[751]:


df_auth_order_repay_first['credit_use_days_bins'].value_counts(dropna=False)


# # 1.授信表

# In[705]:


df_auth['tacticskq_name'].value_counts(dropna=False)


# In[706]:


df_auth['apply_month'] = df_auth['apply_date'].str[0:7]


# In[708]:


df_auth['apply_date'].str[0:7].value_counts(sort=False)


# In[709]:


pd.concat([df_auth['auth_status'].value_counts(dropna=False),df_auth['auth_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[710]:


df_auth.groupby(['apply_month', 'auth_status'])['order_no'].count().unstack()


# In[27]:


pd.concat([df1['education'].value_counts(dropna=False),df1['education'].value_counts(dropna=False,normalize=True)],axis=1)


# In[719]:


df_auth_num = df_auth.groupby(['id_no_des'])['order_no'].count().reset_index()
df_auth_num['bins'] = pd.cut(df_auth_num['order_no'], bins=[0,1,2,3,4,5,10,20, 30,99999])
df_auth_num['bins'].value_counts(dropna=False)


# In[28]:


pd.merge(df1, df2, how='inner', on='user_id').shape


# In[49]:


df1[df1['auth_status']==6]['auth_credit_amount'].describe()


# # 2.提现表

# In[29]:


pd.concat([df2['cus_level'].value_counts(dropna=False),df2['cus_level'].value_counts(dropna=False,normalize=True)],axis=1)


# In[30]:


pd.concat([df2['cust_type'].value_counts(dropna=False),df2['cust_type'].value_counts(dropna=False,normalize=True)],axis=1)


# In[31]:


pd.concat([df2['tacticskq_name'].value_counts(dropna=False),df2['tacticskq_name'].value_counts(dropna=False,normalize=True)],axis=1)


# In[711]:


pd.concat([df_order['order_status'].value_counts(dropna=False),df_order['order_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[712]:


df_order['apply_date'].str[0:7].value_counts(sort=False)


# In[713]:


df_order['apply_month'] = df_order['apply_date'].str[0:7]


# In[714]:


df_order.groupby(['apply_month', 'order_status'])['order_no'].count().unstack()


# In[50]:


df2[df2['order_status']==6]['loan_amount'].describe()


# In[704]:


df_order['order_status'].value_counts(dropna=False)


# # 3.还款计划表

# In[55]:


xx = df3[['order_no', 'loan_rate','total_periods']].drop_duplicates(subset=['order_no'], keep='first')
xx.groupby(['loan_rate', 'total_periods'])['order_no'].count().unstack()

# pd.concat([df3['loan_rate'].value_counts(dropna=False),df3['loan_rate'].value_counts(dropna=False,normalize=True)],axis=1)


# In[40]:


pd.concat([df3['order_status'].value_counts(dropna=False),df3['order_status'].value_counts(dropna=False,normalize=True)],axis=1)


# In[42]:


pd.concat([df3['total_periods'].value_counts(dropna=False),df3['total_periods'].value_counts(dropna=False,normalize=True)],axis=1)


# In[43]:


xx = df3[['order_no', 'lending_time']].drop_duplicates(subset=['order_no'], keep='first')
xx['lending_time'] = df3['lending_time'].str[0:7]
xx.groupby(['lending_time_1'])['order_no'].count().unstack()


# In[46]:


df3['repay_date'].str[0:7].value_counts(sort=False)


# In[44]:


pd.merge(df2, df3, how='inner', on='order_no').shape


# In[51]:


df3['loan_rate'].describe()


# In[52]:


df3['period_settle_date'].str[0:7].value_counts(dropna=True)


# In[54]:


xx = df3[['order_no', 'lending_time','total_periods']].drop_duplicates(subset=['order_no'], keep='first')
xx['lending_time_1'] = df3['lending_time'].str[0:7]
xx.groupby(['lending_time_1', 'total_periods'])['order_no'].count().unstack()


# # 4. 标签定义

# In[417]:


cols = ['order_no','lending_time','loan_amount','loan_rate','total_periods','period', 'repay_date','period_settle_date','repid_time',
        'is_settle_period','overdue_day','actual_overdue_days','principal','already_repaid_principal','repay_amount','interest',
        'already_repaid_interest']
tmp = df3[df3['order_no']=='23021415553678917563'].sort_values('period')[cols]
tmp


# In[324]:


# 借据层定义标签
df_repay = df3[['order_no','id_no_des','user_id','create_date','period_settle_date','lending_time','repay_date','period',
                'total_periods','overdue_day','actual_overdue_days','is_settle_period',
                'loan_amount','loan_rate','repay_plan_id']]

df_repay.info(show_counts=True)


# In[325]:


df_repay['period_settle_date'].fillna('2023-08-08', inplace=True)


# In[326]:


# 格式转换
df_repay['repay_date'] = pd.to_datetime(df_repay['repay_date'], format='%Y-%m-%d')
df_repay['period_settle_date'] = pd.to_datetime(df_repay['period_settle_date'], format='%Y-%m-%d')
# 计算逾期天数
df_repay['yqts'] = df_repay['period_settle_date'] - df_repay['repay_date']
df_repay['yqts'] = df_repay['yqts'].dt.days
df_repay['yqts'] = df_repay['yqts'].apply(lambda x: x if x> 0 else 0 if x<=0 else np.nan)
# 截止2023-08-08,是否逾期15天以上或者30天以上
df_repay['ove_15_days'] = df_repay['yqts'].apply(lambda x: 1 if x>=15 else 0 if x==0 else 0.5 if x>0 and x<15 else np.nan)
df_repay['ove_30_days'] = df_repay['yqts'].apply(lambda x: 1 if x>=30 else 0 if x==0 else 0.5 if x>0 and x<30 else np.nan)
df_repay['ove_60_days'] = df_repay['yqts'].apply(lambda x: 1 if x>=60 else 0 if x==0 else 0.5 if x>0 and x<60 else np.nan)
# 截止2023-08-08，是否满足mob3/mob6
df_repay['Firs3'] = [*map(lambda t1,t2: 1 if t1<datetime(2023,8,9) and t2==3 else 0, df_repay['repay_date'],df_repay['period'])]
df_repay['Firs6'] = [*map(lambda t1,t2: 1 if t1<datetime(2023,8,9) and t2==6 else 0, df_repay['repay_date'],df_repay['period'])]
df_repay['Firs3'] = df_repay.groupby(['order_no'])['Firs3'].transform(lambda x: x.max())
df_repay['Firs6'] = df_repay.groupby(['order_no'])['Firs6'].transform(lambda x: x.max())


# In[343]:


df_repay.groupby('Firs6')['order_no'].nunique()


# In[327]:


df_repay.info(show_counts=True)


# In[328]:


def define_flag(t1, t2):
    if t1==1:
        flag = 1
    elif t1>0 and t1<1:
        flag = 0.5
    elif t1==0 and t2==1:
        flag = 0
    elif t1==0 and t2==0:
        flag = -1
    else:
        flag = np.nan
    
    return flag   


# In[329]:


# 产生首三期的Y标签
df_Firs3 = df_repay.query("period<=3")
# 添加每期还款计划的标签
df_Firs3['Firs3ever15'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs3['ove_15_days'], df_Firs3['Firs3'])]
df_Firs3['Firs3ever30'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs3['ove_30_days'], df_Firs3['Firs3'])]
df_Firs3['Firs3ever60'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs3['ove_60_days'], df_Firs3['Firs3'])]
df_Firs3 = df_Firs3.groupby('order_no').agg({'Firs3ever15':'max','Firs3ever30':'max','Firs3ever60':'max','overdue_day':'max',
                                             'actual_overdue_days':'max',
                                             'lending_time':'min','total_periods':'max','loan_rate':'max','loan_amount':'max'})
df_Firs3 = df_Firs3.reset_index()
print(df_Firs3.shape)
df_Firs3.head()


# In[330]:


tmp = df_Firs3.groupby(['total_periods','Firs3ever15'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[331]:


tmp = df_Firs3.groupby(['total_periods','Firs3ever30'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[332]:


tmp = df_Firs3.groupby(['total_periods','Firs3ever60'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[333]:


# 产生首六期的Y标签
df_Firs6 = df_repay.query("period<=6")
# 添加每期还款计划的标签
df_Firs6['Firs6ever15'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs6['ove_15_days'], df_Firs6['Firs6'])]
df_Firs6['Firs6ever30'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs6['ove_30_days'], df_Firs6['Firs6'])]
df_Firs6['Firs6ever60'] = [*map(lambda t1,t2: define_flag(t1, t2), df_Firs6['ove_60_days'], df_Firs6['Firs6'])]
df_Firs6 = df_Firs6.groupby('order_no').agg({'Firs6ever15':'max','Firs6ever30':'max','Firs6ever60':'max','overdue_day':'max','actual_overdue_days':'max',
                                             'lending_time':'min','total_periods':'max','loan_rate':'max','loan_amount':'max'})
df_Firs6 = df_Firs6.reset_index() 
df_Firs6.head()



# In[334]:


df_Firs6.info(show_counts=True)


# In[335]:


tmp = df_Firs6.groupby(['total_periods','Firs6ever15'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[336]:


tmp = df_Firs6.groupby(['total_periods','Firs6ever30'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[337]:


tmp = df_Firs6.groupby(['total_periods','Firs6ever60'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# In[339]:


df_Firs6['lending_month'] = df_Firs6['lending_time'].str[0:7]
tmp = df_Firs6.groupby(['lending_month','Firs6ever30'])['order_no'].count().unstack()
tmp['total'] = tmp[[0.0, 1.0,]].sum(axis=1)
tmp['bad_rate'] = tmp[1.0]/tmp['total'] 
tmp


# # 5.基础数据

# In[752]:


from datetime import datetime
from dateutil.relativedelta import relativedelta
import calendar

def calc_curr_overdue_days(x):
    if x['repay_date'] >= x['mob_date']:
        due_days = 0
    elif x['is_settle_period'] == 0 and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] >= x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] < x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = 0
    else:
        due_days = 0 
    return due_days

def calc_ever_overdue_days(x):
    if x['repay_date'] >= x['mob_date']:
        due_days = 0
    elif x['is_settle_period'] == 0 and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] >= x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['mob_date'] - x['repay_date']).days
    elif x['is_settle_period'] == 1 and x['period_settle_date'] < x['mob_date'] and x['repay_date'] < x['mob_date']:
        due_days = (x['period_settle_date'] - x['repay_date']).days
    else:
        due_days = 0 
    return due_days


def calculate_month_difference(start_date, end_date):
#     start_date = datetime.strptime(start_date, "%Y-%m-%d")
#     end_date = datetime.strptime(end_date, "%Y-%m-%d")

    # 取得两个月末日期
    start_month_end = (start_date + relativedelta(day=31)).replace(day=1)
    end_month_end = (end_date + relativedelta(day=31)).replace(day=1)

    # 计算月份数
    month_difference = (end_month_end.year - start_month_end.year) * 12 + (end_month_end.month - start_month_end.month)

    return month_difference


def get_month_end_dates(year, month_start, month_end):
    month_end_dates = []
    
    for month in range(month_start, month_end + 1):
        _, last_day = calendar.monthrange(year, month)
        month_end_date = datetime(year, month, last_day)
        month_end_dates.append(month_end_date)
    
    return month_end_dates



def calc_ovedue_status(x):
    if x<=0:
        status=0
    elif x>0 and x<=30:
        status=1
    elif x>30 and x<=60:
        status=2
    elif x>60 and x<=90:
        status=3
    elif x>90 and x<=120:
        status=4
    elif x>120:
        status=5
    else:
        status=np.nan
    return status


# In[753]:


cols = ['order_no','lending_time','loan_amount','loan_rate','total_periods','period', 'repay_date','period_settle_date','repid_time',
        'is_settle_period','overdue_day','actual_overdue_days','principal','already_repaid_principal','repay_amount','interest',
        'already_repaid_interest','repay_plan_id']

df_base = pd.read_csv('dwd_cap_repay_plan_fd_80005.csv')[cols]
df_base = df_base.sort_values(['order_no', 'period']).reset_index(drop=True)
print(df_base.shape)


# In[754]:


time1 = get_month_end_dates(2022, 6, 12)
time2 = get_month_end_dates(2023, 1, 8)
print(time1, time2)


# In[755]:


mob_month_end = pd.DataFrame({'mob_date':[date.strftime("%Y-%m-%d") for date in (time1+time2)]})
mob_month_end['keys'] = 1  


# In[756]:


df_base['keys']=1
df_base = df_base.merge(mob_month_end, on='keys').drop('keys',axis=1)
df_base.shape


# In[757]:


# df_base['mob_month'] = df_base['mob_date'].str[0:7]
# df_base['lending_month'] = df_base['lending_time'].str[0:7]

df_base['repay_date'] = pd.to_datetime(df_base['repay_date'],format='%Y-%m-%d')
df_base['period_settle_date'] = pd.to_datetime(df_base['period_settle_date'],format='%Y-%m-%d')
df_base['lending_time'] = pd.to_datetime(df_base['lending_time'].str[0:10],format='%Y-%m-%d')
df_base['repid_time'] = pd.to_datetime(df_base['repid_time'].str[0:10],format='%Y-%m-%d')
df_base['mob_date'] = pd.to_datetime(df_base['mob_date'],format='%Y-%m-%d')

df_base[['lending_time','repay_date','period_settle_date','mob_date']].head(2)


# In[758]:


df_base = df_base.query("mob_date>=lending_time & mob_date< '2023-08-01' ")
df_base = df_base.reset_index(drop=True)


# In[759]:


df_base.shape


# In[760]:


# 计算账龄
df_base['mob'] = [*map(lambda t1,t2: calculate_month_difference(t1, t2), df_base['lending_time'], df_base['mob_date'])]
# 计算逾期天数
df_base['ever_overdue_days'] = df_base.apply(calc_ever_overdue_days, axis=1)
df_base['curr_overdue_days'] = df_base.apply(calc_curr_overdue_days, axis=1)


# In[761]:


#计算到期应还本金
df_base['due_prin_amt'] = df_base.apply(lambda x: x['principal'] if x['repay_date'] <= x['mob_date'] else 0, axis=1)

#计算到期已还本金
df_base['act_prin_amt'] = df_base.apply(lambda x: x['already_repaid_principal'] if x['repay_date'] <= x['mob_date'] and x['repid_time'] <= x['mob_date'] else 0, axis=1)


#计算逾期状态
df_base['ovd_status_ever'] = df_base['ever_overdue_days'].apply(lambda x: calc_ovedue_status(x))
df_base['ovd_status_curr'] = df_base['curr_overdue_days'].apply(lambda x: calc_ovedue_status(x))


# # 账龄分析

# In[620]:


df1 = df_base.copy()


# In[621]:


#标记逾期31天以上标识
df1['ovd_flag_ever_30'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['ever_overdue_days'] > 30 else 0, axis=1)
#标记逾期61天以上标识
df1['ovd_flag_ever_60'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['ever_overdue_days'] > 60 else 0, axis=1)
#标记逾期91天以上标识
df1['ovd_flag_ever_90'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['ever_overdue_days'] > 90 else 0, axis=1)



df1 = df1.groupby(['order_no', 'lending_time', 'total_periods', 'loan_amount','mob','mob_date']).agg({
                                                                                                     'ovd_flag_ever_30':'max',
                                                                                                     'ovd_flag_ever_60':'max',
                                                                                                     'ovd_flag_ever_90':'max'})


# In[ ]:


#标记逾期31天以上标识
df1['ovd_flag_curr_30'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['curr_overdue_days'] > 30 else 0, axis=1)
#标记逾期61天以上标识
df1['ovd_flag_curr_60'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['curr_overdue_days'] > 60 else 0, axis=1)
#标记逾期91天以上标识
df1['ovd_flag_curr_90'] = df1.apply(lambda x: 1 if x['repay_date'] < x['mob_date'] and x['curr_overdue_days'] > 90 else 0, axis=1)



df1 = df1.groupby(['order_no', 'lending_time', 'total_periods', 'loan_amount','mob','mob_date']).agg({
                                                                                                     'ovd_flag_curr_30':'max',
                                                                                                     'ovd_flag_curr_60':'max',
                                                                                                     'ovd_flag_curr_90':'max'})


# In[622]:


df1 = df1.reset_index()
print(df1.shape)


# In[623]:


df1['lending_month'] = df1['lending_time'].apply(lambda x:str(x)[0:7])
tmp = df1.groupby(['lending_month', 'mob','total_periods']).agg({'order_no':'count', 'ovd_flag_ever_30':'sum',
                                                             'ovd_flag_ever_60':'sum','ovd_flag_ever_90':'sum'})
tmp = tmp.reset_index()


# In[624]:


tmp.head()


# In[625]:


tmp_total = pd.pivot_table(tmp, values='order_no', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_30 = pd.pivot_table(tmp, values='ovd_flag_ever_30', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_60 = pd.pivot_table(tmp, values='ovd_flag_ever_60', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_90 = pd.pivot_table(tmp, values='ovd_flag_ever_90', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)

tmp_30_pct_total = tmp_30/tmp_total
tmp_60_pct_total = tmp_60/tmp_total
tmp_90_pct_total = tmp_90/tmp_total


# In[626]:


tmp_06 = tmp.query("total_periods==6")
tmp_total = pd.pivot_table(tmp_06, values='order_no', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_30 = pd.pivot_table(tmp_06, values='ovd_flag_ever_30', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_60 = pd.pivot_table(tmp_06, values='ovd_flag_ever_60', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_90 = pd.pivot_table(tmp_06, values='ovd_flag_ever_90', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)

tmp_30_pct_06 = tmp_30/tmp_total
tmp_60_pct_06 = tmp_60/tmp_total
tmp_90_pct_06 = tmp_90/tmp_total


# In[627]:


tmp_09 = tmp.query("total_periods==9")
tmp_total = pd.pivot_table(tmp_09, values='order_no', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_30 = pd.pivot_table(tmp_09, values='ovd_flag_ever_30', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_60 = pd.pivot_table(tmp_09, values='ovd_flag_ever_60', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_90 = pd.pivot_table(tmp_09, values='ovd_flag_ever_90', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)

tmp_30_pct_09 = tmp_30/tmp_total
tmp_60_pct_09 = tmp_60/tmp_total
tmp_90_pct_09 = tmp_90/tmp_total


# In[628]:


tmp_12 = tmp.query("total_periods==12")
tmp_total = pd.pivot_table(tmp_12, values='order_no', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_30 = pd.pivot_table(tmp_12, values='ovd_flag_ever_30', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_60 = pd.pivot_table(tmp_12, values='ovd_flag_ever_60', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)
tmp_90 = pd.pivot_table(tmp_12, values='ovd_flag_ever_90', index=['lending_month'],columns=['mob'],aggfunc='sum',margins=True)

tmp_30_pct_12 = tmp_30/tmp_total
tmp_60_pct_12 = tmp_60/tmp_total
tmp_90_pct_12 = tmp_90/tmp_total


# In[629]:


writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\账龄分析_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
tmp_30_pct_total.to_excel(writer,sheet_name='tmp_30_pct_total')
tmp_60_pct_total.to_excel(writer,sheet_name='tmp_60_pct_total')
tmp_90_pct_total.to_excel(writer,sheet_name='tmp_90_pct_total')

tmp_30_pct_06.to_excel(writer,sheet_name='tmp_30_pct_06')
tmp_60_pct_06.to_excel(writer,sheet_name='tmp_60_pct_06')
tmp_90_pct_06.to_excel(writer,sheet_name='tmp_90_pct_06')

tmp_30_pct_09.to_excel(writer,sheet_name='tmp_30_pct_09')
tmp_60_pct_09.to_excel(writer,sheet_name='tmp_60_pct_09')
tmp_90_pct_09.to_excel(writer,sheet_name='tmp_90_pct_09')

tmp_30_pct_12.to_excel(writer,sheet_name='tmp_30_pct_12')
tmp_60_pct_12.to_excel(writer,sheet_name='tmp_60_pct_12')
tmp_90_pct_12.to_excel(writer,sheet_name='tmp_90_pct_12')
writer.save()


# # 6.滚动率分析

# In[762]:


df_roll = df_base.copy()
df_roll.shape


# In[763]:


df_roll = df_roll.groupby(['order_no', 'lending_time', 'total_periods', 'loan_amount','mob','mob_date']).agg({'ever_overdue_days':'max',
                                                                                                              'ovd_status_ever':'max',
                                                                                                             'curr_overdue_days':'max',
                                                                                                             'ovd_status_curr':'max'})
df_roll = df_roll.reset_index()


# In[764]:


df_roll.info()
df_roll.head()


# In[767]:


mob_date_month = get_month_end_dates(2022, 8, 12)+get_month_end_dates(2023, 1, 7)
mob_date_month


# In[772]:


roll_curr = pd.DataFrame(columns=[0,1,2,3,4,5,'All','mob_date_month'])
roll_curr_rate = pd.DataFrame(columns=[0,1,2,3,4,5,'All','mob_date_month'])
for i in range(len(mob_date_month)-1):
    print('--------------{}------------------'.format(mob_date_month[i]))
    df_roll_part1 = df_roll.query("lending_time<= @mob_date_month[@i] & mob_date<= @mob_date_month[@i]")
    df_roll_part1 = df_roll_part1.groupby(['order_no', 'lending_time', 'total_periods']).agg({'curr_overdue_days':'max','ovd_status_curr':'max'})
    df_roll_part1 = df_roll_part1.reset_index()
                                  
    df_roll_part2 = df_roll.query("lending_time<= @mob_date_month[@i] & mob_date<= @mob_date_month[@i+1] & mob_date> @mob_date_month[@i]")
    df_roll_part2 = df_roll_part2.groupby(['order_no', 'lending_time', 'total_periods']).agg({'curr_overdue_days':'max','ovd_status_curr':'max'})
    df_roll_part2 = df_roll_part2.reset_index()

    df_roll_v1 = pd.merge(df_roll_part1, df_roll_part2, how='left',on='order_no')

    tmp_curr = pd.pivot_table(df_roll_v1, values='order_no',index=['ovd_status_curr_x'], columns=['ovd_status_curr_y'],aggfunc='count',margins=True)
    tmp_curr['mob_date_month'] = mob_date_month[i]
    roll_curr = pd.concat([roll_curr, tmp_curr])
  


# In[778]:


list(tmp_curr.columns)


# In[773]:


for j in [0,1,2,3,4,5,'All']:
    roll_curr_rate[j] = roll_curr[j]/roll_curr["All"]
writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\滚动率分析_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
roll_curr.to_excel(writer,sheet_name='roll_curr')
roll_curr_rate.to_excel(writer,sheet_name='roll_curr_rate')
writer.save()  


# In[ ]:


df_qx = df_base.query("mob_date< '2023-08-01'")
df_qx = df_qx.reset_index(drop=True)
df_qx.shape
# 汇总聚合到借据层
df_qx = df_qx.groupby(['order_no', 'lending_time', 'total_periods', 'loan_amount','mob_date']).agg({'ovd_status_curr':'max',
                                                                                                    'curr_overdue_days':'max',
                                                                                                    'ever_overdue_days':'max',
                                                                                                    'ovd_status_ever':'max',
                                                                                                    'act_prin_amt':'sum'})
df_qx = df_qx.reset_index()
df_qx.head()
df_qx['loan_bal'] = df_qx['loan_amount'] - df_qx['act_prin_amt'] #剩余本金
df_qx['mob_month'] = df_qx['mob_date'].apply(lambda x:str(x)[0:7])
for i in range(len(mob_date_month)-1):
    df_qx_tmp = df_roll.query("lending_time<= mob_date_month[i] & mob_date<= mob_date_month[i+1] & mob_date>= mob_date_month[i]")
    df_qx_tmp = df_qx_tmp[]
    df_qx_tmp = df_qx_tmp.groupby([ 'ovd_status_curr']).agg({'order_no':'count','loan_bal':'sum'})
    df_qx_tmp = df_qx_tmp.reset_index()
    tmp_curr_cnt = pd.pivot_table(df_qx_tmp, values='order_no',index=['ovd_status_curr'], columns=['mob_month'],aggfunc='sum',margins=True)
    tmp_curr_amt = pd.pivot_table(df_qx_tmp, values='loan_bal',index=['ovd_status_curr'], columns=['mob_month'],aggfunc='sum',margins=True)


# In[671]:


df_roll_part1 = df_roll.query("mob_date< '2023-02-01' & lending_time< '2023-02-01'")
df_roll_part1 = df_roll_part1.reset_index()


# In[672]:


# 计算过去7个月最大逾期状态
df_roll_part1 = df_roll_part1.groupby(['order_no', 'lending_time', 'total_periods']).agg({'ever_overdue_days':'max','ovd_status_ever':'max'})
df_roll_part1 = df_roll_part1.reset_index()
df_roll_part1.shape


# In[673]:


df_roll_part1.info(show_counts=True)
df_roll_part1.head()


# In[674]:


df_roll_part2 = df_roll.query("lending_time< '2023-02-01' & mob_date< '2023-08-01' & mob_date>= '2023-02-01'")
df_roll_part2 = df_roll_part2.reset_index()
print(df_roll_part2.shape) 


# In[675]:


# 计算过去7个月最大逾期状态
df_roll_part2 = df_roll_part2.groupby(['order_no', 'lending_time', 'total_periods']).agg({'curr_overdue_days':'max','ovd_status_curr':'max'})
df_roll_part2 = df_roll_part2.reset_index()
df_roll_part2.shape


# In[676]:


df_roll_part2.info(show_counts=True)
df_roll_part2.head()


# In[677]:


df_roll_v1 = pd.merge(df_roll_part1, df_roll_part2, how='left',on='order_no')
df_roll_v1.info()


# In[678]:


df_roll_v1.head()


# In[656]:


tmp_ever = pd.pivot_table(df_roll_v1, values='order_no',index=['ovd_status_ever_x'], columns=['ovd_status_ever_y'],aggfunc='count',margins=True)


# In[666]:


tmp_curr = pd.pivot_table(df_roll_v1, values='order_no',index=['ovd_status_curr_x'], columns=['ovd_status_curr_y'],aggfunc='count',margins=True)


# In[679]:


tmp_ever_curr = pd.pivot_table(df_roll_v1, values='order_no',index=['ovd_status_ever'], columns=['ovd_status_curr'],aggfunc='count',margins=True)


# In[680]:


tmp_ever = pd.pivot_table(df_roll_v1, values='order_no',index=['ovd_status_ever_x'], columns=['ovd_status_ever_y'],aggfunc='count',margins=True)
writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\滚动率分析_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
tmp_ever.to_excel(writer,sheet_name='tmp_ever')
tmp_curr.to_excel(writer,sheet_name='tmp_curr')
tmp_ever_curr.to_excel(writer,sheet_name='tmp_ever_curr')
writer.save()


# # 迁徙率分析

# In[681]:


df_base.info()


# In[682]:


df_qx = df_base.query("mob_date< '2023-08-01'")
df_qx = df_qx.reset_index(drop=True)
df_qx.shape


# In[683]:


# 汇总聚合到借据层
df_qx = df_qx.groupby(['order_no', 'lending_time', 'total_periods', 'loan_amount','mob_date']).agg({'ovd_status_curr':'max',
                                                                                                    'curr_overdue_days':'max',
                                                                                                    'ever_overdue_days':'max',
                                                                                                    'ovd_status_ever':'max',
                                                                                                    'act_prin_amt':'sum'})
df_qx = df_qx.reset_index()
df_qx.head()


# In[684]:


df_qx['loan_bal'] = df_qx['loan_amount'] - df_qx['act_prin_amt'] #剩余本金
df_qx['mob_month'] = df_qx['mob_date'].apply(lambda x:str(x)[0:7])


# In[690]:


df_qx_tmp = df_qx.groupby(['mob_month', 'ovd_status_curr']).agg({'order_no':'count','loan_bal':'sum'})
df_qx_tmp = df_qx_tmp.reset_index()


# In[688]:


tmp_ever_cnt = pd.pivot_table(df_qx_tmp, values='order_no',index=['ovd_status_ever'], columns=['mob_month'],aggfunc='sum',margins=True)
tmp_ever_amt = pd.pivot_table(df_qx_tmp, values='loan_bal',index=['ovd_status_ever'], columns=['mob_month'],aggfunc='sum',margins=True)


# In[691]:


tmp_curr_cnt = pd.pivot_table(df_qx_tmp, values='order_no',index=['ovd_status_curr'], columns=['mob_month'],aggfunc='sum',margins=True)
tmp_curr_amt = pd.pivot_table(df_qx_tmp, values='loan_bal',index=['ovd_status_curr'], columns=['mob_month'],aggfunc='sum',margins=True)


# In[692]:


writer=pd.ExcelWriter(r"D:\liuyedao\转转渠道\result\迁徙率分析_"+str(datetime.today())[:10].replace('-','')+'.xlsx')
tmp_ever_cnt.to_excel(writer,sheet_name='tmp_ever_cnt')
tmp_ever_amt.to_excel(writer,sheet_name='tmp_ever_amt')
tmp_curr_cnt.to_excel(writer,sheet_name='tmp_curr_cnt')
tmp_ever_amt.to_excel(writer,sheet_name='tmp_curr_amt')
writer.save()


# In[602]:


xx1 = df_qx.query("mob_month=='2023-01'& ovd_status_curr==5")
xx1.shape


# In[603]:


xx2 = df_qx.query("mob_month=='2022-12'& ovd_status_curr==4")
xx2.shape


# In[604]:


xx =pd.merge(xx1, xx2, how='left', on='order_no')
xx [xx['total_periods_y'].notnull()].shape


# In[606]:


xx =pd.merge(xx1, xx2, how='left', on='order_no')
xx [xx['total_periods_y'].isnull()]


# In[552]:


xx['order_no'].nunique()


# In[ ]:







#==============================================================================
# File: 转转渠道三方数据匹配-授信层.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_row',None)
pd.set_option('display.width',1000)


# In[2]:


# 运行函数脚本
get_ipython().run_line_magic('run', 'function.ipynb')


# ## 读取三方数据并合并

# In[3]:


# 获取167文件下的所有csv文件
file_dir = r'D:\juzi\0816\80005'
file_name_167 = get_filename(file_dir)
len(file_name_167)


# In[4]:


file_name_zz = file_name_167[1:]


# In[5]:


len('dwd_beforeloan_third_combine_id_80005_')


# In[6]:


file_name_zz[1][38:-11]


# In[7]:


data_source_name = []
for i, iterm in enumerate(file_name_zz):
    if i%50==0:
        print('-----{}---{}------'.format(i, iterm))
    data_source_name.append(iterm[38:-11])
data_source_name = list(set(data_source_name))
data_source_name = sorted(data_source_name)
print(len(data_source_name))
print(data_source_name)


# In[8]:


def merge_csv_file(data_source_name, file_name_zz):
    """
    data_source_name:数据源名称
    file_name_zz:csv文件列表
    """
    data_list = []
    # 读取文件夹下的csv文件数据
    for i, iterm in enumerate(file_name_zz):
        if data_source_name == iterm[38:-11]:
            data = 'data_{}'.format(i)
            globals()[data] = pd.read_csv(r'D:\juzi\0816\80005\{}'.format(iterm))   
            data_list.append(globals()[data])
    if data_list:
        merge_df = pd.concat(data_list, axis=0)
        return merge_df
    else:
        print('----------{}:无数据-------------'.format(data_source_name))
        return None


# In[ ]:


for ds in data_source_name:
    data = 'df_{}'.format(ds)
    globals()[data] = merge_csv_file(ds, file_name_zz)


# ## 授信表

# In[9]:


# 授信表
df_auth = pd.read_csv(r'D:\liuyedao\转转渠道\dwd_beforeloan_auth_examine_fd_80005.csv')


# In[10]:


print(df_auth['order_no'].nunique(),df_auth.shape)


# ## 授信层基础表

# In[11]:


usecols = ['order_no', 'user_id','id_no_des', 'channel_id', 'auth_status','apply_date','apply_time','auth_credit_amount']
df_auth_base = df_auth[usecols]
df_auth_base.shape


# # 三方数据匹配

# ### baihang_1

# In[12]:


ds = 'baihang_1'
df_baihang_1 = merge_csv_file('baihang_1', file_name_zz)
df_baihang_1.dropna(how='all', axis=1, inplace=True)
df_baihang_1.shape


# In[13]:


df_baihang_1['return_massage'].value_counts(dropna=False)


# In[14]:


df_baihang_1 = df_baihang_1[df_baihang_1['return_massage']=='请求成功']
print(df_baihang_1['order_no'].nunique(), df_baihang_1.shape)


# In[16]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_baihang_1, cols_right, needcols=needcols, suffix='baihang_1')
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### baihang_4

# In[17]:


ds = 'baihang_4'
df_baihang_4 = merge_csv_file(ds, file_name_zz)
df_baihang_4.dropna(how='all', axis=1, inplace=True)
df_baihang_4.shape


# In[18]:


df_baihang_4['return_massage'].value_counts(dropna=False)


# In[19]:


df_baihang_4 = df_baihang_4[df_baihang_4['return_massage']=='查询成功']
df_baihang_4.shape


# In[20]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_baihang_4, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[21]:


df_three.head()


# ### rong360_4

# In[22]:


ds = 'rong360_4'
df_rong360 = merge_csv_file(ds, file_name_zz)
df_rong360.dropna(how='all', axis=1, inplace=True)
df_rong360.shape


# In[23]:


df_rong360['return_massage'].value_counts()


# In[24]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_rong360, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[25]:


df_three.head()


# ### rong360_5

# In[26]:


ds = 'rong360_5'
df_rong360_5 = merge_csv_file(ds, file_name_zz)
df_rong360_5.dropna(how='all', axis=1, inplace=True)
df_rong360_5.shape


# In[27]:


df_rong360_5['value_002'].value_counts(dropna=False)


# In[28]:


df_rong360_5 = df_rong360_5[df_rong360_5['value_002']==0.0]
df_rong360_5.shape


# In[29]:


needcols = ['value_006']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_rong360_5, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### tengxun_1

# In[30]:


ds = 'tengxun_1'
df_tengxun_1 = merge_csv_file(ds, file_name_zz)
df_tengxun_1.dropna(how='all', axis=1, inplace=True)
df_tengxun_1.shape


# In[31]:


print(df_tengxun_1['order_no'].nunique(), df_tengxun_1.shape)


# In[32]:


df_tengxun_1.groupby(['return_massage','value_005'])['order_no'].count().unstack()


# In[33]:


df_tengxun_1['value_005'].value_counts(dropna=False)


# In[34]:


df_tengxun_1['return_massage'].value_counts(dropna=False)


# In[35]:


df_tengxun_1 = df_tengxun_1.query("value_005==0.0")
df_tengxun_1.shape


# In[37]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_tengxun_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### xinyongsuanli_1

# In[38]:


ds = 'xinyongsuanli_1'
df_xinyongsuanli_1 = merge_csv_file(ds, file_name_zz)
df_xinyongsuanli_1.dropna(how='all', axis=1, inplace=True)
df_xinyongsuanli_1.shape


# In[39]:


df_xinyongsuanli_1.groupby(['return_massage','value_003'])['order_no'].count().unstack()


# In[40]:


df_xinyongsuanli_1 = df_xinyongsuanli_1[df_xinyongsuanli_1['return_massage'].isin(['请求成功','处理成功'])]


# In[41]:


df_xinyongsuanli_1['value_002'].value_counts(dropna=False)


# In[42]:


df_xinyongsuanli_1.reset_index(drop=True,inplace=True)

tmp = pd.get_dummies(df_xinyongsuanli_1['value_002'])
df_xinyongsuanli_1 = pd.concat([df_xinyongsuanli_1, tmp.mul(df_xinyongsuanli_1['model_score_01'], axis=0)], axis=1)


# In[43]:


df_xinyongsuanli_1.info()


# In[44]:


df_xinyongsuanli_1.rename(columns={1.0:'model_score_01_1', 2.0:'model_score_01_2', 3.0:'model_score_01_3', 4.0:'model_score_01_4'},inplace=True)


# In[47]:


df_xinyongsuanli_1 = df_xinyongsuanli_1.groupby(['order_no','id_no_des'])['create_time','model_score_01_1','model_score_01_2','model_score_01_3','model_score_01_4'].max()
df_xinyongsuanli_1 = df_xinyongsuanli_1.reset_index()


# In[48]:


df_xinyongsuanli_1.info()


# In[52]:


needcols = ['model_score_01_1','model_score_01_2','model_score_01_3','model_score_01_4']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_xinyongsuanli_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[53]:


df_three.head()


# ### sub_bairong_5

# In[269]:


ds = 'sub_bairong_5'
df_sub_bairong_5 = merge_csv_file(ds, file_name_zz)
df_sub_bairong_5.dropna(how='all', axis=1, inplace=True)
df_sub_bairong_5.shape


# In[270]:


df_sub_bairong_5.info(show_counts=True)


# In[271]:


df_sub_bairong_5['return_massage'].value_counts(dropna=False)


# In[272]:


df_sub_bairong_5 = df_sub_bairong_5[df_sub_bairong_5['return_massage']=='请求成功']
df_sub_bairong_5.shape


# In[273]:


df_sub_bairong_5['create_time'].min()


# In[274]:


df_sub_bairong_5.select_dtypes(include='object').columns


# In[275]:


print(df_sub_bairong_5.select_dtypes(include='number').columns)


# In[276]:


tmp = df_sub_bairong_5.select_dtypes(include='number').columns


# In[277]:


needcols = list(tmp[tmp.str.contains('value_')])
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_sub_bairong_5, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[278]:


df_auth_base.info()


# In[279]:


df_auth_base.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_total_three_part_data.csv',index=False)


# ### bairong_1

# In[257]:


ds = 'bairong_1'
df_bairong_1 = merge_csv_file(ds, file_name_zz)
df_bairong_1.dropna(how='all', axis=1, inplace=True)
df_bairong_1.shape


# In[258]:


df_bairong_1.info(show_counts=True)


# In[259]:


df_auth_base.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_no_bairong_1_or_sub.csv',index=False)


# In[260]:


df_bairong_1['created_at'].min()


# In[262]:


df_bairong_1.select_dtypes(include='object').columns


# In[263]:


df_bairong_1['apply_loan_result_json_flag_applyloanstr'].head()


# In[264]:


df_bairong_1.rename(columns={'created_at':'create_time','id_card_encrypt':'id_no_des'},inplace=True)


# In[265]:


len(list(df_bairong_1.columns[df_bairong_1.columns.str.contains("als")]))


# In[266]:


needcols = list(df_bairong_1.columns[df_bairong_1.columns.str.contains("als|json")])
print(len(needcols))
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[267]:


df_auth_base.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_no_bairong_sub.csv',index=False)


# In[268]:


df_auth_base.info()


# ### bairong_6

# In[220]:


ds = 'bairong_6'
df_bairong_6 = merge_csv_file(ds, file_name_zz)
df_bairong_6.dropna(how='all', axis=1, inplace=True)
df_bairong_6.shape


# In[221]:


df_bairong_6['return_massage'].value_counts(dropna=False)


# In[222]:


df_bairong_6 = df_bairong_6[df_bairong_6['return_massage']=='请求成功']
df_bairong_6.shape


# In[224]:


df_bairong_6.head()


# In[225]:


df_bairong_6['return_code'].value_counts(dropna=False)


# In[226]:


df_bairong_6['model_score_01'] = df_bairong_6['return_code']


# In[227]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_6, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[228]:


df_three.head()


# ### bairong_8

# In[229]:


ds = 'bairong_8'
df_bairong_8 = merge_csv_file(ds, file_name_zz)
df_bairong_8.dropna(how='all', axis=1, inplace=True)
df_bairong_8.shape


# In[230]:


df_bairong_8['return_massage'].value_counts(dropna=False)


# In[231]:


df_bairong_8['value_001'].value_counts(dropna=False)


# In[232]:


df_bairong_8['value_002'].value_counts(dropna=False)


# In[233]:


df_bairong_8.reset_index(drop=True,inplace=True)

tmp = pd.get_dummies(df_bairong_8['value_002'])
df_bairong_8 = pd.concat([df_bairong_8, tmp.mul(df_bairong_8['model_score_01'], axis=0)], axis=1)
df_bairong_8.rename(columns={'ScoreCust2':'model_score_01_2', 'ScoreCust3':'model_score_01_3'},inplace=True)
df_bairong_8 = df_bairong_8.groupby(['order_no','id_no_des'])['create_time','model_score_01_2' ,'model_score_01_3'].max()
df_bairong_8 = df_bairong_8.reset_index()


# In[234]:


needcols = ['model_score_01_2', 'model_score_01_3']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_8, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[236]:


df_three.head()


# ### bairong_13

# In[237]:


ds = 'bairong_13'
df_bairong_13 = merge_csv_file(ds, file_name_zz)
df_bairong_13.dropna(how='all', axis=1, inplace=True)
df_bairong_13.shape


# In[238]:


df_bairong_13['return_massage'].value_counts(dropna=False)


# In[239]:


df_bairong_13['value_012'].value_counts(dropna=False)


# In[240]:


df_bairong_13['value_011'].value_counts(dropna=False)


# In[241]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_13, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### bairong_14

# In[242]:


ds = 'bairong_14'
df_bairong_14 = merge_csv_file(ds, file_name_zz)
df_bairong_14.dropna(how='all',axis=1, inplace=True)
df_bairong_14.shape


# In[243]:


df_bairong_14['return_massage'].value_counts(dropna=False)


# In[244]:


df_bairong_14['value_012'].value_counts(dropna=False)


# In[245]:


df_bairong_14['value_011'].value_counts(dropna=False)


# In[247]:


needcols = list(df_bairong_14.select_dtypes(include='number').columns)
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_14, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### bairong_15

# In[248]:


ds = 'bairong_15'
df_bairong_15 = merge_csv_file(ds, file_name_zz)
df_bairong_15.dropna(how='all', axis=1, inplace=True)
df_bairong_15.shape


# In[249]:


df_bairong_15['return_massage'].value_counts(dropna=False)


# In[250]:


df_bairong_15['value_012'].value_counts(dropna=False)


# In[251]:


df_bairong_15['value_011'].value_counts(dropna=False)


# In[253]:


print(list(df_bairong_15.select_dtypes(include='number').columns))


# In[254]:


needcols = list(df_bairong_15.select_dtypes(include='number').columns)
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bairong_15, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### tianchuang_1

# In[54]:


ds = 'tianchuang_1'
df_tianchuang_1 = merge_csv_file(ds, file_name_zz)
df_tianchuang_1.dropna(how='all', axis=1, inplace=True)
df_tianchuang_1.shape


# In[55]:


df_tianchuang_1['return_massage'].value_counts(dropna=False)


# In[56]:


df_tianchuang_1['value_004'].value_counts(dropna=False)


# In[57]:


df_tianchuang_1['create_time'].min()


# In[58]:


df_tianchuang_1 = df_tianchuang_1[df_tianchuang_1['return_massage']=='请求成功']
print(df_tianchuang_1['order_no'].nunique(), df_tianchuang_1.shape)


# In[59]:


df_tianchuang_1['value_001'].value_counts(dropna=False)


# In[60]:


tianchuang_part_q = df_tianchuang_1.query("value_001=='LBMQ150101'")
print(tianchuang_part_q['order_no'].nunique(), tianchuang_part_q.shape)

tianchuang_part_r = df_tianchuang_1.query("value_001=='LBMR150101'")
print(tianchuang_part_r['order_no'].nunique(), tianchuang_part_r.shape)


# In[61]:


df_tianchuang_1 = pd.merge(tianchuang_part_q, tianchuang_part_r, how='outer',on=['order_no','id_no_des', 'channel_id','create_time'])


# In[62]:


df_tianchuang_1.rename(columns={'model_score_01_x':'model_score_01_q','model_score_01_y':'model_score_01_r'},inplace=True)


# In[63]:


needcols = ['model_score_01_q', 'model_score_01_r']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_tianchuang_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### tianchuang_4

# In[64]:


ds = 'tianchuang_4'
df_tianchuang_4 = merge_csv_file(ds, file_name_zz)
df_tianchuang_4.dropna(how='all', axis=1, inplace=True)
df_tianchuang_4.shape


# In[65]:


df_tianchuang_4['value_011'].value_counts(dropna=False)


# In[66]:


df_tianchuang_4['value_012'].value_counts(dropna=False)


# In[67]:


df_tianchuang_4 = df_tianchuang_4[df_tianchuang_4['value_011']=='响应成功']
df_tianchuang_4.shape


# In[69]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_tianchuang_4, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### fulin_1

# In[88]:


ds = 'fulin_1'
df_fulin_1 = merge_csv_file(ds, file_name_zz)
df_fulin_1.dropna(how='all', axis=1, inplace=True)
df_fulin_1.shape


# In[89]:


df_fulin_1['return_massage'].value_counts(dropna=False).head()


# In[90]:


df_fulin_1['return_code'].value_counts(dropna=False)


# In[91]:


df_fulin_1['create_time'].min()


# In[92]:


df_fulin_1 = df_fulin_1[df_fulin_1['return_massage'].isin(['success','查询成功'])]


# In[94]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_fulin_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[95]:


df_three.head()


# ### pudao_3

# In[167]:


ds = 'pudao_3'
df_pudao_3 = merge_csv_file(ds, file_name_zz)
df_pudao_3.dropna(how='all', axis=1, inplace=True)
df_pudao_3.shape


# In[169]:


df_pudao_3['value_011'].value_counts(dropna=False)


# In[170]:


df_pudao_3['create_time'].min()


# In[171]:


df_pudao_3.info()


# In[172]:


df_pudao_3 = df_pudao_3[df_pudao_3['value_011']==0.0]
df_pudao_3 = df_pudao_3.reset_index(drop=True)


# In[173]:


import json
import ast
df_pudao_3['value_012'] = df_pudao_3['value_012'].apply(lambda x: json.loads(x) if pd.notnull(x) else {})
# [json.loads(x) if pd.notnull(x) else {} for x in df_pudao_3['value_012'].to_list()]
# df_pudao_3['value_012'] = df_pudao_3['value_012'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})


# In[177]:


df_pudao_3.info()


# In[178]:


df_pudao_3 = pd.concat([df_pudao_3.drop(['value_012'], axis=1), df_pudao_3['value_012'].apply(pd.Series)], axis=1)
df_pudao_3.shape


# In[179]:


df_pudao_3.head(1)


# In[180]:


len(list(df_pudao_3.columns[df_pudao_3.columns.str.contains("ppdi|bank")]))


# In[181]:


needcols = list(df_pudao_3.columns[df_pudao_3.columns.str.contains("ppdi|bank")])
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_pudao_3, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### pudao_4

# In[183]:


ds = 'pudao_4'
df_pudao_4 = merge_csv_file(ds, file_name_zz)
df_pudao_4.dropna(how='all', axis=1, inplace=True)
df_pudao_4.shape


# In[184]:


df_pudao_4['value_011'].value_counts(dropna=False)


# In[187]:


df_pudao_4['create_time'].min()


# In[188]:


df_pudao_4 = df_pudao_4[df_pudao_4['value_011']==0.0]


# In[191]:


print(list(df_pudao_4.columns[df_pudao_4.columns.str.contains("value")])[8:])


# In[192]:


needcols = list(df_pudao_4.columns[df_pudao_4.columns.str.contains("value")])[8:]
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_pudao_4, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### pudao_15

# In[193]:


ds = 'pudao_15'
df_pudao_15 = merge_csv_file(ds, file_name_zz)
df_pudao_15.dropna(how='all', axis=1, inplace=True)
df_pudao_15.shape


# In[194]:


df_pudao_15['return_massage'].value_counts(dropna=False)


# In[195]:


df_pudao_15['value_012'].value_counts(dropna=False)


# In[196]:


df_pudao_15['value_011'].value_counts(dropna=False)


# In[197]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_pudao_15, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[198]:


df_three.head()


# ### pudao_16

# In[199]:


ds = 'pudao_16'
df_pudao_16 = merge_csv_file(ds, file_name_zz)
df_pudao_16.dropna(how='all', axis=1, inplace=True)
df_pudao_16.shape


# In[200]:


df_pudao_16['return_massage'].value_counts(dropna=False)


# In[201]:


df_pudao_16['value_012'].value_counts(dropna=False)


# In[202]:


df_pudao_16['value_011'].value_counts(dropna=False)


# In[203]:


df_pudao_16 = df_pudao_16[df_pudao_16['return_massage']=='调用成功']
df_pudao_16.shape


# In[204]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_pudao_16, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[205]:


df_auth_base.info()


# In[206]:


df_auth_base.to_csv(r'D:\liuyedao\转转渠道\mid_result\auth_no_bairong.csv',index=False)


# ### ruizhi_4

# In[97]:


ds = 'ruizhi_4'
df_ruizhi_4 = merge_csv_file(ds, file_name_zz)
df_ruizhi_4.dropna(how='all', axis=1, inplace=True)
df_ruizhi_4.shape


# In[98]:


df_ruizhi_4['return_massage'].value_counts(dropna=False)


# In[99]:


df_ruizhi_4['value_011'].value_counts(dropna=False)


# In[100]:


df_ruizhi_4['value_012'].value_counts(dropna=False)


# In[101]:


df_ruizhi_4 = df_ruizhi_4[df_ruizhi_4['return_massage']=='通过']
df_ruizhi_4.shape


# In[102]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_ruizhi_4, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[103]:


df_three.head()


# ### ruizhi_6

# In[104]:


ds = 'ruizhi_6'
df_ruizhi_6 = merge_csv_file(ds, file_name_zz)
df_ruizhi_6.dropna(how='all', axis=1, inplace=True)
df_ruizhi_6.shape


# In[105]:


df_ruizhi_6['return_massage'].value_counts(dropna=False)


# In[106]:


df_ruizhi_6['create_time'].min()


# In[107]:


df_ruizhi_6 = df_ruizhi_6[df_ruizhi_6['return_massage']=='调用成功']
df_ruizhi_6.shape


# In[108]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_ruizhi_6, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[109]:


df_three.head()


# ### bileizhen_1

# In[110]:


ds = 'bileizhen_1'
df_bileizhen_1 = merge_csv_file(ds, file_name_zz)
df_bileizhen_1.dropna(how='all', axis=1, inplace=True)
df_bileizhen_1.shape


# In[111]:


df_bileizhen_1['return_massage'].value_counts(dropna=False)


# In[112]:


df_bileizhen_1['value_002'].value_counts(dropna=False)


# In[113]:


df_bileizhen_1['value_001'].value_counts(dropna=False)


# In[114]:


df_bileizhen_1 = df_bileizhen_1.query("value_002==0.0")
df_bileizhen_1.shape


# In[115]:


df_bileizhen_1['create_time'].min()


# In[116]:


tmp = pd.get_dummies(df_bileizhen_1['value_001'])
df_bileizhen_1 = pd.concat([df_bileizhen_1, tmp.mul(df_bileizhen_1['model_score_01'], axis=0)], axis=1)
df_bileizhen_1.rename(columns={1.0:'model_score_01_1', 2.0:'model_score_01_2', 3.0:'model_score_01_3'},inplace=True)
df_bileizhen_1 = df_bileizhen_1.groupby(['order_no','id_no_des'])['create_time','model_score_01_1','model_score_01_2','model_score_01_3'].max()
df_bileizhen_1 = df_bileizhen_1.reset_index()


# In[118]:


needcols = ['model_score_01_1', 'model_score_01_2', 'model_score_01_3']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_bileizhen_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[119]:


df_three.head()


# ### duxiaoman_1

# In[121]:


ds = 'duxiaoman_1'
df_duxiaoman_1 = merge_csv_file(ds, file_name_zz)
df_duxiaoman_1.dropna(how='all', axis=1, inplace=True)
df_duxiaoman_1.shape


# In[122]:


df_duxiaoman_1['create_time'].min()


# In[123]:


df_duxiaoman_1['return_massage'].value_counts(dropna=False)


# In[124]:


df_duxiaoman_1['value_003'].value_counts(dropna=False)


# In[125]:


df_duxiaoman_1 = df_duxiaoman_1[df_duxiaoman_1['value_003']==0.0]


# In[126]:


df_duxiaoman_1['value_001'].value_counts(dropna=False)


# In[127]:


df_duxiaoman_1['value_002'].value_counts(dropna=False)


# In[128]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_duxiaoman_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[129]:


df_three.head()


# ### duxiaoman_6

# In[130]:


ds = 'duxiaoman_6'
df_duxiaoman_6 = merge_csv_file(ds, file_name_zz)
df_duxiaoman_6.dropna(how='all', axis=1, inplace=True)
df_duxiaoman_6.shape


# In[132]:


df_duxiaoman_6['return_massage'].value_counts(dropna=False)


# In[133]:


df_duxiaoman_6['value_017'].value_counts(dropna=False)


# In[134]:


df_duxiaoman_6['create_time'].min()


# In[135]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_duxiaoman_6, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[136]:


df_three.head()


# # hangliezhi_1

# In[137]:


ds = 'hangliezhi_1'
df_hangliezhi_1 = merge_csv_file(ds, file_name_zz)
df_hangliezhi_1.dropna(how='all', axis=1, inplace=True)
df_hangliezhi_1.shape


# In[138]:


df_hangliezhi_1['return_massage'].value_counts(dropna=False)


# In[139]:


df_hangliezhi_1 = df_hangliezhi_1[df_hangliezhi_1['return_massage']=='查询成功']
df_hangliezhi_1.shape


# In[140]:


df_hangliezhi_1['create_time'].min()


# In[141]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_hangliezhi_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[142]:


df_three.head()


# ### hengpu_4

# In[143]:


ds = 'hengpu_4'
df_hengpu_4 = merge_csv_file(ds, file_name_zz)
df_hengpu_4.dropna(how='all', axis=1, inplace=True)
df_hengpu_4.shape


# In[144]:


df_hengpu_4['value_012'].value_counts(dropna=False)


# In[145]:


df_hengpu_4['return_massage'].value_counts(dropna=False)


# In[146]:


df_hengpu_4['value_011'].value_counts(dropna=False)


# In[147]:


df_hengpu_4 = df_hengpu_4[df_hengpu_4['return_massage']=='请求成功']
df_hengpu_4.shape


# In[149]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols
# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_hengpu_4, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ### my_1

# In[150]:


ds = 'my_1'
df_my_1 = merge_csv_file(ds, file_name_zz)
df_my_1.dropna(how='all', axis=1, inplace=True)
df_my_1.shape


# In[152]:


df_my_1['return_massage'].value_counts(dropna=False)


# In[153]:


df_my_1['value_011'].value_counts(dropna=False)


# In[154]:


df_my_1['value_012'].value_counts(dropna=False)


# In[156]:


df_my_1 = df_my_1.query("value_012==0.0")


# In[157]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_my_1, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# In[158]:


df_three.head()


# ### aliyun_2

# In[159]:


ds = 'aliyun_2'
df_aliyun_2 = merge_csv_file(ds, file_name_zz)
df_aliyun_2.dropna(how='all', axis=1, inplace=True)
df_aliyun_2.shape


# In[161]:


df_aliyun_2['value_003'].value_counts(dropna=False)


# In[162]:


df_aliyun_2['return_massage'].value_counts(dropna=False)


# In[163]:


df_aliyun_2['value_001'].value_counts(dropna=False)


# In[164]:


df_aliyun_2 = df_aliyun_2[df_aliyun_2['return_massage'].isin(['调用成功,有效数据', '请求成功'])]
df_aliyun_2.shape


# In[165]:


needcols = ['model_score_01']
cols_left = ['order_no','id_no_des','apply_date']
cols_right = ['order_no','id_no_des','create_time'] + needcols

# 返回匹配的三方数据
df_three = process_data(df_auth_base, cols_left, df_aliyun_2, cols_right, needcols=needcols, suffix=ds)
print(df_three.shape)
# 匹配关联
df_auth_base = pd.merge(df_auth_base, df_three, how='left',on='order_no')
print(df_auth_base.shape)


# ## 三方数据匹配率

# In[209]:


# 需要计算匹配率的字段
df_auth_base.columns[0:8]


# In[210]:


# 按渠道统计各字段的匹配情况
df_auth_base['channel_id'].value_counts()


# In[211]:


import toad 

to_drop = list(df_auth_base.columns[0:8])
to_keep = list(df_auth_base.columns[8:])
tmp_total = toad.detect(df_auth_base.drop(to_drop, axis=1))
tmp_6 = toad.detect(df_auth_base.query("auth_status==6").drop(to_drop, axis=1))
tmp_7 = toad.detect(df_auth_base.query("auth_status==7").drop(to_drop, axis=1))


# In[213]:


df_auth_base.insert(7, 'apply_month', df_auth_base['apply_date'].str[0:7])


# In[214]:


to_drop = list(df_auth_base.columns[0:9])
print(to_drop)
to_keep = list(df_auth_base.columns[9:])


# In[217]:


# 按申请年月统计匹配情况
year_month = df_auth_base.groupby(by=['apply_month'])[to_keep].apply(lambda x: x.isna().sum()/x.shape[0])
year_month = year_month.T
year_month['avg_missing_rate'] = year_month.mean(axis=1)
year_month['avg_std_rate'] = year_month.std(axis=1)


# In[218]:


year_month_6 = df_auth_base.query("auth_status==6").groupby(by=['apply_month'])[to_keep].apply(lambda x: x.isna().sum()/x.shape[0])
year_month_6 = year_month_6.T
year_month_6['avg_missing_rate'] = year_month_6.mean(axis=1)
year_month_6['avg_std_rate'] = year_month_6.std(axis=1)

year_month_7 = df_auth_base.query("auth_status==7").groupby(by=['apply_month'])[to_keep].apply(lambda x: x.isna().sum()/x.shape[0])
year_month_7 = year_month_7.T
year_month_7['avg_missing_rate'] = year_month_7.mean(axis=1)
year_month_7['avg_std_rate'] = year_month_7.std(axis=1)


# In[219]:


# 保存统计的数据
writer=pd.ExcelWriter(r'D:\liuyedao\转转渠道\result\转转渠道_auth_三方数据匹配_'+str(datetime.today())[:10].replace('-','')+'.xlsx')
tmp_total.to_excel(writer,sheet_name='总体')
tmp_6.to_excel(writer,sheet_name='通过')
tmp_7.to_excel(writer,sheet_name='拒绝')

year_month.to_excel(writer,sheet_name='总体_年月')
year_month_6.to_excel(writer,sheet_name='通过_年月')
year_month_7.to_excel(writer,sheet_name='拒绝_年月')

writer.save()




#==============================================================================
# File: 逻辑回归_mob3_ever30.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
模型建立
@author: kantt
"""
#%%
import pandas as pd
import numpy as np
import toad
import os 
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
#%% 数据导入与处理
data_tmp1 = pd.read_csv(r'C:\Users\ruizhi\Desktop\kantt\1.base_data\dt_all_loan_1st.csv') 

#列名筛选
col_list=data_tmp1.columns
drop_col=['Unnamed: 0.1','loan_amount_y','Unnamed: 0','order_no_y','channel_id_y','apply_date_y','apply_time','id_no_des_y']

data_tmp1.drop(columns=drop_col,inplace=True)
data_tmp1=data_tmp1.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3","model_score_01_xysl_1.0":"model_score_01_xysl_1"})
#剔除灰样本确认模型样本
data = data_tmp1[(data_tmp1['Firs3ever30'].isin([0.,2.]))&(~data_tmp1.apply_month.isin(['2023-03','2023-04']))]
# 	channel	0.0	2.0
# 0	167	81941	3161
# 1	174	43487	1954

cal1=data.groupby(['apply_month','Firs3ever30'])['user_id'].count().unstack().reset_index()
cal2=data.groupby(['channel_id_x','Firs3ever30'])['user_id'].count().unstack().reset_index()
print('数据大小：', data.shape) #(157317, 145)

data['target']=data['Firs3ever30']/2
#%%
#STEP1. - 数据集切分  训练集测试集
model = data[data.apply_month.isin(['2022-05','2022-06','2022-07','2022-08','2022-09','2022-10','2022-11','2022-12','2023-01','2023-02'])]
#oot = data[data.apply_month.isin(['2022-12','2023-01','2023-02'])]

allFeatures = list(model.columns.drop(['id_no_des_x','order_no_x','apply_date_x','order_status','loan_period','channel_id_x','loan_amount_x','lending_time','loan_rate',
'total_periods','Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30','apply_month','create_time',
'channel_name','cert_type','auth_status','auth_credit_amount','cust_type','close_reason','time_diff']))

X1 = model[model.channel_id_x==167][allFeatures]
Y1 = model[model.channel_id_x==167]['target']
X2 = model[model.channel_id_x==174][allFeatures]
Y2 = model[model.channel_id_x==174]['target']

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1 ,Y1, test_size=0.3, random_state=88, stratify=Y1)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2 ,Y2, test_size=0.3, random_state=88, stratify=Y2)

X_train=pd.concat([X_train1,X_train2],axis=0)
X_test=pd.concat([X_test1,X_test2],axis=0)
y_train=pd.concat([y_train1,y_train2],axis=0)
y_test=pd.concat([y_test1,y_test2],axis=0)

model.loc[X_train.index,'sample_set']='train'
model.loc[X_test.index,'sample_set']='test'

dt_train=model[model.sample_set=='train'] #59094
dt_test=model[model.sample_set=='test'] #25327

cal3=dt_train[dt_train.channel_id_x==174].groupby(['apply_month','Firs3ever30'])['user_id'].count().unstack().reset_index()
cal4=dt_test[dt_test.channel_id_x==174].groupby(['apply_month','Firs3ever30'])['user_id'].count().unstack().reset_index()
#%%
#STEP2. - 数据概况 --缺失率&分位数
var_cal_train=toad.detector.detect(dt_train[allFeatures])
var_cal_test=toad.detector.detect(dt_test[allFeatures])
#var_cal_oot=toad.detector.detect(oot[allFeatures])
#数据概况 --iv
var_iv_train=toad.quality(dt_train[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_test=toad.quality(dt_test[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
# var_iv_oot=toad.quality(oot[allFeatures],
#                           target='target',
#                           method='chi',
#                           n_bins=5,
#                           iv_only=True)
var_iv_train.sort_values('iv',ascending=False)
var_iv_test.sort_values('iv',ascending=False)
# var_iv_oot.sort_values('iv',ascending=False)

train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.7, return_drop=True)
print(train_selected.shape)#(66721, 29)
col_target=train_selected.columns
col_target_key=['crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1','model_score_01_xysl_3','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin','model_score_01_bh_fraud','model_score_01_fico']
col_target_key_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_tengxun_b','model_score_01_x_tianchuang_b','model_score_01_xysl_1_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','model_score_01br_score_b','utl_b','model_score_01_rong360_b','model_score_01_fulin_b','model_score_01_bh_fraud_b','model_score_01_fico_b']
#%%

#STEP3. - 数据切分
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='chi', min_samples=None, n_bins=10, empty_separate=True) 
bins_result = c.export()
train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins['WEIGHT']=1

col_target_keep=['target','crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1','model_score_01_xysl_3','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin','model_score_01_bh_fraud','model_score_01_fico']


train_selected_adj=train_selected[col_target_keep].fillna(-9999)
train_selected_adj=train_selected_adj.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
train_selected_adj['WEIGHT']=1
#%%
eps=0.0000001
def iv(data,columns):
    w=pd.DataFrame()
    for i in columns:
        a=pd.DataFrame()
        a['人数']=data['WEIGHT'].groupby(data[i]).sum()
        a['人数占比']=data['WEIGHT'].groupby(data[i]).sum()/data['WEIGHT'].sum()
        a['字段']=i
        a['分段']=data['WEIGHT'].groupby(data[i]).sum().index
        a['B数量']=data['target'].groupby(data[i]).sum()
        a['G数量']=a['人数']-a['B数量']
        a['bad_rate']=a['B数量']/a['人数']
        a['G占比']=a['G数量']/a['G数量'].sum()+eps
        a['B占比']=a['B数量']/a['B数量'].sum()+eps
        a['iv_tmp']=np.log(a['B占比']/a['G占比'])*(a['B占比']-a['G占比'])
        a['iv']=a['iv_tmp'].sum()
        a['woe']=np.log(a['B占比']/a['G占比'])
        w=w.append(a,ignore_index=True)
    return w
#%%
#手动分箱
# train_selected_adj['crd_loan_gap_b']=pd.qcut(train_selected_adj.crd_loan_gap,10,duplicates='drop')
# train_selected_adj['model_score_01_b']=pd.qcut(train_selected_adj.model_score_01,10,duplicates='drop')
# train_selected_adj['model_score_01_tengxun_b']=pd.qcut(train_selected_adj.model_score_01_tengxun,10,duplicates='drop')
# train_selected_adj['model_score_01_x_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_x_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_1_b']=pd.qcut(train_selected_adj['model_score_01_xysl_1'],10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_3_b']=pd.qcut(train_selected_adj['model_score_01_xysl_3'],10,duplicates='drop')
# train_selected_adj['model_score_01_y_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_y_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_zr_tdzx_b']=pd.qcut(train_selected_adj.model_score_01_zr_tdzx,10,duplicates='drop')
# train_selected_adj['model_score_01br_score_b']=pd.qcut(train_selected_adj.model_score_01br_score,10,duplicates='drop')
# train_selected_adj['model_score_01_rong360_b']=pd.qcut(train_selected_adj.model_score_01_rong360,10,duplicates='drop')
# train_selected_adj['model_score_01_fulin_b']=pd.qcut(train_selected_adj.model_score_01_fulin,10,duplicates='drop')
# train_selected_adj['utl_b']=pd.qcut(train_selected_adj.utl,10,duplicates='drop')

# train_selected_adj['model_score_01_bh_fraud_b']=pd.qcut(train_selected_adj.model_score_01_bh_fraud,10,duplicates='drop')
# train_selected_adj['model_score_01_fico_b']=pd.qcut(train_selected_adj.model_score_01_fico,10,duplicates='drop')

# bin_train=iv(train_selected_adj,['model_score_01_tengxun_b'])
# bin_train=pd.DataFrame()
# bin_train=iv(train_selected_adj,col_target_key_adj)

#%%
train_selected_adj['crd_loan_gap_b']=pd.cut(train_selected_adj.crd_loan_gap,[-10000,0,7,30,np.inf])
train_selected_adj['model_score_01_b']=pd.cut(train_selected_adj.model_score_01,[-10000,0,720,740,770,850])
train_selected_adj['model_score_01_x_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_x_tianchuang,[-10000,0,600,700,850])
train_selected_adj['model_score_01_xysl_3_b']=pd.cut(train_selected_adj['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
train_selected_adj['model_score_01_y_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_y_tianchuang,[-10000,0,580,850])
train_selected_adj['model_score_01_zr_tdzx_b']=pd.cut(train_selected_adj.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
train_selected_adj['utl_b']=pd.cut(train_selected_adj.utl,[0,0.5,0.9,np.inf])

train_selected_adj['model_score_01_bh_fraud_b']=pd.cut(train_selected_adj.model_score_01_bh_fraud,[-10000,0,650,np.inf])
train_selected_adj['model_score_01_fico_b']=pd.cut(train_selected_adj.model_score_01_fico,[-10000,0,800,np.inf])

use_col_keep_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','utl_b','model_score_01_bh_fraud_b','model_score_01_fico_b']

bin_train=iv(train_selected_adj,use_col_keep_adj)

#%%
adj_bins={'crd_loan_gap': [-10000,0,7,30,np.inf],
 'model_score_01': [-10000,0,720,740,770,850],
 'model_score_01_x_tianchuang': [-10000,0,600,700,850],
 'model_score_01_xysl_3': [-10000,0,600,620,645,670,700,np.inf],
 'model_score_01_y_tianchuang': [-10000,0,580,850],
 'model_score_01_zr_tdzx': [-10000,0,660,700,750,800,900],
# 'model_score_01br_score': [-10000,0,500,545,np.inf],
 'utl':[0,0.5,0.9,np.inf],
 'model_score_01_bh_fraud':[-10000,0,650,np.inf],
 'model_score_01_fico':[-10000,0,800,np.inf]}
use_col_keep=['user_id','target','crd_loan_gap','model_score_01','model_score_01_x_tianchuang','model_score_01_xysl_3',
              'model_score_01_y_tianchuang','model_score_01_zr_tdzx','utl','model_score_01_bh_fraud', 'model_score_01_fico']

# 更新分箱
c.update(adj_bins)
test_selected=dt_test[use_col_keep]
test_selected=test_selected[use_col_keep].fillna(-9999)
# oot_selected=oot[use_col_keep]
# oot_selected=oot_selected[use_col_keep].fillna(-9999)

#test_selected_bins = c.transform(test_selected, labels=True)
test_selected['crd_loan_gap_b']=pd.cut(test_selected.crd_loan_gap,[-10000,0,7,30,np.inf])
test_selected['model_score_01_b']=pd.cut(test_selected.model_score_01,[-10000,0,720,740,770,850])
test_selected['model_score_01_x_tianchuang_b']=pd.cut(test_selected.model_score_01_x_tianchuang,[-10000,0,600,700,850])
test_selected['model_score_01_xysl_3_b']=pd.cut(test_selected['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
test_selected['model_score_01_y_tianchuang_b']=pd.cut(test_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
test_selected['model_score_01_zr_tdzx_b']=pd.cut(test_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
test_selected['utl_b']=pd.cut(test_selected.utl,[0,0.5,0.9,np.inf])
test_selected['model_score_01_bh_fraud_b']=pd.cut(test_selected.model_score_01_bh_fraud,[-10000,0,650,np.inf])
test_selected['model_score_01_fico_b']=pd.cut(test_selected.model_score_01_fico,[-10000,0,800,np.inf])
test_selected['WEIGHT']=1

bin_test=pd.DataFrame()

bin_test=iv(test_selected,use_col_keep_adj)

#oot_selected_bins = c.transform(oot_selected, labels=True)
# oot_selected['crd_loan_gap_b']=pd.cut(oot_selected.crd_loan_gap,[-10000,0,7,30,np.inf])
# oot_selected['model_score_01_b']=pd.cut(oot_selected.model_score_01, [-10000,0,720,740,750,850])
# oot_selected['model_score_01_x_tianchuang_b']=pd.cut(oot_selected.model_score_01_x_tianchuang,[-10000,0,600,700,850])
# oot_selected['model_score_01_xysl_3_b']=pd.cut(oot_selected['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
# oot_selected['model_score_01_y_tianchuang_b']=pd.cut(oot_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
# oot_selected['model_score_01_zr_tdzx_b']=pd.cut(oot_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
# oot_selected['utl_b']=pd.cut(oot_selected.utl,[0,0.5,0.9,np.inf])

# oot_selected['WEIGHT']=1

# bin_oot=pd.DataFrame()

# bin_oot=iv(oot_selected,use_col_keep_adj)

#%%
#WOE转化
train_bins=train_selected_adj[['target','crd_loan_gap_b','model_score_01_b', 'model_score_01_x_tianchuang_b',
       'model_score_01_xysl_3_b', 'model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b', 'utl_b','model_score_01_bh_fraud_b','model_score_01_fico_b']]
transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_bins), train_bins['target'], exclude=['target'])
#相关性检验
corr_train=train_woe.loc[:,use_col_keep_adj].corr()
#%% glm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.formula.api as smf
import statsmodels.api as sm
#train_woe.rename(columns={"model_score_01_xysl_3.0":'model_score_01_xysl_3',"model_score_01_xysl_3.0_b":'model_score_01_xysl_3_b'},inplace=True)
#model_names_all=['model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01br_score_b','utl_b']
model_name1=['crd_loan_gap_b','utl_b','model_score_01_zr_tdzx_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_y_tianchuang_b','model_score_01_xysl_3_b',]

#model_names_shortest=['model_score_01_y_tianchuang_b','crd_loan_gap_b','model_score_01_b','model_score_01_xysl_3_b','utl_b']

formula="{} ~ {} +1".format('target','+'.join(model_name1))
formula

model_1=smf.glm(formula,train_woe[model_name1+['target']],family=sm.families.Binomial(sm.families.links.logit())).fit()

print(model_1.summary())
train_selected_adj['lr_prob']=model_1.predict(train_woe[model_name1])
print('train_KS:',toad.metrics.KS(train_selected_adj['lr_prob'],train_selected_adj['target']))
print('train_AUC:',toad.metrics.AUC(train_selected_adj['lr_prob'],train_selected_adj['target']))
# train_KS: 0.1893672906736823
# train_AUC: 0.6309943534797793
#%%
#VIF检验
import warnings
warnings.filterwarnings('ignore')

vif=pd.DataFrame()
X = np.matrix(train_woe[model_name1])
vif['features']=model_name1
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
#%%
#模型验证
#PART1.测试集
test_selected1 = test_selected[list(train_woe.columns)]
test_woe = transer.transform(c.transform(test_selected1))

test_selected['lr_prob']=model_1.predict(test_woe[model_name1])
print('test_KS:',toad.metrics.KS(test_selected['lr_prob'],test_selected['target']))
print('test_AUC:',toad.metrics.AUC(test_selected['lr_prob'],test_selected['target']))
# test_KS: 0.18184092119525985
# test_AUC: 0.6256792048758679
#PART2.验证集
# oot_selected1 = oot_selected[list(train_woe.columns)]
# oot_woe = transer.transform(c.transform(oot_selected))
# oot_selected['lr_prob']=model_1.predict(oot_woe[model_name1])

# print('test_KS:',toad.metrics.KS(oot_selected['lr_prob'],oot_selected['target']))
# print('test_AUC:',toad.metrics.AUC(oot_selected['lr_prob'],oot_selected['target']))
# test_KS: 0.12530964842627595
# test_AUC: 0.5839134464129571

#%%
#模型验证
#PART1.测试集
test_selected1 = test_selected[list(train_woe.columns)]
test_woe = transer.transform(c.transform(test_selected1))
# test_KS: 0.17841887367599363
# test_AUC: 0.6221942290628473
#PART2.验证集
# oot_selected1 = oot_selected[list(train_woe.columns)]
# oot_woe = transer.transform(c.transform(oot_selected1))


test_selected['lr_prob']=model_1.predict(test_woe[model_name1])
print('test_KS:',toad.metrics.KS(test_selected['lr_prob'],test_selected['target']))
print('test_AUC:',toad.metrics.AUC(test_selected['lr_prob'],test_selected['target']))
# oot_selected['lr_prob']=model_1.predict(oot_woe[model_name1])
# print('oot_KS:',toad.metrics.KS(oot_selected['lr_prob'],oot_selected['target']))
# print('oot_AUC:',toad.metrics.AUC(oot_selected['lr_prob'],oot_selected['target']))
#%%
import matplotlib.pyplot as plt
import matplotlib
from sklearn import metrics
from sklearn.metrics import roc_curve

def plot_roc(y_label,y_pred):
    tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred)
    fig=plt.figure(figsize=(6,4))
    ax=fig.add_subplot(1,1,1)
    ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC)
    ax.plot([0,1],[0,1],'r--')
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    ax.set_title('ROC')
    ax.legend(loc='best')
    return plt.show(ax)

from sklearn.metrics import roc_auc_score,roc_curve
y_pred=model_1.predict(train_woe[model_name1])

#计算AUC值
AUC=roc_auc_score(train_woe.target,y_pred)
print('AUC:',AUC)

#ROC曲线
plot_roc(train_woe.target,y_pred)
fpr,tpr,thresholds_train=roc_curve(train_woe.target,y_pred)
KS=np.max(tpr-fpr)
print('KS:',KS)

#  KS: 0.1703041002849328
# AUC: 0.61895733707338
#%%#评分转换
def creditCards(paramsEst,
                bin_woe_map_df,
                basepoints, 
                baseodds, 
                PDO,
                odds_new,
                odds_old,
                has_intercept_score):
    
    """
    output credit card for each var in model
    --------------------------------------------
    ParamsEst: pandas Series, 模型的参数估计结果，index为变量名, value为变量系数估计值
    bin_woe_map_df：变量分bin和woe的对应表
    basepoints: 标准odds，即baseodds时的标准评分
    baseodds: 标准odds，指定的标准odds
    PDO: odds增加翻倍时，评分变化的分数，风险模型，若要1的概率越高分数越低，PDO取负值；若要1的概率越高分数越高，PDO取正值
    odds_new：抽样建模样本的odds：1的数量/0的数量
    odds_old：原始未抽样样本的odds：1的数量/0的数量
    has_intercept_score: 是否含截距项分数
    -------------------------------------------
    Return
    creditCard: 评分卡结果，pandas dataframe
    """

    # 计算A&B
    beta = PDO/np.log(2)
    alpha = basepoints + beta*np.log(baseodds)
    
    #alpha, beta = _score_cal(basepoints, baseodds, PDO)
    odds_ratio = odds_new/odds_old
    
    # 计算截距项基础分
    if has_intercept_score:
        points_0 = round(alpha - beta * paramsEst['const'] - beta * np.log(odds_ratio))
    else:
        points_0 = alpha - beta * paramsEst['const'] - beta * np.log(odds_ratio)
    # 变量个数
    num_vars = len(paramsEst)-1
      
    print('标准odds: ' + str(baseodds))
    print('标准odds时的标准评分: ' + str(basepoints))
    print('odds翻倍时评分变化PDO：' + str(PDO))
    print('评分计算公式截距系数alpha = ' + str(alpha))
    print('评分计算公式斜率系数beta = ' + str(beta))
    print('建模样本的总体odds：' + str(odds_new))
    print('原始样本的总体odds：' + str(odds_old))
    print('模型变量个数: ' + str(num_vars))
    print('若评分卡包含截距项评分，截距项评分未: ' + str(round(points_0)))
    print('若评分卡不包含截距项评分，每个变量分摊截距评分or变量缺失默认评分: ' + str(round(points_0/num_vars)))
  
    # woe
    var_list = list(paramsEst.index)[1:]
    woe_maps_dict = {}
    for var in var_list:
        var_bin_woe_df = bin_woe_map_df[bin_woe_map_df['var']==var]
        var_bin_woe_df_dict = var_bin_woe_df.pivot(index='var', columns='bin', values='woe').to_dict('index')
        woe_maps_dict[var] = var_bin_woe_df_dict[var]
        
    # 根据各段woe，计算相应得分
    points = pd.DataFrame()
    for k in woe_maps_dict.keys():
        d = pd.DataFrame(woe_maps_dict[k], index=[k]).T
        if has_intercept_score:
            d['points'] = (-beta * d.loc[:, k] * paramsEst[k]).round()
        else:
            d['points'] = (points_0*1.0/num_vars - beta * d.loc[:, k] * paramsEst[k]).round()
        d = d.rename(columns={k: 'var_woe'})
        # range
        bin_map = bin_woe_map_df[bin_woe_map_df['var']==k]
        bin_map.index = bin_map['bin']
        bin_map.index.name = None
        bin_map = bin_map[['bin', 'woe']]
        bin_map['var'] = k
        d = d.merge(bin_map, left_index=True, right_index=True)
        # 构造新的index
        n = len(d)
        ind_0 = []
        i = 0
        while i < n:
            ind_0.append(k)
            i += 1
        d.index = [ind_0,list(d.index)]
        points = pd.concat([points, d], axis=0)
    points_df = points[['var', 'bin',  'points', 'var_woe']]    

    #输出评分卡
    if has_intercept_score:
        points_0_df = pd.DataFrame([['basePoints', '0', points_0, 1]], 
                                   columns = ['var', 'bin', 'points', 'var_woe'])
        points_0_df.index=[['basePoints'], ['0']]
        credit_card = pd.concat([points_0_df, points_df], axis=0)
        credit_card.index.names = ["varname", "binCode"]
    else:
        credit_card = points_df
        credit_card.index.names = ["varname", "binCode"]
    return credit_card
#%%
baseodds = 35
basepoints = 700
PDO = 60  
odds_old = 60510/6211
odds_new = 60510/6211
 
params = {'const':-2.2765
          ,'crd_loan_gap_b': 0.7177
          ,'utl_b':0.9094
          ,'model_score_01_zr_tdzx_b':0.4670
          ,'model_score_01_b':0.5001
          ,'model_score_01_x_tianchuang_b':0.6286
          ,'model_score_01_y_tianchuang_b':0.5071
          ,'model_score_01_xysl_3_b':0.8476} 

paramsEst = pd.Series(params)
paramsEst.index = [k.replace("_b", "") for k in paramsEst.index]
bin_woe_map_df=pd.read_excel(r'C:\Users\ruizhi\Desktop\kantt\0.cal_summary\bin_woe_map.xlsx')

credit_card_df = creditCards(paramsEst = paramsEst,     # 模型参数估计，Series，index为模型变量（不带后缀"_WOE"）
                            bin_woe_map_df = bin_woe_map_df,   # 变量分bin和woe的对应表
                            basepoints = basepoints,    # # 标准odds，1的占比：0的占比（1的数量：0的数量）
                            baseodds = baseodds,  # 标准odds时的标准评分
                            PDO= PDO,   # odds增加一倍时，分数变化
                            odds_new = odds_new,   # 抽样建模样本的odds：1的数量/0的数量
                            odds_old = odds_old,    # 原始未抽样样本的odds：1的数量/0的数量
                            has_intercept_score = False   # 评分卡是否包含截距项
                            )
#%%
########################打总分
def p_to_score(p,pdo,base,odds):
    B = pdo/np.log(2)
    A = base + B*np.log(odds)
    score = A-B*np.log(p/(1-p))
    return round(score,0)
#%%
train_selected_adj['score']=p_to_score(train_selected_adj['lr_prob'],pdo=60,base=700,odds=1)
test_selected['score']=p_to_score(test_selected['lr_prob'],pdo=60,base=700,odds=1)

train_adj=pd.concat([train_selected_adj,dt_train[['channel_id_x']]],axis=1)
test_adj=pd.concat([test_selected,dt_test[['channel_id_x']]],axis=1)

train_adj['score_band']=pd.qcut(train_adj.score,10,duplicates='drop')
test_adj['score_band']=pd.cut(test_adj.score,[-np.inf,937,951,962,972,981,991,1003,1017,1038,np.inf])

cal_score1=train_adj.groupby('score_band')['target'].count().reset_index()
cal_score2=test_adj.groupby('score_band')['target'].count().reset_index()

cal_score3=train_adj.groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()

cal_score4=test_adj.groupby(['score_band','target'])['user_id'].count().unstack().reset_index()

cal_score1=train_adj[train_adj.channel_id_x==174].groupby('score_band')['target'].count().reset_index()
cal_score2=test_adj[test_adj.channel_id_x==174].groupby('score_band')['target'].count().reset_index()

cal_score3=train_adj[train_adj.channel_id_x==174].groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()
cal_score4=test_adj[test_adj.channel_id_x==174].groupby(['score_band','target'])['user_id'].count().unstack().reset_index()

print('train_KS:',toad.metrics.KS(train_adj[train_adj.channel_id_x==174]['lr_prob'],train_adj[train_adj.channel_id_x==174]['target']))
print('train_AUC:',toad.metrics.AUC(train_adj[train_adj.channel_id_x==174]['lr_prob'],train_adj[train_adj.channel_id_x==174]['target']))
print('test_KS:',toad.metrics.KS(test_adj[test_adj.channel_id_x==174]['lr_prob'],test_adj[test_adj.channel_id_x==174]['target']))
print('test_AUC:',toad.metrics.AUC(test_adj[test_adj.channel_id_x==174]['lr_prob'],test_adj[test_adj.channel_id_x==174]['target']))
train_KS: 0.17104400961932537
train_AUC: 0.6082721617530511
test_KS: 0.20142121885600656
test_AUC: 0.6261367476378004
print('train_KS:',toad.metrics.KS(train_adj[train_adj.channel_id_x==167]['lr_prob'],train_adj[train_adj.channel_id_x==167]['target']))
print('train_AUC:',toad.metrics.AUC(train_adj[train_adj.channel_id_x==167]['lr_prob'],train_adj[train_adj.channel_id_x==167]['target']))
print('test_KS:',toad.metrics.KS(test_adj[test_adj.channel_id_x==167]['lr_prob'],test_adj[test_adj.channel_id_x==167]['target']))
print('test_AUC:',toad.metrics.AUC(test_adj[test_adj.channel_id_x==167]['lr_prob'],test_adj[test_adj.channel_id_x==167]['target']))
train_KS: 0.17799778849611614
train_AUC: 0.6193057430268272
test_KS: 0.14869805655868412
test_AUC: 0.6015536901190438





#==============================================================================
# File: 逻辑回归打分.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
模型建立
@author: kantt
"""
#%%
import pandas as pd
import numpy as np
import toad
import os 
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
#%% 数据导入与处理
data_tmp1 = pd.read_csv(r'C:\Users\ruizhi\Desktop\kantt\1.base_data\valid_cust_first.csv') 

#列名筛选
col_list=data_tmp1.columns
drop_col=['Unnamed: 0.1','loan_amount_y','Unnamed: 0','order_no_y','channel_id_y','apply_date_y','apply_time','id_no_des_y','channel_id','apply_date_real']
data_tmp1.drop(columns=drop_col,inplace=True)

#剔除灰样本确认模型样本
data = data_tmp1[data_tmp1['Firs6ever30'].isin([0.,2.])]
#    channel_id_x  Firs6ever30  user_id
# 0           167          0.0    61704
# 1           167          2.0     5994
# 2           174          0.0    24740
# 3           174          2.0     2879

cal1=data.groupby(['apply_month','Firs6ever30','channel_id_x'])['user_id'].count().unstack().reset_index()

print('数据大小：', data.shape) #(95317, 143)

data['target']=data['Firs6ever30']/2
#%%
#STEP1. - 数据集切分  训练集测试集
model = data
#[data.apply_month.isin(['2022-05','2022-06','2022-07','2022-08','2022-09'])]
#oot = data[data.apply_month.isin(['2022-10','2022-11','2022-12'])]

allFeatures = list(model.columns.drop(['id_no_des_x','order_no_x','apply_date_x','order_status','loan_period','channel_id_x','loan_amount_x','lending_time','loan_rate',
'total_periods','Firs3ever15','Firs3ever30','Firs6ever15','Firs6ever30','apply_month','create_time',
'channel_name','cert_type','auth_status','auth_credit_amount','cust_type','close_reason','time_diff']))

X1 = model[model.channel_id_x==167][allFeatures]
Y1 = model[model.channel_id_x==167]['target']
X2 = model[model.channel_id_x==174][allFeatures]
Y2 = model[model.channel_id_x==174]['target']

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1 ,Y1, test_size=0.3, random_state=88, stratify=Y1)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2 ,Y2, test_size=0.3, random_state=88, stratify=Y2)

X_train=pd.concat([X_train1,X_train2],axis=0)
X_test=pd.concat([X_test1,X_test2],axis=0)
y_train=pd.concat([y_train1,y_train2],axis=0)
y_test=pd.concat([y_test1,y_test2],axis=0)

model.loc[X_train.index,'sample_set']='train'
model.loc[X_test.index,'sample_set']='test'

dt_train=model[model.sample_set=='train'] #59094
dt_test=model[model.sample_set=='test'] #25327


cal2=dt_train.groupby(['apply_month','Firs6ever30','channel_id_x'])['user_id'].count().reset_index()
cal3=dt_test.groupby(['apply_month','Firs6ever30','channel_id_x'])['user_id'].count().reset_index()
#%%
#STEP2. - 数据概况 --缺失率&分位数
var_cal_train=toad.detector.detect(dt_train[allFeatures])
var_cal_test=toad.detector.detect(dt_test[allFeatures])
#数据概况 --iv
var_iv_train=toad.quality(dt_train[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)
var_iv_test=toad.quality(dt_test[allFeatures],
                          target='target',
                          method='chi',
                          n_bins=5,
                          iv_only=True)

var_iv_train.sort_values('iv',ascending=False)
var_iv_test.sort_values('iv',ascending=False)

train_selected, dropped = toad.selection.select(dt_train[allFeatures], target='target', empty=0.9, iv=0.01, corr=0.7, return_drop=True)
print(train_selected.shape)#(66721, 29)
col_target=train_selected.columns
col_target_key=['crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']
col_target_key_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_tengxun_b','model_score_01_x_tianchuang_b','model_score_01_xysl_1.0_b','model_score_01_xysl_3.0_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','model_score_01br_score_b','utl_b','model_score_01_rong360_b','model_score_01_fulin_b']
#%%

#STEP3. - 数据切分
c = toad.transform.Combiner()
c.fit(train_selected, y='target', method='chi', min_samples=None, n_bins=10, empty_separate=True) 
bins_result = c.export()
train_selected_bins = c.transform(train_selected, labels=True)
train_selected_bins['WEIGHT']=1

col_target_keep=['target','crd_loan_gap','model_score_01','model_score_01_tengxun','model_score_01_x_tianchuang','model_score_01_xysl_1.0','model_score_01_xysl_3.0','model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl','model_score_01_rong360','model_score_01_fulin']

train_selected_adj=train_selected[col_target_keep]
train_selected_adj=train_selected[col_target_keep].fillna(-9999)
train_selected_adj=train_selected_adj.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})
train_selected_adj['WEIGHT']=1

bin_train_auto=iv(train_selected_bins,train_selected_bins.columns)
#%%
eps=0.0000001
def iv(data,columns):
    w=pd.DataFrame()
    for i in columns:
        a=pd.DataFrame()
        a['人数']=data['WEIGHT'].groupby(data[i]).sum()
        a['人数占比']=data['WEIGHT'].groupby(data[i]).sum()/data['WEIGHT'].sum()
        a['字段']=i
        a['分段']=data['WEIGHT'].groupby(data[i]).sum().index
        a['B数量']=data['target'].groupby(data[i]).sum()
        a['G数量']=a['人数']-a['B数量']
        a['bad_rate']=a['B数量']/a['人数']
        a['G占比']=a['G数量']/a['G数量'].sum()+eps
        a['B占比']=a['B数量']/a['B数量'].sum()+eps
        a['iv_tmp']=np.log(a['B占比']/a['G占比'])*(a['B占比']-a['G占比'])
        a['iv']=a['iv_tmp'].sum()
        a['woe']=np.log(a['B占比']/a['G占比'])
        w=w.append(a,ignore_index=True)
    return w
#%%
#自动分箱
# train_selected_adj['crd_loan_gap_b']=pd.qcut(train_selected_adj.crd_loan_gap,10,duplicates='drop')
# train_selected_adj['model_score_01_b']=pd.qcut(train_selected_adj.model_score_01,10,duplicates='drop')
# train_selected_adj['model_score_01_tengxun_b']=pd.qcut(train_selected_adj.model_score_01_tengxun,10,duplicates='drop')
# train_selected_adj['model_score_01_x_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_x_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_1.0_b']=pd.qcut(train_selected_adj['model_score_01_xysl_1.0'],10,duplicates='drop')
# train_selected_adj['model_score_01_xysl_3_b']=pd.qcut(train_selected_adj['model_score_01_xysl_3'],10,duplicates='drop')
# train_selected_adj['model_score_01_y_tianchuang_b']=pd.qcut(train_selected_adj.model_score_01_y_tianchuang,10,duplicates='drop')
# train_selected_adj['model_score_01_zr_tdzx_b']=pd.qcut(train_selected_adj.model_score_01_zr_tdzx,10,duplicates='drop')
# train_selected_adj['model_score_01br_score_b']=pd.qcut(train_selected_adj.model_score_01br_score,10,duplicates='drop')
# train_selected_adj['model_score_01_rong360_b']=pd.qcut(train_selected_adj.model_score_01_rong360,10,duplicates='drop')
# train_selected_adj['model_score_01_fulin_b']=pd.qcut(train_selected_adj.model_score_01_fulin,10,duplicates='drop')
# train_selected_adj['utl_b']=pd.qcut(train_selected_adj.utl,10,duplicates='drop')

# bin_train=pd.DataFrame()
# bin_train=iv(train_selected_adj,col_target_key_adj)


# train_selected['value_015_bairong_b']=pd.qcut(train_selected.value_015_bairong,10,duplicates='drop')
# train_selected['value_021_bairong_b']=pd.qcut(train_selected.value_021_bairong,10,duplicates='drop')
# train_selected['value_024_bairong_b']=pd.qcut(train_selected.value_024_bairong,10,duplicates='drop')
# train_selected['value_044_bairong_b']=pd.qcut(train_selected.value_044_bairong,10,duplicates='drop')

# train_selected['value_069_bairong_b']=pd.qcut(train_selected['value_069_bairong'],10,duplicates='drop')
# train_selected['value_072_bairong_b']=pd.qcut(train_selected['value_072_bairong'],10,duplicates='drop')
# train_selected['value_078_bairong_b']=pd.qcut(train_selected.value_078_bairong,10,duplicates='drop')
# train_selected['value_084_bairong_b']=pd.qcut(train_selected.value_084_bairong,10,duplicates='drop')
# train_selected['value_086_bairong_b']=pd.qcut(train_selected.value_086_bairong,10,duplicates='drop')
# train_selected['value_087_bairong_b']=pd.qcut(train_selected.value_087_bairong,10,duplicates='drop')
# train_selected['WEIGHT']=1

# list_try=['value_015_bairong_b','value_021_bairong_b','value_024_bairong_b','value_044_bairong_b',
#           'value_069_bairong_b','value_072_bairong_b','value_078_bairong_b','value_084_bairong_b','value_086_bairong_b','value_087_bairong_b']
# bin_train=iv(train_selected,list_try)


#%% 
#手动分箱
train_selected_adj['crd_loan_gap_b']=pd.cut(train_selected_adj.crd_loan_gap,[-10000,0,7,np.inf])
train_selected_adj['model_score_01_b']=pd.cut(train_selected_adj.model_score_01,[-10000,0,690,720,740,770,850])
train_selected_adj['model_score_01_x_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_x_tianchuang,[-10000,0,575,645,850])
train_selected_adj['model_score_01_xysl_3_b']=pd.cut(train_selected_adj['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
train_selected_adj['model_score_01_y_tianchuang_b']=pd.cut(train_selected_adj.model_score_01_y_tianchuang,[-10000,0,580,850])
train_selected_adj['model_score_01_zr_tdzx_b']=pd.cut(train_selected_adj.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
#train_selected_adj['model_score_01br_score_b']=pd.cut(train_selected_adj.model_score_01br_score,[-10000,0,500,545,np.inf])
train_selected_adj['utl_b']=pd.cut(train_selected_adj.utl,[0,0.5,0.9,np.inf])


use_col_keep_adj=['crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','utl_b']
bin_train=pd.DataFrame()
bin_train=iv(train_selected_adj,use_col_keep_adj)

# train_selected_adj['model_score_01_b']=pd.cut(train_selected_adj.model_score_01,[-10000,0,690,720,740,770,850])
# bin_train_tmp=pd.DataFrame()
# bin_train_tmp=iv(train_selected_adj,['model_score_01_b'])

#%%
adj_bins={'crd_loan_gap': [-10000,0,7,np.inf],
 'model_score_01': [-10000,0,690,720,740,770,850],
 'model_score_01_x_tianchuang': [-10000,0,575,645,850],
 'model_score_01_xysl_3.0': [-10000,0,580,600,620,645,670,700,np.inf],
 'model_score_01_y_tianchuang': [-10000,0,580,850],
 'model_score_01_zr_tdzx': [-10000,0,660,700,750,800,900],
# 'model_score_01br_score': [-10000,0,500,545,np.inf],
 'utl':[0,0.5,0.9,np.inf]}
use_col_keep=['user_id','target','crd_loan_gap','model_score_01','model_score_01_x_tianchuang','model_score_01_xysl_3.0',
              'model_score_01_y_tianchuang','model_score_01_zr_tdzx','model_score_01br_score','utl']

# 更新分箱
c.update(adj_bins)
test_selected=dt_test[use_col_keep]
test_selected=test_selected[use_col_keep].fillna(-9999)
test_selected=test_selected.rename(columns={"model_score_01_xysl_3.0":"model_score_01_xysl_3"})

#test_selected_bins = c.transform(test_selected, labels=True)
test_selected['crd_loan_gap_b']=pd.cut(test_selected.crd_loan_gap,[-10000,0,7,np.inf])
test_selected['model_score_01_b']=pd.cut(test_selected.model_score_01,[-10000,0,690,720,740,770,850])
test_selected['model_score_01_x_tianchuang_b']=pd.cut(test_selected.model_score_01_x_tianchuang,[-10000,0,575,645,850])
test_selected['model_score_01_xysl_3_b']=pd.cut(test_selected['model_score_01_xysl_3'],[-10000,0,600,620,645,670,700,np.inf])
test_selected['model_score_01_y_tianchuang_b']=pd.cut(test_selected.model_score_01_y_tianchuang,[-10000,0,580,850])
test_selected['model_score_01_zr_tdzx_b']=pd.cut(test_selected.model_score_01_zr_tdzx,[-10000,0,660,700,750,800,900])
#test_selected['model_score_01br_score_b']=pd.cut(test_selected.model_score_01br_score,[-10000,0,500,545,np.inf])
test_selected['utl_b']=pd.cut(test_selected.utl,[0,0.5,0.9,np.inf])

test_selected['WEIGHT']=1

bin_test=pd.DataFrame()

bin_test=iv(test_selected,use_col_keep_adj)

#%%
#WOE转化
train_bins=train_selected_adj[['target','crd_loan_gap_b','model_score_01_b', 'model_score_01_x_tianchuang_b',
       'model_score_01_xysl_3_b', 'model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b', 'utl_b']]
transer = toad.transform.WOETransformer()
train_woe = transer.fit_transform(c.transform(train_bins), train_bins['target'], exclude=['target'])
#相关性检验
corr_train=train_woe.loc[:,use_col_keep_adj].corr()
#%% glm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.formula.api as smf
import statsmodels.api as sm
#train_woe.rename(columns={"model_score_01_xysl_3.0":'model_score_01_xysl_3',"model_score_01_xysl_3.0_b":'model_score_01_xysl_3_b'},inplace=True)
#model_names_all=['model_score_01_y_tianchuang_b','model_score_01_zr_tdzx_b','crd_loan_gap_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_xysl_3_b','model_score_01br_score_b','utl_b']
model_name1=['crd_loan_gap_b','utl_b','model_score_01_zr_tdzx_b','model_score_01_b','model_score_01_x_tianchuang_b','model_score_01_y_tianchuang_b','model_score_01_xysl_3_b']

#model_names_shortest=['model_score_01_y_tianchuang_b','crd_loan_gap_b','model_score_01_b','model_score_01_xysl_3_b','utl_b']

formula="{} ~ {} +1".format('target','+'.join(model_name1))
formula

model_1=smf.glm(formula,train_woe[model_name1+['target']],family=sm.families.Binomial(sm.families.links.logit())).fit()

print(model_1.summary())
train_selected_adj['lr_prob']=model_1.predict(train_woe[model_name1])
print('train_KS:',toad.metrics.KS(train_selected_adj['lr_prob'],train_selected_adj['target']))
print('train_AUC:',toad.metrics.AUC(train_selected_adj['lr_prob'],train_selected_adj['target']))
# train_KS: 0.17051911380326734
# train_AUC: 0.6185083155545703
#%%
#VIF检验
import warnings
warnings.filterwarnings('ignore')

vif=pd.DataFrame()
X = np.matrix(train_woe[model_name1])
vif['features']=model_name1
vif['VIF_Factor']=[variance_inflation_factor(np.matrix(X),i) for i in range(X.shape[1])]
#%%
#模型验证
#PART1.测试集
test_selected1 = test_selected[list(train_woe.columns)]
test_woe = transer.transform(c.transform(test_selected1))

test_selected['lr_prob']=model_1.predict(test_woe[model_name1])
print('test_KS:',toad.metrics.KS(test_selected['lr_prob'],test_selected['target']))
print('test_AUC:',toad.metrics.AUC(test_selected['lr_prob'],test_selected['target']))
# test_KS: 0.16481799692996335
# test_AUC: 0.6088489480057364
#%%
import matplotlib.pyplot as plt
import matplotlib
from sklearn import metrics
from sklearn.metrics import roc_curve

def plot_roc(y_label,y_pred):
    tpr,fpr,threshold = metrics.roc_curve(y_label,y_pred)
    fig=plt.figure(figsize=(6,4))
    ax=fig.add_subplot(1,1,1)
    ax.plot(tpr,fpr,color='blue',label='AUC=%.3f'%AUC)
    ax.plot([0,1],[0,1],'r--')
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    ax.set_title('ROC')
    ax.legend(loc='best')
    return plt.show(ax)

from sklearn.metrics import roc_auc_score,roc_curve
y_pred=model_1.predict(train_woe[model_name1])

#计算AUC值
AUC=roc_auc_score(train_woe.target,y_pred)
print('AUC:',AUC)

#ROC曲线
plot_roc(train_woe.target,y_pred)
fpr,tpr,thresholds_train=roc_curve(train_woe.target,y_pred)
KS=np.max(tpr-fpr)
print('KS:',KS)



#%%#评分转换
def creditCards(paramsEst,
                bin_woe_map_df,
                basepoints, 
                baseodds, 
                PDO,
                odds_new,
                odds_old,
                has_intercept_score):
    
    """
    output credit card for each var in model
    --------------------------------------------
    ParamsEst: pandas Series, 模型的参数估计结果，index为变量名, value为变量系数估计值
    bin_woe_map_df：变量分bin和woe的对应表
    basepoints: 标准odds，即baseodds时的标准评分
    baseodds: 标准odds，指定的标准odds
    PDO: odds增加翻倍时，评分变化的分数，风险模型，若要1的概率越高分数越低，PDO取负值；若要1的概率越高分数越高，PDO取正值
    odds_new：抽样建模样本的odds：1的数量/0的数量
    odds_old：原始未抽样样本的odds：1的数量/0的数量
    has_intercept_score: 是否含截距项分数
    -------------------------------------------
    Return
    creditCard: 评分卡结果，pandas dataframe
    """

    # 计算A&B
    beta = PDO/np.log(2)
    alpha = basepoints + beta*np.log(baseodds)
    
    #alpha, beta = _score_cal(basepoints, baseodds, PDO)
    odds_ratio = odds_new/odds_old
    
    # 计算截距项基础分
    if has_intercept_score:
        points_0 = round(alpha - beta * paramsEst['const'] - beta * np.log(odds_ratio))
    else:
        points_0 = alpha - beta * paramsEst['const'] - beta * np.log(odds_ratio)
    # 变量个数
    num_vars = len(paramsEst)-1
      
    print('标准odds: ' + str(baseodds))
    print('标准odds时的标准评分: ' + str(basepoints))
    print('odds翻倍时评分变化PDO：' + str(PDO))
    print('评分计算公式截距系数alpha = ' + str(alpha))
    print('评分计算公式斜率系数beta = ' + str(beta))
    print('建模样本的总体odds：' + str(odds_new))
    print('原始样本的总体odds：' + str(odds_old))
    print('模型变量个数: ' + str(num_vars))
    print('若评分卡包含截距项评分，截距项评分未: ' + str(round(points_0)))
    print('若评分卡不包含截距项评分，每个变量分摊截距评分or变量缺失默认评分: ' + str(round(points_0/num_vars)))
  
    # woe
    var_list = list(paramsEst.index)[1:]
    woe_maps_dict = {}
    for var in var_list:
        var_bin_woe_df = bin_woe_map_df[bin_woe_map_df['var']==var]
        var_bin_woe_df_dict = var_bin_woe_df.pivot(index='var', columns='bin', values='woe').to_dict('index')
        woe_maps_dict[var] = var_bin_woe_df_dict[var]
        
    # 根据各段woe，计算相应得分
    points = pd.DataFrame()
    for k in woe_maps_dict.keys():
        d = pd.DataFrame(woe_maps_dict[k], index=[k]).T
        if has_intercept_score:
            d['points'] = (-beta * d.loc[:, k] * paramsEst[k]).round()
        else:
            d['points'] = (points_0*1.0/num_vars - beta * d.loc[:, k] * paramsEst[k]).round()
        d = d.rename(columns={k: 'var_woe'})
        # range
        bin_map = bin_woe_map_df[bin_woe_map_df['var']==k]
        bin_map.index = bin_map['bin']
        bin_map.index.name = None
        bin_map = bin_map[['bin', 'woe']]
        bin_map['var'] = k
        d = d.merge(bin_map, left_index=True, right_index=True)
        # 构造新的index
        n = len(d)
        ind_0 = []
        i = 0
        while i < n:
            ind_0.append(k)
            i += 1
        d.index = [ind_0,list(d.index)]
        points = pd.concat([points, d], axis=0)
    points_df = points[['var', 'bin',  'points', 'var_woe']]    

    #输出评分卡
    if has_intercept_score:
        points_0_df = pd.DataFrame([['basePoints', '0', points_0, 1]], 
                                   columns = ['var', 'bin', 'points', 'var_woe'])
        points_0_df.index=[['basePoints'], ['0']]
        credit_card = pd.concat([points_0_df, points_df], axis=0)
        credit_card.index.names = ["varname", "binCode"]
    else:
        credit_card = points_df
        credit_card.index.names = ["varname", "binCode"]
    return credit_card
#%%
baseodds = 35
basepoints = 700
PDO = 60  
odds_old = 60510/6211
odds_new = 60510/6211
 
params = {'const':-2.2765
          ,'crd_loan_gap_b': 0.6865 
          ,'utl_b':0.9112
          ,'model_score_01_zr_tdzx_b':0.3585
          ,'model_score_01_b':0.5163
          ,'model_score_01_x_tianchuang_b':0.6534
          ,'model_score_01_y_tianchuang_b':0.5606
          ,'model_score_01_xysl_3_b':0.8483} 

paramsEst = pd.Series(params)
paramsEst.index = [k.replace("_b", "") for k in paramsEst.index]
bin_woe_map_df=pd.read_excel(r'C:\Users\ruizhi\Desktop\kantt\0.cal_summary\bin_woe_map.xlsx')


credit_card_df = creditCards(paramsEst = paramsEst,     # 模型参数估计，Series，index为模型变量（不带后缀"_WOE"）
                            bin_woe_map_df = bin_woe_map_df,   # 变量分bin和woe的对应表
                            basepoints = basepoints,    # # 标准odds，1的占比：0的占比（1的数量：0的数量）
                            baseodds = baseodds,  # 标准odds时的标准评分
                            PDO= PDO,   # odds增加一倍时，分数变化
                            odds_new = odds_new,   # 抽样建模样本的odds：1的数量/0的数量
                            odds_old = odds_old,    # 原始未抽样样本的odds：1的数量/0的数量
                            has_intercept_score = False   # 评分卡是否包含截距项
                            )
#%%
########################打总分
def p_to_score(p,pdo,base,odds):
    B = pdo/np.log(2)
    A = base + B*np.log(odds)
    score = A-B*np.log(p/(1-p))
    return round(score,0)
#%%
train_selected_adj['score']=p_to_score(train_selected_adj['lr_prob'],pdo=60,base=700,odds=1)
test_selected['score']=p_to_score(test_selected['lr_prob'],pdo=60,base=700,odds=1)

train_adj=pd.concat([train_selected_adj,dt_train[['channel_id_x']]],axis=1)
test_adj=pd.concat([test_selected,dt_test[['channel_id_x']]],axis=1)

train_adj['score_band']=pd.qcut(train_adj.score,10,duplicates='drop')
# test_adj['score_band']=pd.qcut(test_adj.score,10,duplicates='drop')
test_adj['score_band']=pd.cut(test_adj.score,[-np.inf,856,871,882,891,901,911,922,936,954,np.inf])


cal_score1=train_adj.groupby('score_band')['target'].count().reset_index()
cal_score2=test_adj.groupby('score_band')['target'].count().reset_index()

cal_score3=train_adj.groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()

cal_score4=test_adj.groupby(['score_band','target'])['user_id'].count().unstack().reset_index()

cal_score1=train_adj[train_adj.channel_id_x==167].groupby('score_band')['target'].count().reset_index()
cal_score2=test_adj[test_adj.channel_id_x==167].groupby('score_band')['target'].count().reset_index()

cal_score3=train_adj[train_adj.channel_id_x==167].groupby(['score_band','target'])['model_score_01_tengxun'].count().unstack().reset_index()
cal_score4=test_adj[test_adj.channel_id_x==167].groupby(['score_band','target'])['user_id'].count().unstack().reset_index()

print('train_KS:',toad.metrics.KS(train_adj[train_adj.channel_id_x==174]['lr_prob'],train_adj[train_adj.channel_id_x==174]['target']))
print('train_AUC:',toad.metrics.AUC(train_adj[train_adj.channel_id_x==174]['lr_prob'],train_adj[train_adj.channel_id_x==174]['target']))
print('test_KS:',toad.metrics.KS(test_adj[test_adj.channel_id_x==174]['lr_prob'],test_adj[test_adj.channel_id_x==174]['target']))
print('test_AUC:',toad.metrics.AUC(test_adj[test_adj.channel_id_x==174]['lr_prob'],test_adj[test_adj.channel_id_x==174]['target']))
train_KS: 0.16932086611070624
train_AUC: 0.6127211406998614
test_KS: 0.17433343812689003
test_AUC: 0.6086488523858
print('train_KS:',toad.metrics.KS(train_adj[train_adj.channel_id_x==167]['lr_prob'],train_adj[train_adj.channel_id_x==167]['target']))
print('train_AUC:',toad.metrics.AUC(train_adj[train_adj.channel_id_x==167]['lr_prob'],train_adj[train_adj.channel_id_x==167]['target']))
print('test_KS:',toad.metrics.KS(test_adj[test_adj.channel_id_x==167]['lr_prob'],test_adj[test_adj.channel_id_x==167]['target']))
print('test_AUC:',toad.metrics.AUC(test_adj[test_adj.channel_id_x==167]['lr_prob'],test_adj[test_adj.channel_id_x==167]['target']))
train_KS: 0.16841357568776205
train_AUC: 0.6179256866628375
test_KS: 0.15817861101790814
test_AUC: 0.6058247820251639




#==============================================================================
# End of batch 4
#==============================================================================
