# Auto-merged batch 4/4
# Total files in this batch: 57



#==============================================================================
# File: äººè¡Œç‰¹å¾å˜é‡ä¸€è‡´æ€§ç›‘æ§0905.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
ç‰¹å¾ä¸€è‡´æ€§ç›‘æ§è„šæœ¬ï¼ˆæ”¯æŒä» txt è¯»å–ç›‘æ§å˜é‡ï¼‰
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import smtplib
import requests
import logging
import os
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

# =============================
# 1. é…ç½®åŒº
# =============================

# ç½‘æ˜“ä¼ä¸šé‚®ç®±é…ç½®
EMAIL_CONFIG = {
    'host': 'smtphz.qiye.163.com',
    'port': 465,
    'user': 'liaoxilin@hulianshuzhi.com',
    'password': 'Life2010.',
    'recipients': ['lianghuiyi@juzishuke.com','chendonggen@juzishuke.com','chenshengwen@juzishuke.com','tanxing@hulianshuzhi.com','jileilei@hulianshuzhi.com','liyi@hulianshuzhi.com','youpengyu@hulianshuzhi.com','liaoxilin@hulianshuzhi.com'],
    'title': 'ã€äººè¡Œç‰¹å¾ä¸€è‡´æ€§ç›‘æ§ã€‘{date}'
}

# é£ä¹¦æœºå™¨äºº Webhook
FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/8ac2ce64-33fb-45b6-9cad-11a530761ef9"

# å˜é‡åæ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªå˜é‡åï¼‰

VARIABLES_FILE = "/data/home/liaoxilin/æ•°æ®ç›‘æ§/äººè¡Œç‰¹å¾å˜é‡ç›‘æ§/pboc_variable_file_v2.txt"  # æ”¯æŒç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
SCRIPT_DIR = Path(VARIABLES_FILE).parent.resolve()

VARS_FILE   = SCRIPT_DIR / "pboc_variable_file_v2.txt"
VARS_DESC_FILE   = SCRIPT_DIR / "äººè¡Œå˜é‡æ¸…å•250820.xlsx"
LOG_FILE    = SCRIPT_DIR / "consistency_monitor_v2.log"
OUTPUT_FILE = SCRIPT_DIR / "ä¸ä¸€è‡´æ€§æ˜ç»†_v2.csv"

# æ—¥æœŸ
EXECUTION_DATE = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
DS = EXECUTION_DATE.replace('-','')


# SQLï¼šçº¿ä¸Šç‰¹å¾ï¼ˆå« value_006ï¼‰
SQL_ONLINE = f"""
        select 
         t.order_no
        ,t.encrypt_id_card as id_no_des
        ,t.channel_id
        ,t.ds
        ,t.result_json as value_006
        ,t.create_time
        ,t.update_time
        from znzz_fintech_ods.ods_risk_jk_third_hl_jk_yhx_credit_authorization_pbc_dd  as t
        where t.ds="{DS}" 
          and result_json is not null
          and SUBSTR(create_time, 12, 8) < '23:59:40'
"""


# SQLï¼šçº¿ä¸‹ç‰¹å¾è¡¨ï¼ˆè¯·æ ¹æ®å®é™…ä¿®æ”¹ï¼‰
SQL_OFFLINE = f"""

select
 t.order_no
,t.id_no_des
,acan_zbva_xavc_bbvf_n5
,acao_bbvb_ldve_n12
,acao_bbvb_ldve_n3
,acbg_zbva_bbvf_levg_n12
,acbg_zbva_bbvf_levg_n9
,acdb_zbve_n4
,agaa_zbva_xavh_bbvh_md
,agaa_zbva_xawd_bbvf_mf
,agaa_zbvg_xava_bbvh_mf
,agaa_zbvg_xavh_bbvf_mf
,agaa_zbvg_xawh_bbvh_md
,agaa_zbvg_xawi_bbvf_md
,acda_zbva_n1
,acda_zbvg_n5
,crab_zbva_beva_lavh_n11
,crab_zbva_beva_lavh_n9
,crad_lbvc_n9
,crad_lbvh_n9
,cral_zbva_xava_bbvf_men11
,cral_zbva_xava_bbvf_mfn3
,cral_zbva_xavc_bbvf_mdn10
,cral_zbva_xavc_bbvf_mdn11
,cral_zbva_xavc_bbvf_mdn5
,cral_zbva_xavc_bbvf_men3
,cral_zbva_xavc_bbvf_mfn2
,cral_zbva_xavh_bbvf_mfn10
,cral_zbva_xavh_bbvf_mfn3
,cral_zbva_xavh_bbvf_mfn5
,cral_zbva_xawd_bbvf_mcn10
,cral_zbva_xawd_bbvf_mcn11
,cral_zbva_xawd_bbvf_men4
,cral_zbva_xawd_bbvf_men5
,cral_zbva_xawe_bbvf_mcn9
,cral_zbva_xawe_bbvf_mfn11
,cral_zbva_xawh_bbvf_mcn11
,cral_zbva_xawh_bbvf_mfn3
,cral_zbvg_xava_bbvf_mdn11
,cral_zbvg_xava_bbvf_mfn11
,cral_zbvg_xava_bbvf_mfn2
,cral_zbvg_xavc_bbvf_mcn10
,cral_zbvg_xave_bbvf_mcn2
,cral_zbvg_xave_bbvf_mcn9
,cral_zbvg_xave_bbvf_mdn2
,cral_zbvg_xave_bbvf_mdn3
,cral_zbvg_xave_bbvf_mdn4
,cral_zbvg_xave_bbvf_men4
,cral_zbvg_xave_bbvf_mfn5
,cral_zbvg_xawd_bbvf_mcn10
,cral_zbvg_xawd_bbvf_mcn11
,cral_zbvg_xawd_bbvf_mdn3
,cral_zbvg_xawd_bbvf_mfn11
,cral_zbvg_xawe_bbvf_mcn5
,cral_zbvg_xawe_bbvf_men4
,cral_zbvg_xawh_bbvf_mcn10
,cral_zbvg_xawh_bbvf_mdn11
,cral_zbvg_xawh_bbvf_mdn9
,acaf_zbvb_bevg_n4
,acaf_zbvb_bevg_n5
,cram_bbvb_ldvn_mfn11
,cram_bbvf_ldvn_mcn11
,cram_bbvf_ldvn_mfn11
,cram_bbvh_ldve_mcn9
,cram_bbvh_ldvn_mfn11
,crbf_zcva_xeve
,crbf_zcvc_xeve
,deac_zbvg_n5
,deaa_zbva_xavc_bbvf_mco3
,deaa_zbva_xawd_bbvf_mdo2
,deaa_zbvg_xawd_bbvb_mco3
,deaa_zbvg_xawd_bbvf_mdo2
,redg_zbva_xavc_mc
,redg_zbva_xavc_md
,candlted12m10cnyua_cam
,candlted24m10cnycal
,candlted24m10cnyuas
,candlted24mbcnycal
,candlted24mnbcnyua_uuar
,candlted2m10ua_a9r
,candlted2mnbcnycal
,candlted3mnbcnycaa
,candltedallm30ua_a75r
,candltedallmnbcnyuam
,candltedallmnbcnyuual
,candnlted1m10cnyuual
,candnlted1mbcnycal
,candnlted2mnbcnycal
,candltdd_ed31dfl
,candlted3_6mnbmor
,candltrt_ed10dfl
,candltrt_ed10dfm
,candnlted12_24m10cnyua_ar
,candnlted12m10cnyuuas
,candnlted12m20cnyua_cam
,candnlted12mbcnyua_uuam_l
,candnlted12mbcnyuaa
,candnlted12mnbcnyuas
,candnlted12mnbeddfm
,candnlted1mnbeddfm
,candnlted24_allmbcnyuuar
,candnlted24_allmnbcnyuar
,candnlted24m10cnycla
,candnlted24m10cnycls
,candnlted24mbcnyua_uuam_l
,candnlted24mnbcnycla
,candnlted3m10cnycas
,candnlted3m10cnyua_clr
,candnlted3m10cnyuas
,candnlted3mbcnycal
,candnlted3mnbcnycal
,candnlted6_12mbcnyuar
,candnlted6m10cnycll
,candnltedallm10cnycla
,candnltedallm10cnyuuas
,candnltedallmnbcnyua_clr
,candnltedallmnbcnyuas
,candnltedallmnbua_a75c
,s02jstoas
,s06jhstoam
,s24jhstoam
,swwjstoam
,trrba11r
,dsdlldn
,tisr1anc
,q01_ncfc_qryorg_c
,q24_rfi_c
,d07_q03_fgcqc_oorg_r
,d07_q06_oorgqc_r
,d07_q09_bfiqc_r
,d07_q09_rfigea_nqi_r
,d15_q03_mfcqc_r
,d15_q09_rfigea_nqi_r
,d15_q24_fl_nqi_r
,q01_q03_nbfiqc_r
,q01_q06_nbfiqc_r
,q01_q24_fl_nqi_r
,q01_rfila_nqi_rrgr_r
,q03_q24_rficca_qc_r
,q03_tqc_ncfcqc_r
,q06_q24_cfcqc_r
,q06_q24_rficca_qc_r
,q06_tqi_cfc_nqi_r
,q06_tqi_rfila_nqi_r
,q09_tqi_rficca_nqi_r
,q12_bfiqc_rrgr_r
,q12_fgcqc_oorg_rrgr_r
,q12_q06_mfcqc_s
,q12_q24_nbfiqc_r
,q12_rfigea_qc_rrgr_r
,q24_q12_mfcqc_s
,mdal
,o01cno03cnr
,t03cnt12cnr
,t06cnt12cnr
,c03crchr
,c24crchr
,o12cwwxcdr
,owwcwwncdr
,o01callcho03callchr
,o12cwwchr
,t03crcht12crchr
,t06callcht12callchr
,pbocch01id2mcb
,pbocch01id4mcb
,pbocch02id5mc
,pbocch02id7mc
,pbocch03id20mc
,pbocch03id4mc
,pbocch03id6mc
,pbocch03id7mc
,pbocch06id10mc
,pbocch06id15mc
,pbocch06id20mc
,pbocch06id5mc
,pbocch12id10mc
,pbocch12id15mc
,pbocch12id20mc
,pbocch12id2mc
,pbocch12id2mcb
,pbocch12id3mc
,pbocch12id3mcb
,pbocch12id4mc
,pbocch12id7mc
,pbocchidc0102r
,pbocchidc0103r
,pbocchidc0203r
,pbocchidc0206r
,pbocchidc0306r
,pbocchidc0312r
,pbocchidc0612r
,pbocchidcb0103r
,pbocchidcb0206r
,pbocchidcb0306r
,pbocchidcb0312r
,pbocchidcb0612r


from 
    (
        select order_no,encrypt_id_card as id_no_des
        from znzz_fintech_ods.ods_risk_jk_third_hl_jk_yhx_credit_authorization_pbc_dd as t
        where t.ds="{DS}"
          and SUBSTR(create_time, 12, 8) < '23:59:40'
          and result_json is not null 
        group by order_no,encrypt_id_card
    ) as t

left join 
(
select t.*
from znzz_fintech_ads.dm_f_lxl_test_pboc_query_vars_01 as t 
where dt = "{EXECUTION_DATE}"
  
) as t1 on t.order_no = t1.order_no

left join 
(
select t.*
from znzz_fintech_ads.dm_f_lxl_test_pboc_query_vars_02 as t 
where dt = "{EXECUTION_DATE}"
  
) as t2 on t.order_no = t2.order_no 

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_insurance_housing_fund_info_detail_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t7 on t.order_no = t7.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_large_installments_detail_df_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t8 on t.order_no = t8.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_large_installments_detail_dt_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t9 on t.order_no = t9.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_mobile_phone_number_info_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t10 on t.order_no = t10.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_df as t 
where dt = "{EXECUTION_DATE}"
  
) as t11 on t.order_no = t11.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_ds as t 
where dt = "{EXECUTION_DATE}"
  
) as t12 on t.order_no = t12.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_dt as t 
where dt = "{EXECUTION_DATE}"
  
) as t13 on t.order_no = t13.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_special_trans_info_detail_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t14 on t.order_no = t14.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_special_trans_info_detail_view_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t15 on t.order_no = t15.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t16 on t.order_no = t16.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_Rh as t 
where dt = "{EXECUTION_DATE}"
  
) as t17 on t.order_no = t17.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R0 as t 
where dt = "{EXECUTION_DATE}"
  
) as t18 on t.order_no = t18.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R3 as t 
where dt = "{EXECUTION_DATE}"
  
) as t19 on t.order_no = t19.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R4 as t 
where dt = "{EXECUTION_DATE}"
  
) as t20 on t.order_no = t20.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_Rhn as t 
where dt = "{EXECUTION_DATE}"
  
) as t21 on t.order_no = t21.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_ac_01_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t22 on t.order_no = t22.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_ac_07_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t23 on t.order_no = t23.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_co_02_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t24 on t.order_no = t24.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_03_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t25 on t.order_no = t25.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_04_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t26 on t.order_no = t26.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_06_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t27 on t.order_no = t27.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_de_05_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t28 on t.order_no = t28.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_alert_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t29 on t.order_no = t29.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_overdue_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t30 on t.order_no = t30.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_recovered_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t31 on t.order_no = t31.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_repay_responsibility_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t32 on t.order_no = t32.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t33 on t.order_no = t33.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_1_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t34 on t.order_no = t34.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_2_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t35 on t.order_no = t35.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_3_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t36 on t.order_no = t36.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_4_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t37 on t.order_no = t37.order_no

left join 
(
select t.*
from znzz_fintech_ads.beforeloan_pboc_trans_info_detail_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t39 on t.order_no = t39.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_trans_info_detail_repay_status_del_month_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t40 on t.order_no = t40.order_no
;

"""

# ä¸€è‡´ç‡é˜ˆå€¼
THRESHOLD = 0.9999

# æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# =============================
# 2. è¯»å–å˜é‡å txt æ–‡ä»¶
# =============================
def read_variables_from_txt(file_path):
    """
    è¯»å–ç›‘æ§å˜é‡åæ–‡ä»¶
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            variables = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        logging.info(f"âœ… ä» {file_path} è¯»å–åˆ° {len(variables)} ä¸ªç›‘æ§å˜é‡ï¼š{variables}")
        return variables
    except Exception as e:
        logging.error(f"âŒ è¯»å–å˜é‡æ–‡ä»¶å¤±è´¥: {e}")
        raise

# =============================
# 3. è·å–æ•°æ®å‡½æ•°ï¼ˆODPSï¼‰
# =============================
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    logging.info('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()

    try:
        instance = conn.execute_sql(sql)
        if not instance.is_successful():
            raise Exception(f"SQL æ‰§è¡Œå¤±è´¥: {instance.get_logview_address()}")

        with instance.open_reader() as reader:
            data = reader.to_pandas(n_process=n_process)
        end = time.time()
        logging.info(f'ç»“æŸè·‘æ•°ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        logging.info(f"è¿è¡Œæ—¶é—´ï¼š{end - start:.2f}ç§’")
        return data

    except Exception as e:
        logging.error(f"æ•°æ®è·å–å¤±è´¥: {e}")
        raise

# =============================
# 4. è§£æ value_006 JSON
# =============================
def parse_value_006(df):
    def safe_parse(x):
        try:
            return json.loads(x) if pd.notna(x) else {}
        except:
            return {}
    
    parsed = df['value_006'].apply(safe_parse)
    features_df = pd.json_normalize(parsed)
    return pd.concat([df.drop(columns=['value_006']), features_df], axis=1)


# =============================
# 5. è®¡ç®—ä¸€è‡´æ€§ï¼ˆåªå¯¹æ¯” txt ä¸­çš„å˜é‡ï¼‰
# =============================
def calculate_consistency(online_df, offline_df, monitor_vars):

    # åªä¿ç•™è¦ç›‘æ§çš„å˜é‡ï¼ˆä¸”å­˜åœ¨äºä¸¤ä¸ª DataFrame ä¸­ï¼‰
    available_online = set(online_df.columns) & set(monitor_vars)
    available_offline = set(offline_df.columns) & set(monitor_vars)
    valid_vars = list(available_online & available_offline)

    if not valid_vars:
        logging.warning("âš ï¸ æœªåœ¨æ•°æ®ä¸­æ‰¾åˆ°ä»»ä½•è¦ç›‘æ§çš„å˜é‡")
        return pd.DataFrame(), pd.DataFrame()

    logging.info(f"ğŸ“Š æ­£åœ¨å¯¹æ¯”ä»¥ä¸‹ {len(valid_vars)} ä¸ªå˜é‡: {valid_vars}")

    ## 4.1æ•°æ®ç±»å‹è½¬æ¢
    for col in valid_vars:
        # åˆ¤æ–­æ˜¯å¦ä¸º object ç±»å‹
        if online_df[col].dtype == 'object':
            online_df[col] = pd.to_numeric(online_df[col], errors='coerce')               

        if offline_df[col].dtype == 'object':
            offline_df[col] = pd.to_numeric(offline_df[col])

    ## 4.2æ•°æ®å¤„ç†ç¼ºå¤±å€¼
    # online_df.replace([-997],np.nan,inplace=True) 
    offline_df.fillna(-999, inplace=True)

    ## 4.3çº¿ä¸‹æ•°æ®å’Œçº¿ä¸Šæ•°æ®ç²¾åº¦ä¿æŒä¸€è‡´
    online_df[valid_vars]= online_df[valid_vars].round(9) 
    s_vars = ['candnlted12_24m10cnyua_ar','crbf_zcva_xeve','acdb_zbve_n4','crbf_zcvc_xeve','crad_lbvc_n9','crad_lbvh_n9','acda_zbva_n1','acda_zbvg_n5']
    online_df[s_vars]= online_df[s_vars].round(4) 
    offline_df[valid_vars]= offline_df[valid_vars].round(9)
    offline_df[s_vars]= offline_df[s_vars].round(4)
    
    merged = pd.merge(
        online_df[['order_no','id_no_des'] + valid_vars],
        offline_df[['order_no','id_no_des'] + valid_vars],
        on=['order_no','id_no_des'],
        suffixes=('_online', '_offline'),
        how='inner'
    )

    if merged.empty:
        logging.warning("âš ï¸ åˆå¹¶åæ•°æ®ä¸ºç©º")
        return pd.DataFrame(), pd.DataFrame()

    results = []
    inconsistent_orders = []

    
    for var in valid_vars:
        online_col = merged[f'{var}_online']
        offline_col = merged[f'{var}_offline']
        both_numeric = online_col.notna() & offline_col.notna()
        within_tolerance = (online_col - offline_col).abs() < 1e-7
        is_equal = (online_col == offline_col) | (online_col.isna() & offline_col.isna()) | (both_numeric & within_tolerance)  
        rate = is_equal.mean()
        count = (~is_equal).sum()

        results.append({
            'variable': var,
            'consistency_rate': round(rate * 100, 4),
            'inconsistent_count': int(count)
        })
        

        # é‡ç‚¹ï¼šè®°å½•ä¸ä¸€è‡´çš„è®¢å•åŠå…¶çº¿ä¸Š/çº¿ä¸‹å€¼
        if rate < THRESHOLD or count > 0:  # å³ä½¿ rate é«˜ä½†æœ‰å°‘é‡ä¸ä¸€è‡´ä¹Ÿè®°å½•
            bad = merged.loc[~is_equal, ['id_no_des','order_no']].copy()
            bad['variable'] = var
            bad['consistency_rate'] = rate
            bad['online_value'] = merged.loc[~is_equal, f'{var}_online']
            bad['offline_value'] = merged.loc[~is_equal, f'{var}_offline']

            inconsistent_orders.append(bad)
    
    # è¯»å–å˜é‡æ¸…å•,æè¿°å˜é‡
    df_vars_des = pd.read_excel(VARS_DESC_FILE, sheet_name='Sheet1')
    df_vars_des.drop(columns=['type'],inplace=True)
    df_vars_des.rename(columns={'var':'variable'},inplace=True)
    df_vars_des['variable']=df_vars_des['variable'].str.lower()
    # ç”Ÿæˆæœ€ç»ˆç»“æœ
    consistency_df = pd.DataFrame(results).sort_values('consistency_rate', ascending=False).reset_index(drop=True)
    consistency_df['çº¿ä¸Šçº¿ä¸‹åˆå¹¶åçš„è®°å½•æ•°']=merged.shape[0]
    consistency_df['çº¿ä¸Šæ•°æ®è®°å½•æ•°']=online_df.shape[0]
    consistency_df['çº¿ä¸‹æ•°æ®è®°å½•æ•°']=offline_df.shape[0]
    consistency_df=pd.merge(consistency_df, df_vars_des, how='left',on=['variable'])
    
    all_inconsistent_df = pd.concat(inconsistent_orders, ignore_index=True) if inconsistent_orders \
        else pd.DataFrame(columns=['id_no_des','order_no', 'variable', 'consistency_rate',
            'online_value', 'offline_value'])
    all_inconsistent_df=pd.merge(all_inconsistent_df,online_df[['id_no_des','order_no','ds','create_time','update_time']],\
                             how='left',on=['id_no_des','order_no'])
    all_inconsistent_df=pd.merge(all_inconsistent_df, df_vars_des, how='left',on=['variable'])

    # ä¿å­˜æ•°æ®ä¸ºcsv
    if len(all_inconsistent_df) > 0:
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            # å†™å…¥æ€»è§ˆ Sheet
            consistency_df.to_excel(writer, sheet_name='ä¸€è‡´ç‡æ€»è§ˆ', index=False)
            all_inconsistent_df.to_excel(writer, sheet_name='ä¸ä¸€è‡´æ˜ç»†', index=False)
#             all_inconsistent_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        logging.info(f"ğŸ’¾ ä¸ä¸€è‡´è®°å½•å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    else:
        logging.info("âœ… æ‰€æœ‰å˜é‡åœ¨å®¹å¿åº¦èŒƒå›´å†…å®Œå…¨ä¸€è‡´ï¼Œæœªç”Ÿæˆä¸ä¸€è‡´è®°å½•æ–‡ä»¶")
    
    # å»é‡é€»è¾‘ï¼šåŒä¸€ä¸ªèº«ä»½è¯å·åªä¿ç•™ä¸€ä¸ªè®¢å•ï¼ŒåŒä¸€ä¸ªç‰¹å¾å˜é‡ä¹Ÿåªä¿ç•™ä¸€ä¸ª
    inconsistent_df = all_inconsistent_df.drop_duplicates(subset=['id_no_des', 'variable'], keep='first')
    inconsistent_df = inconsistent_df.reset_index(drop=True)
    logging.info(f"ğŸ“Š ä¸ä¸€è‡´æ ·æœ¬å»é‡åè®°å½•æ•°ï¼š{len(inconsistent_df)} æ¡ï¼ˆåŸ {len(all_inconsistent_df)} æ¡ï¼‰")

    # âœ… åœ¨ send_email å‰è¿›è¡Œé‡‡æ ·
    if len(inconsistent_df) > 6000:
        # æŒ‰ variable åˆ†ç»„ï¼Œæ¯ç»„æœ€å¤šå– 10 æ¡
        inconsistent_df_sample = inconsistent_df.groupby('variable').head(10).reset_index(drop=True)
        logging.info(f"ğŸ“Š ä¸ä¸€è‡´æ ·æœ¬å·²é‡‡æ ·ï¼š{len(inconsistent_df_sample)} æ¡ï¼ˆåŸ {len(inconsistent_df)} æ¡ï¼‰")
    else:
        inconsistent_df_sample = inconsistent_df.copy()    
    
    return consistency_df, inconsistent_df_sample


# =============================
# 6. å‘é€é‚®ä»¶ï¼ˆExcel é™„ä»¶ç‰ˆï¼‰
# =============================
def send_email(consistency_df, inconsistent_df):
    cfg = EMAIL_CONFIG
    title = cfg['title'].format(date=datetime.now().strftime('%Y-%m-%d'))

    msg = MIMEMultipart()
    msg['Subject'] = title
    msg['From'] = cfg['user']
    msg['To'] = ", ".join(cfg['recipients'])

    low_count = (consistency_df['consistency_rate'] < 99.99).sum() if len(consistency_df) > 0 else 0

    html = f"""
    <h3>äººè¡Œç‰¹å¾ä¸€è‡´æ€§ç›‘æ§æŠ¥å‘Š</h3>
    <p><strong>æ‰§è¡Œæ—¶é—´ï¼š</strong>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    <p><strong>æ•°æ®æ—¥æœŸï¼š</strong>{EXECUTION_DATE}</p>
    <p><strong>ç›‘æ§å˜é‡æ•°ï¼š</strong>{len(consistency_df)}</p>
    <p><strong>ä¸€è‡´ç‡ä½äº99.99%çš„å˜é‡æ•°ï¼š</strong><span style="color:red">{low_count}</span></p>
    """

    msg.attach(MIMEText(html, 'html', 'utf-8'))

    # ========== ç”Ÿæˆ Excel é™„ä»¶ ==========
    if len(consistency_df) > 0 or len(inconsistent_df) > 0:
        import io
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils.dataframe import dataframe_to_rows

        # åˆ›å»ºå†…å­˜ä¸­çš„ Excel
        excel_buffer = io.BytesIO()

        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            # å†™å…¥æ€»è§ˆ Sheet
            if len(consistency_df) > 0:
                consistency_df.to_excel(writer, sheet_name='ä¸€è‡´ç‡æ€»è§ˆ', index=False)
                ws1 = writer.sheets['ä¸€è‡´ç‡æ€»è§ˆ']
                # è‡ªåŠ¨åˆ—å®½
                for col in ws1.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws1.column_dimensions[column].width = adjusted_width

                # è¡¨å¤´åŠ ç²— + èƒŒæ™¯è‰²
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                for cell in ws1[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center")

            # å†™å…¥ä¸ä¸€è‡´æ˜ç»† Sheet
            if len(inconsistent_df) > 0:
                inconsistent_df.to_excel(writer, sheet_name='éƒ¨åˆ†ä¸ä¸€è‡´æ˜ç»†', index=False)
                ws2 = writer.sheets['éƒ¨åˆ†ä¸ä¸€è‡´æ˜ç»†']
                for col in ws2.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws2.column_dimensions[column].width = adjusted_width

                # è¡¨å¤´æ ¼å¼
                for cell in ws2[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center")

        # è·å–äºŒè¿›åˆ¶æ•°æ®
        excel_data = excel_buffer.getvalue()

        # æ·»åŠ é™„ä»¶
        attachment = MIMEApplication(excel_data, _subtype="xlsx")
        attachment.add_header('Content-Disposition', 'attachment', filename=f'äººè¡Œç‰¹å¾ä¸€è‡´æ€§æŠ¥å‘Š_{EXECUTION_DATE}.xlsx')
        msg.attach(attachment)

        logging.info("ğŸ“ å·²ç”Ÿæˆå¹¶é™„åŠ  Excel æŠ¥å‘Š")
    else:
        logging.warning("âš ï¸ æ— æ•°æ®å¯ç”Ÿæˆ Excel é™„ä»¶")

    # ========== å‘é€é‚®ä»¶ ==========
    try:
        server = smtplib.SMTP_SSL(cfg['host'], cfg['port'])
        server.login(cfg['user'], cfg['password'])
        server.sendmail(cfg['user'], cfg['recipients'], msg.as_string())
        server.quit()
        logging.info("ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸ")
    except Exception as e:
        logging.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")


# =============================
# 7. é£ä¹¦é¢„è­¦
# =============================
def send_feishu_alert(consistency_df):
    low_count = (consistency_df['consistency_rate'] < 99.99).sum() if len(consistency_df) > 0 else 0

    if low_count == 0:
        logging.info("âœ… æ‰€æœ‰å˜é‡ä¸€è‡´ç‡ â‰¥ 99.99%ï¼Œè·³è¿‡é£ä¹¦é¢„è­¦")
        return

    content = f"""
âš ï¸ã€äººè¡Œç‰¹å¾ä¸€è‡´æ€§é¢„è­¦ã€‘âš ï¸
ğŸ‘¤ è´Ÿè´£äººï¼šå»–ç¦§æ—
ğŸ“… æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}
ğŸ“‰ ä¸€è‡´ç‡ä½äº99.99%çš„å˜é‡æ•°é‡ï¼š{low_count} ä¸ª
ğŸ“Œ è¯·ç«‹å³æ£€æŸ¥ï¼

è¯¦æƒ…è§é‚®ä»¶ã€‚
    """.strip()

    payload = {"msg_type": "text", "content": {"text": content}}
    try:
        resp = requests.post(FEISHU_WEBHOOK, data=json.dumps(payload), headers={'Content-Type': 'application/json'}, timeout=10)
        if resp.status_code == 200 and resp.json().get('code') == 0:
            logging.info("ğŸš€ é£ä¹¦é¢„è­¦å‘é€æˆåŠŸ")
        else:
            logging.error(f"âŒ é£ä¹¦å‘é€å¤±è´¥: {resp.text}")
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦å‘é€å¼‚å¸¸: {e}")


# =============================
# 8. ä¸»å‡½æ•°
# =============================
def main():
    logging.info("========== äººè¡Œç‰¹å¾ä¸€è‡´æ€§ç›‘æ§ä»»åŠ¡å¯åŠ¨ ==========")

    try:
        # 1. è¯»å–ç›‘æ§å˜é‡å
        monitor_vars = read_variables_from_txt(VARS_FILE)

        # 2. è·å–æ•°æ®
        df_online_raw = get_data(SQL_ONLINE)
        
        df_offline = get_data(SQL_OFFLINE)

        logging.info(f"çº¿ä¸Šæ•°æ®è¡Œæ•°: {len(df_online_raw)}")
        logging.info(f"çº¿ä¸‹æ•°æ®è¡Œæ•°: {len(df_offline)}")

        # 3. è§£æ value_006
        df_online_parsed = parse_value_006(df_online_raw)

        # 4. è®¡ç®—ä¸€è‡´æ€§       
        consistency_df, inconsistent_df = calculate_consistency(df_online_parsed, df_offline, monitor_vars)
        
        # 5. å‘é€é€šçŸ¥
        send_email(consistency_df, inconsistent_df)
        send_feishu_alert(consistency_df)

        logging.info("âœ… äººè¡Œç‰¹å¾ä¸€è‡´æ€§ç›‘æ§ä»»åŠ¡å®Œæˆ")

    except Exception as e:
        logging.critical(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        raise

# =============================
# 9. è¿è¡Œ
# =============================
if __name__ == '__main__':
    main()


#==============================================================================
# File: åˆ†ç®±åç”»å›¾.py
#==============================================================================


# è°ƒç”¨å‡½æ•°ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
def plot_stacked_bar(df, var, month_col, bins, values, filename=None):
    # å‡è®¾dfæ˜¯ä¸€ä¸ªDataFrameï¼ŒåŒ…å«æ‚¨çš„æ•°æ®
    # month_col æ˜¯æœˆä»½åˆ—
    # bins æ˜¯åˆ†ç®±åˆ—
    # values æ˜¯è¦ç»˜åˆ¶çš„å€¼åˆ—
    # åˆ›å»ºä¸€ä¸ªé€è§†è¡¨
    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values, fill_value=0)

    # åˆå§‹åŒ–å›¾å½¢
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # è·å–æ‰€æœ‰çš„åˆ†ç®±ç±»åˆ«
    bins_list = pivot_df.columns.tolist()
    
    # è®¡ç®—æ¯ä¸ªæŸ±å­çš„å®½åº¦
    bar_width = 0.8
    
    # è®¡ç®—xè½´ä¸Šçš„ä½ç½®
    x_pos = range(len(pivot_df.index))
    
    # åˆå§‹åŒ–åº•éƒ¨
    bottom = [0] * len(x_pos)
    
    # éå†æ¯ä¸ªåˆ†ç®±ï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾
    for bin in bins_list:
        # ä½¿ç”¨fillna(0)å¤„ç†NaNå€¼
        pivot_df[bin] = pivot_df[bin].fillna(0)
        
        ax.bar(x_pos,
               pivot_df[bin],
               width=bar_width,
               label=bin,
               bottom=bottom,
               align='center',
               alpha=0.8)
        
        # æ›´æ–°åº•éƒ¨çš„å€¼
        bottom = [b + v for b, v in zip(bottom, pivot_df[bin])]
    
    # è®¾ç½®å›¾å½¢å±æ€§
    ax.set_title(f'{values}â€”â€”{var}')
    # ax.set_xlabel('Month')
    ax.set_ylabel(f'{values}')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(pivot_df.index)
    ax.legend()

    plt.grid(axis='y', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show() 
    
# è°ƒç”¨å‡½æ•°ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾
def draw_time_series(df, var, month_col, bins, values, filename=None):

    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values)
    
    # ç»˜åˆ¶æ—¶é—´åºåˆ—æŠ˜çº¿å›¾
    plt.figure(figsize=(14, 7))
    for bin in pivot_df.columns:
        i = list(pivot_df.index)
        j = list(pivot_df[bin])
        plt.plot(i, j, label=bin, marker='o')

    plt.title(f'{values}â€”â€”{var}')
    # plt.xlabel('Month')
    plt.ylabel(f'{values}')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show()


def draw_line_bar(df, col, bin, var1, var2, var3, filename=None):
    # å¤„ç†æ•°æ®
    bins = df[bin]
    varsname = df[col].unique()[0]
    values1 = list(df[var1].fillna(0))
    values2 = list(df[var2].fillna(0))
    values3 = df[var3].unique()[0]

    # åˆ›å»ºå›¾å½¢å’Œä¸»è½´
    fig, ax1 = plt.subplots()

    # ä¸»çºµè½´ - æŸ±çŠ¶å›¾ (åˆ†ç®±å æ¯”)
    color = 'tab:blue'
    ax1.set_xlabel(f'{varsname}')
    ax1.set_ylabel(f'{var1}', color=color)
    bars = ax1.bar(bins, values1, color=color)
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        yval = bar.get_height()
        # va='bottom' to place label below the bar
        ax1.text(bar.get_x() + bar.get_width()/2.0, yval, f'{round(yval, 3)}', ha='center', va='top') 
    
    ax1.tick_params(axis='y', labelcolor=color, rotation=45)

    # å‰¯çºµè½´ - æŠ˜çº¿å›¾ (åå æ¯”)
    ax2 = ax1.twinx()  # åˆ›å»ºç¬¬äºŒä¸ªçºµåæ ‡è½´
    color = 'tab:red'
    ax2.set_ylabel(f'{var2}', color=color)  # è®¾ç½®æ ‡ç­¾é¢œè‰²
    _ = ax2.plot(values2, color=color, marker='o')  # ç»˜åˆ¶æŠ˜çº¿å›¾
    # åœ¨æŠ˜çº¿å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for xtick, txt in zip(ax1.get_xticks(), values2):
        ax2.text(xtick, txt, f'{round(txt, 3)}', ha='center', va='top', rotation=45)
    ax2.tick_params(axis='y', labelcolor=color)

    # è·å–xè½´çš„åˆ»åº¦ä½ç½®
    xtick_positions = range(len(bins))
    # è®¾ç½®Xè½´æ ‡ç­¾è‡ªåŠ¨è°ƒæ•´
    ax1.set_xticks(xtick_positions)
    ax1.set_xticklabels(bins, rotation=45, ha='right')
    
    # è®¾ç½®æ ‡é¢˜å’Œç½‘æ ¼
    ax1.set_title(f'{varsname}')
    ax1.grid(False)

    # åœ¨å·¦ä¸Šè§’æ·»åŠ æ–‡æœ¬
    ax1.text(0.05, 0.95, f'IV value:{round(values3,3)}', transform=ax1.transAxes, verticalalignment='top')
    # è°ƒæ•´å¸ƒå±€
    # fig.tight_layout()
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()


#==============================================================================
# File: å˜é‡woeç¦»æ•£åŒ–.py
#==============================================================================


# coding: utf-8

# In[ ]:


# å˜é‡woeç¦»æ•£åŒ–

# å˜é‡woeç»“æœè¡¨
def woe_df_concat(bin_df):
    """
    bin_df:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    
    return :woeç»“æœè¡¨
    """
    woe_df_list =[]
    for df in bin_df:
        woe_df = df.reset_index().assign(col=df.index.name).rename(columns={df.index.name:'bin'})
        woe_df_list.append(woe_df)
    woe_result = pd.concat(woe_df_list,axis=0)
    # ä¸ºäº†ä¾¿äºæŸ¥çœ‹ï¼Œå°†å­—æ®µååˆ—ç§»åˆ°ç¬¬ä¸€åˆ—çš„ä½ç½®ä¸Š
    woe_result1 = woe_result['col']
    woe_result2 = woe_result.iloc[:,:-1]
    woe_result_df = pd.concat([woe_result1,woe_result2],axis=1)
    woe_result_df = woe_result_df.reset_index(drop=True)
    return woe_result_df

# woeè½¬æ¢
def woe_transform(df,target,df_woe):
    """
    df:æ•°æ®é›†
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    df_woe:woeç»“æœè¡¨
    
    return:woeè½¬åŒ–ä¹‹åçš„æ•°æ®é›†
    """
    df2 = df.copy()
    for col in df2.drop([target],axis=1).columns:
        x = df2[col]
        bin_map = df_woe[df_woe.col==col]
        bin_res = np.array([0]*x.shape[0],dtype=float)
        for i in bin_map.index:
            lower = bin_map['min_bin'][i]
            upper = bin_map['max_bin'][i]
            if lower == upper:
                x1 = x[np.where(x == lower)[0]]
            else:
                x1 = x[np.where((x>=lower)&(x<=upper))[0]]
            mask = np.in1d(x,x1)
            bin_res[mask] = bin_map['woe'][i]
        bin_res = pd.Series(bin_res,index=x.index)
        bin_res.name = x.name
        df2[col] = bin_res
    return df2




#==============================================================================
# File: å˜é‡åˆ†ç®±.py
#==============================================================================


# coding: utf-8

# In[ ]:


# å˜é‡åˆ†ç®±

# ç±»åˆ«æ€§å˜é‡çš„åˆ†ç®± 
def binning_cate(df,col_list,target):
    """
    df:æ•°æ®é›†
    col_list:å˜é‡listé›†åˆ
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    
    return: 
    bin_df :listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    iv_value:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„IVå€¼
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total-bad
    all_odds = good*1.0/bad
    bin_df =[]
    iv_value=[]
    for col in col_list:
        d1 = df.groupby([col],as_index=True)
        d2 = pd.DataFrame()
        d2['min_bin'] = d1[col].min()
        d2['max_bin'] = d1[col].max()
        d2['total'] = d1[target].count()
        d2['totalrate'] = d2['total']/total
        d2['bad'] = d1[target].sum()
        d2['badrate'] = d2['bad']/d2['total']
        d2['good'] = d2['total'] - d2['bad']
        d2['goodrate'] = d2['good']/d2['total']
        d2['badattr'] = d2['bad']/bad
        d2['goodattr'] = (d2['total']-d2['bad'])/good
        d2['odds'] = d2['good']/d2['bad']
        GB_list=[]
        for i in d2.odds:
            if i>=all_odds:
                GB_index = str(round((i/all_odds)*100,0))+str('G')
            else:
                GB_index = str(round((all_odds/i)*100,0))+str('B')
            GB_list.append(GB_index)
        d2['GB_index'] = GB_list
        d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
        d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
        d2['IV'] = d2['bin_iv'].sum()
        iv = d2['bin_iv'].sum().round(3)
        print('å˜é‡å:{}'.format(col))
        print('IV:{}'.format(iv))
        print('\t')
        bin_df.append(d2)
        iv_value.append(iv)
    return bin_df,iv_value


# ç±»åˆ«æ€§å˜é‡ivçš„æ˜ç»†è¡¨
def iv_cate(df,col_list,target):
    """
    df:æ•°æ®é›†
    col_list:å˜é‡listé›†åˆ
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    
    return:å˜é‡çš„ivæ˜ç»†è¡¨
    """
    bin_df,iv_value = binning_cate(df,col_list,target)
    iv_df = pd.DataFrame({'col':col_list,
                          'iv':iv_value})
    iv_df = iv_df.sort_values('iv',ascending=False)
    return iv_df


# æ•°å€¼å‹å˜é‡çš„åˆ†ç®± 

# å…ˆç”¨å¡æ–¹åˆ†ç®±è¾“å‡ºå˜é‡çš„åˆ†å‰²ç‚¹
def split_data(df,col,split_num):
    """
    df: åŸå§‹æ•°æ®é›†
    col:éœ€è¦åˆ†ç®±çš„å˜é‡
    split_num:åˆ†å‰²ç‚¹çš„æ•°é‡
    """
    df2 = df.copy()
    count = df2.shape[0] # æ€»æ ·æœ¬æ•°
    n = math.floor(count/split_num) # æŒ‰ç…§åˆ†å‰²ç‚¹æ•°ç›®ç­‰åˆ†åæ¯ç»„çš„æ ·æœ¬æ•°
    split_index = [i*n for i in range(1,split_num)] # åˆ†å‰²ç‚¹çš„ç´¢å¼•
    values = sorted(list(df2[col])) # å¯¹å˜é‡çš„å€¼ä»å°åˆ°å¤§è¿›è¡Œæ’åº
    split_value = [values[i] for i in split_index] # åˆ†å‰²ç‚¹å¯¹åº”çš„value
    split_value = sorted(list(set(split_value))) # åˆ†å‰²ç‚¹çš„valueå»é‡æ’åº
    return split_value

def assign_group(x,split_bin):
    """
    x:å˜é‡çš„value
    split_bin:split_dataå¾—å‡ºçš„åˆ†å‰²ç‚¹list
    """
    n = len(split_bin)
    if x<=min(split_bin):   
        return min(split_bin) # å¦‚æœxå°äºåˆ†å‰²ç‚¹çš„æœ€å°å€¼ï¼Œåˆ™xæ˜ å°„ä¸ºåˆ†å‰²ç‚¹çš„æœ€å°å€¼
    elif x>max(split_bin): # å¦‚æœxå¤§äºåˆ†å‰²ç‚¹çš„æœ€å¤§å€¼ï¼Œåˆ™xæ˜ å°„ä¸ºåˆ†å‰²ç‚¹çš„æœ€å¤§å€¼
        return 10e10
    else:
        for i in range(n-1):
            if split_bin[i]<x<=split_bin[i+1]:# å¦‚æœxåœ¨ä¸¤ä¸ªåˆ†å‰²ç‚¹ä¹‹é—´ï¼Œåˆ™xæ˜ å°„ä¸ºåˆ†å‰²ç‚¹è¾ƒå¤§çš„å€¼
                return split_bin[i+1]

def bin_bad_rate(df,col,target,grantRateIndicator=0):
    """
    df:åŸå§‹æ•°æ®é›†
    col:åŸå§‹å˜é‡/å˜é‡æ˜ å°„åçš„å­—æ®µ
    target:ç›®æ ‡å˜é‡çš„å­—æ®µ
    grantRateIndicator:æ˜¯å¦è¾“å‡ºæ€»ä½“çš„è¿çº¦ç‡
    """
    total = df.groupby([col])[target].count()
    bad = df.groupby([col])[target].sum()
    total_df = pd.DataFrame({'total':total})
    bad_df = pd.DataFrame({'bad':bad})
    regroup = pd.merge(total_df,bad_df,left_index=True,right_index=True,how='left')
    regroup = regroup.reset_index()
    regroup['bad_rate'] = regroup['bad']/regroup['total']  # è®¡ç®—æ ¹æ®colåˆ†ç»„åæ¯ç»„çš„è¿çº¦ç‡
    dict_bad = dict(zip(regroup[col],regroup['bad_rate'])) # è½¬ä¸ºå­—å…¸å½¢å¼
    if grantRateIndicator==0:
        return (dict_bad,regroup)
    total_all= df.shape[0]
    bad_all = df[target].sum()
    all_bad_rate = bad_all/total_all # è®¡ç®—æ€»ä½“çš„è¿çº¦ç‡
    return (dict_bad,regroup,all_bad_rate)

def cal_chi2(df,all_bad_rate):
    """
    df:bin_bad_rateå¾—å‡ºçš„regroup
    all_bad_rate:bin_bad_rateå¾—å‡ºçš„æ€»ä½“è¿çº¦ç‡
    """
    df2 = df.copy()
    df2['expected'] = df2['total']*all_bad_rate # è®¡ç®—æ¯ç»„çš„åç”¨æˆ·æœŸæœ›æ•°é‡
    combined = zip(df2['expected'],df2['bad']) # éå†æ¯ç»„çš„åç”¨æˆ·æœŸæœ›æ•°é‡å’Œå®é™…æ•°é‡
    chi = [(i[0]-i[1])**2/i[0] for i in combined] # è®¡ç®—æ¯ç»„çš„å¡æ–¹å€¼
    chi2 = sum(chi) # è®¡ç®—æ€»çš„å¡æ–¹å€¼
    return chi2

def assign_bin(x,cutoffpoints):
    """
    x:å˜é‡çš„value
    cutoffpoints:åˆ†ç®±çš„åˆ‡å‰²ç‚¹
    """
    bin_num = len(cutoffpoints)+1 # ç®±ä½“ä¸ªæ•°
    if x<=cutoffpoints[0]:  # å¦‚æœxå°äºæœ€å°çš„cutoffç‚¹ï¼Œåˆ™æ˜ å°„ä¸ºBin 0
        return 'Bin 0'
    elif x>cutoffpoints[-1]: # å¦‚æœxå¤§äºæœ€å¤§çš„cutoffç‚¹ï¼Œåˆ™æ˜ å°„ä¸ºBin(bin_num-1)
        return 'Bin {}'.format(bin_num-1)
    else:
        for i in range(0,bin_num-1):
            if cutoffpoints[i]<x<=cutoffpoints[i+1]: # å¦‚æœxåœ¨ä¸¤ä¸ªcutoffç‚¹ä¹‹é—´ï¼Œåˆ™xæ˜ å°„ä¸ºBin(i+1)
                return 'Bin {}'.format(i+1)

def ChiMerge(df,col,target,max_bin=5,min_binpct=0):
    col_unique = sorted(list(set(df[col]))) # å˜é‡çš„å”¯ä¸€å€¼å¹¶æ’åº
    n = len(col_unique) # å˜é‡å”¯ä¸€å€¼å¾—ä¸ªæ•°
    df2 = df.copy()
    if n>100:  # å¦‚æœå˜é‡çš„å”¯ä¸€å€¼æ•°ç›®è¶…è¿‡100ï¼Œåˆ™å°†é€šè¿‡split_dataå’Œassign_groupå°†xæ˜ å°„ä¸ºsplitå¯¹åº”çš„value
        split_col = split_data(df2,col,100)  # é€šè¿‡è¿™ä¸ªç›®çš„å°†å˜é‡çš„å”¯ä¸€å€¼æ•°ç›®äººä¸ºè®¾å®šä¸º100
        df2['col_map'] = df2[col].map(lambda x:assign_group(x,split_col))
    else:
        df2['col_map'] = df2[col]  # å˜é‡çš„å”¯ä¸€å€¼æ•°ç›®æ²¡æœ‰è¶…è¿‡100ï¼Œåˆ™ä¸ç”¨åšæ˜ å°„
    # ç”Ÿæˆdict_bad,regroup,all_bad_rateçš„å…ƒç»„
    (dict_bad,regroup,all_bad_rate) = bin_bad_rate(df2,'col_map',target,grantRateIndicator=1)
    col_map_unique = sorted(list(set(df2['col_map'])))  # å¯¹å˜é‡æ˜ å°„åçš„valueè¿›è¡Œå»é‡æ’åº
    group_interval = [[i] for i in col_map_unique]  # å¯¹col_map_uniqueä¸­æ¯ä¸ªå€¼åˆ›å»ºlistå¹¶å­˜å‚¨åœ¨group_intervalä¸­
    
    while (len(group_interval)>max_bin): # å½“group_intervalçš„é•¿åº¦å¤§äºmax_binæ—¶ï¼Œæ‰§è¡Œwhileå¾ªç¯
        chi_list=[]
        for i in range(len(group_interval)-1):
            temp_group = group_interval[i]+group_interval[i+1] # temp_group ä¸ºç”Ÿæˆçš„åŒºé—´,listå½¢å¼ï¼Œä¾‹å¦‚[1,3]
            chi_df = regroup[regroup['col_map'].isin(temp_group)]
            chi_value = cal_chi2(chi_df,all_bad_rate) # è®¡ç®—æ¯ä¸€å¯¹ç›¸é‚»åŒºé—´çš„å¡æ–¹å€¼
            chi_list.append(chi_value)
        best_combined = chi_list.index(min(chi_list)) # æœ€å°çš„å¡æ–¹å€¼çš„ç´¢å¼•
        # å°†å¡æ–¹å€¼æœ€å°çš„ä¸€å¯¹åŒºé—´è¿›è¡Œåˆå¹¶
        group_interval[best_combined] = group_interval[best_combined]+group_interval[best_combined+1]
        # åˆ é™¤åˆå¹¶å‰çš„å³åŒºé—´
        group_interval.remove(group_interval[best_combined+1])
        # å¯¹åˆå¹¶åæ¯ä¸ªåŒºé—´è¿›è¡Œæ’åº
    group_interval = [sorted(i) for i in group_interval]
    # cutoffç‚¹ä¸ºæ¯ä¸ªåŒºé—´çš„æœ€å¤§å€¼
    cutoffpoints = [max(i) for i in group_interval[:-1]]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç®±åªæœ‰å¥½æ ·æœ¬æˆ–è€…åªæœ‰åæ ·æœ¬
    df2['col_map_bin'] = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints)) # å°†col_mapæ˜ å°„ä¸ºå¯¹åº”çš„åŒºé—´Bin
    # è®¡ç®—æ¯ä¸ªåŒºé—´çš„è¿çº¦ç‡
    (dict_bad,regroup) = bin_bad_rate(df2,'col_map_bin',target)
    # è®¡ç®—æœ€å°å’Œæœ€å¤§çš„è¿çº¦ç‡
    [min_bad_rate,max_bad_rate] = [min(dict_bad.values()),max(dict_bad.values())]
    # å½“æœ€å°çš„è¿çº¦ç‡ç­‰äº0ï¼Œè¯´æ˜åŒºé—´å†…åªæœ‰å¥½æ ·æœ¬ï¼Œå½“æœ€å¤§çš„è¿çº¦ç‡ç­‰äº1ï¼Œè¯´æ˜åŒºé—´å†…åªæœ‰åæ ·æœ¬
    while min_bad_rate==0 or max_bad_rate==1:
        bad01_index = regroup[regroup['bad_rate'].isin([0,1])].col_map_bin.tolist()# è¿çº¦ç‡ä¸º1æˆ–0çš„åŒºé—´
        bad01_bin = bad01_index[0]
        if bad01_bin==max(regroup.col_map_bin):
            cutoffpoints = cutoffpoints[:-1] # å½“bad01_binæ˜¯æœ€å¤§çš„åŒºé—´æ—¶ï¼Œåˆ é™¤æœ€å¤§çš„cutoffç‚¹
        elif bad01_bin==min(regroup.col_map_bin):
            cutoffpoints = cutoffpoints[1:] # å½“bad01_binæ˜¯æœ€å°çš„åŒºé—´æ—¶ï¼Œåˆ é™¤æœ€å°çš„cutoffç‚¹
        else:
            bad01_bin_index = list(regroup.col_map_bin).index(bad01_bin) # æ‰¾å‡ºbad01_binçš„ç´¢å¼•
            prev_bin = list(regroup.col_map_bin)[bad01_bin_index-1] # bad01_binå‰ä¸€ä¸ªåŒºé—´
            df3 = df2[df2.col_map_bin.isin([prev_bin,bad01_bin])] 
            (dict_bad,regroup1) = bin_bad_rate(df3,'col_map_bin',target)
            chi1 = cal_chi2(regroup1,all_bad_rate)  # è®¡ç®—å‰ä¸€ä¸ªåŒºé—´å’Œbad01_binçš„å¡æ–¹å€¼
            later_bin = list(regroup.col_map_bin)[bad01_bin_index+1] # bin01_binçš„åä¸€ä¸ªåŒºé—´
            df4 = df2[df2.col_map_bin.isin([later_bin,bad01_bin])] 
            (dict_bad,regroup2) = bin_bad_rate(df4,'col_map_bin',target)
            chi2 = cal_chi2(regroup2,all_bad_rate) # è®¡ç®—åä¸€ä¸ªåŒºé—´å’Œbad01_binçš„å¡æ–¹å€¼
            if chi1<chi2:  # å½“chi1<chi2æ—¶,åˆ é™¤å‰ä¸€ä¸ªåŒºé—´å¯¹åº”çš„cutoffç‚¹
                cutoffpoints.remove(cutoffpoints[bad01_bin_index-1])
            else:  # å½“chi1>=chi2æ—¶,åˆ é™¤bin01å¯¹åº”çš„cutoffç‚¹
                cutoffpoints.remove(cutoffpoints[bad01_bin_index])
        df2['col_map_bin'] = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints))
        (dict_bad,regroup) = bin_bad_rate(df2,'col_map_bin',target)
        # é‡æ–°å°†col_mapæ˜ å°„è‡³åŒºé—´ï¼Œå¹¶è®¡ç®—æœ€å°å’Œæœ€å¤§çš„è¿çº¦ç‡ï¼Œç›´è¾¾ä¸å†å‡ºç°è¿çº¦ç‡ä¸º0æˆ–1çš„æƒ…å†µï¼Œå¾ªç¯åœæ­¢
        [min_bad_rate,max_bad_rate] = [min(dict_bad.values()),max(dict_bad.values())]
    
    # æ£€æŸ¥åˆ†ç®±åçš„æœ€å°å æ¯”
    if min_binpct>0:
        group_values = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints))
        df2['col_map_bin'] = group_values # å°†col_mapæ˜ å°„ä¸ºå¯¹åº”çš„åŒºé—´Bin
        group_df = group_values.value_counts().to_frame() 
        group_df['bin_pct'] = group_df['col_map']/n # è®¡ç®—æ¯ä¸ªåŒºé—´çš„å æ¯”
        min_pct = group_df.bin_pct.min() # å¾—å‡ºæœ€å°çš„åŒºé—´å æ¯”
        while min_pct<min_binpct and len(cutoffpoints)>2: # å½“æœ€å°çš„åŒºé—´å æ¯”å°äºmin_pctä¸”cutoffç‚¹çš„ä¸ªæ•°å¤§äº2ï¼Œæ‰§è¡Œå¾ªç¯
            # ä¸‹é¢çš„é€»è¾‘åŸºæœ¬ä¸â€œæ£€éªŒæ˜¯å¦æœ‰ç®±ä½“åªæœ‰å¥½/åæ ·æœ¬â€çš„ä¸€è‡´
            min_pct_index = group_df[group_df.bin_pct==min_pct].index.tolist()
            min_pct_bin = min_pct_index[0]
            if min_pct_bin == max(group_df.index):
                cutoffpoints=cutoffpoints[:-1]
            elif min_pct_bin == min(group_df.index):
                cutoffpoints=cutoffpoints[1:]
            else:
                minpct_bin_index = list(group_df.index).index(min_pct_bin)
                prev_pct_bin = list(group_df.index)[minpct_bin_index-1]
                df5 = df2[df2['col_map_bin'].isin([min_pct_bin,prev_pct_bin])]
                (dict_bad,regroup3) = bin_bad_rate(df5,'col_map_bin',target)
                chi3 = cal_chi2(regroup3,all_bad_rate)
                later_pct_bin = list(group_df.index)[minpct_bin_index+1]
                df6 = df2[df2['col_map_bin'].isin([min_pct_bin,later_pct_bin])]
                (dict_bad,regroup4) = bin_bad_rate(df6,'col_map_bin',target)
                chi4 = cal_chi2(regroup4,all_bad_rate)
                if chi3<chi4:
                    cutoffpoints.remove(cutoffpoints[minpct_bin_index-1])
                else:
                    cutoffpoints.remove(cutoffpoints[minpct_bin_index])
    return cutoffpoints

# æ•°å€¼å‹å˜é‡çš„åˆ†ç®±ï¼ˆå¡æ–¹åˆ†ç®±ï¼‰
def binning_num(df,target,col_list,max_bin=None,min_binpct=None):
    """
    df:æ•°æ®é›†
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    col_list:å˜é‡listé›†åˆ
    max_bin:æœ€å¤§çš„åˆ†ç®±ä¸ªæ•°
    min_binpct:åŒºé—´å†…æ ·æœ¬æ‰€å æ€»ä½“çš„æœ€å°æ¯”
    
    return:
    bin_df :listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    iv_value:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„IVå€¼
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total-bad
    all_odds = good/bad
    inf = float('inf')
    ninf = float('-inf')
    bin_df=[]
    iv_value=[]
    for col in col_list:
        cut = ChiMerge(df,col,target,max_bin=max_bin,min_binpct=min_binpct)
        cut.insert(0,ninf)
        cut.append(inf)
        bucket = pd.cut(df[col],cut)
        d1 = df.groupby(bucket)
        d2 = pd.DataFrame()
        d2['min_bin'] = d1[col].min()
        d2['max_bin'] = d1[col].max()
        d2['total'] = d1[target].count()
        d2['totalrate'] = d2['total']/total
        d2['bad'] = d1[target].sum()
        d2['badrate'] = d2['bad']/d2['total']
        d2['good'] = d2['total'] - d2['bad']
        d2['goodrate'] = d2['good']/d2['total']
        d2['badattr'] = d2['bad']/bad
        d2['goodattr'] = (d2['total']-d2['bad'])/good
        d2['odds'] = d2['good']/d2['bad']
        GB_list=[]
        for i in d2.odds:
            if i>=all_odds:
                GB_index = str(round((i/all_odds)*100,0))+str('G')
            else:
                GB_index = str(round((all_odds/i)*100,0))+str('B')
            GB_list.append(GB_index)
        d2['GB_index'] = GB_list
        d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
        d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
        d2['IV'] = d2['bin_iv'].sum()
        iv = d2['bin_iv'].sum().round(3)
        print('å˜é‡å:{}'.format(col))
        print('IV:{}'.format(iv))
        print('\t')
        bin_df.append(d2)
        iv_value.append(iv)
    return bin_df,iv_value


# æ•°å€¼å‹å˜é‡çš„ivæ˜ç»†è¡¨
def iv_num(df,target,col_list,max_bin=None,min_binpct=None):
    """
    df:æ•°æ®é›†
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    col_list:å˜é‡listé›†åˆ
    max_bin:æœ€å¤§çš„åˆ†ç®±ä¸ªæ•°
    min_binpct:åŒºé—´å†…æ ·æœ¬æ‰€å æ€»ä½“çš„æœ€å°æ¯”
    
    return :å˜é‡çš„ivæ˜ç»†è¡¨
    """
    bin_df,iv_value = binning_num(df,target,col_list,max_bin=max_bin,min_binpct=min_binpct)
    iv_df = pd.DataFrame({'col':col_list,
                          'iv':iv_value})
    iv_df = iv_df.sort_values('iv',ascending=False)
    return iv_df


# è‡ªå®šä¹‰åˆ†ç®±
def binning_self(df,col,target,cut=None,right_border=True):
    """
    df: æ•°æ®é›†
    col:åˆ†ç®±çš„å•ä¸ªå˜é‡å
    cut:åˆ’åˆ†åŒºé—´çš„list
    right_borderï¼šè®¾å®šå·¦å¼€å³é—­ã€å·¦é—­å³å¼€
    
    return: 
    bin_df: dfå½¢å¼ï¼Œå•ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    iv_value: å•ä¸ªå˜é‡çš„iv
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total - bad
    all_odds = good/bad
    bucket = pd.cut(df[col],cut,right=right_border)
    d1 = df.groupby(bucket)
    d2 = pd.DataFrame()
    d2['min_bin'] = d1[col].min()
    d2['max_bin'] = d1[col].max()
    d2['total'] = d1[target].count()
    d2['totalrate'] = d2['total']/total
    d2['bad'] = d1[target].sum()
    d2['badrate'] = d2['bad']/d2['total']
    d2['good'] = d2['total'] - d2['bad']
    d2['goodrate'] = d2['good']/d2['total']
    d2['badattr'] = d2['bad']/bad
    d2['goodattr'] = (d2['total']-d2['bad'])/good
    d2['odds'] = d2['good']/d2['bad']
    GB_list=[]
    for i in d2.odds:
        if i>=all_odds:
            GB_index = str(round((i/all_odds)*100,0))+str('G')
        else:
            GB_index = str(round((all_odds/i)*100,0))+str('B')
        GB_list.append(GB_index)
    d2['GB_index'] = GB_list
    d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
    d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
    d2['IV'] = d2['bin_iv'].sum()
    iv_value = d2['bin_iv'].sum().round(3)
    print('å˜é‡å:{}'.format(col))
    print('IV:{}'.format(iv_value))
    bin_df = d2.copy()
    return bin_df,iv_value


# å˜é‡åˆ†ç®±ç»“æœçš„æ£€æŸ¥

# woeçš„å¯è§†åŒ–
def plot_woe(bin_df,hspace=0.4,wspace=0.4,plt_size=None,plt_num=None,x=None,y=None):
    """
    bin_df:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    hspace :å­å›¾ä¹‹é—´çš„é—´éš”(yè½´æ–¹å‘)
    wspace :å­å›¾ä¹‹é—´çš„é—´éš”(xè½´æ–¹å‘)
    plt_size :å›¾çº¸çš„å°ºå¯¸
    plt_num :å­å›¾çš„æ•°é‡
    x :å­å›¾çŸ©é˜µä¸­ä¸€è¡Œå­å›¾çš„æ•°é‡
    y :å­å›¾çŸ©é˜µä¸­ä¸€åˆ—å­å›¾çš„æ•°é‡
    
    return :æ¯ä¸ªå˜é‡çš„woeå˜åŒ–è¶‹åŠ¿å›¾
    """
    plt.figure(figsize=plt_size)
    plt.subplots_adjust(hspace=hspace,wspace=wspace)
    for i,df in zip(range(1,plt_num+1,1),bin_df):
        col_name = df.index.name
        df = df.reset_index()
        plt.subplot(x,y,i)
        plt.title(col_name)
        sns.barplot(data=df,x=col_name,y='woe')
        plt.xlabel('')
        plt.xticks(rotation=30)
    return plt.show()


# æ£€éªŒwoeæ˜¯å¦å•è°ƒ 
def woe_monoton(bin_df):
    """
    bin_df:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    
    return :
    woe_notmonoton_col :woeæ²¡æœ‰å‘ˆå•è°ƒå˜åŒ–çš„å˜é‡ï¼Œlistå½¢å¼
    woe_judge_df :dfå½¢å¼ï¼Œæ¯ä¸ªå˜é‡çš„æ£€éªŒç»“æœ
    """
    woe_notmonoton_col =[]
    col_list = []
    woe_judge=[]
    for woe_df in bin_df:
        col_name = woe_df.index.name
        woe_list = list(woe_df.woe)
        if woe_df.shape[0]==2:
            #print('{}æ˜¯å¦å•è°ƒ: True'.format(col_name))
            col_list.append(col_name)
            woe_judge.append('True')
        else:
            woe_not_monoton = [(woe_list[i]<woe_list[i+1] and woe_list[i]<woe_list[i-1])                                or (woe_list[i]>woe_list[i+1] and woe_list[i]>woe_list[i-1])                                for i in range(1,len(woe_list)-1,1)]
            if True in woe_not_monoton:
                #print('{}æ˜¯å¦å•è°ƒ: False'.format(col_name))
                woe_notmonoton_col.append(col_name)
                col_list.append(col_name)
                woe_judge.append('False')
            else:
                #print('{}æ˜¯å¦å•è°ƒ: True'.format(col_name))
                col_list.append(col_name)
                woe_judge.append('True')
    woe_judge_df = pd.DataFrame({'col':col_list,
                                 'judge_monoton':woe_judge})
    return woe_notmonoton_col,woe_judge_df


# æ£€æŸ¥æŸä¸ªåŒºé—´çš„woeæ˜¯å¦å¤§äº1
def woe_large(bin_df):
    """
    bin_df:listå½¢å¼ï¼Œé‡Œé¢å­˜å‚¨æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æœ
    
    return:
    woe_large_col: æŸä¸ªåŒºé—´woeå¤§äº1çš„å˜é‡ï¼Œlisté›†åˆ
    woe_judge_df :dfå½¢å¼ï¼Œæ¯ä¸ªå˜é‡çš„æ£€éªŒç»“æœ
    """
    woe_large_col=[]
    col_list =[]
    woe_judge =[]
    for woe_df in bin_df:
        col_name = woe_df.index.name
        woe_list = list(woe_df.woe)
        woe_large = list(filter(lambda x:x>=1,woe_list))
        if len(woe_large)>0:
            col_list.append(col_name)
            woe_judge.append('True')
            woe_large_col.append(col_name)
        else:
            col_list.append(col_name)
            woe_judge.append('False')
    woe_judge_df = pd.DataFrame({'col':col_list,
                                 'judge_large':woe_judge})
    return woe_large_col,woe_judge_df




#==============================================================================
# File: å˜é‡ç­›é€‰.py
#==============================================================================


# coding: utf-8

# In[ ]:


# å˜é‡ç­›é€‰ 

# xgboostç­›é€‰å˜é‡ 
def select_xgboost(df,target,imp_num=None):
    """
    df:æ•°æ®é›†
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    imp_num:ç­›é€‰å˜é‡çš„ä¸ªæ•°
    
    return:
    xg_fea_imp:å˜é‡çš„ç‰¹å¾é‡è¦æ€§
    xg_select_col:ç­›é€‰å‡ºçš„å˜é‡
    """
    x = df.drop([target],axis=1)
    y = df[target]
    xgmodel = XGBClassifier(random_state=0)
    xgmodel = xgmodel.fit(x,y,eval_metric='auc')
    xg_fea_imp = pd.DataFrame({'col':list(x.columns),
                               'imp':xgmodel.feature_importances_})
    xg_fea_imp = xg_fea_imp.sort_values('imp',ascending=False).reset_index(drop=True).iloc[:imp_num,:]
    xg_select_col = list(xg_fea_imp.col)
    return xg_fea_imp,xg_select_col


# éšæœºæ£®æ—ç­›é€‰å˜é‡ 
def select_rf(df,target,imp_num=None):
    """
    df:æ•°æ®é›†
    target:ç›®æ ‡å˜é‡çš„å­—æ®µå
    imp_num:ç­›é€‰å˜é‡çš„ä¸ªæ•°
    
    return:
    rf_fea_imp:å˜é‡çš„ç‰¹å¾é‡è¦æ€§
    rf_select_col:ç­›é€‰å‡ºçš„å˜é‡
    """
    x = df.drop([target],axis=1)
    y = df[target]
    rfmodel = RandomForestClassifier(random_state=0)
    rfmodel = rfmodel.fit(x,y)
    rf_fea_imp = pd.DataFrame({'col':list(x.columns),
                               'imp':rfmodel.feature_importances_})
    rf_fea_imp = rf_fea_imp.sort_values('imp',ascending=False).reset_index(drop=True).iloc[:imp_num,:]
    rf_select_col = list(rf_fea_imp.col)
    return rf_fea_imp,rf_select_col


# ç›¸å…³æ€§å¯è§†åŒ–
def plot_corr(df,col_list,threshold=None,plt_size=None,is_annot=True):
    """
    df:æ•°æ®é›†
    col_list:å˜é‡listé›†åˆ
    threshold: ç›¸å…³æ€§è®¾å®šçš„é˜ˆå€¼
    plt_size:å›¾çº¸å°ºå¯¸
    is_annot:æ˜¯å¦æ˜¾ç¤ºç›¸å…³ç³»æ•°å€¼
    
    return :ç›¸å…³æ€§çƒ­åŠ›å›¾
    """
    corr_df = df.loc[:,col_list].corr()
    plt.figure(figsize=plt_size)
    sns.heatmap(corr_df,annot=is_annot,cmap='rainbow',vmax=1,vmin=-1,mask=np.abs(corr_df)<=threshold)
    return plt.show()


# ç›¸å…³æ€§å‰”é™¤
def forward_delete_corr(df,col_list,threshold=None):
    """
    df:æ•°æ®é›†
    col_list:å˜é‡listé›†åˆ
    threshold: ç›¸å…³æ€§è®¾å®šçš„é˜ˆå€¼
    
    return:ç›¸å…³æ€§å‰”é™¤åçš„å˜é‡
    """
    list_corr = col_list[:]
    for col in list_corr:
        corr = df.loc[:,list_corr].corr()[col]
        corr_index= [x for x in corr.index if x!=col]
        corr_values  = [x for x in corr.values if x!=1]
        for i,j in zip(corr_index,corr_values):
            if abs(j)>=threshold:
                list_corr.remove(i)
    return list_corr


# ç›¸å…³æ€§å˜é‡æ˜ å°„å…³ç³» 
def corr_mapping(df,col_list,threshold=None):
    """
    df:æ•°æ®é›†
    col_list:å˜é‡listé›†åˆ
    threshold: ç›¸å…³æ€§è®¾å®šçš„é˜ˆå€¼
    
    return:å¼ºç›¸å…³æ€§å˜é‡ä¹‹é—´çš„æ˜ å°„å…³ç³»è¡¨
    """
    corr_df = df.loc[:,col_list].corr()
    col_a = []
    col_b = []
    corr_value = []
    for col,i in zip(col_list[:-1],range(1,len(col_list),1)):
        high_corr_col=[]
        high_corr_value=[]
        corr_series = corr_df[col][i:]
        for i,j in zip(corr_series.index,corr_series.values):
            if abs(j)>=threshold:
                high_corr_col.append(i)
                high_corr_value.append(j)
        col_a.extend([col]*len(high_corr_col))
        col_b.extend(high_corr_col)
        corr_value.extend(high_corr_value)

    corr_map_df = pd.DataFrame({'col_A':col_a,
                                'col_B':col_b,
                                'corr':corr_value})
    return corr_map_df


# æ˜¾è‘—æ€§ç­›é€‰,åœ¨ç­›é€‰å‰éœ€è¦åšwoeè½¬æ¢
def forward_delete_pvalue(x_train,y_train):
    """
    x_train -- xè®­ç»ƒé›†
    y_train -- yè®­ç»ƒé›†
    
    return :æ˜¾è‘—æ€§ç­›é€‰åçš„å˜é‡
    """
    col_list = list(x_train.columns)
    pvalues_col=[]
    for col in col_list:
        pvalues_col.append(col)
        x_train2 = sm.add_constant(x_train.loc[:,pvalues_col])
        sm_lr = sm.Logit(y_train,x_train2)
        sm_lr = sm_lr.fit()
        for i,j in zip(sm_lr.pvalues.index[1:],sm_lr.pvalues.values[1:]): 
            if j>=0.05:
                pvalues_col.remove(i)
    
    x_new_train = x_train.loc[:,pvalues_col]
    x_new_train2 = sm.add_constant(x_new_train)
    lr = sm.Logit(y_train,x_new_train2)
    lr = lr.fit()
    print(lr.summary2())
    return pvalues_col


# é€»è¾‘å›å½’ç³»æ•°ç¬¦å·ç­›é€‰,åœ¨ç­›é€‰å‰éœ€è¦åšwoeè½¬æ¢
def forward_delete_coef(x_train,y_train):
    """
    x_train -- xè®­ç»ƒé›†
    y_train -- yè®­ç»ƒé›†
    
    return :
    coef_colå›å½’ç³»æ•°ç¬¦å·ç­›é€‰åçš„å˜é‡
    lr_coeï¼šæ¯ä¸ªå˜é‡çš„ç³»æ•°å€¼
    """
    col_list = list(x_train.columns)
    coef_col = []
    for col in col_list:
        coef_col.append(col)
        x_train2 = x_train.loc[:,coef_col]
        sk_lr = LogisticRegression(random_state=0).fit(x_train2,y_train)
        coef_df = pd.DataFrame({'col':coef_col,'coef':sk_lr.coef_[0]})
        if coef_df[coef_df.coef<0].shape[0]>0:
            coef_col.remove(col)
    
    x_new_train = x_train.loc[:,coef_col]
    lr = LogisticRegression(random_state=0).fit(x_new_train,y_train)
    lr_coe = pd.DataFrame({'col':coef_col,
                           'coef':lr.coef_[0]})
    return coef_col,lr_coe




#==============================================================================
# File: å¢ç›Šè¯„ä¼°.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'å¢ç›Šè¯„ä¼°'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./å¢ç›Šè¯„ä¼°'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. æ•°æ®è¯»å–

# In[ ]:


print(result_path)


# In[ ]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data


# In[4]:


df1 = pd.read_csv('å‰ç­›å®æ—¶æ¨¡å‹250919_ys.csv')
df1.info(show_counts=True)
df1.head()


# In[5]:


df2 = pd.read_csv('å‰ç­›å®æ—¶æ¨¡å‹250906.csv')
df2.info(show_counts=True)
df2.head()


# In[6]:


df_sample_ = pd.merge(df2, df1[['order_no','m1b0070','m1b0077']], how='left',on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[7]:


varsname = ['id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2', 'm1b0070', 'm1b0071', 'm1b0074', 'm1b0075', 'm1b0077', 'umeng_sdk_score', 'tianchuang_score', 'fico_model', 'haina_model']
print(len(varsname))
print(varsname)


# In[8]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[9]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[10]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[11]:


df_sample = df_sample_.copy()
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[12]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[13]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-14'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-02-15' & apply_date<='2025-02-28'").index, 'data_set']='2_test'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-09'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-10' & apply_date<='2025-04-15'").index, 'data_set']='3_oot2'


# In[14]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[87]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[ ]:


print(df_sample[target].value_counts())


# In[ ]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[ ]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[ ]:


# # 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[ ]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[ ]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[ ]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set)


# In[ ]:



def calculate_iv_by_group(df, 
                          group_col, 
                          target, 
                          variables,
                          is_return_unique=False,
                          method='quantile', 
                          n_bins=10):
    """
    æŒ‰åˆ†ç»„å­—æ®µè®¡ç®—æ¯ä¸ªå˜é‡åœ¨å„ç»„ä¸­çš„ IV å’Œ unique å€¼
    è¾“å‡ºæ ¼å¼ï¼šæ‰€æœ‰ IV åˆ—åœ¨å‰ï¼Œæ‰€æœ‰ UNIQUE åˆ—åœ¨å
    
    Parameters:
    -----------
    df : pd.DataFrame
        åŸå§‹æ•°æ®
    group_col : str
        åˆ†ç»„åˆ—åï¼ˆå¦‚ 'sample_type', 'year_month' ç­‰ï¼‰
    target : str
        ç›®æ ‡å˜é‡åˆ—å
    variables : list
        è¦åˆ†æçš„å˜é‡ååˆ—è¡¨
    method : str
        toad.quality çš„åˆ†ç®±æ–¹æ³•
    n_bins : int
        åˆ†ç®±æ•°é‡
    
    Returns:
    --------
    pd.DataFrame
        ç´¢å¼•: variable
        åˆ—:  {group1}_iv, {group2}_iv, ..., {group1}_unique, {group2}_unique, ...
    """
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = [group_col, target] + variables
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame.")
    
    # å­˜å‚¨æ¯ç»„çš„ç»“æœ
    iv_data = {}   # å­˜å‚¨æ‰€æœ‰ iv åˆ—
    unique_data = {}  # å­˜å‚¨æ‰€æœ‰ unique åˆ—
    
    # æŒ‰ group_col åˆ†ç»„
    for group_name, group_df in df.groupby(group_col):
        data = group_df[variables + [target]].copy()
        
        if data.empty:
            print(f"Warning: No data in group '{group_name}'")
            continue
        
        try:
            # ä½¿ç”¨ toad è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality = toad.quality(
                data,
                target=target,
                iv_only=True,
                method=method,
                n_bins=n_bins
            )
            
            # åªä¿ç•™ iv å’Œ unique
            cols_to_keep = ['iv', 'unique']
            existing_cols = [c for c in cols_to_keep if c in quality.columns]
            quality = quality[existing_cols]
            
            # åˆ†å¼€å­˜å‚¨
            if 'iv' in quality.columns:
                iv_data[f"{group_name}_iv"] = quality['iv']
            
            if 'unique' in quality.columns:
                unique_data[f"{group_name}_unique"] = quality['unique']
                
        except Exception as e:
            print(f"Error processing group '{group_name}': {str(e)}")
            continue
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not iv_data and not unique_data:
        raise ValueError("No valid group data processed.")
    
    # è½¬æ¢ä¸º DataFrame
    df_iv = pd.DataFrame(iv_data)
    df_unique = pd.DataFrame(unique_data)
    
    # åˆå¹¶ï¼šIV åœ¨å‰ï¼ŒUnique åœ¨å
    if is_return_unique:
        result = pd.concat([df_iv, df_unique], axis=1)
    else:
        result = df_iv.copy()
    
    # å¡«å……ç¼ºå¤±å€¼
    result = result.fillna(value=pd.NA)
    
    # è®¾ç½®ç´¢å¼•å
    result.index.name = 'variable'
    
    return result


# In[ ]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='quantile', n_bins=10)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[ ]:



df_iv = calculate_iv_by_group(
    df=df_sample,
    group_col='data_set',
    target=target,
    variables=varsname
)
df_iv


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[ ]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(to_drop2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[ ]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[ ]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[ ]:


target


# In[ ]:


varsname_v2 = varsname[:]


# In[ ]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[ ]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[ ]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[ ]:


new_bins_dict


# In[ ]:


combiner.load(new_bins_dict)


# In[ ]:


combiner.export()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[ ]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[ ]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[ ]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[ ]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[ ]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[ ]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[15]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24åˆ©ç‡'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[16]:


from itertools import combinations

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # ä½¿ç”¨å½“å‰æœ€ä½³ç‰¹å¾é›†è®­ç»ƒæ¨¡å‹
        # âœ… æ„å»ºå½“å‰ç‰¹å¾å­é›†çš„æ•°æ®
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # å¦‚æœæ˜¯ numpy arrayï¼Œç”¨ä½ç½®ç´¢å¼•
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # æ›´æ–°æœ€ä½³ç‰¹å¾é›†
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'å‹ç›Ÿè”åˆå»ºæ¨¡_v6_20250717140214.bin')  # è‡ªåŠ¨è¯†åˆ« .txt/.bin/.json


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[ ]:


df_sample[target] = pd.to_numeric(df_sample[target])


# In[17]:


df_sample[target].value_counts()


# In[18]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[19]:


df_sample[modeltrian_target].value_counts()


# In[20]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[ ]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[21]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[22]:


df_sample['channel_types'].value_counts()


# In[23]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[ ]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[ ]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.07
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 80


# In[ ]:



### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[ ]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[ ]:


varsname


# In[33]:


varsname_base = ['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0074',
 'm1b0075',
 'm1b0077']


# In[34]:


print(len(varsname_base))
print(varsname_base)


# In[ ]:


def model_train(data, selected_vars, target, opt_params):
    X_train = data[data['data_set']=='1_train'][selected_vars]
    y_train = data[data['data_set']=='1_train'][target]
    X_test = data[data['data_set']=='2_test'][selected_vars]
    y_test = data[data['data_set']=='2_test'][target]      
    
    best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, opt_params)
    
    dtrain = lgb.Dataset(X_train[best_features], label=y_train)
    dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)
    lgb_model = lgb.train(opt_params, dtrain, valid_sets=dtest, num_boost_round=10000)
    
    return lgb_model

from itertools import combinations

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # ä½¿ç”¨å½“å‰æœ€ä½³ç‰¹å¾é›†è®­ç»ƒæ¨¡å‹
        # âœ… æ„å»ºå½“å‰ç‰¹å¾å­é›†çš„æ•°æ®
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # å¦‚æœæ˜¯ numpy arrayï¼Œç”¨ä½ç½®ç´¢å¼•
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # æ›´æ–°æœ€ä½³ç‰¹å¾é›†
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


vars_combiner_list = []
vars_combiner_dict = {}


# In[ ]:



for three_score in ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']:
    print(f"===========å¼€å§‹å˜é‡ï¼š{three_score}===========")
    selected_vars = varsname_base + [three_score]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([three_score, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()

    gc.collect()


# In[ ]:



for score_a, score_b in combinations(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'], 2):
    print(f"===========å¼€å§‹å˜é‡ç»„åˆï¼š{score_a} vs {score_b}===========")
       
    selected_vars = varsname_base + [score_a, score_b]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([score_a, score_b, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()
    gc.collect()




# In[ ]:



for score_a, score_b, score_c in combinations(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'], 3):
    print(f"===========å¼€å§‹å˜é‡ç»„åˆï¼š{score_a} vs {score_b} vs {score_c}===========")
       
    selected_vars = varsname_base + [score_a, score_b, score_c]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([score_a, score_b, score_c, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()
    gc.collect()




# In[ ]:



selected_vars = varsname_base + ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

varname = "@".join(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model', '_main'])        
df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
vars_combiner_list.append(varname)
vars_combiner_dict[varname]=lgb_model.feature_name()
gc.collect()


# In[ ]:





# In[ ]:





# In[ ]:


# # ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
# X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
# y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
# print(df_sample.groupby(['data_set'])['order_no'].count())
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(X_train.shape)
# print(df_sample.groupby(['data_set'])['order_no'].count())


# In[ ]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[ ]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[ ]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[ ]:


df_sample["customer_tags"].value_counts()


# In[ ]:


df_sample['ficoæ•°æ®æ˜¯å¦ç¼ºå¤±']=df_sample['fico_model'].apply(lambda x: '1_ä¸ç¼ºå¤±' if pd.notna(x) else '2_æœ‰ç¼ºå¤±' )
df_sample['ficoæ•°æ®æ˜¯å¦ç¼ºå¤±'].value_counts()


# In[ ]:


# # æŒ‰ flag åˆ†ç»„è®¡ç®—
# df_ks_auc_set_all = (
#     df_sample[df_sample['fico_model'].notna()]
#     .groupby(['customer_tags'])
#     .apply(
#         lambda g: calculate_ks_auc(g, modeltrian_target, target, 'm1b0074', 'data_set')
#     )
#     .reset_index()
# )
# df_ks_auc_set_all


# In[ ]:


(
    df_sample[df_sample['m1b0072'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'm1b0072', 'data_set')
    )
    .reset_index()
)


# In[ ]:


df_sample[df_sample['fico_model'].notna()]['apply_date'].max()


# In[ ]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤') 
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤') 
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 å‚æ•°ä¼˜åŒ–

# In[27]:


from itertools import combinations
import pandas as pd
import numpy as np
import lightgbm as lgb
from hyperopt import fmin, tpe, hp, Trials
from hyperopt.early_stop import no_progress_loss
from sklearn.metrics import roc_auc_score
import gc
import pickle
import os
from typing import List, Tuple, Dict, Any


# In[28]:



# ================================
# 1. å·¥å…·å‡½æ•°å®šä¹‰
# ================================

def feature_importance(model: lgb.Booster) -> pd.DataFrame:
    """è·å–ç‰¹å¾é‡è¦æ€§"""
    importance = model.feature_importance(importance_type='gain')
    features = model.feature_name()
    return pd.DataFrame({'feature': features, 'gain': importance}).sort_values(by='gain', ascending=False)

def rfe_with_lgb(
    X_train: pd.DataFrame, y_train: pd.Series,
    X_test: pd.DataFrame, y_test: pd.Series,
    params: Dict[str, Any]
) -> List[str]:
    """é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰"""
    feature_names = list(X_train.columns)
    best_features = feature_names[:]

    while len(best_features) > 0:
        train_set = lgb.Dataset(X_train[best_features], label=y_train)
        valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)

        try:
            model = lgb.train(params, train_set, valid_sets=[valid_set], num_boost_round=10000, verbose_eval=False)
            df_importance = feature_importance(model).reset_index(drop=True)
        except Exception as e:
            print(f"  âš ï¸ RFE è®­ç»ƒå¤±è´¥: {e}")
            break

        if all(df_importance['gain'] > 0):
            break

        best_features = df_importance[df_importance['gain'] > 0]['feature'].tolist()
        gc.collect()

    return best_features

def param_hyperopt(
    param_spaces: Dict, X_train, y_train, X_test, y_test,
    num_boost_round=10000, max_evals=50
) -> Tuple[Dict, Trials]:
    """è´å¶æ–¯è°ƒå‚ä¸»å‡½æ•°ï¼ˆå…¼å®¹è€ç‰ˆ hyperoptï¼‰"""
    def lgb_hyperopt_object(params):
        max_depth = int(params['max_depth'])
        num_leaves = int(params['num_leaves'])
        
        # â›”ï¸ çº¦æŸæ£€æŸ¥ï¼šnum_leaves <= 2^max_depth
        if num_leaves >= 2 ** max_depth:
            return 1.0  # æƒ©ç½šï¼Œæ‹’ç»éæ³•ç»„åˆ

        param = {
            'objective': 'binary',
            'boosting': 'gbdt',
            'metric': 'auc',
            'learning_rate': params['learning_rate'],
            'num_leaves': num_leaves,
            'max_depth': max_depth,
            'min_data_in_leaf': int(params['min_data_in_leaf']),
            'feature_fraction': params['feature_fraction'],
            'bagging_fraction': params['bagging_fraction'],
            'lambda_l1': params['lambda_l1'],
            'lambda_l2': params['lambda_l2'],
            'min_gain_to_split': 10,
            'early_stopping_rounds': 30,
            'scale_pos_weight': 1,
            'seed': 1,
            'verbose': -1
        }

        train_set = lgb.Dataset(X_train, label=y_train)
        valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)

        try:
            clf_obj = lgb.train(param, train_set, valid_sets=[valid_set], num_boost_round=num_boost_round, verbose_eval=False)
            pred = clf_obj.predict(X_test)
            loss = 1 - roc_auc_score(y_test, pred)
        except Exception as e:
            loss = 1.0  # å¼‚å¸¸ä¹Ÿè¿”å›é«˜æŸå¤±

        return loss

    trials = Trials()
    early_stop_fn = no_progress_loss(50)

    best_params = fmin(
        fn=lgb_hyperopt_object,
        space=param_spaces,
        algo=tpe.suggest,
        max_evals=max_evals,
        trials=trials,
        early_stop_fn=early_stop_fn,
        show_progressbar=True
    )

    # è½¬æ¢æ•´æ•°å‚æ•°
    for key in ['num_leaves', 'max_depth', 'min_data_in_leaf']:
        if key in best_params:
            best_params[key] = int(best_params[key])

    return best_params, trials


# In[30]:



# ================================
# 2. å®šä¹‰æœç´¢ç©ºé—´ï¼ˆå…¼å®¹è€ç‰ˆ hyperoptï¼‰
# ================================

spaces = {
    "learning_rate": hp.uniform('learning_rate', 0.05, 0.1),
    'max_depth': hp.quniform('max_depth', 2, 6, 1),
    "num_leaves": hp.quniform("num_leaves", 4, 64, 1),  # ç”¨çº¦æŸæ£€æŸ¥ä¿è¯åˆæ³•æ€§
    "min_data_in_leaf": hp.quniform("min_data_in_leaf", 100, 1000, 50),
    "feature_fraction": hp.uniform("feature_fraction", 0.5, 0.9),
    "bagging_fraction": hp.uniform("bagging_fraction", 0.5, 0.9),
    "lambda_l1": hp.uniform("lambda_l1", 0, 10),
    "lambda_l2": hp.uniform("lambda_l2", 1, 300)
}

# åŸºç¡€å‚æ•°
base_params = {
    'objective': 'binary',
    'boosting': 'gbdt',
    'metric': 'auc',
    'min_gain_to_split': 80,
    'early_stopping_rounds': 30,
    'scale_pos_weight': 1,
    'seed': 1,
    'verbose': -1
}


# In[31]:



# ================================
# 3. åˆå§‹åŒ–å­˜å‚¨
# ================================

score_vars = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
vars_combiner_list = []          # å­˜å‚¨æ–°å˜é‡å
vars_combiner_dict = {}          # {varname: feature_list}
model_results = []               # å­˜å‚¨æ¯æ¬¡è®­ç»ƒçš„å…ƒä¿¡æ¯


# In[45]:



# ================================
# 4. éå†æ‰€æœ‰éç©ºå­é›†ï¼ˆ1~4ä¸ªå˜é‡ï¼‰
# ================================

print("ğŸš€ å¼€å§‹éå†æ‰€æœ‰è¯„åˆ†å˜é‡ç»„åˆï¼ˆ1~4ä¸ªï¼‰...")

for r in range(1, 5):  # 1, 2, 3, 4
    for combo in combinations(score_vars, r):
        score_names = list(combo)
        combo_name = "@".join(score_names + ['_main'])  # å¦‚ fico_model@umeng_sdk_score_main
        print(f"\n=========== å¼€å§‹ç»„åˆï¼š{combo_name} ===========")

        # é€‰æ‹©å˜é‡
        selected_vars = varsname_base + score_names  # varsname_base æ˜¯ä½ çš„åŸºç¡€å˜é‡åˆ—è¡¨

        # åˆ’åˆ†æ•°æ®
        X_train = df_sample[df_sample['data_set'] == '1_train'][selected_vars]
        y_train = df_sample[df_sample['data_set'] == '1_train'][modeltrian_target]
        X_test = df_sample[df_sample['data_set'] == '2_test'][selected_vars]
        y_test = df_sample[df_sample['data_set'] == '2_test'][modeltrian_target]

        if len(X_train) == 0 or len(X_test) == 0:
            print("  âš ï¸ è®­ç»ƒ/æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡...")
            continue

        # æ­¥éª¤1ï¼šè´å¶æ–¯è°ƒå‚
        print("  â†’ å¼€å§‹è´å¶æ–¯è°ƒå‚...")
        opt_params, _ = param_hyperopt(spaces, X_train, y_train, X_test, y_test, max_evals=20)

        # åˆå¹¶å‚æ•°
        final_params = {**base_params, **opt_params}

        # æ­¥éª¤2ï¼šRFE ç‰¹å¾é€‰æ‹©
        print("  â†’ å¼€å§‹é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰...")
        best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, final_params)

        # æ­¥éª¤3ï¼šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        dtrain = lgb.Dataset(X_train[best_features], label=y_train)
        dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)

        lgb_model = lgb.train(
            final_params,
            dtrain,
            valid_sets=[dtest],
            num_boost_round=10000,
            verbose_eval=50
        )

        # æ­¥éª¤4ï¼šç”Ÿæˆé¢„æµ‹å˜é‡
        df_sample[combo_name] = lgb_model.predict(df_sample[best_features], num_iteration=lgb_model.best_iteration)

        # æ­¥éª¤5ï¼šè®°å½•
        vars_combiner_list.append(combo_name)
        vars_combiner_dict[combo_name] = best_features

        model_results.append({
            'combination': combo_name,
            'n_scores': len(combo),
            'scores': list(combo),
            'params': final_params,
            'selected_features': best_features,
            'n_features': len(best_features),
            'best_iteration': lgb_model.best_iteration
        })

        gc.collect()

print(f"\nâœ… æ‰€æœ‰ {len(model_results)} ä¸ªç»„åˆè®­ç»ƒå®Œæˆï¼")


# In[48]:



# ================================
# 5. ä¿å­˜ç»“æœ
# ================================

# ä¿å­˜ç»“æœå­—å…¸
with open(result_path + 'vars_combiner_list.pkl', 'wb') as f:
    pickle.dump(vars_combiner_list, f)

with open(result_path + 'vars_combiner_dict.pkl', 'wb') as f:
    pickle.dump(vars_combiner_dict, f)

with open(result_path + 'model_results_all.pkl', 'wb') as f:
    pickle.dump(model_results, f)

# å¯¼å‡ºæ‘˜è¦ CSV
results_df = pd.DataFrame([
    {
        'combination': r['combination'],
        'n_scores': r['n_scores'],
        'n_features': r['n_features'],
        'learning_rate': r['params']['learning_rate'],
        'num_leaves': r['params']['num_leaves'],
        'max_depth': r['params']['max_depth'],
        'lambda_l2': r['params']['lambda_l2'],
        'feature_fraction': r['params']['feature_fraction'],
        'bagging_fraction': r['params']['bagging_fraction']
    }
    for r in model_results
])

results_df.to_csv(result_path + 'hyperopt_summary_all_combinations.csv', index=False)
print("ğŸ“Š ç»“æœå·²ä¿å­˜ï¼š")
print("   - vars_combiner_list.pkl")
print("   - vars_combiner_dict.pkl")
print("   - model_results_all.pkl")
print("   - hyperopt_summary_all_combinations.csv")


# In[71]:


results_df


# In[79]:


model_results[6]


# In[85]:


model_results[0]


# In[ ]:



for three_score in ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']:
    print(f"=========== å¼€å§‹å˜é‡ï¼š{three_score} ===========")
    selected_vars = varsname_base + [three_score]
    
    # è°ƒç”¨å¸¦è°ƒå‚çš„è®­ç»ƒå‡½æ•°
    result = model_train_with_hyperopt(
        data=df_sample,
        selected_vars=selected_vars,
        target=modeltrian_target,
        base_params=base_params,
        num_boost_round=10000,
        max_evals=30  # å¯è°ƒæ•´
    )
    
    lgb_model = result['model']
    final_params = result['params']
    best_features = result['features']
    
    # ç”Ÿæˆæ–°å˜é‡å
    varname = "@".join([three_score, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[best_features], num_iteration=lgb_model.best_iteration)
    
    # ä¿å­˜ç»“æœ
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname] = best_features
    
    # ä¿å­˜æ¨¡å‹çš„å‚æ•°å’Œå˜é‡ï¼ˆå¯ç”¨äºåç»­åˆ†ææˆ–å¯¼å‡ºï¼‰
    model_results.append({
        'score_name': three_score,
        'model_params': final_params,
        'selected_features': best_features,
        'num_features': len(best_features),
        'best_iteration': lgb_model.best_iteration,
        'auc_train': lgb_model.best_score['training']['auc'],
        'auc_valid': lgb_model.best_score['valid_0']['auc']
    })
    
    gc.collect()


# In[ ]:


# 5ï¼Œç»˜åˆ¶æœç´¢è¿‡ç¨‹
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'umeng_sdk_score@_main', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[88]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'umeng_sdk_score@_main', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# æŒ‰ flag åˆ†ç»„è®¡ç®—
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')  
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[49]:


df_sample.info(show_counts=True)


# In[ ]:


vars_combiner_dict


# In[ ]:


vars_combiner_list


# In[50]:


without_fico= load_model_from_pkl('pre_selection_20250919_without_fico.pkl')
without_fico = lgb.Booster(model_str = without_fico._handle)
print(without_fico.feature_name())
df_sample['tianchuang_id5@_main'] = without_fico.predict(df_sample[without_fico.feature_name()], num_iteration=without_fico.best_iteration)
df_sample['tianchuang_id5@_main'].head()


# In[51]:


df_sample['tianchuang_id5@_main'] = 1 - df_sample['tianchuang_id5@_main']


# In[52]:


fico_= load_model_from_pkl('pre_selection_20250919.pkl')
fico_ = lgb.Booster(model_str = fico_._handle)
print(fico_.feature_name())
df_sample['fico_model@tianchuang_score_id@_main'] = fico_.predict(df_sample[fico_.feature_name()], num_iteration=fico_.best_iteration)
df_sample['fico_model@tianchuang_score_id@_main'].head()


# In[53]:


df_sample['fico_model@tianchuang_score_id@_main'] = 1 - df_sample['fico_model@tianchuang_score_id@_main']


# In[56]:


filepath = '/home/liaoxilin/è”åˆå»ºæ¨¡/å‹ç›Ÿsdk&ç™¾è¡Œå¤šå¤´/'
umeng_model= load_model_from_pkl(filepath + 'result_å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹/å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_sample['id5_off_cpd30_2508'] = umeng_model.predict(df_sample[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_sample['id5_off_cpd30_2508'].head()


# ### 5.3.1æ•°æ®å¤„ç†

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[ ]:


df_evalue = df_sample.copy()
df_evalue.info(show_counts=True)
df_evalue.head()


# In[ ]:


list(vars_combiner_dict.values())


# In[57]:


# è·å–æ‰€æœ‰å€¼åˆ—è¡¨
lists = list(vars_combiner_dict.values())

# å°†ç¬¬ä¸€ä¸ªåˆ—è¡¨è½¬æ¢ä¸ºé›†åˆï¼Œä½œä¸ºåˆå§‹äº¤é›†
common_elements = set(lists[0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ—è¡¨

# ä¸å…¶ä½™æ¯ä¸ªåˆ—è¡¨æ±‚äº¤é›†
for lst in lists[1:]:
    common_elements &= set(lst)  # ç­‰åŒäº common_elements = common_elements.intersection(set(lst))

print("æ‰€æœ‰åˆ—è¡¨å…±æœ‰çš„å…ƒç´ :", common_elements)


# In[58]:


varsname_base = ['md5_off_m4d30_2509v2', 'id5_off_m4d30_2509v2']


# In[59]:


print(len(vars_combiner_list))
print(vars_combiner_list)


# In[61]:


list1 = [item.replace('@_main', '').split('@') for item in vars_combiner_list[1:]]
print(len(list1), list1)


# In[62]:


list2 = vars_combiner_list[1:]
print(len(list2), list2)

list3 = [['id5_off_m3d30_2507','id5_off_m4d30_2509v2','md5_off_m3d30_2507','md5_off_m4d30_2509v2']] * 15
print(len(list3), list3)


# In[63]:


# ç”Ÿæˆä¸‰å…ƒç»„ï¼š(list1[i], list3[i], list2[i])
triplets = [(list2[i], list3[i], list1[i]) for i in range(len(list2))]

# æ‰“å°ç»“æœ
for triplet in triplets:
    print(triplet)
    score_1, score_2, score_3 = triplet
    print(score_1, score_2, score_3)


# In[ ]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[ ]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[ ]:


filepath = '/home/liaoxilin/è”åˆå»ºæ¨¡/å‹ç›Ÿsdk&ç™¾è¡Œå¤šå¤´/'
umeng_model= load_model_from_pkl(filepath + 'result_å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹/å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[ ]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_å‹ç›ŸFico/å‹ç›ŸFicoç¦»çº¿èåˆ_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[ ]:


fico_model_v2= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹/ficoè”åˆåˆ†èåˆæ¨¡å‹_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[ ]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v2/ficoè”åˆåˆ†èåˆæ¨¡å‹v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v2_20250915112542.pkl


# In[ ]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[ ]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[ ]:


# æ–¹æ³•2ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆæ›´é«˜æ•ˆï¼‰
a_has_data = df_evalue['fico_model'].notna()  # ç­‰ä»·äº ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score' umeng_sdk_score].notna()

df_evalue['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±'] = np.where(
    a_has_data & b_has_data & c_has_data,  # éƒ½æœ‰æ•°æ®ï¼ˆéƒ½ä¸æ˜¯NaNï¼‰
    '1_éƒ½ä¸ç¼ºå¤±',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # éƒ½æ— æ•°æ®ï¼ˆéƒ½æ˜¯NaNï¼‰
        '2_éƒ½æœ‰ç¼ºå¤±',
        None  # å…¶ä»–æƒ…å†µï¼ˆä¸€ä¸ªæœ‰æ•°æ®ä¸€ä¸ªæ— æ•°æ®ï¼‰
    )
)


# In[ ]:


df_evalue['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±'].value_counts(dropna=False)


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[64]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    data = pd.Series({'KS': ks_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
#         df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[65]:


df_evalue = df_sample.dropna(subset=['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'],how='all')
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)
df_evalue.head()


# In[66]:


map_dict = {'3_oot2':'20250310-20250415','3_oot1':'20250301-20250309','2_test':'20250215-20250228','1_train':'20250101-20250214'}


# In[67]:


def rename_models(original_list):
    # å®šä¹‰æ›¿æ¢æ˜ å°„ï¼ˆæŒ‰é•¿åº¦é™åºæ’åˆ—ï¼Œé¿å…çŸ­åç§°å¹²æ‰°é•¿åç§°ï¼Œæ¯”å¦‚ umeng è¢« fico_model åŒ…å«ï¼‰
    replacements = {
        'umeng_sdk_score': 'å‹ç›Ÿ',
        'fico_model': 'fico',
        'haina_model': 'æµ·çº³',
        'tianchuang_score': 'å¤©åˆ›'
    }
    
    result = []
    for name in original_list:
        new_name = name
        for old, new in replacements.items():
            new_name = new_name.replace(old, new)
        result.append(new_name)
    
    return result


# In[70]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_å¢ç›Šè¯„ä¼°250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list[1:] + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[72]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_å¢ç›Šè¯„ä¼°250922_v3.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list[1:] + varsname_base + ['m1b0077','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[74]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_è´å¶æ–¯è°ƒå‚_æ— fico_å¢ç›Šè¯„ä¼°250922_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' not in col]
    e_score3 = ['md5_off_m4d30_2509v2','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[76]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_è´å¶æ–¯è°ƒå‚_æœ‰fico_å¢ç›Šè¯„ä¼°250922_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model', 'fico_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' in col]
    e_score3 = ['md5_off_m4d30_2509v2','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[83]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_3ä¸ªå­åˆ†_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' not in col]
    e_score3 = ['id5_off_cpd30_2508','tianchuang_id5@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[84]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_ficoå­åˆ†_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' in col]
    e_score3 = ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[ ]:



with pd.ExcelWriter('æˆä¿¡åœºæ™¯m4d30_å¢ç›Šè¯„ä¼°250922_v3.xlsx') as writer:

    for i, triplet in enumerate(triplets):
        score_1, score_2, score_3 = triplet
        score_list = [score_1] + score_2 + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
        print(len(score_list),score_list)
        
        target_list = ['target_mob4dpd30_1'] 
        labels_models_dict = {target: score_list for target in target_list}
        print(labels_models_dict)

        print(df_evalue.shape[0])
        tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
        tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_3].notna().any(axis=1),:]
        print(tmp_df_evalue.shape[0])
        # æ•´ä½“å®¢ç¾¤
        groupkeys2 = ['channel_types', 'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

        df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        # åˆ†å®¢ç¾¤
        groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'customer_tags', 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

        df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        
        # åˆå¹¶æ•°æ®
        df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
        df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)
        score_2_new = [f"KS_{col}" for col in score_2]
        df_auc_ks_all['KSæœ€å¤§å€¼'] = df_auc_ks_all[score_2_new].max(axis=1)
        ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
        df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
        df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)
        original_list = [score_1]
        new_original_list = rename_models(original_list)
        score_2_new = new_original_list[0]
        df_auc_ks_all.to_excel(writer, sheet_name=f'{score_2_new}')
        
        gc.collect()


# In[ ]:


(df_auc_ks_all[f"KS_{score_1}"] - df_auc_ks_all[score_2_new]).min(axis=1)


# In[ ]:


df_auc_ks_all[[f"KS_{score_1}"]].subtract(df_auc_ks_all[score_2_new], axis=0)


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2.insert(0, 'time_windowns', value=df_ksauc_all2['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all2


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all4.insert(1, 'time_windowns', value=df_ksauc_all4['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all4


# In[ ]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all2.to_excel(writer, sheet_name='æ•´ä½“_æœ‰æ— æ•°æ®')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')
    df_ksauc_all4.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤_æœ‰æ— æ•°æ®')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all2.to_excel(writer, sheet_name='æ•´ä½“_æœ‰æ— æ•°æ®')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')
    df_ksauc_all4.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤_æœ‰æ— æ•°æ®')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all2.to_excel(writer, sheet_name='æ•´ä½“_æœ‰æ— æ•°æ®')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')
    df_ksauc_all4.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤_æœ‰æ— æ•°æ®')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("ficoæ•°æ®æ˜¯å¦ç¼ºå¤±=='1_ä¸ç¼ºå¤±' & flag=='1_æ–°å®¢' & channel=='é‡‘ç§‘æ¸ é“'").reset_index(drop=True)


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


score = 'y_pred_v3'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[ ]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[ ]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix_month)


# In[ ]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[ ]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[ ]:


df_evalue.to_csv(result_path + 'ficoç¦»çº¿èåˆ_report.csv',index=False)
print(result_path + 'ficoç¦»çº¿èåˆ_report.csv')


# # 7.æ¨¡å‹éƒ¨ç½²å’Œå›æº¯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## å­—æ®µåç§°
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## æœ¬åœ°æ–‡ä»¶ï¼Œæ³¨æ„ï¼šåªå†™æ–‡ä»¶åå³å¯ï¼Œå‚æ•°æ˜¯ list ç±»å‹
        csv_filename_list=['å¤©åˆ›æ¨¡å‹åˆ†æ•°v2.csv'],
        ## æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œæ³¨æ„ï¼šéœ€è¦æœ¬åœ°çš„ç»å¯¹è·¯å¾„
        input_path='/data/home/liaoxilin/è”åˆå»ºæ¨¡/å‹ç›Ÿsdk&ç™¾è¡Œå¤šå¤´/',                    
        ## ä¸Šä¼ çš„æ•°æ®åº“
        database='znzz_fintech_ads',        
        ## ä¸Šä¼ çš„è¡¨å
        table_name='lxl_model_',
        # åˆ†åŒºå­—æ®µ
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # è‡ªå®šä¹‰åˆ†éš”ç¬¦
        delimiter='|'
       ) 




#==============================================================================
# File: å®æ—¶æ¨¡å‹_æˆä¿¡fpd30_æ´ä¾¦ç»­ä¾¦.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result/{task_name}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('===================')
        data = reader.to_pandas()

    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data


# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


df_sample_dict = {}


# In[7]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-08', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
--ç¦»çº¿fpd30èåˆæ¨¡å‹å­åˆ†
,t1.standard_score
,t1.bad_score
,t2.bh_a005
,t2.bh_a007
,t2.bh_a019
,t2.bh_a043
,t2.bh_a053
,t2.bh_a057
,t2.bh_a060
,t2.bh_a062
,t2.bh_a070
,t2.bh_a072
,t2.bh_a076
,t2.bh_a094
,t2.bh_a101
,t2.bh_a103
,t2.bh_a111
,t2.bh_a114
,t2.bh_a118
,t2.bh_a120
,t2.bh_a126
,t2.bh_a132
,t2.bh_a139
,t2.bh_a152
,t2.bh_a154
,t2.bh_a163
,t2.bh_a165
,t2.bh_a166
,t2.bh_a168
,t2.bh_a170
,t2.bh_a179
,t2.bh_a187
,t2.bh_a190
,t2.bh_a191
,t2.bh_a207
,t2.bh_a212
,t2.bh_a217
,t2.bh_a218
,t2.bh_a288
,t2.bh_a292
,t2.bh_a293
,t2.bh_a295
,t2.bh_a297
,t2.bh_a299
,t2.bh_a330
,t2.bh_a332
,t2.bh_a335
,t2.bh_a354
,t2.bh_a366
,t2.bh_a380
,t2.bh_a389
,t2.bh_a392
,t2.bh_a393
,t2.bh_b226
,t2.bh_b236
,t2.bh_b245
,t2.bh_b264
,t2.bh_b267
,t2.bh_b275
,t2.bh_b279
,t2.bh_b290
,t2.bh_b292
,t2.bh_b300
,t2.bh_c013
,t2.bh_c014
,t2.bh_c031
,t2.bh_c034
,t2.bh_c053
,t2.bh_c057
,t2.bh_c067
,t2.bh_c077
,t2.bh_c079
,t2.bh_d016
,t2.bh_d028
,t2.bh_d034
,t2.bh_e031
,t2.bh_e044
,t2.bh_e048
,t2.bh_e049
,t2.bh_e050
,t2.bh_f018
,t2.bh_f021
,t2.bh_f028
,t2.bh_g022
,t2.bh_g036
,t2.bh_g042
,t2.bh_g044
,t2.bh_h068
,t2.bh_h074
,t2.bh_h079
,t2.bh_h084
,t2.bh_h098
,t2.bh_h099
,t2.bh_h104
,t2.bh_h111
,t2.bh_h119
,t2.bh_h121
,t2.bh_h123
,t2.bh_h124
,t2.bh_h131
,t2.bh_h135
,t2.bh_h136
,t2.bh_h138
,t2.bh_h150
,t2.bh_h151
,t2.bh_h153
,t2.bh_h155
,t2.bh_h158
,t2.bh_h159
,t2.bh_h161
,t2.bh_h167
,t2.bh_h168
,t2.bh_h170
,t2.bh_h171
,t2.bh_h173
,t2.bh_qu005
,t2.bh_qu006
,t2.bh_qu009
,t2.bh_qu011
,t2.bh_qu015
,t2.bh_qu016
,t2.bh_qu018
,t2.bh_qu019
,t2.bh_qu022
,t2.bh_x075
,t2.bh_x090
,t2.bh_x113
,t3.value_016
,t3.value_048
,t3.value_077
,t3.value_082
,t3.value_108
,t3.value_110
,t3.value_115
,t3.value_123
,t3.value_125
,t3.value_132
,t3.value_145
,t3.value_151
,t3.value_178
,t3.value_181
,t3.value_193
,t3.value_202
,t3.value_210
,t3.value_220
,t3.value_223
,t3.value_232
,t3.value_236
,t3.value_240
,t3.value_241
,t3.value_251
,t3.value_266
,t3.value_276
,t3.value_285
,t3.value_299
,t3.value_305
,t3.value_306
,t3.value_313
,t3.value_329
,t3.value_335
,t3.value_341
,t3.value_342
,t3.value_351
,t3.value_368
,t3.value_377
,t3.value_380
,t3.value_384
,t3.value_392
,t3.value_398
,t3.value_408
,t3.value_414
,t3.value_415
,t3.value_416

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 2) 
      and lending_time='{run_day}'
    ) as t 
--ç¦»çº¿é£é™©æ¨¡å‹å­åˆ†
left join 
    (
    select t.* 
    from znzz_fintech_ads.dm_f_lxl_test_behave_model_merge_fpd30_score as t 
    where dt=date_sub('{run_day}',1)
    ) as t1 on t.id_no_des=t1.id_no_des

--æ´ä¾¦å˜é‡
left join
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk 
    from znzz_fintech_dwd.dwd_beforeloan_data_source_bh_fqz_djv3_id as t
    where dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t2 on t.id_no_des=t2.id_no_des and t2.rk=1

--ç»­ä¾¦å˜é‡
left join 
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk
    from znzz_fintech_dwd.dwd_beforeloan_third_combine_sub_id as t  
    where ds='jzhl_thirds_platform_intf_bh_nfacq932_20240529'
      and dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t3 on t.id_no_des=t3.id_no_des and t3.rk=1
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[8]:


data_time = pd.DataFrame({'run_day':list(df_sample_dict.keys())})
data_time['run_day'].value_counts()


# In[9]:


df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[10]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['id_no_des'].nunique())


# In[11]:


print(df_sample_['lending_time'].min(), df_sample_['lending_time'].max())


# In[ ]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[ ]:


df_sample['channel_type'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rate'] = df_sample['channel_id'].apply(channel_rate)


# In[12]:


df_sample_.to_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_åŸå§‹æ•°æ®é›†_250110_part1.csv',index=False)
print(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_åŸå§‹æ•°æ®é›†_250110_part1.csv')


# In[13]:


df_sample = df_sample_.query("diff_days<=30")
df_sample.info()


# In[14]:


df_sample.to_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110_part1.csv',index=False)
print(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110_part1.csv')


# In[18]:


del df_sample
gc.collect()


# In[19]:


df_sample_part1 = pd.read_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110_part1.csv')
df_sample_part1.info()


# In[15]:


df_sample_part2 = pd.read_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110_part2.csv')
df_sample_part2.info()


# In[20]:


df_sample = pd.concat([df_sample_part1, df_sample_part2.query("lending_time>='2024-11-09'")], ignore_index=True)
df_sample.info()
df_sample.head()


# In[21]:


print(df_sample.shape, df_sample['order_no'].nunique(), df_sample['id_no_des'].nunique())


# In[22]:


df_sample.columns.to_list()[:12]


# In[23]:


varsname = [col for col in df_sample.columns.to_list()[12:]]

print(varsname[:10], varsname[-10:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[24]:


print(result_path)


# In[25]:


# for i, col in enumerate(varsname[773:]):
#     if df_sample[col].dtype=='object':
#         print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
#         df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')


# In[34]:


pd.set_option('display.max_row',None)
df_sample.groupby(['lending_time','fpd20'])['order_no'].count().unstack()


# In[33]:


df_sample['fpd20'] = df_sample['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[35]:


df_sample.loc[df_sample.query("lending_time>='2024-11-10'").index, 'fpd30']= -1
df_sample.loc[df_sample.query("lending_time>='2024-11-20'").index, 'fpd20']= -1


# In[28]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-10-01' & lending_time<='2024-11-09'").index, 'data_set']='3_oot'


# In[29]:


target = 'fpd30'


# In[30]:


df_sample[[target]+varsname].info(show_counts=True)
df_sample[[target]+varsname].head()


# In[36]:


df_sample.to_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110.csv',index=False)
print(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110.csv')


# In[37]:


df_model = df_sample.copy()
df_sample = df_sample.query("lending_time<='2024-11-09'")


# # 1. æ ·æœ¬æ¦‚å†µ

# In[38]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[39]:


print(df_sample[target].value_counts())


# In[40]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[41]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[42]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[43]:


task_name


# In[44]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[45]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])


# In[46]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[48]:


# df_sample = df_model.copy()df_sample[varsname]
df_sample = df_sample.replace(-1, np.nan)
gc.collect()


# ## 2.2 æ•°æ®æ¢ç´¢

# In[50]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[51]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[53]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[55]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[56]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[57]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[58]:


df_iv.loc[to_drop_iv,:].head()


# In[59]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[62]:


# df_sample[varsname_v1+[target]].info()
df_sample = df_sample.reset_index(drop=True)


# In[63]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]].drop('standard_score',axis=1),
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[65]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[66]:


df_iv.loc[to_drop2,:].head()


# In[67]:


tmp = df_iv.loc[to_drop2,:].sort_values(by='iv', ascending=False)
tmp.head(20)


# In[68]:


# to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# In[70]:


# df_sample[varsname_v2].info()


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[71]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            if any(x == 0 for x in regroup['bad']) or any(x == 0 for x in regroup['good']):
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
                regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10 
            else:
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
                regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            if any(x == 0 for x in regroup['bad']) or any(x == 0 for x in regroup['good']):
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
                regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10 
            else:
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
                regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[72]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[73]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[74]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[75]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[77]:


new_bins_dict


# In[78]:


combiner.load(new_bins_dict)


# In[79]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[80]:


to_drop_mode


# In[81]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), varsname_v2,                                    'lending_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[82]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[83]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[84]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[85]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[86]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# In[ ]:





# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[87]:


len(list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index))


# In[88]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[89]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[90]:


df_psi_by_set.loc[drop_by_psi_set,:].head()


# In[91]:


df_miss_set.info()


# In[92]:


to_drop3 = [col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1]
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[93]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[94]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[95]:


df_sample_woe.head()


# In[96]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.85
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.corr(method=method)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[97]:


gc.collect()


# In[98]:


df_iv_by_set.info()
df_iv_by_set.head()


# In[108]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe[varsname_v3],
                                                     df_iv_by_set['3_oot'],
                                                     method='kendall',
                                                     threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[109]:


df_high_corr.info()
df_high_corr.head()


# In[111]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[112]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[113]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'lending_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='kendalltau'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[114]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[115]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[116]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[ ]:





# ## 4.3 é€æ­¥å›å½’ç­›é€‰

# In[78]:


# # å°†woeè½¬åŒ–åçš„æ•°æ®åšé€æ­¥å›å½’
# train_woe = df_sample_woe.query("data_set=='1_train'")[varsname_v3+[target]]
# final_data, to_drop6 = toad.selection.stepwise(train_woe, target=target, estimator='ols', direction = 'both', \
#                                      criterion = 'aic', exclude = None, return_drop=True)

# print(final_data.shape) # é€æ­¥å›å½’ä»31ä¸ªå˜é‡ä¸­é€‰å‡ºäº†10ä¸ª


# In[118]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[119]:


gc.collect()


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.1 æ¨¡å‹è®­ç»ƒ

# In[120]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance

# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[122]:


# 2 å®šä¹‰è¶…å‚ç©ºé—´
# hp.quniform("å‚æ•°åç§°",ä¸‹ç•Œ,ä¸Šç•Œ,æ­¥é•¿)-é€‚ç”¨äºç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æµ®ç‚¹ç‚¹æ•°
# hp.uniform("å‚æ•°åç§°",ä¸‹ç•Œ, ä¸‹ç•Œ)-é€‚ç”¨äºè¿ç»­éšæœºåˆ†å¸ƒçš„æµ®ç‚¹æ•°
# hp.randint("å‚æ•°åç§°",ä¸Šç•Œ)-é€‚ç”¨äº[0,ä¸Šç•Œ)çš„æ•´æ•°,åŒºé—´ä¸ºå·¦é—­å³å¼€
# hp.choice("å‚æ•°åç§°",["å­—ç¬¦ä¸²1","å­—ç¬¦ä¸²2",...])-é€‚ç”¨äºå­—ç¬¦ä¸²ç±»å‹,æœ€ä¼˜å‚æ•°ç”±ç´¢å¼•è¡¨ç¤º
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# å¯ä»¥æ ¹æ®éœ€è¦ï¼Œæ³¨é‡Šæ‰ååçš„ä¸€äº›ä¸å¤ªé‡è¦çš„è¶…å‚

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",21,200,1),
          "max_depth":2,
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",30,200,1),
          "feature_fraction":hp.uniform("feature_fraction",0.5,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.5,1.0),
#           "feature_fraction":1.0,
#           "bagging_fraction":1.0,
          "min_gain_to_split":10,
#           "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 10.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 100, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 50
          }
spaces


# In[123]:


# 3ï¼Œæ‰§è¡Œè¶…å‚æœç´¢
# æœ‰äº†ç›®æ ‡å‡½æ•°å’Œå‚æ•°ç©ºé—´,æ¥ä¸‹æ¥è¦è¿›è¡Œä¼˜åŒ–,éœ€è¦äº†è§£ä»¥ä¸‹å‚æ•°:
# fmin:è‡ªå®šä¹‰ä½¿ç”¨çš„ä»£ç†æ¨¡å‹(å‚æ•°algo),hyperoptæ”¯æŒå¦‚ä¸‹æœç´¢ç®—æ³•ï¼š
#       éšæœºæœç´¢(hyperopt.rand.suggest)
#       æ¨¡æ‹Ÿé€€ç«(hyperopt.anneal.suggest)
#       TPEç®—æ³•ï¼ˆhyperopt.tpe.suggestï¼Œç®—æ³•å…¨ç§°ä¸ºTree-structured Parzen Estimator Approachï¼‰
# partial:ä¿®æ”¹ç®—æ³•æ¶‰åŠåˆ°çš„å…·ä½“å‚æ•°,åŒ…æ‹¬æ¨¡å‹å…·ä½“ä½¿ç”¨äº†å¤šå°‘å°‘ä¸ªåˆå§‹è§‚æµ‹å€¼(å‚æ•°n_start_jobs),
#         ä»¥åŠåœ¨è®¡ç®—é‡‡é›†å‡½æ•°å€¼æ—¶ç©¶ç«Ÿè€ƒè™‘å¤šå°‘ä¸ªæ ·æœ¬(å‚æ•°n_EI_candidates)
# trials:è®°å½•æ•´ä¸ªè¿­ä»£è¿‡ç¨‹,ä»hyperoptåº“ä¸­å¯¼å…¥çš„æ–¹æ³•Trials(),ä¼˜åŒ–å®Œæˆä¹‹å,
#        å¯ä»¥ä»ä¿å­˜å¥½çš„trialsä¸­æŸ¥çœ‹æŸå¤±ã€å‚æ•°ç­‰å„ç§ä¸­é—´ä¿¡æ¯
# early_stop_fn:æå‰åœæ­¢å‚æ•°,ä»hyperoptåº“å¯¼å…¥çš„æ–¹æ³•no_progresss_loss(),å¯ä»¥è¾“å…¥å…·ä½“çš„æ•°å­—n,
#               è¡¨ç¤ºå½“æŸå¤±è¿ç»­næ¬¡æ²¡æœ‰ä¸‹é™æ—¶,è®©ç®—æ³•æå‰åœæ­¢
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds, init_model=None):

        """å®šä¹‰ç›®æ ‡å‡½æ•°"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1,
                'num_threads': -1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                init_model=init_model,
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set,                                 num_boost_round=num_boost_round, init_model=init_model)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(30)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[124]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[125]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[128]:


len(varsname_v5)


# In[127]:


varsname_v5.remove('bad_score')


# In[129]:


# è®­ç»ƒæ•°æ®é›†
X_train = df_sample.query("data_set!='3_oot'")[varsname_v5]
y_train = df_sample.query("data_set!='3_oot'")[target]
print(X_train.shape)


# In[ ]:


# 4ï¼Œè·å–æœ€ä¼˜å‚æ•°ï¼Œè°ƒå‚è¿‡ç¨‹
# ç¡®å®šä¸€ä¸ªè¾ƒé«˜çš„å­¦ä¹ ç‡
# å¯¹å†³ç­–æ ‘åŸºæœ¬å‚æ•°è°ƒå‚
# æ­£åˆ™åŒ–å‚æ•°è°ƒå‚
# é™ä½å­¦ä¹ ç‡
best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=10)


# In[131]:


# 5ï¼Œç»˜åˆ¶æœç´¢è¿‡ç¨‹
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[132]:


print("æœ€ä¼˜å‚æ•°best_params: ", best_params)


# In[384]:


### æ·»åŠ æ— éœ€è°ƒå‚çš„é€šç”¨å‚æ•°
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
bst_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
bst_params['learning_rate'] = spaces['learning_rate']
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
bst_params['bagging_fraction'] = best_params['bagging_fraction']    
bst_params['feature_fraction'] = best_params['feature_fraction'] 
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = spaces['lambda_l2']
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = spaces['max_depth']
# è°ƒå‚åçš„å…¶ä»–å‚
bst_params['min_gain_to_split'] = spaces['min_gain_to_split']


# In[385]:


print("æœ€ä¼˜å‚æ•°bst_params: ", bst_params)


# In[386]:


# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[387]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[389]:


gc.collect()


# In[390]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[391]:


# è¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_month = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month = pd.concat(ks_auc_dict_month.values(), keys=ks_auc_dict_month.keys())
df_ks_auc_month = df_ks_auc_month.reset_index()
df_ks_auc_month.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[392]:


df_ks_auc_month


# In[394]:


# è¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_set = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set = pd.concat(ks_auc_dict_set.values(), keys=ks_auc_dict_set.keys())
df_ks_auc_set = df_ks_auc_set.reset_index()
df_ks_auc_set.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[395]:


df_ks_auc_set


# In[396]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month = feature_importance(lgb_model) 
df_importance_month = pd.merge(df_importance_month, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month = df_importance_month.reset_index()
df_importance_month = df_importance_month.rename(columns={'index':'varsname'})
df_importance_month.info()
df_importance_month.head()


# In[397]:



# æ•ˆæœè¯„ä¼°åæ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set = pd.merge(df_importance_set, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set['3_oot_iv']/df_importance_set['1_train_iv'] - 1
df_importance_set.drop(columns=['1_train_psi'], inplace=True)
df_importance_set = df_importance_set.reset_index()
df_importance_set = df_importance_set.rename(columns={'index':'varsname'})
df_importance_set.info()
df_importance_set.head()


# In[399]:


# # æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
# print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
# print(result_path + f'{task_name}_{timestamp}.pkl')
# print(result_path + f'{task_name}_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹ç»“æœ_{task_name}_{timestamp}.xlsx') as writer:
    df_importance_month.to_excel(writer, sheet_name='df_importance_month')
    df_importance_set.to_excel(writer, sheet_name='df_importance_set')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
#     df_ks_auc_month_30.to_excel(writer, sheet_name='df_ks_auc_month_30')
#     df_ks_auc_set_30.to_excel(writer, sheet_name='df_ks_auc_set_30')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'4_æ¨¡å‹ç»“æœ_{task_name}_{timestamp}.xlsx')


# ## 5.2 æ¨¡å‹ä¼˜åŒ–

# ### 5.2.1å‚æ•°ä¼˜åŒ–

# In[ ]:


best_params:  {'bagging_fraction': 0.6138533923759146, 'feature_fraction': 0.6011434207908619, 'min_data_in_leaf': 75.0, 'num_leaves': 198.0}


# In[ ]:


bst_params:  {'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'bagging_freq': 1, 'scale_pos_weight': 1, 'seed': 1, 'num_threads': -1, 'learning_rate': 0.1, 'bagging_fraction': 0.6138533923759146, 'feature_fraction': 0.6011434207908619, 'lambda_l1': 0, 'lambda_l2': 300, 'early_stopping_rounds': 50, 'num_leaves': 198, 'min_data_in_leaf': 75, 'max_depth': 2, 'min_gain_to_split': 10}


# In[227]:


### ä¼˜åŒ–è°ƒå‚1
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6138533923759146     
opt_params['feature_fraction'] = 0.6011434207908619
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 75
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[228]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[229]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[230]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[231]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[232]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[233]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[258]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_month_v1 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v1', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v1[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v1[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v1[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v1 = pd.concat(ks_auc_dict_month_v1.values(), keys=ks_auc_dict_month_v1.keys())
df_ks_auc_month_v1 = df_ks_auc_month_v1.reset_index()
df_ks_auc_month_v1.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[259]:


df_ks_auc_month_v1


# In[261]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_set_v1 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v1', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v1[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v1[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v1[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v1 = pd.concat(ks_auc_dict_set_v1.values(), keys=ks_auc_dict_set_v1.keys())
df_ks_auc_set_v1 = df_ks_auc_set_v1.reset_index()
df_ks_auc_set_v1.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[262]:


df_ks_auc_set_v1


# In[263]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = df_importance_month_v1.rename(columns={'index':'varsname'})
df_importance_month_v1.head()


# In[265]:


df_importance_set_v1 = feature_importance(lgb_model) 
tmp = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True, right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='inner', left_index=True, right_index=True)
df_importance_set_v1['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set_v1['3_oot_iv']/df_importance_set_v1['1_train_iv'] - 1
df_importance_set_v1.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = df_importance_set_v1.rename(columns={'index':'varsname'})
df_importance_set_v1.head()


# In[238]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_paramsopt_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_paramsopt_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_paramsopt_{timestamp}.pkl')
print(result_path + f'{task_name}_paramsopt_{timestamp}.bin')


# In[266]:


# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_paramsopt_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_paramsopt_{timestamp}.xlsx')


# ### 5.2.2 ç‰¹å¾ä¼˜åŒ–

# In[460]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[461]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[462]:


to_drop6 = ['standard_score']
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[463]:


# ä½¿ç”¨çš„æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v6],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[469]:


varsname_v7 = ['y_prob_v2','standard_score']


# In[470]:


# ä½¿ç”¨çš„æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v7],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[476]:


### ä¼˜åŒ–è°ƒå‚2
opt_params_v2 = {}
opt_params_v2['boosting'] = 'gbdt'
opt_params_v2['objective'] = 'binary'
opt_params_v2['metric'] = 'auc'
opt_params_v2['bagging_freq'] = 1
opt_params_v2['scale_pos_weight'] = 1 
opt_params_v2['seed'] = 1 
opt_params_v2['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params_v2['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params_v2['bagging_fraction'] = 0.6138533923759146     
opt_params_v2['feature_fraction'] = 0.6011434207908619
opt_params_v2['lambda_l1'] = 0
opt_params_v2['lambda_l2'] = 300
opt_params_v2['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params_v2['num_leaves'] = 21
opt_params_v2['min_data_in_leaf'] = 75
opt_params_v2['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params_v2['min_gain_to_split'] = 10
print(opt_params_v2)


# In[477]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params_v2, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[466]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[478]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v2_merge'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[479]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_month_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v2[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v2 = pd.concat(ks_auc_dict_month_v2.values(), keys=ks_auc_dict_month_v2.keys())
df_ks_auc_month_v2 = df_ks_auc_month_v2.reset_index()
df_ks_auc_month_v2.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[480]:


df_ks_auc_month_v2


# In[481]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_month_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2_merge', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v2[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2_merge', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2_merge', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v2 = pd.concat(ks_auc_dict_month_v2.values(), keys=ks_auc_dict_month_v2.keys())
df_ks_auc_month_v2 = df_ks_auc_month_v2.reset_index()
df_ks_auc_month_v2.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[482]:


df_ks_auc_month_v2


# In[284]:


df_ks_auc_month_v2


# In[285]:



# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_set_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v2[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v2 = pd.concat(ks_auc_dict_set_v2.values(), keys=ks_auc_dict_set_v2.keys())
df_ks_auc_set_v2 = df_ks_auc_set_v2.reset_index()
df_ks_auc_set_v2.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[286]:


df_ks_auc_set_v2


# In[287]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = df_importance_month_v2.rename(columns={'index':'varsname'})
df_importance_month_v2


# In[294]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = df_importance_month_v2.rename(columns={'index':'varsname'})
df_importance_month_v2


# In[288]:


# æ•ˆæœè¯„ä¼°åæ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v2 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v2 = pd.merge(df_importance_set_v2, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v2['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set_v2['3_oot_iv']/df_importance_set_v2['1_train_iv'] - 1
df_importance_set_v2.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = df_importance_set_v2.rename(columns={'index':'varsname'})
df_importance_set_v2


# In[311]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_featureopt_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_featureopt_{timestamp}.bin')
# print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
# print(result_path + f'{task_name}_featureopt_{timestamp}.pkl')
# print(result_path + f'{task_name}_featureopt_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_featureopt_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')     
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_featureopt_{timestamp}.xlsx')


# ### 5.2.3å¢é‡å­¦ä¹ 

# In[428]:


### ä¼˜åŒ–è°ƒå‚3
opt_params_v3 = {}
opt_params_v3['boosting'] = 'gbdt'
opt_params_v3['objective'] = 'binary'
opt_params_v3['metric'] = 'auc'
opt_params_v3['bagging_freq'] = 1
opt_params_v3['scale_pos_weight'] = 1 
opt_params_v3['seed'] = 1 
opt_params_v3['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params_v3['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params_v3['bagging_fraction'] = 0.6138533923759146     
opt_params_v3['feature_fraction'] = 0.6011434207908619
opt_params_v3['lambda_l1'] = 0
opt_params_v3['lambda_l2'] = 300
opt_params_v3['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params_v3['num_leaves'] = 198
opt_params_v3['min_data_in_leaf'] = 75
opt_params_v3['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params_v3['min_gain_to_split'] = 10
print(opt_params_v3)


# In[429]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[430]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[431]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[432]:


init_model = load_model_from_pkl('./result/æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30/æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30_20250113103851.pkl')


# In[433]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=init_model)


# In[434]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[435]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_month_v3 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v3', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v3[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v3[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v3[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v3 = pd.concat(ks_auc_dict_month_v3.values(), keys=ks_auc_dict_month_v3.keys())
df_ks_auc_month_v3 = df_ks_auc_month_v3.reset_index()
df_ks_auc_month_v3.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[436]:


df_ks_auc_month_v3


# In[437]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
ks_auc_dict_set_v3 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v3', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v3[('å…¨æ¸ é“',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v3[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v3[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v3 = pd.concat(ks_auc_dict_set_v3.values(), keys=ks_auc_dict_set_v3.keys())
df_ks_auc_set_v3 = df_ks_auc_set_v3.reset_index()
df_ks_auc_set_v3.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[438]:


df_ks_auc_set_v3


# In[439]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = df_importance_month_v3.rename(columns={'index':'varsname'})
df_importance_month_v3.info()
df_importance_month_v3.head()


# In[440]:



# æ•ˆæœè¯„ä¼°åæ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v3 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v3['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set_v3['3_oot_iv']/df_importance_set_v3['1_train_iv'] - 1
df_importance_set_v3.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = df_importance_set_v3.rename(columns={'index':'varsname'})
df_importance_set_v3.info()
df_importance_set_v3.head()


# In[441]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_initmodel_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_initmodel_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_initmodel_{timestamp}.pkl')
print(result_path + f'{task_name}_initmodel_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_initmodel_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')     
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_initmodel_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# ### 5.3.1æ•°æ®å¤„ç†

# In[483]:


usecols= ['order_no','channel_id', 'lending_time','lending_month', 'mob',          'maxdpd', 'fpd', 'fpd10', 'fpd30', 'mob4dpd30', 'diff_days'] + varsname_v5
df1 = pd.read_csv(result_path+'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_åŸå§‹æ•°æ®é›†_250110_part1.csv',usecols=usecols)
df2 = pd.read_csv(result_path+'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_åŸå§‹æ•°æ®é›†_250117_part2.csv',usecols=usecols)
df_model_vars = pd.concat([df1, df2.query("lending_time>='2024-11-09'")],ignore_index=True)
df_model_vars.info(show_counts=True)
df_model_vars.head()


# In[486]:


print(df_model_vars.shape[0], df_model_vars['order_no'].nunique())


# In[487]:


df_model_vars[varsname_v5].describe().T


# In[488]:


df_model_vars[varsname_v5] = df_model_vars[varsname_v5].replace(-1, np.nan)
gc.collect()


# In[489]:


df_model_vars[varsname_v5].describe().T


# In[490]:


# è¡ç”ŸYæ ‡ç­¾
print(df_model_vars['fpd'].min(), df_model_vars['fpd'].max())
print(df_model_vars['fpd30'].min(), df_model_vars['fpd30'].max())


# In[491]:


# è¡ç”ŸYæ ‡ç­¾
# df_model_vars['fpd10'] = df_model_vars['fpd'].apply(lambda x: 1 if x>10 else 0)
df_model_vars['fpd20'] = df_model_vars['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[494]:


df_model_vars.groupby(["lending_time",'fpd10'])['order_no'].count().unstack()


# In[495]:


df_model_vars = df_model_vars.query("lending_time<='2024-12-05'").reset_index(drop=True)
df_model_vars.loc[df_model_vars.query("lending_time>='2024-11-17'").index, 'fpd30'] = -1
df_model_vars.loc[df_model_vars.query("lending_time>='2024-11-27'").index, 'fpd20'] = -1


# In[496]:


# æ·»åŠ å®¢ç¾¤æ ‡ç­¾
def diff_days_(x):
    if x<=30:
        days = 'T30-'
    elif x>30:
        days = 'T30+'
    else:
        days = np.nan
    return days

df_model_vars['å®¢ç¾¤'] = df_model_vars['diff_days'].apply(diff_days_)


# In[497]:


# åŸºç¡€è®­ç»ƒæ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl(result_path + 'æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30_20250113103851.pkl')
df_model_vars['y_prob'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[498]:


# ç¬¬ä¸€æ¬¡ä¼˜åŒ–ï¼šå‚æ•°ä¼˜åŒ–æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl(result_path + 'æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30_paramsopt_20250113115717.pkl')
df_model_vars['y_prob_v1'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[499]:


# ç¬¬ä¸‰æ¬¡ä¼˜åŒ–ï¼šå¢é‡å­¦ä¹ æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl(result_path + 'æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30_initmodel_20250117103621.pkl')
df_model_vars['y_prob_v3'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[357]:


# # æœ€æ–°æ¨¡å‹æ•°æ®è¡¨ç°ç°æ¨¡å‹æ•°æ® 
# sql="""
# select t2.*
# from
# (
#     select order_no 
#     from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
#     where dt=date_sub(current_date(), 2) 
#       and lending_time>='2024-07-21'
#       and lending_time<='2024-11-05'
# ) as t1 
# inner join znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t2 on t1.order_no=t2.order_no
# ;
# """
# df_tx_bj = get_data(sql)
# df_tx_bj.info(show_counts=True)
# df_tx_bj.head() 


# In[358]:


# df_tx_bj['t_beha3_fpd'] = pd.to_numeric(df_tx_bj['t_beha3_fpd'])
# df_tx_bj['t_beha3_mob4'] = pd.to_numeric(df_tx_bj['t_beha3_mob4'])


# In[359]:


# selected_cols = df_tx_bj.columns.to_list()[3:]
# df_tx_bj[selected_cols].describe().T


# In[360]:


# df_tx_bj.to_csv(result_path + 'å…¨æ¸ é“å…¶ä»–æç°æ¨¡å‹åˆ†æ•°_241218.csv')
# print(result_path + 'å…¨æ¸ é“å…¶ä»–æç°æ¨¡å‹åˆ†æ•°_241218.csv')


# In[361]:


# # å¥½åˆ†æ•°è½¬ä¸ºååˆ†æ•°
# for i, col in enumerate(selected_cols):
#     print(f'ç¬¬{i}ä¸ªå˜é‡ï¼š{col}')
#     df_tx_bj[col] = 1 - df_tx_bj[col]


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[500]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col, percentile=0.95):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    badrate = df[label_col].mean()
    
    if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
    elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
    else:
        print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")

    if badrate>0 and pct_n_badrate>0:
        lift_n = pct_n_badrate/badrate
    else:
        lift_n = np.nan
    return pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}',
                                                    'top5lift':f'top5lift_{score_}'})
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[511]:


# df_evalue = df_model_vars.copy()
df_evalue = df_evalue.query("standard_score==standard_score")
df_evalue.info(show_counts=True)


# In[502]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[503]:


df_evalue['channel_type'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_rate'] = df_evalue['channel_id'].apply(channel_rate)


# In[504]:


df_evalue['bad_score'] = 1000 - df_evalue['standard_score'] 


# In[505]:


colsname = ['y_prob', 'y_prob_v1', 'y_prob_v3', 'bad_score']

print(colsname)
target_list = ['fpd10', 'fpd20', 'fpd30']
labels_models_dict = {target: colsname for target in target_list}
print(labels_models_dict)


# In[512]:


df_evalue['æ¸ é“'] = 'å…¨æ¸ é“'
groupkeys1 = ['å®¢ç¾¤', 'æ¸ é“', 'lending_month']
df_ksauc_all_v1 = cal_ks_auc(df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.head()


# In[513]:



groupkeys2 = ['å®¢ç¾¤', 'channel_type', 'lending_month']
df_ksauc_all_v2 = cal_ks_auc(df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_type':'æ¸ é“'}, inplace=True)
df_ksauc_all_v2.head()


# In[518]:


df_ksauc_all_v2


# In[514]:



groupkeys4 = ['å®¢ç¾¤', 'channel_rate', 'lending_month']
df_ksauc_all_v4 = cal_ks_auc(df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rate':'æ¸ é“'}, inplace=True)
df_ksauc_all_v4.head()


# In[515]:


df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0, ignore_index=True)
df_ksauc_all_1.rename(columns={'lending_month':'æœˆä»½', 'target_type':'æ ‡ç­¾'},inplace=True)
df_ksauc_all_1.head()


# In[516]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_æ¨¡å‹å¯¹æ¯”åˆ†æ_{task_name}_{timestamp}_éƒ¨åˆ†è¦†ç›–.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}_éƒ¨åˆ†è¦†ç›–.xlsx')


# In[ ]:





# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[517]:


result_path


# In[484]:


df_sample.to_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250117.csv',index=False)
print(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250117.csv')


# In[376]:


score = 'y_prob_v3'


# In[377]:


df_sample['lending_month'].value_counts()


# In[378]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[379]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), 
                                                [score], 'lending_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[381]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: å®æ—¶æ¨¡å‹_æˆä¿¡_fpd30_æ¡”å­å•†åŸ.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æ¡”å­å•†åŸæˆä¿¡æ¨¡å‹fpd30'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result/{task_name}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('===================')
        data = reader.to_pandas()

    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data


# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


df_sample_dict = {}


# In[8]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-15', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
--ç¦»çº¿fpd30èåˆæ¨¡å‹å­åˆ†
,t1.standard_score
--æ´ä¾¦å˜é‡
,t2.bh_a001
,t2.bh_a002
,t2.bh_a003
,t2.bh_a004
,t2.bh_a005
,t2.bh_a006
,t2.bh_a007
,t2.bh_a008
,t2.bh_a009
,t2.bh_a010
,t2.bh_a011
,t2.bh_a012
,t2.bh_a013
,t2.bh_a014
,t2.bh_a015
,t2.bh_a016
,t2.bh_a017
,t2.bh_a018
,t2.bh_a019
,t2.bh_a020
,t2.bh_a021
,t2.bh_a022
,t2.bh_a023
,t2.bh_a024
,t2.bh_a025
,t2.bh_a026
,t2.bh_a027
,t2.bh_a028
,t2.bh_a029
,t2.bh_a030
,t2.bh_a031
,t2.bh_a032
,t2.bh_a033
,t2.bh_a034
,t2.bh_a035
,t2.bh_a036
,t2.bh_a037
,t2.bh_a038
,t2.bh_a039
,t2.bh_a040
,t2.bh_a041
,t2.bh_a042
,t2.bh_a043
,t2.bh_a044
,t2.bh_a045
,t2.bh_a046
,t2.bh_a047
,t2.bh_a048
,t2.bh_a049
,t2.bh_a050
,t2.bh_a051
,t2.bh_a052
,t2.bh_a053
,t2.bh_a054
,t2.bh_a055
,t2.bh_a056
,t2.bh_a057
,t2.bh_a058
,t2.bh_a059
,t2.bh_a060
,t2.bh_a061
,t2.bh_a062
,t2.bh_a063
,t2.bh_a064
,t2.bh_a065
,t2.bh_a066
,t2.bh_a067
,t2.bh_a068
,t2.bh_a069
,t2.bh_a070
,t2.bh_a071
,t2.bh_a072
,t2.bh_a073
,t2.bh_a074
,t2.bh_a075
,t2.bh_a076
,t2.bh_a077
,t2.bh_a078
,t2.bh_a079
,t2.bh_a080
,t2.bh_a081
,t2.bh_a082
,t2.bh_a083
,t2.bh_a084
,t2.bh_a085
,t2.bh_a086
,t2.bh_a087
,t2.bh_a088
,t2.bh_a089
,t2.bh_a090
,t2.bh_a091
,t2.bh_a092
,t2.bh_a093
,t2.bh_a094
,t2.bh_a095
,t2.bh_a096
,t2.bh_a097
,t2.bh_a098
,t2.bh_a099
,t2.bh_a100
,t2.bh_a101
,t2.bh_a102
,t2.bh_a103
,t2.bh_a104
,t2.bh_a105
,t2.bh_a106
,t2.bh_a107
,t2.bh_a108
,t2.bh_a109
,t2.bh_a110
,t2.bh_a111
,t2.bh_a112
,t2.bh_a113
,t2.bh_a114
,t2.bh_a115
,t2.bh_a116
,t2.bh_a117
,t2.bh_a118
,t2.bh_a119
,t2.bh_a120
,t2.bh_a121
,t2.bh_a122
,t2.bh_a123
,t2.bh_a124
,t2.bh_a125
,t2.bh_a126
,t2.bh_a127
,t2.bh_a128
,t2.bh_a129
,t2.bh_a130
,t2.bh_a131
,t2.bh_a132
,t2.bh_a133
,t2.bh_a134
,t2.bh_a135
,t2.bh_a136
,t2.bh_a137
,t2.bh_a138
,t2.bh_a139
,t2.bh_a140
,t2.bh_a141
,t2.bh_a142
,t2.bh_a143
,t2.bh_a144
,t2.bh_a145
,t2.bh_a146
,t2.bh_a147
,t2.bh_a148
,t2.bh_a149
,t2.bh_a150
,t2.bh_a151
,t2.bh_a152
,t2.bh_a153
,t2.bh_a154
,t2.bh_a155
,t2.bh_a156
,t2.bh_a157
,t2.bh_a158
,t2.bh_a159
,t2.bh_a160
,t2.bh_a161
,t2.bh_a162
,t2.bh_a163
,t2.bh_a164
,t2.bh_a165
,t2.bh_a166
,t2.bh_a167
,t2.bh_a168
,t2.bh_a169
,t2.bh_a170
,t2.bh_a171
,t2.bh_a172
,t2.bh_a173
,t2.bh_a174
,t2.bh_a175
,t2.bh_a176
,t2.bh_a177
,t2.bh_a178
,t2.bh_a179
,t2.bh_a180
,t2.bh_a181
,t2.bh_a182
,t2.bh_a183
,t2.bh_a184
,t2.bh_a185
,t2.bh_a186
,t2.bh_a187
,t2.bh_a188
,t2.bh_a189
,t2.bh_a190
,t2.bh_a191
,t2.bh_a192
,t2.bh_a193
,t2.bh_a194
,t2.bh_a195
,t2.bh_a196
,t2.bh_a197
,t2.bh_a198
,t2.bh_a199
,t2.bh_a200
,t2.bh_a201
,t2.bh_a202
,t2.bh_a203
,t2.bh_a204
,t2.bh_a205
,t2.bh_a206
,t2.bh_a207
,t2.bh_a208
,t2.bh_a209
,t2.bh_a210
,t2.bh_a211
,t2.bh_a212
,t2.bh_a213
,t2.bh_a214
,t2.bh_a215
,t2.bh_a216
,t2.bh_a217
,t2.bh_a218
,t2.bh_a219
,t2.bh_a220
,t2.bh_b001
,t2.bh_b002
,t2.bh_b003
,t2.bh_b004
,t2.bh_b005
,t2.bh_b006
,t2.bh_b007
,t2.bh_b008
,t2.bh_b009
,t2.bh_b010
,t2.bh_b011
,t2.bh_b012
,t2.bh_b013
,t2.bh_b014
,t2.bh_b015
,t2.bh_b016
,t2.bh_b017
,t2.bh_b018
,t2.bh_b019
,t2.bh_b020
,t2.bh_b021
,t2.bh_b022
,t2.bh_b023
,t2.bh_b024
,t2.bh_b025
,t2.bh_b026
,t2.bh_b027
,t2.bh_b028
,t2.bh_b029
,t2.bh_b030
,t2.bh_b031
,t2.bh_b032
,t2.bh_b033
,t2.bh_b034
,t2.bh_b035
,t2.bh_b036
,t2.bh_b037
,t2.bh_b038
,t2.bh_b039
,t2.bh_b040
,t2.bh_b041
,t2.bh_b042
,t2.bh_b043
,t2.bh_b044
,t2.bh_b045
,t2.bh_b046
,t2.bh_b047
,t2.bh_b048
,t2.bh_b049
,t2.bh_b050
,t2.bh_b051
,t2.bh_b052
,t2.bh_b053
,t2.bh_b054
,t2.bh_b055
,t2.bh_b056
,t2.bh_b057
,t2.bh_b058
,t2.bh_b059
,t2.bh_b060
,t2.bh_b061
,t2.bh_b062
,t2.bh_b063
,t2.bh_b064
,t2.bh_b065
,t2.bh_b066
,t2.bh_b067
,t2.bh_b068
,t2.bh_b069
,t2.bh_b070
,t2.bh_b071
,t2.bh_b072
,t2.bh_b073
,t2.bh_b074
,t2.bh_b075
,t2.bh_b076
,t2.bh_b077
,t2.bh_b078
,t2.bh_b079
,t2.bh_b080
,t2.bh_b081
,t2.bh_b082
,t2.bh_b083
,t2.bh_b084
,t2.bh_b085
,t2.bh_b086
,t2.bh_b087
,t2.bh_b088
,t2.bh_b089
,t2.bh_b090
,t2.bh_b091
,t2.bh_b092
,t2.bh_b093
,t2.bh_b094
,t2.bh_b095
,t2.bh_b096
,t2.bh_b097
,t2.bh_b098
,t2.bh_b099
,t2.bh_b100
,t2.bh_b101
,t2.bh_b102
,t2.bh_b103
,t2.bh_b104
,t2.bh_b105
,t2.bh_b106
,t2.bh_b107
,t2.bh_b108
,t2.bh_b109
,t2.bh_b110
,t2.bh_b111
,t2.bh_b112
,t2.bh_b113
,t2.bh_b114
,t2.bh_b115
,t2.bh_b116
,t2.bh_b117
,t2.bh_b118
,t2.bh_b119
,t2.bh_b120
,t2.bh_b121
,t2.bh_b122
,t2.bh_b123
,t2.bh_b124
,t2.bh_b125
,t2.bh_b126
,t2.bh_b127
,t2.bh_b128
,t2.bh_b129
,t2.bh_b130
,t2.bh_b131
,t2.bh_b132
,t2.bh_b133
,t2.bh_b134
,t2.bh_b135
,t2.bh_b136
,t2.bh_b137
,t2.bh_b138
,t2.bh_b139
,t2.bh_b140
,t2.bh_b141
,t2.bh_b142
,t2.bh_b143
,t2.bh_b144
,t2.bh_b145
,t2.bh_b146
,t2.bh_b147
,t2.bh_b148
,t2.bh_b149
,t2.bh_b150
,t2.bh_b151
,t2.bh_b152
,t2.bh_b153
,t2.bh_b154
,t2.bh_b155
,t2.bh_b156
,t2.bh_b157
,t2.bh_b158
,t2.bh_b159
,t2.bh_b160
,t2.bh_b161
,t2.bh_b162
,t2.bh_b163
,t2.bh_b164
,t2.bh_b165
,t2.bh_b166
,t2.bh_b167
,t2.bh_b168
,t2.bh_b169
,t2.bh_b170
,t2.bh_b171
,t2.bh_b172
,t2.bh_b173
,t2.bh_b174
,t2.bh_b175
,t2.bh_b176
,t2.bh_b177
,t2.bh_b178
,t2.bh_b179
,t2.bh_b180
,t2.bh_b181
,t2.bh_b182
,t2.bh_b183
,t2.bh_b184
,t2.bh_b185
,t2.bh_b186
,t2.bh_b187
,t2.bh_b188
,t2.bh_b189
,t2.bh_b190
,t2.bh_b191
,t2.bh_b192
,t2.bh_b193
,t2.bh_b194
,t2.bh_b195
,t2.bh_b196
,t2.bh_b197
,t2.bh_b198
,t2.bh_b199
,t2.bh_b200
,t2.bh_b201
,t2.bh_b202
,t2.bh_b203
,t2.bh_b204
,t2.bh_b205
,t2.bh_b206
,t2.bh_b207
,t2.bh_b208
,t2.bh_b209
,t2.bh_b210
,t2.bh_b211
,t2.bh_b212
,t2.bh_b213
,t2.bh_b214
,t2.bh_b215
,t2.bh_b216
,t2.bh_b217
,t2.bh_b218
,t2.bh_b219
,t2.bh_b220
,t2.bh_b221
,t2.bh_b222
,t2.bh_b223
,t2.bh_b224
,t2.bh_b225
,t2.bh_b226
,t2.bh_b227
,t2.bh_b228
,t2.bh_b229
,t2.bh_b230
,t2.bh_b231
,t2.bh_b232
,t2.bh_b233
,t2.bh_b234
,t2.bh_b235
,t2.bh_b236
,t2.bh_b237
,t2.bh_b238
,t2.bh_b239
,t2.bh_b240
,t2.bh_b241
,t2.bh_b242
,t2.bh_b243
,t2.bh_b244
,t2.bh_b245
,t2.bh_b246
,t2.bh_b247
,t2.bh_b248
,t2.bh_b249
,t2.bh_b250
,t2.bh_b251
,t2.bh_b252
,t2.bh_b253
,t2.bh_b254
,t2.bh_b255
,t2.bh_b256
,t2.bh_b257
,t2.bh_b258
,t2.bh_b259
,t2.bh_b260
,t2.bh_b261
,t2.bh_b262
,t2.bh_b263
,t2.bh_b264
,t2.bh_b265
,t2.bh_b266
,t2.bh_b267
,t2.bh_b268
,t2.bh_b269
,t2.bh_b270
,t2.bh_b271
,t2.bh_b272
,t2.bh_b273
,t2.bh_b274
,t2.bh_b275
,t2.bh_b276
,t2.bh_b277
,t2.bh_b278
,t2.bh_b279
,t2.bh_b280
,t2.bh_b281
,t2.bh_b282
,t2.bh_b283
,t2.bh_b284
,t2.bh_b285
,t2.bh_b286
,t2.bh_b287
,t2.bh_b288
,t2.bh_b289
,t2.bh_b290
,t2.bh_b291
,t2.bh_b292
,t2.bh_b293
,t2.bh_b294
,t2.bh_b295
,t2.bh_b296
,t2.bh_b297
,t2.bh_b298
,t2.bh_b299
,t2.bh_b300
,t2.bh_b301
,t2.bh_b302
,t2.bh_b303
,t2.bh_b304
,t2.bh_b305
,t2.bh_b306
,t2.bh_b307
,t2.bh_b308
,t2.bh_b309
,t2.bh_b310
,t2.bh_b311
,t2.bh_b312
,t2.bh_b313
,t2.bh_b314
,t2.bh_b315
,t2.bh_b316
,t2.bh_b317
,t2.bh_c001
,t2.bh_c002
,t2.bh_c003
,t2.bh_c004
,t2.bh_c005
,t2.bh_c006
,t2.bh_c007
,t2.bh_c008
,t2.bh_c009
,t2.bh_c010
,t2.bh_c011
,t2.bh_c012
,t2.bh_c013
,t2.bh_c014
,t2.bh_c015
,t2.bh_c016
,t2.bh_c017
,t2.bh_c018
,t2.bh_c019
,t2.bh_c020
,t2.bh_c021
,t2.bh_c022
,t2.bh_c023
,t2.bh_c024
,t2.bh_c025
,t2.bh_c026
,t2.bh_c027
,t2.bh_c028
,t2.bh_c029
,t2.bh_c030
,t2.bh_c031
,t2.bh_c032
,t2.bh_c033
,t2.bh_c034
,t2.bh_c035
,t2.bh_c036
,t2.bh_c037
,t2.bh_c038
,t2.bh_c039
,t2.bh_c040
,t2.bh_c041
,t2.bh_c042
,t2.bh_c043
,t2.bh_c044
,t2.bh_c045
,t2.bh_c046
,t2.bh_c047
,t2.bh_c048
,t2.bh_c049
,t2.bh_c050
,t2.bh_c051
,t2.bh_c052
,t2.bh_c053
,t2.bh_c054
,t2.bh_c055
,t2.bh_c056
,t2.bh_c057
,t2.bh_c058
,t2.bh_c059
,t2.bh_c060
,t2.bh_c061
,t2.bh_c062
,t2.bh_c063
,t2.bh_c064
,t2.bh_c065
,t2.bh_c066
,t2.bh_c067
,t2.bh_c068
,t2.bh_c069
,t2.bh_c070
,t2.bh_c071
,t2.bh_c072
,t2.bh_c073
,t2.bh_c074
,t2.bh_c075
,t2.bh_c076
,t2.bh_c077
,t2.bh_c078
,t2.bh_c079
,t2.bh_c080
,t2.bh_d001
,t2.bh_d002
,t2.bh_d003
,t2.bh_d004
,t2.bh_d005
,t2.bh_d006
,t2.bh_d007
,t2.bh_d008
,t2.bh_d009
,t2.bh_d010
,t2.bh_d011
,t2.bh_d012
,t2.bh_d013
,t2.bh_d014
,t2.bh_d015
,t2.bh_d016
,t2.bh_d017
,t2.bh_d018
,t2.bh_d019
,t2.bh_d020
,t2.bh_d021
,t2.bh_d022
,t2.bh_d023
,t2.bh_d024
,t2.bh_d025
,t2.bh_d026
,t2.bh_d027
,t2.bh_d028
,t2.bh_d029
,t2.bh_d030
,t2.bh_d031
,t2.bh_d032
,t2.bh_d033
,t2.bh_d034
,t2.bh_d035
,t2.bh_d036
,t2.bh_d037
,t2.bh_d038
,t2.bh_d039
,t2.bh_d040
,t2.bh_d041
,t2.bh_d042
,t2.bh_d043
,t2.bh_d044
,t2.bh_d045
,t2.bh_e001
,t2.bh_e002
,t2.bh_e003
,t2.bh_e004
,t2.bh_e005
,t2.bh_e006
,t2.bh_e007
,t2.bh_e008
,t2.bh_e009
,t2.bh_e010
,t2.bh_e011
,t2.bh_e012
,t2.bh_e013
,t2.bh_e014
,t2.bh_e015
,t2.bh_e016
,t2.bh_e017
,t2.bh_e018
,t2.bh_e019
,t2.bh_e020
,t2.bh_e021
,t2.bh_e022
,t2.bh_e023
,t2.bh_e024
,t2.bh_e025
,t2.bh_e026
,t2.bh_e027
,t2.bh_e028
,t2.bh_e029
,t2.bh_e030
,t2.bh_e031
,t2.bh_e032
,t2.bh_e033
,t2.bh_e034
,t2.bh_e035
,t2.bh_e036
,t2.bh_e037
,t2.bh_e038
,t2.bh_e039
,t2.bh_e040
,t2.bh_e041
,t2.bh_e042
,t2.bh_e043
,t2.bh_e044
,t2.bh_e045
,t2.bh_e046
,t2.bh_e047
,t2.bh_e048
,t2.bh_e049
,t2.bh_e050
,t2.bh_e051
,t2.bh_e052
,t2.bh_e053
,t2.bh_e054
,t2.bh_e055
,t2.bh_e056
,t2.bh_e057
,t2.bh_e058
,t2.bh_e059
,t2.bh_e060
,t2.bh_e061
,t2.bh_e062
,t2.bh_e063
,t2.bh_e064
,t2.bh_e065
,t2.bh_e066
,t2.bh_f001
,t2.bh_f002
,t2.bh_f003
,t2.bh_f004
,t2.bh_f005
,t2.bh_f006
,t2.bh_f007
,t2.bh_f008
,t2.bh_f009
,t2.bh_f010
,t2.bh_f011
,t2.bh_f012
,t2.bh_f013
,t2.bh_f014
,t2.bh_f015
,t2.bh_f016
,t2.bh_f017
,t2.bh_f018
,t2.bh_f019
,t2.bh_f020
,t2.bh_f021
,t2.bh_f022
,t2.bh_f023
,t2.bh_f024
,t2.bh_f025
,t2.bh_f026
,t2.bh_f027
,t2.bh_f028
,t2.bh_f029
,t2.bh_f030
,t2.bh_g001
,t2.bh_g002
,t2.bh_g003
,t2.bh_g004
,t2.bh_g005
,t2.bh_g006
,t2.bh_g007
,t2.bh_g008
,t2.bh_g009
,t2.bh_g010
,t2.bh_g011
,t2.bh_g012
,t2.bh_g013
,t2.bh_g014
,t2.bh_g015
,t2.bh_g016
,t2.bh_g017
,t2.bh_g018
,t2.bh_g019
,t2.bh_g020
,t2.bh_g021
,t2.bh_g022
,t2.bh_g023
,t2.bh_g024
,t2.bh_g025
,t2.bh_g026
,t2.bh_g027
,t2.bh_g028
,t2.bh_g029
,t2.bh_g030
,t2.bh_g031
,t2.bh_g032
,t2.bh_g033
,t2.bh_g034
,t2.bh_g035
,t2.bh_g036
,t2.bh_g037
,t2.bh_g038
,t2.bh_g039
,t2.bh_g040
,t2.bh_g041
,t2.bh_g042
,t2.bh_g043
,t2.bh_g044
,t2.bh_g045
,t2.bh_g046
,t2.bh_g047
,t2.bh_g048
,t2.bh_g049
,t2.bh_g050
,t2.bh_g051
,t2.bh_g052
,t2.bh_g053
,t2.bh_g054
,t2.bh_g055
,t2.bh_g056
,t2.bh_g057
,t2.bh_g058
,t2.bh_g059
,t2.bh_g060
,t2.bh_h001
,t2.bh_h002
,t2.bh_h003
,t2.bh_h004
,t2.bh_h005
,t2.bh_h006
,t2.bh_h007
,t2.bh_h008
,t2.bh_h009
,t2.bh_h010
,t2.bh_h011
,t2.bh_h012
,t2.bh_h013
,t2.bh_h014
,t2.bh_h015
,t2.bh_h016
,t2.bh_h017
,t2.bh_h018
,t2.bh_h019
,t2.bh_h020
,t2.bh_h021
,t2.bh_h022
,t2.bh_h023
,t2.bh_h024
,t2.bh_h025
,t2.bh_h026
,t2.bh_h027
,t2.bh_h028
,t2.bh_h029
,t2.bh_h030
,t2.bh_h031
,t2.bh_h032
,t2.bh_h033
,t2.bh_h034
,t2.bh_h035
,t2.bh_h036
,t2.bh_h037
,t2.bh_h038
,t2.bh_h039
,t2.bh_h040
,t2.bh_h041
,t2.bh_h042
,t2.bh_h043
,t2.bh_h044
,t2.bh_h045
,t2.bh_h046
,t2.bh_h047
,t2.bh_h048
,t2.bh_h049
,t2.bh_h050
,t2.bh_h051
,t2.bh_h052
,t2.bh_h053
,t2.bh_h054
,t2.bh_h055
,t2.bh_h056
,t2.bh_h057
,t2.bh_h058
,t2.bh_h059
,t2.bh_h060
,t2.bh_h061
,t2.bh_h062
,t2.bh_h063
,t2.bh_h064
,t2.bh_h065
,t2.bh_h066
,t2.bh_h067
,t2.bh_h068
,t2.bh_h069
,t2.bh_h070
,t2.bh_h071
,t2.bh_h072
,t2.bh_h073
,t2.bh_h074
,t2.bh_h075
,t2.bh_h076
,t2.bh_h077
,t2.bh_h078
,t2.bh_h079
,t2.bh_h080
,t2.bh_h081
,t2.bh_h082
,t2.bh_h083
,t2.bh_h084
,t2.bh_h085
,t2.bh_h086
,t2.bh_h087
,t2.bh_h088
,t2.bh_h089
,t2.bh_h090
,t2.bh_h091
,t2.bh_h092
,t2.bh_h093
,t2.bh_h094
,t2.bh_h095
,t2.bh_h096
,t2.bh_h097
,t2.bh_h098
,t2.bh_h099
,t2.bh_h100
,t2.bh_h101
,t2.bh_h102
,t2.bh_h103
,t2.bh_h104
,t2.bh_h105
,t2.bh_h106
,t2.bh_h107
,t2.bh_h108
,t2.bh_h109
,t2.bh_h110
,t2.bh_h111
,t2.bh_h112
,t2.bh_h113
,t2.bh_h114
,t2.bh_h115
,t2.bh_h116
,t2.bh_h117
,t2.bh_h118
,t2.bh_h119
,t2.bh_h120
,t2.bh_h121
,t2.bh_h122
,t2.bh_h123
,t2.bh_h124
,t2.bh_h125
,t2.bh_h126
,t2.bh_h127
,t2.bh_h128
,t2.bh_h129
,t2.bh_h130
,t2.bh_h131
,t2.bh_h132
,t2.bh_h133
,t2.bh_h134
,t2.bh_h135
,t2.bh_h136
,t2.bh_h137
,t2.bh_h138
,t2.bh_h139
,t2.bh_h140
,t2.bh_h141
,t2.bh_h142
,t2.bh_h143
,t2.bh_h144
,t2.bh_h145
,t2.bh_h146
,t2.bh_h147
,t2.bh_h148
,t2.bh_h149
,t2.bh_h150
,t2.bh_h151
,t2.bh_h152
,t2.bh_h153
,t2.bh_h154
,t2.bh_h155
,t2.bh_h156
,t2.bh_h157
,t2.bh_h158
,t2.bh_h159
,t2.bh_h160
,t2.bh_h161
,t2.bh_h162
,t2.bh_h163
,t2.bh_h164
,t2.bh_h165
,t2.bh_h166
,t2.bh_h167
,t2.bh_h168
,t2.bh_h169
,t2.bh_h170
,t2.bh_h171
,t2.bh_h172
,t2.bh_h173
,t2.bh_a221
,t2.bh_a222
,t2.bh_a223
,t2.bh_a224
,t2.bh_a225
,t2.bh_a226
,t2.bh_a227
,t2.bh_a228
,t2.bh_a229
,t2.bh_a230
,t2.bh_a231
,t2.bh_a232
,t2.bh_a233
,t2.bh_a234
,t2.bh_a235
,t2.bh_a236
,t2.bh_a237
,t2.bh_a238
,t2.bh_a239
,t2.bh_a240
,t2.bh_a241
,t2.bh_a242
,t2.bh_a243
,t2.bh_a244
,t2.bh_a245
,t2.bh_a246
,t2.bh_a247
,t2.bh_a248
,t2.bh_a249
,t2.bh_a250
,t2.bh_a251
,t2.bh_a252
,t2.bh_a253
,t2.bh_a254
,t2.bh_a255
,t2.bh_a256
,t2.bh_a257
,t2.bh_a258
,t2.bh_a259
,t2.bh_a260
,t2.bh_a261
,t2.bh_a262
,t2.bh_a263
,t2.bh_a264
,t2.bh_a265
,t2.bh_a266
,t2.bh_a267
,t2.bh_a268
,t2.bh_a269
,t2.bh_a270
,t2.bh_a271
,t2.bh_a272
,t2.bh_a273
,t2.bh_a274
,t2.bh_a275
,t2.bh_a276
,t2.bh_a277
,t2.bh_a278
,t2.bh_a279
,t2.bh_a280
,t2.bh_a281
,t2.bh_a282
,t2.bh_a283
,t2.bh_a284
,t2.bh_a285
,t2.bh_a286
,t2.bh_a287
,t2.bh_a288
,t2.bh_a289
,t2.bh_a290
,t2.bh_a291
,t2.bh_a292
,t2.bh_a293
,t2.bh_a294
,t2.bh_a295
,t2.bh_a296
,t2.bh_a297
,t2.bh_a298
,t2.bh_a299
,t2.bh_a300
,t2.bh_a301
,t2.bh_a302
,t2.bh_a303
,t2.bh_a304
,t2.bh_a305
,t2.bh_a306
,t2.bh_a307
,t2.bh_a308
,t2.bh_a309
,t2.bh_a310
,t2.bh_a311
,t2.bh_a312
,t2.bh_a313
,t2.bh_a314
,t2.bh_a315
,t2.bh_a316
,t2.bh_a317
,t2.bh_a318
,t2.bh_a319
,t2.bh_a320
,t2.bh_a321
,t2.bh_a322
,t2.bh_a323
,t2.bh_a324
,t2.bh_a325
,t2.bh_a326
,t2.bh_a327
,t2.bh_a328
,t2.bh_a329
,t2.bh_a330
,t2.bh_a331
,t2.bh_a332
,t2.bh_a333
,t2.bh_a334
,t2.bh_a335
,t2.bh_a336
,t2.bh_a337
,t2.bh_a338
,t2.bh_a339
,t2.bh_a340
,t2.bh_a341
,t2.bh_a342
,t2.bh_a343
,t2.bh_a344
,t2.bh_a345
,t2.bh_a346
,t2.bh_a347
,t2.bh_a348
,t2.bh_a349
,t2.bh_a350
,t2.bh_a351
,t2.bh_a352
,t2.bh_a353
,t2.bh_a354
,t2.bh_a355
,t2.bh_a356
,t2.bh_a357
,t2.bh_a358
,t2.bh_a359
,t2.bh_a360
,t2.bh_a361
,t2.bh_a362
,t2.bh_a363
,t2.bh_a364
,t2.bh_a365
,t2.bh_a366
,t2.bh_a367
,t2.bh_a368
,t2.bh_a369
,t2.bh_a370
,t2.bh_a371
,t2.bh_a372
,t2.bh_a373
,t2.bh_a374
,t2.bh_a375
,t2.bh_a376
,t2.bh_a377
,t2.bh_a378
,t2.bh_a379
,t2.bh_a380
,t2.bh_a381
,t2.bh_a382
,t2.bh_a383
,t2.bh_a384
,t2.bh_a385
,t2.bh_a386
,t2.bh_a387
,t2.bh_a388
,t2.bh_a389
,t2.bh_a390
,t2.bh_a391
,t2.bh_a392
,t2.bh_a393
,t2.bh_qu001
,t2.bh_qu002
,t2.bh_qu003
,t2.bh_qu004
,t2.bh_qu005
,t2.bh_qu006
,t2.bh_qu007
,t2.bh_qu008
,t2.bh_qu009
,t2.bh_qu010
,t2.bh_qu011
,t2.bh_qu012
,t2.bh_qu013
,t2.bh_qu014
,t2.bh_qu015
,t2.bh_qu016
,t2.bh_qu017
,t2.bh_qu018
,t2.bh_qu019
,t2.bh_qu020
,t2.bh_qu021
,t2.bh_qu022
,t2.bh_qu023
,t2.bh_qu024
,t2.bh_x001
,t2.bh_x002
,t2.bh_x003
,t2.bh_x004
,t2.bh_x005
,t2.bh_x006
,t2.bh_x007
,t2.bh_x008
,t2.bh_x009
,t2.bh_x010
,t2.bh_x011
,t2.bh_x012
,t2.bh_x013
,t2.bh_x014
,t2.bh_x015
,t2.bh_x016
,t2.bh_x017
,t2.bh_x018
,t2.bh_x019
,t2.bh_x020
,t2.bh_x021
,t2.bh_x022
,t2.bh_x023
,t2.bh_x024
,t2.bh_x025
,t2.bh_x026
,t2.bh_x027
,t2.bh_x028
,t2.bh_x029
,t2.bh_x030
,t2.bh_x031
,t2.bh_x032
,t2.bh_x033
,t2.bh_x034
,t2.bh_x035
,t2.bh_x036
,t2.bh_x037
,t2.bh_x038
,t2.bh_x039
,t2.bh_x040
,t2.bh_x041
,t2.bh_x042
,t2.bh_x043
,t2.bh_x044
,t2.bh_x045
,t2.bh_x046
,t2.bh_x047
,t2.bh_x048
,t2.bh_x049
,t2.bh_x050
,t2.bh_x051
,t2.bh_x052
,t2.bh_x053
,t2.bh_x054
,t2.bh_x055
,t2.bh_x056
,t2.bh_x057
,t2.bh_x058
,t2.bh_x059
,t2.bh_x060
,t2.bh_x061
,t2.bh_x062
,t2.bh_x063
,t2.bh_x064
,t2.bh_x065
,t2.bh_x066
,t2.bh_x067
,t2.bh_x068
,t2.bh_x069
,t2.bh_x070
,t2.bh_x071
,t2.bh_x072
,t2.bh_x073
,t2.bh_x074
,t2.bh_x075
,t2.bh_x076
,t2.bh_x077
,t2.bh_x078
,t2.bh_x079
,t2.bh_x080
,t2.bh_x081
,t2.bh_x082
,t2.bh_x083
,t2.bh_x084
,t2.bh_x085
,t2.bh_x086
,t2.bh_x087
,t2.bh_x088
,t2.bh_x089
,t2.bh_x090
,t2.bh_x091
,t2.bh_x092
,t2.bh_x093
,t2.bh_x094
,t2.bh_x095
,t2.bh_x096
,t2.bh_x097
,t2.bh_x098
,t2.bh_x099
,t2.bh_x100
,t2.bh_x101
,t2.bh_x102
,t2.bh_x103
,t2.bh_x104
,t2.bh_x105
,t2.bh_x106
,t2.bh_x107
,t2.bh_x108
,t2.bh_x109
,t2.bh_x110
,t2.bh_x111
,t2.bh_x112
,t2.bh_x113
,t2.bh_x114
,t2.bh_x115
,t2.bh_x116
,t2.bh_x117
,t2.bh_x118
,t2.bh_x119
,t2.bh_x120
,t2.bh_x121
,t2.bh_x122
,t2.bh_x123
,t2.bh_x124
,t2.bh_x125
,t2.bh_x126
,t2.bh_x127
,t2.bh_x128
,t2.bh_x129
,t2.bh_x130
,t2.bh_x131
,t2.bh_x132
,t2.bh_x133
,t2.bh_x134
,t2.bh_x135
,t2.bh_x136
,t2.bh_x137
,t2.bh_x138
,t2.bh_x139
,t2.bh_x140
,t2.bh_x141
,t2.bh_x142
,t2.bh_x143
,t2.bh_x144
,t2.bh_q001_q0
,t2.bh_q001_q5
,t2.bh_q001_q10
,t2.bh_q001_q15
,t2.bh_q001_q20
,t2.bh_q001_q25
,t2.bh_q001_q30
,t2.bh_q001_q35
,t2.bh_q001_q40
,t2.bh_q001_q45
,t2.bh_q001_q50
,t2.bh_q001_q55
,t2.bh_q001_q60
,t2.bh_q001_q65
,t2.bh_q001_q70
,t2.bh_q001_q75
,t2.bh_q001_q80
,t2.bh_q001_q85
,t2.bh_q001_q90
,t2.bh_q001_q95
,t2.bh_q001_q100
,t2.bh_q002_q0
,t2.bh_q002_q5
,t2.bh_q002_q10
,t2.bh_q002_q15
,t2.bh_q002_q20
,t2.bh_q002_q25
,t2.bh_q002_q30
,t2.bh_q002_q35
,t2.bh_q002_q40
,t2.bh_q002_q45
,t2.bh_q002_q50
,t2.bh_q002_q55
,t2.bh_q002_q60
,t2.bh_q002_q65
,t2.bh_q002_q70
,t2.bh_q002_q75
,t2.bh_q002_q80
,t2.bh_q002_q85
,t2.bh_q002_q90
,t2.bh_q002_q95
,t2.bh_q002_q100
,t2.bh_q003_q0
,t2.bh_q003_q5
,t2.bh_q003_q10
,t2.bh_q003_q15
,t2.bh_q003_q20
,t2.bh_q003_q25
,t2.bh_q003_q30
,t2.bh_q003_q35
,t2.bh_q003_q40
,t2.bh_q003_q45
,t2.bh_q003_q50
,t2.bh_q003_q55
,t2.bh_q003_q60
,t2.bh_q003_q65
,t2.bh_q003_q70
,t2.bh_q003_q75
,t2.bh_q003_q80
,t2.bh_q003_q85
,t2.bh_q003_q90
,t2.bh_q003_q95
,t2.bh_q003_q100
,t2.bh_q004_q0
,t2.bh_q004_q5
,t2.bh_q004_q10
,t2.bh_q004_q15
,t2.bh_q004_q20
,t2.bh_q004_q25
,t2.bh_q004_q30
,t2.bh_q004_q35
,t2.bh_q004_q40
,t2.bh_q004_q45
,t2.bh_q004_q50
,t2.bh_q004_q55
,t2.bh_q004_q60
,t2.bh_q004_q65
,t2.bh_q004_q70
,t2.bh_q004_q75
,t2.bh_q004_q80
,t2.bh_q004_q85
,t2.bh_q004_q90
,t2.bh_q004_q95
,t2.bh_q004_q100
,t2.bh_q005_q0
,t2.bh_q005_q5
,t2.bh_q005_q10
,t2.bh_q005_q15
,t2.bh_q005_q20
,t2.bh_q005_q25
,t2.bh_q005_q30
,t2.bh_q005_q35
,t2.bh_q005_q40
,t2.bh_q005_q45
,t2.bh_q005_q50
,t2.bh_q005_q55
,t2.bh_q005_q60
,t2.bh_q005_q65
,t2.bh_q005_q70
,t2.bh_q005_q75
,t2.bh_q005_q80
,t2.bh_q005_q85
,t2.bh_q005_q90
,t2.bh_q005_q95
,t2.bh_q005_q100
,t2.bh_q006_q0
,t2.bh_q006_q5
,t2.bh_q006_q10
,t2.bh_q006_q15
,t2.bh_q006_q20
,t2.bh_q006_q25
,t2.bh_q006_q30
,t2.bh_q006_q35
,t2.bh_q006_q40
,t2.bh_q006_q45
,t2.bh_q006_q50
,t2.bh_q006_q55
,t2.bh_q006_q60
,t2.bh_q006_q65
,t2.bh_q006_q70
,t2.bh_q006_q75
,t2.bh_q006_q80
,t2.bh_q006_q85
,t2.bh_q006_q90
,t2.bh_q006_q95
,t2.bh_q006_q100
,t2.bh_q007_q0
,t2.bh_q007_q5
,t2.bh_q007_q10
,t2.bh_q007_q15
,t2.bh_q007_q20
,t2.bh_q007_q25
,t2.bh_q007_q30
,t2.bh_q007_q35
,t2.bh_q007_q40
,t2.bh_q007_q45
,t2.bh_q007_q50
,t2.bh_q007_q55
,t2.bh_q007_q60
,t2.bh_q007_q65
,t2.bh_q007_q70
,t2.bh_q007_q75
,t2.bh_q007_q80
,t2.bh_q007_q85
,t2.bh_q007_q90
,t2.bh_q007_q95
,t2.bh_q007_q100
,t2.bh_q008_q0
,t2.bh_q008_q5
,t2.bh_q008_q10
,t2.bh_q008_q15
,t2.bh_q008_q20
,t2.bh_q008_q25
,t2.bh_q008_q30
,t2.bh_q008_q35
,t2.bh_q008_q40
,t2.bh_q008_q45
,t2.bh_q008_q50
,t2.bh_q008_q55
,t2.bh_q008_q60
,t2.bh_q008_q65
,t2.bh_q008_q70
,t2.bh_q008_q75
,t2.bh_q008_q80
,t2.bh_q008_q85
,t2.bh_q008_q90
,t2.bh_q008_q95
,t2.bh_q008_q100
,t2.bh_q009_q0
,t2.bh_q009_q5
,t2.bh_q009_q10
,t2.bh_q009_q15
,t2.bh_q009_q20
,t2.bh_q009_q25
,t2.bh_q009_q30
,t2.bh_q009_q35
,t2.bh_q009_q40
,t2.bh_q009_q45
,t2.bh_q009_q50
,t2.bh_q009_q55
,t2.bh_q009_q60
,t2.bh_q009_q65
,t2.bh_q009_q70
,t2.bh_q009_q75
,t2.bh_q009_q80
,t2.bh_q009_q85
,t2.bh_q009_q90
,t2.bh_q009_q95
,t2.bh_q009_q100
,t2.bh_q010_q0
,t2.bh_q010_q5
,t2.bh_q010_q10
,t2.bh_q010_q15
,t2.bh_q010_q20
,t2.bh_q010_q25
,t2.bh_q010_q30
,t2.bh_q010_q35
,t2.bh_q010_q40
,t2.bh_q010_q45
,t2.bh_q010_q50
,t2.bh_q010_q55
,t2.bh_q010_q60
,t2.bh_q010_q65
,t2.bh_q010_q70
,t2.bh_q010_q75
,t2.bh_q010_q80
,t2.bh_q010_q85
,t2.bh_q010_q90
,t2.bh_q010_q95
,t2.bh_q010_q100
,t2.bh_q011_q0
,t2.bh_q011_q5
,t2.bh_q011_q10
,t2.bh_q011_q15
,t2.bh_q011_q20
,t2.bh_q011_q25
,t2.bh_q011_q30
,t2.bh_q011_q35
,t2.bh_q011_q40
,t2.bh_q011_q45
,t2.bh_q011_q50
,t2.bh_q011_q55
,t2.bh_q011_q60
,t2.bh_q011_q65
,t2.bh_q011_q70
,t2.bh_q011_q75
,t2.bh_q011_q80
,t2.bh_q011_q85
,t2.bh_q011_q90
,t2.bh_q011_q95
,t2.bh_q011_q100
,t2.bh_q012_q0
,t2.bh_q012_q5
,t2.bh_q012_q10
,t2.bh_q012_q15
,t2.bh_q012_q20
,t2.bh_q012_q25
,t2.bh_q012_q30
,t2.bh_q012_q35
,t2.bh_q012_q40
,t2.bh_q012_q45
,t2.bh_q012_q50
,t2.bh_q012_q55
,t2.bh_q012_q60
,t2.bh_q012_q65
,t2.bh_q012_q70
,t2.bh_q012_q75
,t2.bh_q012_q80
,t2.bh_q012_q85
,t2.bh_q012_q90
,t2.bh_q012_q95
,t2.bh_q012_q100
,t2.bh_q013_q0
,t2.bh_q013_q5
,t2.bh_q013_q10
,t2.bh_q013_q15
,t2.bh_q013_q20
,t2.bh_q013_q25
,t2.bh_q013_q30
,t2.bh_q013_q35
,t2.bh_q013_q40
,t2.bh_q013_q45
,t2.bh_q013_q50
,t2.bh_q013_q55
,t2.bh_q013_q60
,t2.bh_q013_q65
,t2.bh_q013_q70
,t2.bh_q013_q75
,t2.bh_q013_q80
,t2.bh_q013_q85
,t2.bh_q013_q90
,t2.bh_q013_q95
,t2.bh_q013_q100
,t2.bh_q014_q0
,t2.bh_q014_q5
,t2.bh_q014_q10
,t2.bh_q014_q15
,t2.bh_q014_q20
,t2.bh_q014_q25
,t2.bh_q014_q30
,t2.bh_q014_q35
,t2.bh_q014_q40
,t2.bh_q014_q45
,t2.bh_q014_q50
,t2.bh_q014_q55
,t2.bh_q014_q60
,t2.bh_q014_q65
,t2.bh_q014_q70
,t2.bh_q014_q75
,t2.bh_q014_q80
,t2.bh_q014_q85
,t2.bh_q014_q90
,t2.bh_q014_q95
,t2.bh_q014_q100
,t2.bh_q015_q0
,t2.bh_q015_q5
,t2.bh_q015_q10
,t2.bh_q015_q15
,t2.bh_q015_q20
,t2.bh_q015_q25
,t2.bh_q015_q30
,t2.bh_q015_q35
,t2.bh_q015_q40
,t2.bh_q015_q45
,t2.bh_q015_q50
,t2.bh_q015_q55
,t2.bh_q015_q60
,t2.bh_q015_q65
,t2.bh_q015_q70
,t2.bh_q015_q75
,t2.bh_q015_q80
,t2.bh_q015_q85
,t2.bh_q015_q90
,t2.bh_q015_q95
,t2.bh_q015_q100
,t2.bh_q016_q0
,t2.bh_q016_q5
,t2.bh_q016_q10
,t2.bh_q016_q15
,t2.bh_q016_q20
,t2.bh_q016_q25
,t2.bh_q016_q30
,t2.bh_q016_q35
,t2.bh_q016_q40
,t2.bh_q016_q45
,t2.bh_q016_q50
,t2.bh_q016_q55
,t2.bh_q016_q60
,t2.bh_q016_q65
,t2.bh_q016_q70
,t2.bh_q016_q75
,t2.bh_q016_q80
,t2.bh_q016_q85
,t2.bh_q016_q90
,t2.bh_q016_q95
,t2.bh_q016_q100
,t2.bh_q017_q0
,t2.bh_q017_q5
,t2.bh_q017_q10
,t2.bh_q017_q15
,t2.bh_q017_q20
,t2.bh_q017_q25
,t2.bh_q017_q30
,t2.bh_q017_q35
,t2.bh_q017_q40
,t2.bh_q017_q45
,t2.bh_q017_q50
,t2.bh_q017_q55
,t2.bh_q017_q60
,t2.bh_q017_q65
,t2.bh_q017_q70
,t2.bh_q017_q75
,t2.bh_q017_q80
,t2.bh_q017_q85
,t2.bh_q017_q90
,t2.bh_q017_q95
,t2.bh_q017_q100
,t2.bh_q018_q0
,t2.bh_q018_q5
,t2.bh_q018_q10
,t2.bh_q018_q15
,t2.bh_q018_q20
,t2.bh_q018_q25
,t2.bh_q018_q30
,t2.bh_q018_q35
,t2.bh_q018_q40
,t2.bh_q018_q45
,t2.bh_q018_q50
,t2.bh_q018_q55
,t2.bh_q018_q60
,t2.bh_q018_q65
,t2.bh_q018_q70
,t2.bh_q018_q75
,t2.bh_q018_q80
,t2.bh_q018_q85
,t2.bh_q018_q90
,t2.bh_q018_q95
,t2.bh_q018_q100
,t2.bh_q019_q0
,t2.bh_q019_q5
,t2.bh_q019_q10
,t2.bh_q019_q15
,t2.bh_q019_q20
,t2.bh_q019_q25
,t2.bh_q019_q30
,t2.bh_q019_q35
,t2.bh_q019_q40
,t2.bh_q019_q45
,t2.bh_q019_q50
,t2.bh_q019_q55
,t2.bh_q019_q60
,t2.bh_q019_q65
,t2.bh_q019_q70
,t2.bh_q019_q75
,t2.bh_q019_q80
,t2.bh_q019_q85
,t2.bh_q019_q90
,t2.bh_q019_q95
,t2.bh_q019_q100

--ç»­ä¾¦å˜é‡
,t3.value_012
,t3.value_013
,t3.value_014
,t3.value_015
,t3.value_016
,t3.value_017
,t3.value_018
,t3.value_019
,t3.value_020
,t3.value_021
,t3.value_022
,t3.value_023
,t3.value_024
,t3.value_025
,t3.value_026
,t3.value_027
,t3.value_028
,t3.value_029
,t3.value_030
,t3.value_031
,t3.value_032
,t3.value_033
,t3.value_034
,t3.value_035
,t3.value_036
,t3.value_037
,t3.value_038
,t3.value_039
,t3.value_040
,t3.value_041
,t3.value_042
,t3.value_043
,t3.value_044
,t3.value_045
,t3.value_046
,t3.value_047
,t3.value_048
,t3.value_049
,t3.value_050
,t3.value_051
,t3.value_052
,t3.value_053
,t3.value_054
,t3.value_055
,t3.value_056
,t3.value_057
,t3.value_058
,t3.value_059
,t3.value_060
,t3.value_061
,t3.value_062
,t3.value_063
,t3.value_064
,t3.value_065
,t3.value_066
,t3.value_067
,t3.value_068
,t3.value_069
,t3.value_070
,t3.value_071
,t3.value_072
,t3.value_073
,t3.value_074
,t3.value_075
,t3.value_076
,t3.value_077
,t3.value_078
,t3.value_079
,t3.value_080
,t3.value_081
,t3.value_082
,t3.value_083
,t3.value_084
,t3.value_085
,t3.value_086
,t3.value_087
,t3.value_088
,t3.value_089
,t3.value_090
,t3.value_091
,t3.value_092
,t3.value_093
,t3.value_094
,t3.value_095
,t3.value_096
,t3.value_097
,t3.value_098
,t3.value_099
,t3.value_100
,t3.value_101
,t3.value_102
,t3.value_103
,t3.value_104
,t3.value_105
,t3.value_106
,t3.value_107
,t3.value_108
,t3.value_109
,t3.value_110
,t3.value_111
,t3.value_112
,t3.value_113
,t3.value_114
,t3.value_115
,t3.value_116
,t3.value_117
,t3.value_118
,t3.value_119
,t3.value_120
,t3.value_121
,t3.value_122
,t3.value_123
,t3.value_124
,t3.value_125
,t3.value_126
,t3.value_127
,t3.value_128
,t3.value_129
,t3.value_130
,t3.value_131
,t3.value_132
,t3.value_133
,t3.value_134
,t3.value_135
,t3.value_136
,t3.value_137
,t3.value_138
,t3.value_139
,t3.value_140
,t3.value_141
,t3.value_142
,t3.value_143
,t3.value_144
,t3.value_145
,t3.value_146
,t3.value_147
,t3.value_148
,t3.value_149
,t3.value_150
,t3.value_151
,t3.value_152
,t3.value_153
,t3.value_154
,t3.value_155
,t3.value_156
,t3.value_157
,t3.value_158
,t3.value_159
,t3.value_160
,t3.value_161
,t3.value_162
,t3.value_163
,t3.value_164
,t3.value_165
,t3.value_166
,t3.value_167
,t3.value_168
,t3.value_169
,t3.value_170
,t3.value_171
,t3.value_172
,t3.value_173
,t3.value_174
,t3.value_175
,t3.value_176
,t3.value_177
,t3.value_178
,t3.value_179
,t3.value_180
,t3.value_181
,t3.value_182
,t3.value_183
,t3.value_184
,t3.value_185
,t3.value_186
,t3.value_187
,t3.value_188
,t3.value_189
,t3.value_190
,t3.value_191
,t3.value_192
,t3.value_193
,t3.value_194
,t3.value_195
,t3.value_196
,t3.value_197
,t3.value_198
,t3.value_199
,t3.value_200
,t3.value_201
,t3.value_202
,t3.value_203
,t3.value_204
,t3.value_205
,t3.value_206
,t3.value_207
,t3.value_208
,t3.value_209
,t3.value_210
,t3.value_211
,t3.value_212
,t3.value_213
,t3.value_214
,t3.value_215
,t3.value_216
,t3.value_217
,t3.value_218
,t3.value_219
,t3.value_220
,t3.value_221
,t3.value_222
,t3.value_223
,t3.value_224
,t3.value_225
,t3.value_226
,t3.value_227
,t3.value_228
,t3.value_229
,t3.value_230
,t3.value_231
,t3.value_232
,t3.value_233
,t3.value_234
,t3.value_235
,t3.value_236
,t3.value_237
,t3.value_238
,t3.value_239
,t3.value_240
,t3.value_241
,t3.value_242
,t3.value_243
,t3.value_244
,t3.value_245
,t3.value_246
,t3.value_247
,t3.value_248
,t3.value_249
,t3.value_250
,t3.value_251
,t3.value_252
,t3.value_253
,t3.value_254
,t3.value_255
,t3.value_256
,t3.value_257
,t3.value_258
,t3.value_259
,t3.value_260
,t3.value_261
,t3.value_262
,t3.value_263
,t3.value_264
,t3.value_265
,t3.value_266
,t3.value_267
,t3.value_268
,t3.value_269
,t3.value_270
,t3.value_271
,t3.value_272
,t3.value_273
,t3.value_274
,t3.value_275
,t3.value_276
,t3.value_277
,t3.value_278
,t3.value_279
,t3.value_280
,t3.value_281
,t3.value_282
,t3.value_283
,t3.value_284
,t3.value_285
,t3.value_286
,t3.value_287
,t3.value_288
,t3.value_289
,t3.value_290
,t3.value_291
,t3.value_292
,t3.value_293
,t3.value_294
,t3.value_295
,t3.value_296
,t3.value_297
,t3.value_298
,t3.value_299
,t3.value_300
,t3.value_301
,t3.value_302
,t3.value_303
,t3.value_304
,t3.value_305
,t3.value_306
,t3.value_307
,t3.value_308
,t3.value_309
,t3.value_310
,t3.value_311
,t3.value_312
,t3.value_313
,t3.value_314
,t3.value_315
,t3.value_316
,t3.value_317
,t3.value_318
,t3.value_319
,t3.value_320
,t3.value_321
,t3.value_322
,t3.value_323
,t3.value_324
,t3.value_325
,t3.value_326
,t3.value_327
,t3.value_328
,t3.value_329
,t3.value_330
,t3.value_331
,t3.value_332
,t3.value_333
,t3.value_334
,t3.value_335
,t3.value_336
,t3.value_337
,t3.value_338
,t3.value_339
,t3.value_340
,t3.value_341
,t3.value_342
,t3.value_343
,t3.value_344
,t3.value_345
,t3.value_346
,t3.value_347
,t3.value_348
,t3.value_349
,t3.value_350
,t3.value_351
,t3.value_352
,t3.value_353
,t3.value_354
,t3.value_355
,t3.value_356
,t3.value_357
,t3.value_358
,t3.value_359
,t3.value_360
,t3.value_361
,t3.value_362
,t3.value_363
,t3.value_364
,t3.value_365
,t3.value_366
,t3.value_367
,t3.value_368
,t3.value_369
,t3.value_370
,t3.value_371
,t3.value_372
,t3.value_373
,t3.value_374
,t3.value_375
,t3.value_376
,t3.value_377
,t3.value_378
,t3.value_379
,t3.value_380
,t3.value_381
,t3.value_382
,t3.value_383
,t3.value_384
,t3.value_385
,t3.value_386
,t3.value_387
,t3.value_388
,t3.value_389
,t3.value_390
,t3.value_391
,t3.value_392
,t3.value_393
,t3.value_394
,t3.value_395
,t3.value_396
,t3.value_397
,t3.value_398
,t3.value_399
,t3.value_400
,t3.value_401
,t3.value_402
,t3.value_403
,t3.value_404
,t3.value_405
,t3.value_406
,t3.value_407
,t3.value_408
,t3.value_409
,t3.value_410
,t3.value_411
,t3.value_412
,t3.value_413
,t3.value_414
,t3.value_415
,t3.value_416
,t3.value_417
,t3.value_418
,t3.value_419
,t3.value_420
,t3.value_421
,t3.value_422
,t3.value_423
,t3.value_424
,t3.value_425
,t3.value_426
,t3.value_427

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 1) 
      and lending_time='{run_day}'
      and channel_id = '1'
    ) as t 
--ç¦»çº¿é£é™©æ¨¡å‹å­åˆ†
left join 
    (
    select t.* 
    from znzz_fintech_ads.dm_f_lxl_test_behave_model_merge_fpd30_score as t 
    where dt=date_sub('{run_day}',1)
    ) as t1 on t.id_no_des=t1.id_no_des

--æ´ä¾¦å˜é‡
left join
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk 
    from znzz_fintech_dwd.dwd_beforeloan_data_source_bh_fqz_djv3_id as t
    where dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t2 on t.id_no_des=t2.id_no_des and t2.rk=1

--ç»­ä¾¦å˜é‡
left join 
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk
    from znzz_fintech_dwd.dwd_beforeloan_third_combine_sub_id as t  
    where ds='jzhl_thirds_platform_intf_bh_nfacq932_20240529'
      and dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t3 on t.id_no_des=t3.id_no_des and t3.rk=1
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[9]:


data_time = pd.DataFrame({'run_day':list(df_sample_dict.keys())})
data_time['run_day'].value_counts()


# In[10]:


df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[11]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['id_no_des'].nunique())


# In[12]:


print(df_sample_['lending_time'].min(), df_sample_['lending_time'].max())


# In[13]:


df_sample_.dropna(how='all', axis=1, inplace=True)
print(df_sample_.shape)


# In[14]:


df_target_dict = {}


# In[15]:


# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-11-06', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
where dt=date_sub(current_date(), 1) 
  and lending_time='{run_day}'
  and channel_id = '1'
;
'''
    print(f'=========================={run_day}=============================')
    df_target_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[16]:


df_target_ = pd.concat(df_target_dict.values(), ignore_index=True)
df_target_.info(show_counts=True)
df_target_.head()


# In[17]:


print(df_target_.shape[0], df_target_['order_no'].nunique())


# In[18]:


selected_cols = ['order_no']+[col for col in df_sample_.columns if col not in df_target_.columns]
print(selected_cols[:5])


# In[19]:


df_sample_=pd.merge(df_target_, df_sample_[selected_cols], how='inner', on=['order_no'])
df_sample_.info(show_counts=True)
# df_sample_.head()


# In[20]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[21]:


df_sample = df_sample_.query("diff_days<=30").reset_index(drop=True)
df_sample.info(show_counts=True)
# df_sample.head()


# In[22]:


df_sample.columns[:12]


# In[23]:


varsname = [col for col in df_sample.columns.to_list()[12:]]

print(varsname[:10], varsname[-10:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[24]:


print(result_path)


# In[32]:


to_del = []


# In[38]:


df_sample[col].value_counts()


# In[39]:


print(col)
to_del.append(col)
print(to_del)
df_sample.drop([col],axis=1,inplace=True)


# In[40]:


for i, col in enumerate(varsname[773:]):
    if df_sample[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')


# In[41]:


df_sample.dropna(how='all', axis=1, inplace=True)
print(df_sample.shape)


# In[42]:


df_sample['bh_q019_q70'].head()


# In[43]:


pd.set_option('display.max_row',None)
df_sample.groupby(['lending_time','fpd30'])['order_no'].count().unstack()


# In[44]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-10-01' & lending_time<='2024-11-06'").index, 'data_set']='3_oot'


# In[45]:


target = 'fpd30'


# In[47]:


# df_sample[[target]+varsname].info(show_counts=True)
# df_sample[[target]+varsname].head()


# In[50]:


df_sample.to_csv(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250108.csv',index=False)
print(result_path + 'æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250108.csv')


# # 1. æ ·æœ¬æ¦‚å†µ

# In[51]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[52]:


print(df_sample[target].value_counts())


# In[53]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[54]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[55]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[56]:


task_name


# In[57]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[59]:


# 2.1 å˜é‡åˆ†å¸ƒ
varsname = df_sample.columns.to_list()[12:-1]
df_explor = toad.detect(df_sample[varsname])


# In[60]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[62]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[63]:


df_sample = df_sample.replace(-1, np.nan)
gc.collect()


# ## 2.2 æ•°æ®æ¢ç´¢

# In[64]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[65]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[66]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[67]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[68]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[69]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[70]:


df_iv.loc[to_drop_iv,:].head()


# In[71]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[105]:


df_sample[varsname_v1+[target]].info()


# In[106]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]].drop('standard_score',axis=1),
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[107]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[108]:


df_iv.loc[to_drop2,:].head()


# In[109]:


tmp = df_iv.loc[to_drop2,:].sort_values(by='iv', ascending=False)
tmp.head(20)


# In[110]:


# to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# In[111]:


df_sample[varsname_v2].info()


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[112]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[113]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[89]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[92]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[93]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[94]:


new_bins_dict


# In[95]:


combiner.load(new_bins_dict)


# In[96]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[97]:


to_drop_mode


# In[114]:


varsname_v2[-1]


# In[115]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), varsname_v2,                                    'lending_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[116]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[117]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[118]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[119]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[120]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# In[ ]:





# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[123]:


len(list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index))


# In[124]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[130]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[131]:


df_psi_by_set.loc[drop_by_psi_set,:].head()


# In[134]:


df_miss_set.info()


# In[138]:


to_drop3 = [col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1]
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[139]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# In[140]:


# print(varsname_v3)


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[141]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[142]:


df_sample_woe.head()


# In[143]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.85
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.corr(method=method)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[144]:


gc.collect()


# In[145]:


df_iv_by_set.info()
df_iv_by_set.head()


# In[146]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe[varsname_v3],
                                                     df_iv_by_set['3_oot'],
                                                     method='kendall',
                                                     threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[152]:


df_high_corr.info()
df_high_corr.head()


# In[154]:


print(to_drop4)


# In[155]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[157]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[168]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'lending_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='kendalltau'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[169]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[170]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[171]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[ ]:





# ## 4.3 é€æ­¥å›å½’ç­›é€‰

# In[78]:


# # å°†woeè½¬åŒ–åçš„æ•°æ®åšé€æ­¥å›å½’
# train_woe = df_sample_woe.query("data_set=='1_train'")[varsname_v3+[target]]
# final_data, to_drop6 = toad.selection.stepwise(train_woe, target=target, estimator='ols', direction = 'both', \
#                                      criterion = 'aic', exclude = None, return_drop=True)

# print(final_data.shape) # é€æ­¥å›å½’ä»31ä¸ªå˜é‡ä¸­é€‰å‡ºäº†10ä¸ª


# In[172]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[173]:


gc.collect()


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.1 æ¨¡å‹è®­ç»ƒ

# In[209]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance

# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[388]:


# 2 å®šä¹‰è¶…å‚ç©ºé—´
# hp.quniform("å‚æ•°åç§°",ä¸‹ç•Œ,ä¸Šç•Œ,æ­¥é•¿)-é€‚ç”¨äºç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æµ®ç‚¹ç‚¹æ•°
# hp.uniform("å‚æ•°åç§°",ä¸‹ç•Œ, ä¸‹ç•Œ)-é€‚ç”¨äºè¿ç»­éšæœºåˆ†å¸ƒçš„æµ®ç‚¹æ•°
# hp.randint("å‚æ•°åç§°",ä¸Šç•Œ)-é€‚ç”¨äº[0,ä¸Šç•Œ)çš„æ•´æ•°,åŒºé—´ä¸ºå·¦é—­å³å¼€
# hp.choice("å‚æ•°åç§°",["å­—ç¬¦ä¸²1","å­—ç¬¦ä¸²2",...])-é€‚ç”¨äºå­—ç¬¦ä¸²ç±»å‹,æœ€ä¼˜å‚æ•°ç”±ç´¢å¼•è¡¨ç¤º
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# å¯ä»¥æ ¹æ®éœ€è¦ï¼Œæ³¨é‡Šæ‰ååçš„ä¸€äº›ä¸å¤ªé‡è¦çš„è¶…å‚

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",21,200,1),
          "max_depth":2,
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",30,200,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.8,1.0),
#           "feature_fraction":1.0,
#           "bagging_fraction":1.0,
#           "min_gain_to_split":10,
          "min_gain_to_split":10,
#     "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 10.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 100, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 50
          }
spaces


# In[182]:


# 3ï¼Œæ‰§è¡Œè¶…å‚æœç´¢
# æœ‰äº†ç›®æ ‡å‡½æ•°å’Œå‚æ•°ç©ºé—´,æ¥ä¸‹æ¥è¦è¿›è¡Œä¼˜åŒ–,éœ€è¦äº†è§£ä»¥ä¸‹å‚æ•°:
# fmin:è‡ªå®šä¹‰ä½¿ç”¨çš„ä»£ç†æ¨¡å‹(å‚æ•°algo),hyperoptæ”¯æŒå¦‚ä¸‹æœç´¢ç®—æ³•ï¼š
#       éšæœºæœç´¢(hyperopt.rand.suggest)
#       æ¨¡æ‹Ÿé€€ç«(hyperopt.anneal.suggest)
#       TPEç®—æ³•ï¼ˆhyperopt.tpe.suggestï¼Œç®—æ³•å…¨ç§°ä¸ºTree-structured Parzen Estimator Approachï¼‰
# partial:ä¿®æ”¹ç®—æ³•æ¶‰åŠåˆ°çš„å…·ä½“å‚æ•°,åŒ…æ‹¬æ¨¡å‹å…·ä½“ä½¿ç”¨äº†å¤šå°‘å°‘ä¸ªåˆå§‹è§‚æµ‹å€¼(å‚æ•°n_start_jobs),
#         ä»¥åŠåœ¨è®¡ç®—é‡‡é›†å‡½æ•°å€¼æ—¶ç©¶ç«Ÿè€ƒè™‘å¤šå°‘ä¸ªæ ·æœ¬(å‚æ•°n_EI_candidates)
# trials:è®°å½•æ•´ä¸ªè¿­ä»£è¿‡ç¨‹,ä»hyperoptåº“ä¸­å¯¼å…¥çš„æ–¹æ³•Trials(),ä¼˜åŒ–å®Œæˆä¹‹å,
#        å¯ä»¥ä»ä¿å­˜å¥½çš„trialsä¸­æŸ¥çœ‹æŸå¤±ã€å‚æ•°ç­‰å„ç§ä¸­é—´ä¿¡æ¯
# early_stop_fn:æå‰åœæ­¢å‚æ•°,ä»hyperoptåº“å¯¼å…¥çš„æ–¹æ³•no_progresss_loss(),å¯ä»¥è¾“å…¥å…·ä½“çš„æ•°å­—n,
#               è¡¨ç¤ºå½“æŸå¤±è¿ç»­næ¬¡æ²¡æœ‰ä¸‹é™æ—¶,è®©ç®—æ³•æå‰åœæ­¢
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds, init_model=None):

        """å®šä¹‰ç›®æ ‡å‡½æ•°"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1,
                'num_threads': -1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                init_model=init_model,
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set,                                 num_boost_round=num_boost_round, init_model=init_model)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(30)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[385]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[386]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[179]:


len(varsname_v5)


# In[ ]:


selected_cols


# In[387]:


# è®­ç»ƒæ•°æ®é›†
X_train = df_sample.query("data_set!='3_oot'")[selected_cols]
y_train = df_sample.query("data_set!='3_oot'")[target]
print(X_train.shape)


# In[ ]:


# 4ï¼Œè·å–æœ€ä¼˜å‚æ•°ï¼Œè°ƒå‚è¿‡ç¨‹
# ç¡®å®šä¸€ä¸ªè¾ƒé«˜çš„å­¦ä¹ ç‡
# å¯¹å†³ç­–æ ‘åŸºæœ¬å‚æ•°è°ƒå‚
# æ­£åˆ™åŒ–å‚æ•°è°ƒå‚
# é™ä½å­¦ä¹ ç‡
best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=10)


# In[390]:


# 5ï¼Œç»˜åˆ¶æœç´¢è¿‡ç¨‹
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[391]:


print("æœ€ä¼˜å‚æ•°best_params: ", best_params)


# In[392]:


### æ·»åŠ æ— éœ€è°ƒå‚çš„é€šç”¨å‚æ•°
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
bst_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
bst_params['learning_rate'] = spaces['learning_rate']
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
bst_params['bagging_fraction'] = best_params['bagging_fraction']    
bst_params['feature_fraction'] = best_params['feature_fraction'] 
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = spaces['lambda_l2']
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = spaces['max_depth']
# è°ƒå‚åçš„å…¶ä»–å‚
bst_params['min_gain_to_split'] = spaces['min_gain_to_split']


# In[393]:


print("æœ€ä¼˜å‚æ•°bst_params: ", bst_params)


# In[394]:


# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[selected_cols],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[395]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[190]:


gc.collect()


# In[396]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[397]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month = pd.concat([tmp, df_ks_auc_month], axis=1)
print(df_ks_auc_month)


df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set = pd.concat([tmp, df_ks_auc_set], axis=1)
print(df_ks_auc_set)


# In[192]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month = pd.concat([tmp, df_ks_auc_month], axis=1)
print(df_ks_auc_month)


df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set = pd.concat([tmp, df_ks_auc_set], axis=1)
print(df_ks_auc_set)


# In[200]:


usecols = df_sample_.columns.to_list()[:12] + varsname_v5
print(len(usecols))


# In[201]:


print(usecols[:12], usecols[-10:])


# In[225]:


# df_sample_[usecols].to_csv(result_path + 'æ¡”å­å•†åŸfdp30æˆä¿¡æ¨¡å‹_åŸå§‹æ•°æ®é›†.csv', index=False)
print(result_path + 'æ¡”å­å•†åŸfdp30æˆä¿¡æ¨¡å‹_åŸå§‹æ•°æ®é›†.csv')


# In[205]:


df_sample_30 = df_sample_.query("diff_days>30")[usecols].reset_index(drop=True)
for col in varsname_v5:
    print(f"========={col}=========")
    df_sample_30[col] = pd.to_numeric(df_sample_30[col])


# In[206]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_30['y_prob'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[207]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ-30+å®¢ç¾¤
df_ks_auc_month_30 = model_ks_auc(df_sample_30, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_30 = pd.concat([tmp, df_ks_auc_month_30], axis=1)
print(df_ks_auc_month_30)


# df_ks_auc_set_30 = model_ks_auc(df_sample_30, target, 'y_prob', 'data_set')
# tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
# df_ks_auc_set_30 = pd.concat([tmp, df_ks_auc_set_30], axis=1)
# print(df_ks_auc_set_30)


# In[211]:


model_ks_auc(df_sample, target, 'standard_score', 'lending_month')


# In[212]:


model_ks_auc(df_sample_30, target, 'standard_score', 'lending_month')


# In[196]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month = feature_importance(lgb_model) 
df_importance_month = pd.merge(df_importance_month, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month = df_importance_month.reset_index()
df_importance_month = df_importance_month.rename(columns={'index':'varsname'})
df_importance_month.head()


# In[197]:



# æ•ˆæœè¯„ä¼°åæ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set = pd.merge(df_importance_set, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set['3_oot_iv']/df_importance_set['1_train_iv'] - 1
df_importance_set.drop(columns=['1_train_psi'], inplace=True)
df_importance_set = df_importance_set.reset_index()
df_importance_set = df_importance_set.rename(columns={'index':'varsname'})
df_importance_set.head()


# In[198]:


result_path


# In[199]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_{timestamp}.pkl')
print(result_path + f'{task_name}_{timestamp}.bin')


# In[213]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_æ¨¡å‹ç»“æœåˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
    df_importance_month.to_excel(writer, sheet_name='df_importance_month')
    df_importance_set.to_excel(writer, sheet_name='df_importance_set')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
    df_ks_auc_month_30.to_excel(writer, sheet_name='df_ks_auc_month_30')
#     df_ks_auc_set_30.to_excel(writer, sheet_name='df_ks_auc_set_30')
print("æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'4_æ¨¡å‹ç»“æœåˆ†æ_{task_name}_{timestamp}.xlsx')


# ## 5.2 æ¨¡å‹ä¼˜åŒ–

# ### 5.2.3 æ¨¡å‹èåˆ

# In[489]:


### ä¼˜åŒ–è°ƒå‚1
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 65
opt_params['max_depth'] = 1
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10

# opt_params = {
#  'boosting': 'gbdt',
#  'objective': 'binary',
#  'metric': 'auc',
#  'bagging_freq': 1,
#  'scale_pos_weight': 1,
#  'seed': 1,
#  'num_threads': -1,
#  'learning_rate': 0.05,
#  'bagging_fraction': 0.8628008772208227,
#  'feature_fraction': 0.6177619614753441,
#  'lambda_l1': 0,
#  'lambda_l2': 300,
#  'early_stopping_rounds': 600,
#  'num_leaves': 75,
#  'min_data_in_leaf': 95,
#  'max_depth': 2,
#  'min_gain_to_split': 10}


# In[490]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[491]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[492]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[493]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[['standard_score','y_prob_v3']],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[494]:


# # ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
# X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[selected_cols],
#                                                     df_sample.query("data_set!='3_oot'")[target],
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=df_sample.query("data_set!='3_oot'")[target])
# print(X_train.shape, X_test.shape)

# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(df_sample['data_set'].value_counts())


# In[495]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[496]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
lgb_model.params


# In[498]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_ks_auc_month_v2 = model_ks_auc(df_sample, target, 'y_prob_v1', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month_v2 = pd.concat([tmp, df_ks_auc_month_v2], axis=1)
print(df_ks_auc_month_v2)


df_ks_auc_set_v2 = model_ks_auc(df_sample, target, 'y_prob_v1', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
print(df_ks_auc_set_v2)


# In[499]:


model_ks_auc(df_sample, 'fpd10', 'y_prob_v1', 'lending_month')


# In[501]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_30['y_prob_v1'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[502]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ-30+å®¢ç¾¤
df_ks_auc_month_30_v2 = model_ks_auc(df_sample_30, target, 'y_prob_v1', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_30_v2 = pd.concat([tmp, df_ks_auc_month_30_v2], axis=1)
print(df_ks_auc_month_30_v2)


# df_ks_auc_set_30_v2 = model_ks_auc(df_sample.query("diff_days>30"), target, 'y_prob_v1', 'data_set')
# tmp = get_target_summary(df_sample.query("diff_days>30"), target, 'data_set').set_index('bins')
# df_ks_auc_set_30_v2 = pd.concat([tmp, df_ks_auc_set_30_v2], axis=1)
# print(df_ks_auc_set_30_v2)


# In[503]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_v2 = feature_importance(lgb_model) 
df_importance_v2 = pd.merge(df_importance_v2, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_v2 = df_importance_v2.reset_index()
df_importance_v2 = df_importance_v2.rename(columns={'index':'varsname'})
df_importance_v2


# In[604]:





# In[504]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_opt_{timestamp}.pkl')
print(result_path + f'{task_name}_opt_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_opt_{timestamp}.xlsx') as writer:
    df_importance_v2.to_excel(writer, sheet_name='df_importance_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')
    df_ks_auc_month_30_v2.to_excel(writer, sheet_name='df_ks_auc_month_30_v2')
#     df_ks_auc_set_30_v2.to_excel(writer, sheet_name='df_ks_auc_set_30_v2')      
print("æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_æ¨¡å‹èåˆ_{timestamp}.xlsx')


# ### 5.2.2 ç‰¹å¾ä¼˜åŒ–

# In[506]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[507]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[508]:


to_drop6 = ['standard_score']
# + list(df_importance_month_v3[df_importance_month_v3['gain']>0].index
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[509]:


# ä½¿ç”¨çš„æ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v6],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[510]:


opt_params = {
 'boosting': 'gbdt',
 'objective': 'binary',
 'metric': 'auc',
 'bagging_freq': 1,
 'scale_pos_weight': 1,
 'seed': 1,
 'num_threads': -1,
 'learning_rate': 0.1,
 'bagging_fraction': 0.8628008772208227,
 'feature_fraction': 0.6177619614753441,
 'lambda_l1': 0,
 'lambda_l2': 300,
 'early_stopping_rounds': 600,
 'num_leaves': 75,
 'min_data_in_leaf': 95,
 'max_depth': 1,
 'min_gain_to_split': 10}


# In[511]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[512]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[513]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_ks_auc_month_v3 = model_ks_auc(df_sample, target, 'y_prob_v3', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month_v3 = pd.concat([tmp, df_ks_auc_month_v3], axis=1)
print(df_ks_auc_month_v3)


df_ks_auc_set_v3 = model_ks_auc(df_sample, target, 'y_prob_v3', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v3 = pd.concat([tmp, df_ks_auc_set_v3], axis=1)
print(df_ks_auc_set_v3)


# In[514]:


model_ks_auc(df_sample, 'fpd10', 'y_prob_v3', 'lending_month')


# In[517]:


# lgb_model.feature_name()


# In[428]:


df_sample_30['y_prob_v3'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[429]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ-30+å®¢ç¾¤
df_ks_auc_month_v3_30 = model_ks_auc(df_sample_30, target, 'y_prob_v3', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_v3_30 = pd.concat([tmp, df_ks_auc_month_v3_30], axis=1)
print(df_ks_auc_month_v3_30)

# df_ks_auc_set_v3_30 = model_ks_auc(df_sample.query("diff_days>30"), target, 'y_prob_v3', 'data_set')
# tmp = get_target_summary(df_sample.query("diff_days>30"), target, 'data_set').set_index('bins')
# df_ks_auc_set_v3_30 = pd.concat([tmp, df_ks_auc_set_v3_30], axis=1)
# print(df_ks_auc_set_v3_30)


# In[515]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = df_importance_month_v3.rename(columns={'index':'varsname'})
df_importance_month_v3


# In[431]:


# æ•ˆæœè¯„ä¼°åæ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v3 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v3['ivçš„å˜åŒ–å¹…åº¦'] = df_importance_set_v3['3_oot_iv']/df_importance_set_v3['1_train_iv'] - 1
df_importance_set_v3.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = df_importance_set_v3.rename(columns={'index':'varsname'})
df_importance_set_v3


# In[ ]:


selected_cols = 


# In[432]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')


# In[433]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
# print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
# print(result_path + f'{task_name}_v3_{timestamp}.pkl')
# print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')
    df_ks_auc_month_v3_30.to_excel(writer, sheet_name='df_ks_auc_month_v3_30')
#     df_ks_auc_set_v3_30.to_excel(writer, sheet_name='df_ks_auc_set_v3_30')    
print("æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_v3_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# ### 5.3.1æ•°æ®å¤„ç†

# In[518]:


# usecols= ['order_no','channel_id', 'lending_time','lending_month', 'mob',\
#           'maxdpd', 'fpd', 'fpd10', 'fpd30', 'mob4dpd30', 'diff_days'] + varsname_v5
df_model_vars = pd.read_csv('./result/æ´ä¾¦ç»­ä¾¦æ¨¡å‹fpd30/æ¡”å­å•†åŸfpd30æˆä¿¡å®æ—¶æ¨¡å‹_å»ºæ¨¡æ•°æ®é›†_250110.csv')
df_model_vars.info(show_counts=True)
df_model_vars.head()


# In[520]:


df_model_vars[varsname_v6] = df_model_vars[varsname_v6].replace(-1, np.nan)
gc.collect()


# In[521]:


# df_model_vars[varsname_v5].describe().T


# In[708]:


# è¡ç”ŸYæ ‡ç­¾
print(df_model_vars['fpd'].min(), df_model_vars['fpd'].max())
print(df_model_vars['fpd30'].min(), df_model_vars['fpd30'].max())


# In[522]:


# è¡ç”ŸYæ ‡ç­¾
# df_model_vars['fpd10'] = df_model_vars['fpd'].apply(lambda x: 1 if x>10 else 0)
# df_model_vars['fpd20'] = df_model_vars['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[525]:


df_model_vars.groupby(["lending_time",'fpd10'])['order_no'].count().unstack()


# In[526]:


# df_model_vars.loc[df_model_vars.query("lending_time>='2024-10-16'").index, 'fpd30'] = -1
# df_model_vars.loc[df_model_vars.query("lending_time>='2024-10-26'").index, 'fpd20'] = -1


# In[527]:


# df_model_vars.drop(index=df_model_vars.query("lending_time>='2024-11-06'").index, inplace=True)


# In[528]:


# df_model_vars = df_model_vars.reset_index(drop=True)


# In[530]:


# # æ·»åŠ å®¢ç¾¤æ ‡ç­¾
# def diff_days_(x):
#     if x<=30:
#         days = 'T30-'
#     elif x>30:
#         days = 'T30+'
#     else:
#         days = np.nan
#     return days

# df_model_vars['å®¢ç¾¤'] = df_model_vars['diff_days'].apply(diff_days_)


# In[531]:


df_model_vars['diff_days'].max()


# In[532]:


# df_model_vars.info(show_counts=True)


# In[534]:


# ç¬¬ä¸€æ¬¡æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/æ¡”å­å•†åŸæˆä¿¡æ¨¡å‹fpd30/æ¡”å­å•†åŸæˆä¿¡æ¨¡å‹fpd30_v3_20250110145432.pkl')
df_model_vars['y_prob_v3'] = lgb_model.predict(df_model_vars[varsname_v6],
                                               num_iteration=lgb_model.best_iteration)


# In[720]:


result_path


# In[537]:


# æœ€ç»ˆæ¨¡å‹æ‰“åˆ†
lgb_model= load_model_from_pkl('./result/æ¡”å­å•†åŸæˆä¿¡æ¨¡å‹fpd30/æ¡”å­å•†åŸæˆä¿¡æ¨¡å‹fpd30_20250110152634.pkl')
df_model_vars['y_prob_v1'] = lgb_model.predict(df_model_vars[['standard_score','y_prob_v3']],
                                               num_iteration=lgb_model.best_iteration)


# In[1]:


lgb_model.feathure_name()


# In[571]:


# å…¶ä»–æç°æ¨¡å‹æ•°æ® 
sql="""
select t1.order_no,channel_id, fpd10,fpd30,prob
from
(
    select order_no,channel_id, fpd10,fpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 1) 
      and lending_time>='2024-11-01'
      and lending_time<='2024-11-30'
) as t1 
inner join 
(select order_no,prob
from znzz_fintech_ads.fkmodel_ascore_fico_fpd10_v1_score as t 
where dt>='2024-11-01'
)as t2 on t1.order_no=t2.order_no
;

"""
df_tx_bj = get_data(sql)
df_tx_bj.info(show_counts=True)
df_tx_bj.head() 


# In[576]:


df_tx_bj['channel_type'] = df_tx_bj['channel_id'].astype(int).apply(channel_type)
df_tx_bj['channel_type'].value_counts()


# In[577]:


df_tx_bj['channel_rate'] = df_tx_bj['channel_id'].astype(int).apply(channel_rate)
df_tx_bj['channel_rate'].value_counts()


# In[587]:


fpr, tpr, _ = roc_curve(df_tx_bj.query("channel_type=='æ¡”å­å•†åŸ'")['fpd30'], df_tx_bj.query("channel_type=='æ¡”å­å•†åŸ'")['prob'], pos_label=1)
from sklearn.metrics import auc
auc_value = auc(fpr, tpr)
ks_value = max(abs(tpr - fpr))
print(auc_value, ks_value)


# In[540]:


# selected_cols = df_tx_bj.columns.to_list()[3:]
# df_tx_bj[selected_cols].describe().T


# In[541]:


# df_tx_bj_copy = df_tx_bj.copy()


# In[542]:


# result_path


# In[543]:


# df_tx_bj.to_csv(result_path + 'å…¨æ¸ é“å…¶ä»–æç°æ¨¡å‹åˆ†æ•°_241218.csv')
# print(result_path + 'å…¨æ¸ é“å…¶ä»–æç°æ¨¡å‹åˆ†æ•°_241218.csv')


# In[544]:


# # å¥½åˆ†æ•°è½¬ä¸ºååˆ†æ•°
# for i, col in enumerate(selected_cols):
#     print(f'ç¬¬{i}ä¸ªå˜é‡ï¼š{col}')
#     df_tx_bj[col] = 1 - df_tx_bj[col]


# In[545]:


# print(df_model_vars.shape, df_tx_bj.shape)


# In[546]:


# df_evalue = pd.merge(df_model_vars, df_tx_bj, how='left',on=['order_no'])
# print(df_evalue.shape, df_evalue['order_no'].nunique())


# In[547]:


# df_evalue.info(show_counts=True)


# In[548]:


# df_evalue.drop(columns=['apply_date','channel_id_y'], inplace=True)
# df_evalue.rename(columns={'channel_id_x':'channel_id'},inplace=True)


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[549]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col, percentile=0.95):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    badrate = df[label_col].mean()
    
    if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
    elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
    else:
        print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
    if badrate>0 and pct_n_badrate>0:
        lift_n = pct_n_badrate/badrate
    else:
        lift_n = np.nan
    return pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}',
                                                    'top5lift':f'top5lift_{score_}'})
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[559]:


colsname = ['bad_score','y_prob_v3','y_prob_v1']

print(colsname)
target_list = ['fpd10', 'fpd20', 'fpd30']
labels_models_dict = {target: colsname for target in target_list}
print(labels_models_dict)


# In[551]:


df_model_vars['channel_id'].head()


# In[732]:


# groupkeys1 = ['lending_month']
# df_ksauc_all_v1 = cal_ks_auc(df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(loc=(len(groupkeys1)), column='æ¸ é“', value='å…¨æ¸ é“', allow_duplicates=False)
# df_ksauc_all_v1.insert(loc=(len(groupkeys1)+1), column='å®¢ç¾¤', value='å…¨ä½“', allow_duplicates=False)
# df_ksauc_all_v1.head()


# In[553]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[554]:


df_evalue = df_model_vars.copy()


# In[555]:


df_evalue['channel_type'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_type'].value_counts()


# In[556]:


df_evalue['channel_rate'] = df_evalue['channel_id'].apply(channel_rate)
df_evalue['channel_rate'].value_counts()


# In[735]:


# df_evalue['lending_month_new'] = df_evalue['lending_month']
# df_evalue.loc[df_evalue.query("lending_time>='2024-09-01' & lending_time<='2024-09-20'").index, 'lending_month_new']='2024-09_1train'
# df_evalue.loc[df_evalue.query("lending_time>='2024-09-21' & lending_time<='2024-09-30'").index, 'lending_month_new']='2024-09_3oot'


# In[560]:



groupkeys2 = ['lending_month', 'channel_type']
df_ksauc_all_v2 = cal_ks_auc(df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.insert(loc=(len(groupkeys2)), column='å®¢ç¾¤', value='å…¨ä½“', allow_duplicates=False)
df_ksauc_all_v2.head()


# In[749]:



# groupkeys3 = ['lending_month_new', 'å®¢ç¾¤']
# df_ksauc_all_v3 = cal_ks_auc(df_evalue, groupkeys3, labels_models_dict)
# df_ksauc_all_v3.insert(loc=(len(groupkeys3)-1), column='æ¸ é“', value='å…¨æ¸ é“', allow_duplicates=False)
# df_ksauc_all_v3.head()


# In[558]:



groupkeys4 = ['lending_month', 'channel_rate']
df_ksauc_all_v4 = cal_ks_auc(df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.head()


# In[588]:


df_ksauc_all_v2.query("channel_type=='æ¡”å­å•†åŸ'")


# In[564]:


df_ksauc_all_v4.query("channel_rate=='227'")


# In[589]:


# target_list = ['fpd10', 'fpd20', 'fpd30']
# labels_models_dict_2 = {target: ['y_prob'] for target in target_list}
# print(labels_models_dict_2)


# In[590]:


# groupkeys5 = ['lending_month_new', 'æ¸ é“', 'å®¢ç¾¤', 'category']
# df_ksauc_all_v5 = cal_ks_auc(df_evalue, groupkeys5, labels_models_dict_2)
# df_ksauc_all_v5.head()


# In[591]:



# groupkeys6 = ['lending_month_new', 'æ¸ é“', 'category']
# df_ksauc_all_v6 = cal_ks_auc(df_evalue, groupkeys6, labels_models_dict_2)
# df_ksauc_all_v6.insert(loc=(len(groupkeys6)), column='å®¢ç¾¤', value='å…¨ä½“', allow_duplicates=False)
# df_ksauc_all_v6.head()


# In[592]:


df_ksauc_all_1 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_2 = pd.concat([df_ksauc_all_v5,df_ksauc_all_v6], axis=0)


# In[593]:


# result_path


# In[594]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_æ•ˆæœå¯¹æ¯”åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
#     df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹å¯¹æ¯”åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[374]:


result_path


# In[375]:


df_sample.to_csv(result_path + r'å…¨æ¸ é“å®æ—¶æç°è¡Œä¸ºæ¨¡å‹fpd30æ ·æœ¬.csv',index=False)


# In[376]:


score = 'y_prob_v3'


# In[377]:


df_sample['lending_month'].value_counts()


# In[378]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[379]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), 
                                                [score], 'lending_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[381]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: æ‰‹æœºå·md5æˆä¿¡å®æ—¶m4d30èåˆæ¨¡å‹_2501_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æ‰‹æœºå·md5æˆä¿¡å®æ—¶èåˆæ¨¡å‹v2'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./æ‰‹æœºå·md5æˆä¿¡å®æ—¶èåˆæ¨¡å‹v2'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. æ•°æ®è¯»å–

# In[4]:


print(result_path)


# In[5]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data


# In[7]:


df1 = pd.read_csv('å‰ç­›å®æ—¶æ¨¡å‹250919_ys.csv')
df1.info(show_counts=True)
df1.head()


# In[9]:


df2 = pd.read_csv('å‰ç­›å®æ—¶æ¨¡å‹250906.csv')
df2.info(show_counts=True)
df2.head()


# In[11]:


df_sample_ = pd.merge(df2, df1[['order_no','m1b0070','m1b0077']], how='left',on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[12]:


varsname = ['id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2', 'm1b0070', 'm1b0071', 'm1b0074', 'm1b0075', 'm1b0077', 'umeng_sdk_score', 'tianchuang_score', 'fico_model', 'haina_model']
print(len(varsname))
print(varsname)


# In[13]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[14]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[15]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[16]:


df_sample = df_sample_.copy()
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[18]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-14'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-02-15' & apply_date<='2025-02-28'").index, 'data_set']='2_test'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-09'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-10' & apply_date<='2025-04-15'").index, 'data_set']='3_oot2'


# In[19]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[20]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[21]:


print(df_sample[target].value_counts())


# In[22]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[23]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[24]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[25]:


# # 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[26]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[27]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[28]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set)


# In[34]:



def calculate_iv_by_group(df, 
                          group_col, 
                          target, 
                          variables,
                          is_return_unique=False,
                          method='quantile', 
                          n_bins=10):
    """
    æŒ‰åˆ†ç»„å­—æ®µè®¡ç®—æ¯ä¸ªå˜é‡åœ¨å„ç»„ä¸­çš„ IV å’Œ unique å€¼
    è¾“å‡ºæ ¼å¼ï¼šæ‰€æœ‰ IV åˆ—åœ¨å‰ï¼Œæ‰€æœ‰ UNIQUE åˆ—åœ¨å
    
    Parameters:
    -----------
    df : pd.DataFrame
        åŸå§‹æ•°æ®
    group_col : str
        åˆ†ç»„åˆ—åï¼ˆå¦‚ 'sample_type', 'year_month' ç­‰ï¼‰
    target : str
        ç›®æ ‡å˜é‡åˆ—å
    variables : list
        è¦åˆ†æçš„å˜é‡ååˆ—è¡¨
    method : str
        toad.quality çš„åˆ†ç®±æ–¹æ³•
    n_bins : int
        åˆ†ç®±æ•°é‡
    
    Returns:
    --------
    pd.DataFrame
        ç´¢å¼•: variable
        åˆ—:  {group1}_iv, {group2}_iv, ..., {group1}_unique, {group2}_unique, ...
    """
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = [group_col, target] + variables
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame.")
    
    # å­˜å‚¨æ¯ç»„çš„ç»“æœ
    iv_data = {}   # å­˜å‚¨æ‰€æœ‰ iv åˆ—
    unique_data = {}  # å­˜å‚¨æ‰€æœ‰ unique åˆ—
    
    # æŒ‰ group_col åˆ†ç»„
    for group_name, group_df in df.groupby(group_col):
        data = group_df[variables + [target]].copy()
        
        if data.empty:
            print(f"Warning: No data in group '{group_name}'")
            continue
        
        try:
            # ä½¿ç”¨ toad è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality = toad.quality(
                data,
                target=target,
                iv_only=True,
                method=method,
                n_bins=n_bins
            )
            
            # åªä¿ç•™ iv å’Œ unique
            cols_to_keep = ['iv', 'unique']
            existing_cols = [c for c in cols_to_keep if c in quality.columns]
            quality = quality[existing_cols]
            
            # åˆ†å¼€å­˜å‚¨
            if 'iv' in quality.columns:
                iv_data[f"{group_name}_iv"] = quality['iv']
            
            if 'unique' in quality.columns:
                unique_data[f"{group_name}_unique"] = quality['unique']
                
        except Exception as e:
            print(f"Error processing group '{group_name}': {str(e)}")
            continue
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not iv_data and not unique_data:
        raise ValueError("No valid group data processed.")
    
    # è½¬æ¢ä¸º DataFrame
    df_iv = pd.DataFrame(iv_data)
    df_unique = pd.DataFrame(unique_data)
    
    # åˆå¹¶ï¼šIV åœ¨å‰ï¼ŒUnique åœ¨å
    if is_return_unique:
        result = pd.concat([df_iv, df_unique], axis=1)
    else:
        result = df_iv.copy()
    
    # å¡«å……ç¼ºå¤±å€¼
    result = result.fillna(value=pd.NA)
    
    # è®¾ç½®ç´¢å¼•å
    result.index.name = 'variable'
    
    return result


# In[ ]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='quantile', n_bins=10)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[35]:



df_iv = calculate_iv_by_group(
    df=df_sample,
    group_col='data_set',
    target=target,
    variables=varsname
)
df_iv


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[ ]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(to_drop2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[37]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[38]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[39]:


target


# In[40]:


varsname_v2 = varsname[:]


# In[41]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[42]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[43]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[44]:


new_bins_dict


# In[45]:


combiner.load(new_bins_dict)


# In[46]:


combiner.export()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[48]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[49]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[50]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[51]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[52]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[53]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[54]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24åˆ©ç‡'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[ ]:


from itertools import combinations

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # ä½¿ç”¨å½“å‰æœ€ä½³ç‰¹å¾é›†è®­ç»ƒæ¨¡å‹
        # âœ… æ„å»ºå½“å‰ç‰¹å¾å­é›†çš„æ•°æ®
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # å¦‚æœæ˜¯ numpy arrayï¼Œç”¨ä½ç½®ç´¢å¼•
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # æ›´æ–°æœ€ä½³ç‰¹å¾é›†
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'å‹ç›Ÿè”åˆå»ºæ¨¡_v6_20250717140214.bin')  # è‡ªåŠ¨è¯†åˆ« .txt/.bin/.json


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[ ]:


df_sample[target] = pd.to_numeric(df_sample[target])


# In[56]:


df_sample[target].value_counts()


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[modeltrian_target].value_counts()


# In[59]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[ ]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[60]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[61]:


df_sample['channel_types'].value_counts()


# In[62]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[ ]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[ ]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.07
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 80


# In[230]:


opt_params={'objective': 'binary',
  'boosting': 'gbdt',
  'metric': 'auc',
  'min_gain_to_split': 80,
  'early_stopping_rounds': 30,
  'scale_pos_weight': 1,
  'seed': 1,
  'verbose': -1,
  'bagging_fraction': 0.7411882093860374,
  'feature_fraction': 0.7515414702756096,
  'lambda_l1': 1.899961726818411,
  'lambda_l2': 201.84972131273844,
  'learning_rate': 0.09365668958835766,
  'max_depth': 5,
  'min_data_in_leaf': 150,
  'num_leaves': 15}


# In[241]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[265]:


opt_params={'objective': 'binary',
  'boosting': 'gbdt',
  'metric': 'auc',
  'min_gain_to_split': 80,
  'early_stopping_rounds': 30,
  'scale_pos_weight': 1,
  'seed': 1,
  'verbose': -1,
  'bagging_fraction': 0.6548218675559451,
  'feature_fraction': 0.5669296516597987,
  'lambda_l1': 1.6525147188526745,
  'lambda_l2': 138.00050549759732,
  'learning_rate': 0.08292453162668129,
  'max_depth': 6,
  'min_data_in_leaf': 700,
  'num_leaves': 23}

print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[266]:


varsname


# In[267]:


varsname_base=['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0074',
 'm1b0075',
 'm1b0077',
 'umeng_sdk_score',
 'haina_model']


# In[268]:


print(len(varsname_base))
print(varsname_base)


# In[269]:


def model_train(data, selected_vars, target, opt_params):
    X_train = data[data['data_set']=='1_train'][selected_vars]
    y_train = data[data['data_set']=='1_train'][target]
    X_test = data[data['data_set']=='2_test'][selected_vars]
    y_test = data[data['data_set']=='2_test'][target]      
    
    best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, opt_params)
    
    dtrain = lgb.Dataset(X_train[best_features], label=y_train)
    dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)
    lgb_model = lgb.train(opt_params, dtrain, valid_sets=dtest, num_boost_round=10000,verbose_eval=50)
    
    return lgb_model

from itertools import combinations

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # ä½¿ç”¨å½“å‰æœ€ä½³ç‰¹å¾é›†è®­ç»ƒæ¨¡å‹
        # âœ… æ„å»ºå½“å‰ç‰¹å¾å­é›†çš„æ•°æ®
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # å¦‚æœæ˜¯ numpy arrayï¼Œç”¨ä½ç½®ç´¢å¼•
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # æ›´æ–°æœ€ä½³ç‰¹å¾é›†
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# # ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
# X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
# y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
# print(df_sample.groupby(['data_set'])['order_no'].count())
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(X_train.shape)
# print(df_sample.groupby(['data_set'])['order_no'].count())


# In[270]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
lgb_model = model_train(df_sample, varsname_base, modeltrian_target, opt_params)


# In[ ]:





# In[271]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v2'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v2'].head()


# In[272]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'data_set')
df_ks_auc_set_v1


# In[273]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[274]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[275]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[277]:


df_sample["customer_tags"].value_counts()


# In[279]:


# æŒ‰ flag åˆ†ç»„è®¡ç®—
df_ks_auc_set_all = (
    df_sample[(df_sample['umeng_sdk_score'].notna())]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v2', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all


# In[280]:


df_sample[df_sample['umeng_sdk_score'].notna()]['apply_date'].max()


# In[249]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤') 
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[281]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤') 
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 å‚æ•°ä¼˜åŒ–

# In[ ]:


def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=3, max_evals=50):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """å®šä¹‰ç›®æ ‡å‡½æ•°""" 
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': 0,
                'lambda_l2': 300,
                'min_gain_to_split':10,
                'early_stopping_rounds': 30,
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(50)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[ ]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[ ]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
X_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]


# In[ ]:


# 2 å®šä¹‰æœç´¢ç©ºé—´
spaces = {
          # general parameters
          "learning_rate":hp.uniform('learning_rate', 0.05, 0.1),
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",15,31,1),
          'max_depth': hp.quniform('max_depth', 4, 6, 1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",50,150,10),
          "feature_fraction":hp.uniform("feature_fraction",0.7,0.9),
          "bagging_fraction":hp.uniform("bagging_fraction",0.7,0.9)
          }


# In[ ]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=30)
print("best_params: ", best_params)


# In[ ]:


bst_params = {
        #general parameters
        'objective': 'binary',
        'boosting': 'gbdt',
        'metric': 'auc',
        'learning_rate': best_params['learning_rate'], # å­¦ä¹ ç‡,è¶…å‚
        #tuning parameters
        'num_leaves': int(best_params['num_leaves']), # å¶å­èŠ‚ç‚¹æ•°, è¶…å‚
        'min_data_in_leaf': int(best_params['min_data_in_leaf']), # ä¸€ä¸ªå¶å­ä¸Šæ•°æ®çš„æœ€å°æ•°é‡, è¶…å‚
        'max_depth': int(best_params['max_depth']), # æ ‘çš„æ·±åº¦
        'bagging_freq': 1,
        'bagging_fraction': best_params['bagging_fraction'], # æ•°æ®æŠ½æ ·, è¶…å‚
        'feature_fraction': best_params['feature_fraction'], # ç‰¹å¾æŠ½æ ·, è¶…å‚
        'lambda_l1': 0, # l1 æ­£åˆ™åŒ–
        'lambda_l2': 300, # l2 æ­£åˆ™åŒ–
        'min_gain_to_split':10, # åˆ‡åˆ†æœ€å°å¢ç›Š
        'early_stopping_rounds': 30,
        'scale_pos_weight': 1,
        'seed': 1
        }
print("æœ€ä¼˜å‚æ•°bst_params: ", bst_params)


# In[ ]:


# 5ï¼Œç»˜åˆ¶æœç´¢è¿‡ç¨‹
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[ ]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# æŒ‰ flag åˆ†ç»„è®¡ç®—
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')  
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[282]:


df_sample.info(show_counts=True)


# In[83]:


vars_combiner_dict


# In[85]:


vars_combiner_list


# In[96]:


without_fico= load_model_from_pkl('pre_selection_20250919_without_fico.pkl')
without_fico = lgb.Booster(model_str = without_fico._handle)
print(without_fico.feature_name())
df_sample['tianchuang_id5@_main'] = without_fico.predict(df_sample[without_fico.feature_name()], num_iteration=without_fico.best_iteration)
df_sample['tianchuang_id5@_main'].head()


# In[100]:


df_sample['tianchuang_id5@_main'] = 1 - df_sample['tianchuang_id5@_main']


# In[99]:


fico_= load_model_from_pkl('pre_selection_20250919.pkl')
fico_ = lgb.Booster(model_str = fico_._handle)
print(fico_.feature_name())
df_sample['fico_model@tianchuang_score_id@_main'] = fico_.predict(df_sample[fico_.feature_name()], num_iteration=fico_.best_iteration)
df_sample['fico_model@tianchuang_score_id@_main'].head()


# In[101]:


df_sample['fico_model@tianchuang_score_id@_main'] = 1 - df_sample['fico_model@tianchuang_score_id@_main']


# ### 5.3.1æ•°æ®å¤„ç†

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[283]:


df_evalue = df_sample.copy()
df_evalue.info(show_counts=True)
df_evalue.head()


# In[184]:


list(vars_combiner_dict.values())


# In[185]:


# è·å–æ‰€æœ‰å€¼åˆ—è¡¨
lists = list(vars_combiner_dict.values())

# å°†ç¬¬ä¸€ä¸ªåˆ—è¡¨è½¬æ¢ä¸ºé›†åˆï¼Œä½œä¸ºåˆå§‹äº¤é›†
common_elements = set(lists[0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ—è¡¨

# ä¸å…¶ä½™æ¯ä¸ªåˆ—è¡¨æ±‚äº¤é›†
for lst in lists[1:]:
    common_elements &= set(lst)  # ç­‰åŒäº common_elements = common_elements.intersection(set(lst))

print("æ‰€æœ‰åˆ—è¡¨å…±æœ‰çš„å…ƒç´ :", common_elements)


# In[186]:


varsname_base


# In[187]:


print(len(vars_combiner_list))
print(vars_combiner_list)


# In[198]:


list1 = [item.replace('@_main', '').split('@') for item in vars_combiner_list]
print(len(list1), list1)


# In[200]:


list2 = vars_combiner_list
print(len(list2), list2)

list3 = [['id5_off_m3d30_2507','id5_off_m4d30_2509v2','md5_off_m3d30_2507','md5_off_m4d30_2509v2']] * 15
print(len(list3), list3)


# In[202]:


# ç”Ÿæˆä¸‰å…ƒç»„ï¼š(list1[i], list3[i], list2[i])
triplets = [(list2[i], list3[i], list1[i]) for i in range(len(list2))]

# æ‰“å°ç»“æœ
for triplet in triplets:
    print(triplet)
    score_1, score_2, score_3 = triplet
    print(score_1, score_2, score_3)


# In[ ]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[ ]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[284]:


filepath = '/home/liaoxilin/è”åˆå»ºæ¨¡/å‹ç›Ÿsdk&ç™¾è¡Œå¤šå¤´/'
umeng_model= load_model_from_pkl(filepath + 'result_å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹/å‹ç›Ÿè”åˆåˆ†èåˆæ¨¡å‹_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[ ]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_å‹ç›ŸFico/å‹ç›ŸFicoç¦»çº¿èåˆ_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[ ]:


fico_model_v2= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹/ficoè”åˆåˆ†èåˆæ¨¡å‹_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[ ]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v2/ficoè”åˆåˆ†èåˆæ¨¡å‹v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v2_20250915112542.pkl


# In[ ]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[ ]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_ficoè”åˆåˆ†èåˆæ¨¡å‹v3/ficoè”åˆåˆ†èåˆæ¨¡å‹v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[ ]:


# æ–¹æ³•2ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆæ›´é«˜æ•ˆï¼‰
a_has_data = df_evalue['fico_model'].notna()  # ç­‰ä»·äº ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score' umeng_sdk_score].notna()

df_evalue['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±'] = np.where(
    a_has_data & b_has_data & c_has_data,  # éƒ½æœ‰æ•°æ®ï¼ˆéƒ½ä¸æ˜¯NaNï¼‰
    '1_éƒ½ä¸ç¼ºå¤±',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # éƒ½æ— æ•°æ®ï¼ˆéƒ½æ˜¯NaNï¼‰
        '2_éƒ½æœ‰ç¼ºå¤±',
        None  # å…¶ä»–æƒ…å†µï¼ˆä¸€ä¸ªæœ‰æ•°æ®ä¸€ä¸ªæ— æ•°æ®ï¼‰
    )
)


# In[ ]:


df_evalue['å‹ç›Ÿficoæ˜¯å¦ç¼ºå¤±'].value_counts(dropna=False)


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[285]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    data = pd.Series({'KS': ks_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
#         df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[286]:


df_evalue = df_evalue.dropna(subset=['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'],how='all')
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)
df_evalue.head()


# In[287]:


map_dict = {'3_oot2':'20250310-20250415','3_oot1':'20250301-20250309','2_test':'20250215-20250228','1_train':'20250101-20250214'}


# In[288]:


def rename_models(original_list):
    # å®šä¹‰æ›¿æ¢æ˜ å°„ï¼ˆæŒ‰é•¿åº¦é™åºæ’åˆ—ï¼Œé¿å…çŸ­åç§°å¹²æ‰°é•¿åç§°ï¼Œæ¯”å¦‚ umeng è¢« fico_model åŒ…å«ï¼‰
    replacements = {
        'umeng_sdk_score': 'å‹ç›Ÿ',
        'fico_model': 'fico',
        'haina_model': 'æµ·çº³',
        'tianchuang_score': 'å¤©åˆ›'
    }
    
    result = []
    for name in original_list:
        new_name = name
        for old, new in replacements.items():
            new_name = new_name.replace(old, new)
        result.append(new_name)
    
    return result


# In[209]:



with pd.ExcelWriter('æˆä¿¡åœºæ™¯m4d30_å‹ç›Ÿ_æµ·çº³_fico_å¤©åˆ›è”åˆå»ºæ¨¡å¢ç›Šè¯„ä¼°250922_v1.xlsx') as writer:

    for i, triplet in enumerate(triplets):
        score_1, score_2, score_3 = triplet
        score_list = [score_1] + score_2 + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
        print(len(score_list),score_list)
        
        target_list = ['target_mob4dpd30_1'] 
        labels_models_dict = {target: score_list for target in target_list}
        print(labels_models_dict)

        print(df_evalue.shape[0])
        tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
        tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_3].notna().any(axis=1),:]
        print(tmp_df_evalue.shape[0])
        # æ•´ä½“å®¢ç¾¤
        groupkeys2 = ['channel_types', 'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

        df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        # åˆ†å®¢ç¾¤
        groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'customer_tags', 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

        df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        
        # åˆå¹¶æ•°æ®
        df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
        df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)
        score_2_new = [f"KS_{col}" for col in score_2]
        df_auc_ks_all['KSæœ€å¤§å€¼'] = df_auc_ks_all[score_2_new].max(axis=1)
        ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
        df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
        df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)
        original_list = [score_1]
        new_original_list = rename_models(original_list)
        score_2_new = new_original_list[0]
        df_auc_ks_all.to_excel(writer, sheet_name=f'{score_2_new}')
        
        gc.collect()


# In[210]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_å›ºå®šå‚æ•°_å¢ç›Šè¯„ä¼°250922_v1.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[211]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_å›ºå®šå‚æ•°_å¢ç›Šè¯„ä¼°250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[212]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_æ— fico_å›ºå®šå‚æ•°_å¢ç›Šè¯„ä¼°250922_v1.xlsx') as writer:
    score_list = ['umeng_sdk_score', 'tianchuang_score', 'haina_model'] + vars_combiner_list + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[213]:



with pd.ExcelWriter(result_path + 'æˆä¿¡åœºæ™¯m4d30_æ— fico_å›ºå®šå‚æ•°_å¢ç›Šè¯„ä¼°250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'tianchuang_score', 'haina_model'] + vars_combiner_list + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # æ•´ä½“å®¢ç¾¤
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # åˆ†å®¢ç¾¤
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # åˆå¹¶æ•°æ®
    df_ksauc_all_1.insert(1, 'customer_tags', value='å…¨éƒ¨å®¢ç¾¤', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[ ]:


# æ–¹æ³•2ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆæ›´é«˜æ•ˆï¼‰
a_has_data = df_evalue['haina_model'].notna()  # ç­‰ä»·äº ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 

df_evalue['å‹ç›Ÿæµ·çº³æ˜¯å¦ç¼ºå¤±'] = np.where(
    a_has_data & b_has_data & c_has_data,  # éƒ½æœ‰æ•°æ®ï¼ˆéƒ½ä¸æ˜¯NaNï¼‰
    '1_éƒ½ä¸ç¼ºå¤±',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # éƒ½æ— æ•°æ®ï¼ˆéƒ½æ˜¯NaNï¼‰
        '2_éƒ½æœ‰ç¼ºå¤±',
        None  # å…¶ä»–æƒ…å†µï¼ˆä¸€ä¸ªæœ‰æ•°æ®ä¸€ä¸ªæ— æ•°æ®ï¼‰
    )
)


# In[290]:


score_list = ['y_pred_v1','y_pred_v2','id5_off_cpd30_2508','umeng_sdk_score', 'haina_model']
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[292]:


score_list = ['y_pred_v1','y_pred_v2','id5_off_cpd30_2508','umeng_sdk_score', 'haina_model']
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[293]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all2.to_excel(writer, sheet_name='æ•´ä½“_æœ‰æ— æ•°æ®')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')
    df_ksauc_all4.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤_æœ‰æ— æ•°æ®')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='æ•´ä½“')
    df_ksauc_all2.to_excel(writer, sheet_name='æ•´ä½“_æœ‰æ— æ•°æ®')
    df_ksauc_all3.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤')
    df_ksauc_all4.to_excel(writer, sheet_name='åˆ†å®¢ç¾¤_æœ‰æ— æ•°æ®')    
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','ficoæ•°æ®æ˜¯å¦ç¼ºå¤±','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("ficoæ•°æ®æ˜¯å¦ç¼ºå¤±=='1_ä¸ç¼ºå¤±' & flag=='1_æ–°å®¢' & channel=='é‡‘ç§‘æ¸ é“'").reset_index(drop=True)


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[294]:


df_sample['data_set'].value_counts()


# In[304]:


score = 'y_pred_v2'


# In[305]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins2'] = c.transform(df_sample[score], labels=True)


# In[306]:


df_sample['score_bins2'].head()


# In[307]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[308]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[309]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix_month)


# In[310]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins2'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[311]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[303]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[312]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[314]:


df_sample.to_csv(result_path + 'æ‰‹æœºå·md5æˆä¿¡å®æ—¶èåˆæ¨¡å‹v2_report.csv',index=False)
print(result_path + 'æ‰‹æœºå·md5æˆä¿¡å®æ—¶èåˆæ¨¡å‹v2_report.csv')


# # 7.æ¨¡å‹éƒ¨ç½²å’Œå›æº¯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## å­—æ®µåç§°
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## æœ¬åœ°æ–‡ä»¶ï¼Œæ³¨æ„ï¼šåªå†™æ–‡ä»¶åå³å¯ï¼Œå‚æ•°æ˜¯ list ç±»å‹
        csv_filename_list=['å¤©åˆ›æ¨¡å‹åˆ†æ•°v2.csv'],
        ## æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œæ³¨æ„ï¼šéœ€è¦æœ¬åœ°çš„ç»å¯¹è·¯å¾„
        input_path='/data/home/liaoxilin/è”åˆå»ºæ¨¡/å‹ç›Ÿsdk&ç™¾è¡Œå¤šå¤´/',                    
        ## ä¸Šä¼ çš„æ•°æ®åº“
        database='znzz_fintech_ads',        
        ## ä¸Šä¼ çš„è¡¨å
        table_name='lxl_model_',
        # åˆ†åŒºå­—æ®µ
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # è‡ªå®šä¹‰åˆ†éš”ç¬¦
        delimiter='|'
       ) 




#==============================================================================
# File: æ‰¹é‡å¤„ç†ç¦»çº¿å˜é‡.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
from jinja2 import Template
import os


# # é…ç½®å‚æ•°

# In[2]:


# é…ç½®å‚æ•°
BATCH_SIZE = 1500
OUTPUT_DIR = "./generated_sql"
MAIN_SQL_FILE = "main_table.sql"  # ä½ å¯ä»¥æŠŠä¸»è¡¨ SQL å†™å…¥æ–‡ä»¶ä¸­å¤‡ç”¨

os.makedirs(OUTPUT_DIR, exist_ok=True)


# # 1. æ ¸å¿ƒä»£ç 

# In[260]:


# åŠ è½½å˜é‡æ¸…å•
variables_df = pd.read_excel("variables.xlsx")
variables_df = variables_df[variables_df['keep'] == 1]
variables_df = variables_df.sort_values(by='table_name')  # å‡åŒ€åˆ†å¸ƒå˜é‡

# åˆ†æ‰¹å¤„ç†
batches = [variables_df[i:i+BATCH_SIZE] for i in range(0, len(variables_df), BATCH_SIZE)]


# In[791]:


# Jinja2 æ¨¡æ¿ï¼šç‰¹å¾å˜é‡éƒ¨åˆ†
feature_template = Template("""
left join 
(
select 
    t.id_no_des,
    {% for var in variables -%}
        t.{{ var }}{% if not loop.last %}, {% endif %}
    {%- endfor %},
    ROW_NUMBER() OVER (PARTITION BY id_no_des ORDER BY dt DESC) AS rk 
from {{ full_table_name }} as t 
where dt <= date_sub('$[last_day(yyyy-MM-dd)]', 1) 
  and dt >= date_sub('$[last_day(yyyy-MM-dd)]', 100)
) as {{ alias_name }} on t.id_no_des = {{ alias_name }}.id_no_des and {{ alias_name }}.rk = 1
""")


# In[792]:




# ä¸»è¡¨æ¨¡æ¿å ä½ç¬¦
MAIN_TEMPLATE = """
WITH main_base AS (
    -- ä¸»è¡¨ SQL æ”¾åœ¨è¿™é‡Œ
),
features AS (
    -- æ‰€æœ‰ left join çš„ç‰¹å¾è¡¨æ”¾åœ¨è¿™é‡Œ
)
SELECT *
FROM main_base
LEFT JOIN features USING (id_no_des)
"""


# In[ ]:


# è¯»å–ä¸»è¡¨ SQLï¼ˆå‡è®¾ä½ å·²ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼‰
with open(MAIN_SQL_FILE, 'r') as f:
    main_sql = f.read()


# In[793]:





for idx, batch in enumerate(batches):
    print(f"\nProcessing Batch {idx + 1} / {len(batches)}")
    
    # æŒ‰ table_name åˆ†ç»„ï¼Œå¾—åˆ° { 'tableA': ['var1', 'var2'], ... }
    grouped = batch.groupby('table_name')['variable_name'].apply(list).to_dict()
    
    feature_joins = []
    alias_counter = 1
    
    for table_name, vars_list in grouped.items():
        alias_name = f"t{alias_counter}"
        alias_counter += 1
        
        # æ„é€ æ¯ä¸ªç‰¹å¾è¡¨çš„ SELECT å­—æ®µåˆ—è¡¨
        sql_part = feature_template.render(
            full_table_name=table_name,
            variables=vars_list,
            alias_name=alias_name
        )
        
        feature_joins.append(sql_part)
    
    # åˆå¹¶æ‰€æœ‰ left join å­å¥
    all_features_sql = "\n".join(feature_joins)
    
    # å°†ä¸»è¡¨å’Œç‰¹å¾ join åˆå¹¶
    final_sql = f"""
    WITH main_base AS (
        {main_sql}
    ),
    features AS (
        SELECT * FROM main_base
        {all_features_sql}
    )
    SELECT * FROM features
    """
    # æ›¿æ¢æ—¥æœŸå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    final_sql = final_sql.replace("$[last_day(yyyy-MM-dd)]", "2025-06-30")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = os.path.join(OUTPUT_DIR, f"batch_{idx + 1}.sql")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_sql)
    
    print(f"âœ… å·²ç”Ÿæˆ SQL æ–‡ä»¶: {output_file}")


# In[794]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique(), 
      df_sample_['id_no_des'].nunique(), df_sample_['order_no_auth'].nunique())


# In[795]:


df_sample_.select_dtypes('object').columns


# In[797]:


drop_columns = ['fk01s1odc','fk02s1odc','fk03s1odc','fk06s1odc','fk12s1odc','fk01s0odc','fk02s0odc','fk03s0odc','fk06s0odc','fk12s0odc','fk01s1odcr','fk02s1odcr','fk03s1odcr','fk06s1odcr','fk12s1odcr','fk01s0odcr','fk02s0odcr','fk03s0odcr','fk06s0odcr','fk12s0odcr','fk01s1lms','fk02s1lms','fk03s1lms','fk06s1lms','fk12s1lms','fk01s1lmm','fk02s1lmm','fk03s1lmm','fk06s1lmm','fk12s1lmm','fk01s1lml','fk02s1lml','fk03s1lml','fk06s1lml','fk12s1lml','fk01s0lmm','fk02s0lmm','fk03s0lmm','fk06s0lmm','fk12s0lmm','fk01s1lmsr','fk02s1lmsr','fk03s1lmsr','fk06s1lmsr','fk12s1lmsr','fk01s1lmmr','fk02s1lmmr','fk03s1lmmr','fk06s1lmmr','fk12s1lmmr','fk01s0lmsr','fk02s0lmsr','fk03s0lmsr','fk06s0lmsr','fk12s0lmsr','fk01s0lmmr','fk02s0lmmr','fk03s0lmmr','fk06s0lmmr','fk12s0lmmr','fk01chlc','fk02chlc','fk03chlc','fk06chlc','fk12chlc','fk01s1chlc','fk02s1chlc','fk03s1chlc','fk06s1chlc','fk12s1chlc','fk01s0chlc','fk02s0chlc','fk03s0chlc','fk06s0chlc','fk12s0chlc','fk01chlcfk02chlcr','fk01chlcfk03chlcr','fk03chlcfk06chlcr','fk03chlcfk12chlcr','fk06chlcfk12chlcr','fk01s0chlcfk02s0chlcr','fk01s0chlcfk03s0chlcr','fk03s0chlcfk06s0chlcr','fk03s0chlcfk12s0chlcr','fk06s0chlcfk12s0chlcr','od01lt1c','od02lt1c','od03lt1c','od06lt1c','od12lt1c','od01lt2c','od02lt2c','od03lt2c','od06lt2c','od12lt2c','od01lt1cr','od02lt1cr','od03lt1cr','od06lt1cr','od12lt1cr','od01lt2cr','od02lt2cr','od03lt2cr','od06lt2cr','od12lt2cr','od01cod02cr','od01cod03cr','od03cod06cr','od03cod12cr','od06cod12cr','od01lmm','od02lmm','od03lmm','od06lmm','od12lmm','od01lt2lms','od02lt2lms','od03lt2lms','od06lt2lms','od12lt2lms','od01lt2lmm','od02lt2lmm','od03lt2lmm','od06lt2lmm','od12lt2lmm','od01lt1lmsr','od02lt1lmsr','od03lt1lmsr','od06lt1lmsr','od12lt1lmsr','od01lt1lmmr','od02lt1lmmr','od03lt1lmmr','od06lt1lmmr','od12lt1lmmr','od01lt2lmsr','od02lt2lmsr','od03lt2lmsr','od06lt2lmsr','od12lt2lmsr','od01lt2lmmr','od02lt2lmmr','od03lt2lmmr','od06lt2lmmr','od12lt2lmmr','od01chlc','od02chlc','od03chlc','od06chlc','od12chlc','ovd1stdn','p1ovdm15dc','p1ovdm30dc','p1ovds0c','pnovdm15dc','pnovdm30dc','pnovds0c','r01rpdovdfs','r02rpdovdfs','r03rpdovdfs','r06rpdovdfs','r12rpdovdfs','r01rpdovdfsrpdpalsr','r02rpdovdfsrpdpalsr','r03rpdovdfsrpdpalsr','r06rpdovdfsrpdpalsr','r12rpdovdfsrpdpalsr','r01rpdintsrpdpalsr','r02rpdintsrpdpalsr','r03rpdintsrpdpalsr','r06rpdintsrpdpalsr','r12rpdintsrpdpalsr','r01rpdpalsr','r02rpdpalsr','r03rpdpalsr','r06rpdpalsr','r12rpdpalsr','r01p1ovdodc','r02p1ovdodc','r03p1ovdodc','r06p1ovdodc','r12p1ovdodc','r01p1ovdodcr','r02p1ovdodcr','r03p1ovdodcr','r06p1ovdodcr','r12p1ovdodcr','r01p1epyodc','r02p1epyodc','r03p1epyodc','r06p1epyodc','r12p1epyodc','r01p1epyodcr','r02p1epyodcr','r03p1epyodcr','r06p1epyodcr','r12p1epyodcr','r01epys1c','r02epys1c','r03epys1c','r06epys1c','r12epys1c','r01epys1cr','r02epys1cr','r03epys1cr','r06epys1cr','r12epys1cr','r01p1ovdc','r02p1ovdc','r03p1ovdc','r06p1ovdc','r12p1ovdc','r01p1epyc','r02p1epyc','r03p1epyc','r06p1epyc','r12p1epyc','r01ovdds','r02ovdds','r03ovdds','r06ovdds','r12ovdds','r01ovddm','r02ovddm','r03ovddm','r06ovddm','r12ovddm','r01pdovddm','r02pdovddm','r03pdovddm','r06pdovddm','r12pdovddm']
varsname = [col for col in df_sample_.columns.to_list()[15:] if col not in drop_columns]

print(varsname[1:10],varsname[-10:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[798]:


for col in varsname:
    if df_sample_[col].dtype=='object':
        print(col)
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[799]:


df_sample_['order_no'].value_counts().head()


# In[802]:


df_sample_.drop_duplicates(inplace=True)


# In[803]:


df_sample_.shape


# In[804]:


df_sample_.to_csv(r'df_sample_.csv',index=False)


# In[270]:


df_sample = df_sample_[df_sample_.columns.to_list()[:15]+varsname]
df_sample.info()
df_sample.head()


# In[272]:


del df_sample_0907,df_sample_0907_dict
gc.collect()


# In[273]:


for col in varsname:
    if df_sample[col].dtype=='object':
        print(col)
        df_sample[col] = pd.to_numeric(df_sample[col])


# In[274]:


# df_sample = pd.read_csv(r'behave_model_20241112.csv')
# df_sample.info()
# df_sample.head()
print(len(drop_columns))


# In[275]:


print(df_sample['lending_time'].min(), df_sample['lending_time'].max())


# In[276]:


df_sample.select_dtypes('object').columns


# In[435]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-07-31'").index, 'lending_month']='2024-07(1_train)'
df_sample.loc[df_sample.query("lending_time>='2024-08-01' & lending_time<='2024-08-20'").index, 'lending_month']='2024-08(1_train)'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'lending_month']='2024-08(3_oot)'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'lending_month']='2024-09(3_oot)'


# In[330]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-07-31'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-08-01' & lending_time<='2024-08-20'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'data_set']='3_oot'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'data_set']='3_oot'


# In[ ]:





# In[279]:


target = 'fpd30'


# In[280]:


df_sample[[target]+varsname].info()
df_sample[[target]+varsname].head()


# In[281]:


df_sample.to_csv(r'behave_model_fpd30_data_241115.csv',index=False)


# # 1. æ ·æœ¬æ¦‚å†µ

# In[282]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[283]:


print(df_sample[target].value_counts())


# In[284]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[285]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[286]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[287]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./æ¨¡å‹å¼€å‘/è¡Œä¸ºæ¨¡å‹/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[288]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])


# In[289]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[290]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # æå–æœˆä»½ä¿¡æ¯
    # df[timestamp_col] = pd.to_datetime(df[timestamp_col])
    # df['month'] = df[timestamp_col].dt.to_period('M')

    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T
#     missing_rates['mean'] = missing_rates.mean(axis=1)
#     missing_rates['std'] = missing_rates.std(axis=1)
#     missing_rates['cv'] = missing_rates['std'] / missing_rates['mean']

    return missing_rates


# In[291]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss.head())


# In[292]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[293]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[294]:


# timestamp = datetime.now().strftime('%Y%m%d')
# directory = f'D:/è”åˆå»ºæ¨¡/ç™¾è¡Œæ”¯ä»˜/RES-æ˜ä¸œåç¬¬äºŒæ‰¹20W/result_user_fpd30_{timestamp}'
# if not os.path.exists(directory):
#         os.makedirs(directory, exist_ok=True)
# result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss.to_excel(writer, sheet_name='df_miss')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[766]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
# df_miss_drop = df_miss_set.drop(columns=['mean','std','cv'])
to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<0.02].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[767]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[768]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]], target=target, 
                                                empty=0.90, iv=0.02, corr=0.70, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[769]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)), to_drop2)


# In[770]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[300]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            # total = data.groupby(col)[target].count()
            # bad = data.groupby(col)[target].sum()
            # regroup = pd.concat([total, bad], axis=1)
            # regroup.columns = ['total', 'bad']
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
            regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()
    result['mean'] = result.mean(axis=1)
    result['std'] = result.std(axis=1)
    result['cv'] = result['std'] / result['mean']       
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            # total = data.groupby(col)[target].count()
            # bad = data.groupby(col)[target].sum()
            # regroup = pd.concat([total, bad], axis=1)
            # regroup.columns = ['total', 'bad']
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
            regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result

# è°ƒç”¨å‡½æ•°ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
def plot_stacked_bar(df, var, month_col, bins, values, filename=None):
    # å‡è®¾dfæ˜¯ä¸€ä¸ªDataFrameï¼ŒåŒ…å«æ‚¨çš„æ•°æ®
    # month_col æ˜¯æœˆä»½åˆ—
    # bins æ˜¯åˆ†ç®±åˆ—
    # values æ˜¯è¦ç»˜åˆ¶çš„å€¼åˆ—
    # åˆ›å»ºä¸€ä¸ªé€è§†è¡¨
    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values, fill_value=0)

    # åˆå§‹åŒ–å›¾å½¢
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # è·å–æ‰€æœ‰çš„åˆ†ç®±ç±»åˆ«
    bins_list = pivot_df.columns.tolist()
    
    # è®¡ç®—æ¯ä¸ªæŸ±å­çš„å®½åº¦
    bar_width = 0.8
    
    # è®¡ç®—xè½´ä¸Šçš„ä½ç½®
    x_pos = range(len(pivot_df.index))
    
    # åˆå§‹åŒ–åº•éƒ¨
    bottom = [0] * len(x_pos)
    
    # éå†æ¯ä¸ªåˆ†ç®±ï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾
    for bin in bins_list:
        # ä½¿ç”¨fillna(0)å¤„ç†NaNå€¼
        pivot_df[bin] = pivot_df[bin].fillna(0)
        
        ax.bar(x_pos,
               pivot_df[bin],
               width=bar_width,
               label=bin,
               bottom=bottom,
               align='center',
               alpha=0.8)
        
        # æ›´æ–°åº•éƒ¨çš„å€¼
        bottom = [b + v for b, v in zip(bottom, pivot_df[bin])]
    
    # è®¾ç½®å›¾å½¢å±æ€§
    ax.set_title(f'{values}â€”â€”{var}')
    # ax.set_xlabel('Month')
    ax.set_ylabel(f'{values}')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(pivot_df.index)
    ax.legend()

    plt.grid(axis='y', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show() 
    
# è°ƒç”¨å‡½æ•°ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾
def draw_time_series(df, var, month_col, bins, values, filename=None):

    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values)
    
    # ç»˜åˆ¶æ—¶é—´åºåˆ—æŠ˜çº¿å›¾
    plt.figure(figsize=(14, 7))
    for bin in pivot_df.columns:
        i = list(pivot_df.index)
        j = list(pivot_df[bin])
        plt.plot(i, j, label=bin, marker='o')

    plt.title(f'{values}â€”â€”{var}')
    # plt.xlabel('Month')
    plt.ylabel(f'{values}')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show()


def draw_line_bar(df, col, bin, var1, var2, var3, filename=None):
    # å¤„ç†æ•°æ®
    bins = df[bin]
    varsname = df[col].unique()[0]
    values1 = list(df[var1].fillna(0))
    values2 = list(df[var2].fillna(0))
    values3 = df[var3].unique()[0]

    # åˆ›å»ºå›¾å½¢å’Œä¸»è½´
    fig, ax1 = plt.subplots()

    # ä¸»çºµè½´ - æŸ±çŠ¶å›¾ (åˆ†ç®±å æ¯”)
    color = 'tab:blue'
    ax1.set_xlabel(f'{varsname}')
    ax1.set_ylabel(f'{var1}', color=color)
    bars = ax1.bar(bins, values1, color=color)
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        yval = bar.get_height()
        # va='bottom' to place label below the bar
        ax1.text(bar.get_x() + bar.get_width()/2.0, yval, f'{round(yval, 3)}', ha='center', va='top') 
    
    ax1.tick_params(axis='y', labelcolor=color, rotation=45)

    # å‰¯çºµè½´ - æŠ˜çº¿å›¾ (åå æ¯”)
    ax2 = ax1.twinx()  # åˆ›å»ºç¬¬äºŒä¸ªçºµåæ ‡è½´
    color = 'tab:red'
    ax2.set_ylabel(f'{var2}', color=color)  # è®¾ç½®æ ‡ç­¾é¢œè‰²
    _ = ax2.plot(values2, color=color, marker='o')  # ç»˜åˆ¶æŠ˜çº¿å›¾
    # åœ¨æŠ˜çº¿å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for xtick, txt in zip(ax1.get_xticks(), values2):
        ax2.text(xtick, txt, f'{round(txt, 3)}', ha='center', va='top', rotation=45)
    ax2.tick_params(axis='y', labelcolor=color)

    # è·å–xè½´çš„åˆ»åº¦ä½ç½®
    xtick_positions = range(len(bins))
    # è®¾ç½®Xè½´æ ‡ç­¾è‡ªåŠ¨è°ƒæ•´
    ax1.set_xticks(xtick_positions)
    ax1.set_xticklabels(bins, rotation=45, ha='right')
    
    # è®¾ç½®æ ‡é¢˜å’Œç½‘æ ¼
    ax1.set_title(f'{varsname}')
    ax1.grid(False)

    # åœ¨å·¦ä¸Šè§’æ·»åŠ æ–‡æœ¬
    ax1.text(0.05, 0.95, f'IV value:{round(values3,3)}', transform=ax1.transAxes, verticalalignment='top')
    # è°ƒæ•´å¸ƒå±€
    # fig.tight_layout()
    # å¦‚æœæä¾›äº†æ–‡ä»¶åï¼Œåˆ™ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename, dpi=300)
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()
    


# In[301]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=6, min_samples = 0.05, empty_separate=True) 


# In[680]:


# è®¡ç®—psi
# df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07(1_trian)'"), varsname_v2, \
#                                    'lending_month', combiner, return_frame = False)
# print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[303]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[304]:


df_iv_by_month.to_csv(r'df_iv_by_month_241116.csv')


# In[305]:


df_bins = combiner.transform(df_sample[varsname_v2+[target, 'lending_month','data_set']], labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[306]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head() )

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head() )


# In[ ]:


# plot_stacked_bar(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total'"), 
#                  'br_fpd_score', 'groupvars', 'bins', 'total_pct', filename=None)


# In[ ]:


# draw_time_series(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total'"),
#                  'br_fpd_score', 'groupvars', 'bins', 'bad_rate', filename=None)


# In[ ]:


# draw_line_bar(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total' & groupvars=='1_train'"), 
#               'varsname', 'bins', 'total_pct', 'bad_rate', 'iv', filename=None)


# In[307]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[308]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[309]:


drop_by_psi = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index)
print("drop_by_psi: ", len(drop_by_psi))

df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
drop_by_iv1 = list(df_iv_by_month[df_iv_by_month<0.01].dropna(how='all').index)
drop_by_iv2 = ['accuagel','age','ad06odpr','ad06dc_f']
drop_by_iv = drop_by_iv1 + drop_by_iv2
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[771]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[311]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [target, 'lending_month', 'data_set']

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins[varsname_v3+exclude], df_bins[target], exclude=exclude)
print(df_sample_woe.shape)


# In[312]:


df_sample_woe.head()


# In[313]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.85
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.corr(method=method)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 > iv2:
            to_remove.add(var2)
        else:
            to_remove.add(var1)
    
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[314]:


# è°ƒç”¨å‡½æ•°
df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe, df_iv['iv'],                                                     method='kendall', threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[315]:


df_high_corr.head(5)


# In[772]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")


# In[317]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[318]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe, 'lending_month',
                                                                    varsname_v4, target,method='pointbiserialr')

# æŸ¥çœ‹å‰å‡ è¡Œ
print(df_corr_vars_target.head(10))
print(df_pvalue_vars_target.head(10))


# In[319]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[773]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[321]:


print(varsname_v5)


# In[322]:


# timestamp = datetime.now().strftime('%Y%m%d')
# directory = f'D:/è”åˆå»ºæ¨¡/ç™¾è¡Œæ”¯ä»˜/RES-æ˜ä¸œåç¬¬äºŒæ‰¹20W/result_user_fpd30_{timestamp}'
# if not os.path.exists(directory):
#         os.makedirs(directory, exist_ok=True)
# result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        


# In[107]:


# result_path = 'D:/æ¨¡å‹å¼€å‘/æˆä¿¡æ¨¡å‹å¼€å‘240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}stacked_bar_totalpct_{plot_var}_{timestamp}.png'

# plot_stacked_bar(df_group.query("varsname=='br_fpd_score' & bins!='Total'"), 
#                  plot_var, 'groupvars', 'bins', 'total_pct', filename=filename)


# In[108]:


# result_path = 'D:/æ¨¡å‹å¼€å‘/æˆä¿¡æ¨¡å‹å¼€å‘240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}line_char_badrate_{plot_var}_{timestamp}.png'

# draw_time_series( df_group.query("varsname=='br_fpd_score' & bins!='Total'"),
#                  plot_var, 'groupvars', 'bins', 'bad_rate', filename=filename)


# In[109]:


# result_path = 'D:/æ¨¡å‹å¼€å‘/æˆä¿¡æ¨¡å‹å¼€å‘240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}line_chat_iv_{plot_var}_{timestamp}.png'

# draw_time_series( df_group.query("varsname=='br_fpd_score' & bins=='Total'"), 
#                  plot_var, 'groupvars', None, 'iv', filename=filename)


# In[323]:


gc.collect()


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.1 æ¨¡å‹è®­ç»ƒåŠå¯¹æ¯”

# In[ ]:


# æ•°æ®æŠ½æ ·
def resample_negative_samples(X_train, y_train, date_column, amplification_factor, total_samples=200000):
   
    # è®¡ç®—æ¯ä¸ªæœˆçš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    df = pd.concat([X_train, y_train], axis=1)
    df['flag'] = y_train
    df[date_column] = pd.to_datetime(df[date_column])
    df['month'] = df[date_column].dt.to_period('M').astype(str) 
    df_month_ratios = df.pivot_table(index='month', values='flag', aggfunc='mean')
    month_ratios = df_month_ratios['flag'].to_dict()
    
    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
    pos_samples = df[df['flag'] == 1]
    neg_samples = df[df['flag'] == 0]
    
    # è®¡ç®—æ­£æ ·æœ¬æ€»æ•°
    pos_count = len(pos_samples)

    # è®¡ç®—è´Ÿæ ·æœ¬åº”è¯¥æœ‰çš„æ€»æ•°
    neg_count = total_samples - pos_count
       
    # è®¡ç®—æ¯ä¸ªæ—¶é—´çª—å£éœ€è¦çš„ç›®æ ‡è´Ÿæ ·æœ¬æ•°é‡
    target_counts = {}
    for month, ratio in month_ratios.items():
        # è·å–è¯¥æœˆä»½çš„æ­£æ ·æœ¬æ•°é‡
        pos_count = len(pos_samples[pos_samples['month'] == month])
        # è®¡ç®—ç›®æ ‡è´Ÿæ ·æœ¬æ•°é‡
        target_neg_count = pos_count / (amplification_factor * ratio / (1 - ratio))
        target_counts[month] = int(target_neg_count)
    
    # æ ¹æ®æ€»è´Ÿæ ·æœ¬æ•°é‡é‡æ–°åˆ†é…å„æœˆçš„è´Ÿæ ·æœ¬æ•°é‡
    total_target_neg_count = sum(target_counts.values())
    adjusted_counts = {}
    for month, count in target_counts.items():
        adjusted_counts[month] = int((count / total_target_neg_count) * neg_count)
    
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºDataFrameæ¥å­˜å‚¨é‡é‡‡æ ·çš„ç»“æœ
    resampled_neg_samples = pd.DataFrame()

    # å¯¹æ¯ä¸ªæ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬è¿›è¡Œé‡é‡‡æ ·
    for month, target_count in adjusted_counts.items():
        # è·å–å½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬
        current_neg_samples = neg_samples[neg_samples['month'] == month]
        # å¦‚æœå½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬æ•°é‡å°‘äºç›®æ ‡æ•°é‡ï¼Œåˆ™ç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­
        if len(current_neg_samples) <= target_count:
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples])
        else:
            # å¦åˆ™ï¼Œä»å½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬ä¸­éšæœºæŠ½å–ç›®æ ‡æ•°é‡çš„æ ·æœ¬
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples.sample(n=target_count)])

    # åˆå¹¶æ­£æ ·æœ¬å’Œé‡é‡‡æ ·åçš„è´Ÿæ ·æœ¬
    X_resampled = pd.concat([pos_samples, resampled_neg_samples])[X_train.columns]
    y_resampled = y_train.loc[X_resampled.index]

    return (X_resampled, y_resampled)


# In[81]:


# X_train_ = df_sample.query("data_set=='1_train'").drop(columns=[target])
# y_train = df_sample.query("data_set=='1_train'")[target]
# X_test_ = df_sample.query("data_set=='2_test'").drop(columns=[target])
# y_test = df_sample.query("data_set=='2_test'")[target]
# print(X_train_.shape, X_test_.shape)


# In[78]:


# X_resampled, y_resampled = resample_negative_samples(X_train, y_train, 'apply_date', 20)
# print(X_train.shape, y_train.shape)
# print(X_resampled.shape, y_resampled.shape)
# print(y_resampled.value_counts())


# In[82]:


# X_resampled = X_resampled[cols]
# X_train = X_train_[varsname_v5]
# X_test = X_test_[varsname_v5]
# print(X_train.shape, X_test.shape)


# In[324]:


# 2 å®šä¹‰è¶…å‚ç©ºé—´
# hp.quniform("å‚æ•°åç§°",ä¸‹ç•Œ,ä¸Šç•Œ,æ­¥é•¿)-é€‚ç”¨äºç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æµ®ç‚¹ç‚¹æ•°
# hp.uniform("å‚æ•°åç§°",ä¸‹ç•Œ, ä¸‹ç•Œ)-é€‚ç”¨äºè¿ç»­éšæœºåˆ†å¸ƒçš„æµ®ç‚¹æ•°
# hp.randint("å‚æ•°åç§°",ä¸Šç•Œ)-é€‚ç”¨äº[0,ä¸Šç•Œ)çš„æ•´æ•°,åŒºé—´ä¸ºå·¦é—­å³å¼€
# hp.choice("å‚æ•°åç§°",["å­—ç¬¦ä¸²1","å­—ç¬¦ä¸²2",...])-é€‚ç”¨äºå­—ç¬¦ä¸²ç±»å‹,æœ€ä¼˜å‚æ•°ç”±ç´¢å¼•è¡¨ç¤º
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# å¯ä»¥æ ¹æ®éœ€è¦ï¼Œæ³¨é‡Šæ‰ååçš„ä¸€äº›ä¸å¤ªé‡è¦çš„è¶…å‚

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,7,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
#           "feature_fraction":hp.uniform("feature_fraction",0.7,1.0),
#           "bagging_fraction":hp.uniform("bagging_fraction",0.7,1.0),
          "feature_fraction":0.9,
          "bagging_fraction":0.7,
#           "min_gain_to_split":10,
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 1.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 300, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 30
          }


# In[325]:


# 3ï¼Œæ‰§è¡Œè¶…å‚æœç´¢
# æœ‰äº†ç›®æ ‡å‡½æ•°å’Œå‚æ•°ç©ºé—´,æ¥ä¸‹æ¥è¦è¿›è¡Œä¼˜åŒ–,éœ€è¦äº†è§£ä»¥ä¸‹å‚æ•°:
# fmin:è‡ªå®šä¹‰ä½¿ç”¨çš„ä»£ç†æ¨¡å‹(å‚æ•°algo),hyperoptæ”¯æŒå¦‚ä¸‹æœç´¢ç®—æ³•ï¼š
#       éšæœºæœç´¢(hyperopt.rand.suggest)
#       æ¨¡æ‹Ÿé€€ç«(hyperopt.anneal.suggest)
#       TPEç®—æ³•ï¼ˆhyperopt.tpe.suggestï¼Œç®—æ³•å…¨ç§°ä¸ºTree-structured Parzen Estimator Approachï¼‰
# partial:ä¿®æ”¹ç®—æ³•æ¶‰åŠåˆ°çš„å…·ä½“å‚æ•°,åŒ…æ‹¬æ¨¡å‹å…·ä½“ä½¿ç”¨äº†å¤šå°‘å°‘ä¸ªåˆå§‹è§‚æµ‹å€¼(å‚æ•°n_start_jobs),
#         ä»¥åŠåœ¨è®¡ç®—é‡‡é›†å‡½æ•°å€¼æ—¶ç©¶ç«Ÿè€ƒè™‘å¤šå°‘ä¸ªæ ·æœ¬(å‚æ•°n_EI_candidates)
# trials:è®°å½•æ•´ä¸ªè¿­ä»£è¿‡ç¨‹,ä»hyperoptåº“ä¸­å¯¼å…¥çš„æ–¹æ³•Trials(),ä¼˜åŒ–å®Œæˆä¹‹å,
#        å¯ä»¥ä»ä¿å­˜å¥½çš„trialsä¸­æŸ¥çœ‹æŸå¤±ã€å‚æ•°ç­‰å„ç§ä¸­é—´ä¿¡æ¯
# early_stop_fn:æå‰åœæ­¢å‚æ•°,ä»hyperoptåº“å¯¼å…¥çš„æ–¹æ³•no_progresss_loss(),å¯ä»¥è¾“å…¥å…·ä½“çš„æ•°å­—n,
#               è¡¨ç¤ºå½“æŸå¤±è¿ç»­næ¬¡æ²¡æœ‰ä¸‹é™æ—¶,è®©ç®—æ³•æå‰åœæ­¢
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """å®šä¹‰ç›®æ ‡å‡½æ•°"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(50)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[774]:


# 4ï¼Œè·å–æœ€ä¼˜å‚æ•°ï¼Œè°ƒå‚è¿‡ç¨‹
## ç¡®å®šä¸€ä¸ªè¾ƒé«˜çš„å­¦ä¹ ç‡
## å¯¹å†³ç­–æ ‘åŸºæœ¬å‚æ•°è°ƒå‚
## æ­£åˆ™åŒ–å‚æ•°è°ƒå‚
## é™ä½å­¦ä¹ ç‡



X_train, X_test, y_train, y_test = train_test_split(df_sample[df_sample['data_set']=='1_train'][varsname_v5],
                                                    df_sample[df_sample['data_set']=='1_train'][target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample[df_sample['data_set']=='1_train'][target])
df_sample.loc[X_train.index, 'data_type']='1_train'
df_sample.loc[X_test.index, 'data_type']='2_test'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'data_type']='3_oot'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'data_type']='3_oot'



best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=5)


# In[775]:


# 5ï¼Œç»˜åˆ¶æœç´¢è¿‡ç¨‹
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[776]:


print("æœ€ä¼˜å‚æ•°best_params: ", best_params)


# In[709]:


### æ·»åŠ æ— éœ€è°ƒå‚çš„é€šç”¨å‚æ•°
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
bst_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
bst_params['bagging_fraction'] = 0.6    
bst_params['feature_fraction'] = 0.99
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = 1000
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
bst_params['num_leaves'] = 59
bst_params['min_data_in_leaf'] = 115
bst_params['max_depth'] = 4
# è°ƒå‚åçš„å…¶ä»–å‚
bst_params['min_gain_to_split'] = 0.6


# In[784]:


### æ·»åŠ æ— éœ€è°ƒå‚çš„é€šç”¨å‚æ•°
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
bst_params['learning_rate'] = spaces['learning_rate']
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
bst_params['bagging_fraction'] = spaces['bagging_fraction']    
bst_params['feature_fraction'] = spaces['feature_fraction']
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = 1000
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = int(best_params['max_depth'] )
# è°ƒå‚åçš„å…¶ä»–å‚
bst_params['min_gain_to_split'] = best_params['min_gain_to_split']


# In[785]:


print("æœ€ä¼˜å‚æ•°bst_params: ", bst_params)


# In[779]:


to_drop6 = ['au03odcau06odcr_f','r01o00pals','au01dchlm','au03chlcau06chlcr','ad06dc_p',            'au03mnchlm']+list(df_iv[df_iv['iv']<0.02].index)
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[786]:



X_train = df_sample[df_sample['data_type']=='1_train'][varsname_v5]
y_train = df_sample[df_sample['data_type']=='1_train'][target]
X_test = df_sample[df_sample['data_type']=='2_test'][varsname_v5]
y_test = df_sample[df_sample['data_type']=='2_test'][target] 
# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[789]:


# ä¿å­˜æ¨¡å‹
# joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118.pkl')
# joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118_v2.pkl')
joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118_v3.pkl')


# In[1]:


def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[2]:


lgb_model = load_model_from_pkl('./behave_model_fpd30_lgb_241118.pkl')


# In[724]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(tpr-fpr)
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc

def plt_ks_auc(df, target, y_pred, group_col, filename=None):
    fpr, tpr, _ = roc_curve(df[target], df[y_pred], pos_label=1)
    # è®¡ç®— AUC
    auc_ = roc_auc_score(df[target], df[y_pred])
    # KS æ›²çº¿çš„æœ€å¤§è·ç¦»ç‚¹
    ks_x = fpr[np.argmax(tpr - fpr)]
    ks_y = tpr[np.argmax(tpr - fpr)]
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'{group_col} ROC curve (Area = {auc_:.2f})')
    # ç»˜åˆ¶ KS ç‚¹
    plt.plot(ks_x, ks_y, 'ko', label=f'KS Point (KS = {ks_:.2f})')
    # æ·»åŠ  KS ç›´çº¿
    plt.plot([0, ks_x], [ks_y, ks_y], 'k--')
    plt.plot([ks_x, ks_x], [0, ks_y], 'k--')
    # ç»˜åˆ¶å¯¹è§’çº¿
    plt.plot([0,1],[0,1],color='gray', lw=1, linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title(f'ROC curve for {col}',fontsize=25)
    # æ˜¾ç¤ºå›¾ä¾‹
    plt.legend(loc='lower right',fontsize=20)
    # ä¿å­˜å›¾è¡¨
    if filename:
        plt.savefig(filename)
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()


# In[787]:


# è¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
print(df_ks_auc_month)
df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
print(df_ks_auc_set)


# In[640]:


# # è¯„ä¼°æ¨¡å‹æ•ˆæœ
# df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
# df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
# print(df_ks_auc_month)
# df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
# print(df_ks_auc_set)


# In[623]:


# model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')


# In[788]:


model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')


# In[726]:


df_ks_auc_month_227 = model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')
print(df_ks_auc_month_227)


# In[727]:


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# In[728]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_month, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance = pd.merge(df_importance, df_psi_iv, how='inner', left_index=True,right_index=True)
# # df_importance['descrse_iv'] = df_importance['3_oot_iv']/df_importance['2_test_iv'] - 1
df_importance.drop(columns=['1_train'],inplace=True)
# df_importance = df_importance.reset_index()
# df_importance = df_importance.rename(columns={'index':'varsname'})
df_importance.head(10)


# In[643]:


df_importance.to_excel(r'df_importance_1118.xlsx')


# In[729]:


df_sample = df_sample.copy()


# In[736]:


df_sample_all = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_sample_all = pd.concat([df_sample_all, model_ks_auc(df_sample, target, 'y_prob', 'lending_month')], axis=1)
df_sample_all


# In[737]:


df_tmp = df_sample.query("diff_days>30")
df_sample_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_30 = pd.concat([df_sample_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_30


# In[738]:


df_tmp = df_sample.query("channel_id=='227'")
df_sample_227 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_227 = pd.concat([df_sample_227, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_227


# In[739]:


df_tmp = df_sample.query("channel_id=='227' & diff_days>30")
df_sample_227_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_227_30 = pd.concat([df_sample_227_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_227_30


# In[740]:


df_tmp = df_sample.query("channel_id in ('209','213','226','229','231','233','234','235','236')")
df_sample_no_227 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_no_227 = pd.concat([df_sample_no_227, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_no_227


# In[741]:


df_tmp = df_sample.query("channel_id in ('209','213','226','229','231','233','234','235','236') & diff_days>30")
df_sample_no_227_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_no_227_30 = pd.concat([df_sample_no_227_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_no_227_30


# In[743]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./æ¨¡å‹å¼€å‘/è¡Œä¸ºæ¨¡å‹/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹ç»“æœåˆ†æ_{timestamp}.xlsx') as writer:
    df_sample.to_excel(writer, sheet_name='df_sample')
    df_sample_30.to_excel(writer, sheet_name='df_sample_30')
    df_sample_227.to_excel(writer, sheet_name='df_sample_227')
    df_sample_227_30.to_excel(writer, sheet_name='df_sample_227_30')
    df_sample_no_227.to_excel(writer, sheet_name='df_sample_no_227')
    df_sample_no_227_30.to_excel(writer, sheet_name='df_sample_no_227_30')
print("æ•°æ®å­˜å‚¨å®Œæˆï¼")


# In[744]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./æ¨¡å‹å¼€å‘/è¡Œä¸ºæ¨¡å‹/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹ç»“æœåˆ†æ_{timestamp}.xlsx') as writer:
    df_importance.to_excel(writer, sheet_name='df_importance')
#     df_importance_v1.to_excel(writer, sheet_name='df_importance_v1')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
#     df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
#     df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')
print("æ•°æ®å­˜å‚¨å®Œæˆï¼")


# ## 5.2ä¿å­˜æ¨¡å‹å’Œåˆ†æ•°

# In[746]:



# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)


def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

# PMMLæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
# sklearnæ¥å£çš„xgboostï¼Œå¯ä½¿ç”¨sklearn2pmmlç”Ÿæˆpmmlæ–‡ä»¶
def save_model_as_pmml(model, save_file_path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„save_file_path
    :param x: è®­ç»ƒæ•°æ®ç‰¹å¾
    :param y: è®­ç»ƒæ•°æ®æ ‡ç­¾
    :param save_file_path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    # æ¨¡å‹ç»“æœä¿å­˜
    sklearn2pmml(model, save_file_path, with_repr=True)


# PMMLæ ¼å¼è¯»å–
def load_model_from_pmml(load_file_path):
    """
    ä»è·¯å¾„load_file_pathåŠ è½½æ¨¡å‹
    :param load_file_path: pmmlæ–‡ä»¶è·¯å¾„
    """
    model = Model.fromFile(load_file_path)
    return model

# xgbæ¨¡å‹ä¿å­˜ã€åŠ è½½.bin å’Œ .pkl æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜xgbæ¨¡å‹ä¸ºpkl\binæ ¼å¼
    model.save_model(save_file_path)
    # pickle.dump(model, open(f"{file_prefix_name}.pkl", "wb"))

def load_model(model_path):
    #åŠ è½½xgbæ¨¡å‹ï¼Œæ”¯æŒpkl\binæ ¼å¼
    if os.path.splitext(model_path)[-1] == '.pkl':
        return pickle.load(open(model_path, 'rb'))
    else:
        model = xgb.XGBClassifier()
        model._Booster = xgb.Booster(model_file=model_path)
        return model
    
    
def save_lgb_model(lgb_model, file_prefix_name):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºpkl\binæ ¼å¼
    lgb_model.booster_.save_model(f"{file_prefix_name}.bin")
    pickle.dump(lgb_model, open(f"{file_prefix_name}.pkl", "wb"))

def load_lgb_model(model_path):
    #åŠ è½½lgbæ¨¡å‹ï¼Œæ”¯æŒpkl\binæ ¼å¼
    import lightgbm as lgb
    if os.path.splitext(model_path)[-1] == '.pkl':
        return pickle.load(open(model_path, 'rb'))
    else:
        model = lgb.Booster(model_file=model_path)
        return model


# In[764]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./æ¨¡å‹å¼€å‘/è¡Œä¸ºæ¨¡å‹/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
model_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

save_model_as_pkl(lgb_model, model_path + f'model_order_fpd30_{timestamp}.pkl')
save_model_as_bin(lgb_model, model_path + f'model_order_fpd30_{timestamp}.bin')

df_sample.to_csv(model_path + f'model_order_fpd30_score_{timestamp}.csv')
print(model_path + f'model_order_fpd30_{timestamp}.pkl')
print(model_path + f'model_order_fpd30_score_{timestamp}.csv')


# ## 5.3 è¯„åˆ†åˆ†å¸ƒ

# In[748]:


score = 'y_prob'


# In[754]:


df_sample['lending_month'].value_counts()


# In[755]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07(1_train)'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[756]:


df_sample['score_bins'].head()


# In[757]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07(1_train)'"), 
                                                [score], 'lending_month', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[758]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[760]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[ ]:


# df_psi_matrix_set = get_model_psi(df_sample, 'score', 'data_set', c)

# # æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
# print(df_psi_matrix_set)


# In[ ]:


# score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
# score_group_by_dataset_1 = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum',
#                                                  'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
# # df_score_group_by_dataset = score_group_by_dataset.pivot_table(index='bins', columns='groupvars', values='total_pct')
# # print(df_score_group_by_dataset)


# In[ ]:


# score_group_by_dataset_1 = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum',
#                                                  'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
# score_group_by_dataset_1.head(2)


# In[763]:


result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'5_è¯„åˆ†åˆ†å¸ƒ_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
#     score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")




#==============================================================================
# File: æŠ¥å‘Š_æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# # ä¸€ã€æ¨¡å‹ç®€ä»‹

# æœ¬æ¨¡å‹ä¸ºæˆä¿¡å…¨æ¸ é“äººè¡Œmob6dpd30ç¦»çº¿æ¨¡å‹ï¼ŒYæ ‡ç­¾ä¸ºmob6dpd30ã€‚æ¨¡å‹ç®—æ³•ä½¿ç”¨lgbç®—æ³•è¿›ã€‚
# trainæ ·æœ¬ï¼šé€‰å–2024å¹´9æœˆ1æ—¥è‡³2024å¹´11æœˆ17æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾æˆåŠŸçš„ç”¨æˆ·ã€‚
# oot1æ ·æœ¬ï¼šé€‰å–2024å¹´11æœˆ18æ—¥è‡³2024å¹´11æœˆ30æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾æˆåŠŸçš„ç”¨æˆ·ã€‚
# oot2æ ·æœ¬ï¼šé€‰å–2024å¹´12æœˆ1æ—¥è‡³2024å¹´12æœˆ17æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾æˆåŠŸçš„ç”¨æˆ·ã€‚

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# # äºŒã€ æ ·æœ¬æ¦‚å†µ

# In[2]:


def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[3]:


# æœ€ç»ˆæ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”Ÿmob6dpd30ç¦»çº¿æ¨¡å‹_v3_20250721135643.pkl')


# In[ ]:


lgb_model.params


# In[ ]:


varsname = lgb_model.feature_name()
print(len(varsname), varsname)


# In[ ]:


df_sample = pd.read_csv('./result/æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[7]:


# è®¾ç½®æ ‡ç­¾å’Œåˆ†æ•°åˆ—
target = 'target_mob6dpd30'
score = 'y_pred_v4'


# In[8]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[ ]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
total_summary = df_sample[target].agg(total=lambda x: len(x), 
        bad=lambda x: x.sum(), 
        good=lambda x: (x== 0).sum(), 
        bad_rate=lambda x: x.mean()).to_frame().T
total_summary['bins'] = 'Total'
# å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
df_target_summary_month = pd.concat([df_target_summary_month, total_summary], ignore_index=True)
df_target_summary_month


# In[10]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
df_target_summary_set.insert(0, 'time_windows', value=['20240901-20241117','20240901-20241117','20241118-20241130','20241201-20241217'])
df_target_summary_set


# # ä¸‰ã€å˜é‡é‡è¦æ€§ï¼ˆå…¥æ¨¡372ä¸ªï¼Œå±•ç¤ºå‰40ä¸ªï¼‰

# In[14]:


filepath = './result/4_æ¨¡å‹è®­ç»ƒ_æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”Ÿmob6dpd30ç¦»çº¿æ¨¡å‹_v3_20250721135643.xlsx'
file = pd.read_excel(filepath, sheet_name=None) 


# In[ ]:


file.keys()


# In[ ]:


df_importance.info(show_counts=True)


# In[16]:


df_importance = file['df_importance_set_v3'].copy()
df_importance.drop(columns=['Unnamed: 0'],inplace=True)
# df_importance = pd.merge(df_vars_des, df_importance, how='right', on='feature')
df_importance.head(40)


# # å››ã€æ¨¡å‹æ•ˆæœè¯„ä¼°

# In[19]:


df_ks_auc_set_v1 = file['df_ks_auc_set_v3'].copy()
df_ks_auc_set_v1.rename(columns={'Unnamed: 0':'data_set'},inplace=True)
df_ks_auc_set_v1.insert(0,'time_windows', value=['20240901-20241117','20240901-20241117','20241118-20241130','20241201-20241217']*6)
df_ks_auc_set_v1


# In[ ]:


## æ¨¡å‹æ•ˆæœå¯¹æ¯”


# In[ ]:


filepath_c = './result_v4/6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_æç°äººè¡Œå¾ä¿¡æ¨¡å‹fpd30æ ‡ç­¾_20250527191319.xlsx'
file_c = pd.read_excel(filepath_c, sheet_name=None)


# In[ ]:


file_c.keys()


# In[ ]:


fpd30 = file_c['fpd30']
fpd30.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
fpd30.rename(columns={'KS_y_prob_base_v4':'KS_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹','AUC_y_prob_base_v4':'AUC_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹'},inplace=True)
fpd30


# In[ ]:


mob4dpd30 = file_c['mob4dpd30']
mob4dpd30.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
mob4dpd30.rename(columns={'KS_y_prob_base_v4':'KS_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹','AUC_y_prob_base_v4':'AUC_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹'},inplace=True)
mob4dpd30


# In[ ]:


### åœ¨T30+å®¢ç¾¤æ¨¡å‹æ•ˆæœå¯¹æ¯”


# In[ ]:


fpd30_plus = file_c['30_fpd30']
fpd30_plus.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
fpd30_plus.rename(columns={'KS_y_prob_base_v4':'KS_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹','AUC_y_prob_base_v4':'AUC_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹'},inplace=True)
fpd30_plus


# In[ ]:


mob4dpd30_plus = file_c['30_mob4dpd30']
mob4dpd30_plus.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
mob4dpd30_plus.rename(columns={'KS_y_prob_base_v4':'KS_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹','AUC_y_prob_base_v4':'AUC_æœ¬æ¬¡å¼€å‘çš„æ¨¡å‹'},inplace=True)
mob4dpd30_plus


# # äº”ã€è¯„åˆ†åˆ†å¸ƒ

# In[25]:


filepath_score = './result/6_è¯„åˆ†åˆ†å¸ƒ_æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”Ÿmob6dpd30ç¦»çº¿æ¨¡å‹_20250721160250.xlsx'
file_score = pd.read_excel(filepath_score, sheet_name=None)


# In[27]:


df_psi_matrix = file_score['df_psi_matrix'].copy()
df_psi_matrix.rename(columns={'Unnamed: 0':'psi'},inplace=True)
df_psi_matrix.set_index('psi',inplace=True)
df_psi_matrix.T


# In[28]:


score_group_by_dataset = file_score['score_group_by_dataset'].copy()
score_group_by_dataset.drop(columns=['Unnamed: 0'],inplace=True)
score_group_by_dataset = score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[29]:


score_group_by_dataset


# # å…­ã€å˜é‡åˆ†å¸ƒ

# In[39]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname ä¸º ä½ ä¸‹è½½çš„å­—ä½“åº“è·¯å¾„ï¼Œæ³¨æ„ SourceHanSansSC-Bold.otf å­—ä½“çš„è·¯å¾„
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # ä½¿ç”¨æ›´æ·±çš„å¯¹è‰²ç›²å‹å¥½çš„é¢œè‰²
 color_train = '#004494'  # æ·±è“è‰²
 color_oot = '#D66100'    # æ·±æ©™è‰²

 # ç»˜åˆ¶æŸ±çŠ¶å›¾
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT2',
                 color=color_oot, alpha=0.6)

 # æŸ±çŠ¶å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # æŠ˜çº¿å›¾ï¼Œä½¿ç”¨æ›´æ·±çš„é¢œè‰²å’Œæ ‡è®°
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT2')  # ä½¿ç”¨æ–¹å½¢æ ‡è®°
 ax2.set_ylabel('Bad Rate')

 # æŠ˜çº¿å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # æ·»åŠ IVå€¼
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT2 IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # è°ƒæ•´å›¾ä¾‹ä½ç½®
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # è°ƒæ•´å›¾è¡¨å¸ƒå±€ï¼Œç»™é¡¶éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[ ]:


df_importance.head()


# In[47]:


importance_dict = df_importance.set_index('feature')['comment'].to_dict()


# In[42]:


df_group_set = pd.read_excel('./result/3_å˜é‡åˆ†æ_dis_iv_psi_æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”Ÿmob6dpd30ç¦»çº¿æ¨¡å‹_20250722094044.xlsx',sheet_name='df_group_set')


# In[ ]:


df_group_set.head()


# In[50]:



for i, col in enumerate(df_importance['feature'].to_list()):
    
    var_des = importance_dict[col]
    print(f"--------ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col},{var_des}--------")
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot2'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot2'],3)
    # è°ƒç”¨å‡½æ•°
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot',iv_train, iv_oot)


# In[ ]:


# è¾“å‡ºæ¨¡å‹æŠ¥å‘Š
jupyter nbconvert --to html --no-input /data/home/liaoxilin/æ¨¡å‹å¼€å‘/03è´·ä¸­æ¨¡å‹/02æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹/æŠ¥å‘Š_æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.ipynb




#==============================================================================
# File: æŠ¥å‘Š_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# # ä¸€ã€æ¨¡å‹ç®€ä»‹

# æœ¬æ¨¡å‹ä¸ºæˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹ï¼ŒYæ ‡ç­¾ä¸ºfpd30ï¼Œç‰¹å¾å˜é‡æœ‰ç™¾èå­åˆ†ã€æ´ä¾¦å­åˆ†ã€ç»­ä¾¦å­åˆ†ã€å¾ä¿¡å­åˆ†ã€ä¸‰æ–¹å­åˆ†ã€‚æ¨¡å‹ç®—æ³•ä½¿ç”¨lgbç®—æ³•è¿›ã€‚
# trainæ ·æœ¬ï¼šé€‰å–2024å¹´11æœˆ1æ—¥è‡³2025å¹´2æœˆ28æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾çš„å€Ÿæ®ã€‚
# oot1æ ·æœ¬ï¼šé€‰å–2024å¹´10æœˆ1æ—¥è‡³2024å¹´10æœˆ31æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾çš„å€Ÿæ®ã€‚
# oot2æ ·æœ¬ï¼šé€‰å–2025å¹´3æœˆ1æ—¥è‡³2025å¹´3æœˆ31æ—¥ï¼Œapiæ¸ é“ã€é‡‘ç§‘æ¸ é“çš„æˆä¿¡æˆåŠŸä¸”æ”¾æ¬¾çš„å€Ÿæ®ã€‚
# 

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# # äºŒã€ æ ·æœ¬æ¦‚å†µ

# In[2]:


def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[3]:


# æœ€ç»ˆæ¨¡å‹æ‰“åˆ†
lgb_model= load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_v6_20250612152358.pkl')


# In[49]:


varsname = lgb_model.feature_name()
print(len(varsname), varsname)


# In[ ]:


df_sample = pd.read_csv('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_report.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[51]:


usecols = ['order_no','y_prob_base_v6'] + ['duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tianchuang_7', 'pudao_20', 'pudao_68', 'pudao_54', 'baihang_28', 'ali_fraud_score3', 'ppcm_behav_score', 'umeng_score_v5', 'ali_fraud_score9', 'br_fpd', 'pd_fpd', 'br_mob4_2', 'm1a0028_g_p', 'm1a0038_g_p', 'xz_v1_mob4', 'm1a0033_g_p', 'm1a0043_g_p', 'm1a0040_g_p', 'a_bhdj_fpd10_v1', 'm1a0035_g_p', 'dz_v2_fpd', 'xz_v2_fpd', 'm1a0041_g_p', 'm1a0044_g_p', 'm1a0036_g_p']
tmp = df_sample[usecols].head(3)
tmp.to_csv('test.csv',index=False)


# In[6]:


# è®¾ç½®æ ‡ç­¾å’Œåˆ†æ•°åˆ—
target = 'target_fpd30'
score = 'y_prob_base_v6'


# In[7]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[8]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
total_summary = df_sample[target].agg(total=lambda x: len(x), 
        bad=lambda x: x.sum(), 
        good=lambda x: (x== 0).sum(), 
        bad_rate=lambda x: x.mean()).to_frame().T
total_summary['bins'] = 'Total'
# å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
df_target_summary_month = pd.concat([df_target_summary_month, total_summary], ignore_index=True)
df_target_summary_month


# In[ ]:


# # ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

# X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname]
# y_train_ = df_sample.query("data_set not in ('3_oot')")[target]
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'


# In[9]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
df_target_summary_set.insert(0, 'time_windows', value=['20241101-20250228','20241101-20250228','20241001-20241031','20250301-20250331'])
df_target_summary_set


# # ä¸‰ã€å…¥æ¨¡å˜é‡é‡è¦æ€§

# In[10]:


filepath = './result/4_æ¨¡å‹è®­ç»ƒ_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_v6_20250612152358.xlsx'
file = pd.read_excel(filepath, sheet_name=None) 


# In[11]:


vars_des = {
 'dz_v2_fpd':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'xz_v2_fpd':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'m1a0043_g_p':'æç°å…¨æ¸ é“äººè¡Œfpd30æ¨¡å‹v1_2411_2502_å¥½æ¦‚ç‡'
,'m1a0041_g_p':'æç°å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿfpd30æ¨¡å‹'
,'hengpu_4':'æ’æ™®-åæ¬ºè¯ˆåˆ†M3'
,'m1a0033_g_p':'æˆä¿¡å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿfpd30æ¨¡å‹2408_2411_å¥½æ¦‚ç‡åˆ†'
,'pudao_34':'æœ´é“-é¿é›·é’ˆå®šåˆ¶åˆ†V1'
,'ruizhi_6':'FICOè”åˆå»ºæ¨¡å®šåˆ¶åˆ†2'
,'pd_fpd':'æˆä¿¡å…¨æ¸ é“æœ´é“å¤šå¤´æ¨¡å‹fpd30æ ‡ç­¾202504'
,'hengpu_5':'æ’æ™®-å®šåˆ¶ä¿¡ç”¨åˆ†Y'
,'baihang_13':'ç™¾è¡Œ-çµçŠ€äº§å“-å­šä¸´-107'
,'pudao_54':'æœ´é“-å“ˆå•°-hl-ç«çœ¼åˆ†v7'
,'pboc_dpd20':'æˆä¿¡é€šç”¨äººè¡Œæ¨¡å‹dpd20æ ‡ç­¾202410'
,'pudao_20':'æœ´é“-è…¾è®¯å¤©å¾¡åæ¬ºè¯ˆV7é€šç”¨ç‰ˆ'
,'aliyun_5':'æœ´é“-é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆV5'
,'m1a0028_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œmob4dpd30æ¨¡å‹V1_2408_2410_å¥½æ¦‚ç‡'
,'baihang_28':'ficoåæ¬ºè¯ˆæ´è§3.0'
,'pudao_82':'æœ´é“-å¤©åˆ›-ç„è¾°ä¿¡ç”¨åˆ†_AC1501'
,'m1a0035_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œfpd7æ¨¡å‹202408_2411'
,'m1a0038_g_p':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡åˆ†'
,'zhirongfen':'åŒç›¾æ™ºèåˆ†'
,'tianchuang_7':'å¤©åˆ›-è”åˆåˆ†A1502'
,'a_bhdj_fpd10_v1':'æˆä¿¡å…¨æ¸ é“ç™¾è¡Œæ´è§æ¨¡å‹v1fpd10æ ‡ç­¾'
,'duxiaoman_6':'åº¦å°æ»¡-æ¬ºè¯ˆå› å­V4'
,'m1a0044_g_p':'æç°å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡'
,'rong360_4':'æ•°æ®æº_rong360'
,'ali_fraud_score3':'é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆå­åˆ†score3'
,'xz_v1_mob4':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v1å››æœŸæ ‡ç­¾202505'
,'pudao_87':'æœ´é“-å­—èŠ‚-äº’è”ç½‘è¡Œä¸ºè¯„åˆ†25128'
,'m1a0020_g_p':'æˆä¿¡é‡‘ç§‘æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿfpd30æ¨¡å‹202409_2411'
,'a_pboc_fpd0_v1':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v1å…¥å‚¬æ ‡ç­¾202409'
,'br_fpd':'æˆä¿¡ç™¾èå¤šå¤´é¦–æœŸæ ‡ç­¾æ¨¡å‹202404'
,'m1a0040_g_p':'æˆä¿¡å…¨æ¸ é“æœ´é“å¤šå¤´å››æœŸæ¨¡å‹V1_2409_2411_å¥½æ¦‚ç‡'
,'pudao_84':'æœ´é“-å‹ç›Ÿ-å»ºæ¨¡åˆ†score_v3'
,'m1a0037_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡ŒåŠ è¡ç”Ÿfpd20_v1æ¨¡å‹2410_2411'
,'bileizhenv1':'é¿é›·é’ˆv1h1'
,'pudao_68':'æœ´é“-é“¶å•†é“¶æå®šåˆ¶åˆ†'
,'br_mob4_2':'æˆä¿¡ç™¾èå¤šå¤´v2å››æœŸæ ‡ç­¾202407'
,'m1a0036_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡ŒåŠ è¡ç”Ÿfpd1_v2æ¨¡å‹2409_2411'
,'umeng_score_v5':'å‹ç›Ÿ-å°é¢åˆ†V5.0'
,'ppcm_behav_score':'æµ·çº³æ”¯ä»˜è¡Œä¸ºåˆ†'
,'ali_fraud_score9':'é˜¿é‡Œäº‘-ç”³è¯·åæ¬ºè¯ˆå­åˆ†v2_å­åˆ†9'
,'ali_fraud_score3':'é˜¿é‡Œäº‘-ç”³è¯·åæ¬ºè¯ˆå­åˆ†v2_å­åˆ†'
}


# In[ ]:


df_importance = file['df_importance_month_v6'].drop(columns=['Unnamed: 0'])
df_importance['desc'] = df_importance['feature'].map(vars_des)
df_importance.insert(1,'å˜é‡åç§°', df_importance['desc'])
df_importance.drop(columns=['desc'],inplace=True)
df_importance


# In[13]:


file2 = pd.read_excel('./result/3_å˜é‡åˆ†æ_dis_iv_psi_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_20250605194218.xlsx',sheet_name=None)


# In[ ]:


file2.keys()


# In[15]:


file3 = pd.read_excel('./result/2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_20250605172607.xlsx',sheet_name=None)


# In[ ]:


file3.keys()


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§ 
df_iv_by_set = file2['df_iv_by_set'].set_index('Unnamed: 0')
df_iv_by_set.drop(columns=['3_oot1'],inplace=True)
df_psi_by_set = file2['df_psi_by_set'].set_index('Unnamed: 0')
df_psi_by_set.drop(columns=['3_oot1'],inplace=True)
df_miss_set = file3['df_miss_set'].set_index('variable')
df_miss_set.drop(columns=['3_oot1'],inplace=True)
df_iv_psi_miss_set = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set], axis=1)
df_iv_psi_miss_set.columns = [f'{col}_iv' for col in df_iv_by_set.columns]+[f'{col}_psi' for col in df_psi_by_set.columns]+[f'{col}_na' for col in df_miss_set.columns]
df_iv_psi_miss_set = df_iv_psi_miss_set.reset_index()
df_iv_psi_miss_set.rename(columns={"index":"feature"},inplace=True)


# In[ ]:


df_iv_psi_miss_set.head()


# In[ ]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance, df_iv_psi_miss_set, how='left',on='feature')
# df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[17]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_iv_by_month = file2['df_iv_by_month'].set_index('Unnamed: 0')
df_iv_by_month.drop(columns=['2025-04'],inplace=True)
df_psi_by_month = file2['df_psi_by_month'].set_index('Unnamed: 0')
df_psi_by_month.drop(columns=['2025-04'],inplace=True)
df_miss_month = file3['df_miss_month'].set_index('variable')
df_miss_month.drop(columns=['2025-04'],inplace=True)
df_iv_psi_miss = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month], axis=1)
df_iv_psi_miss.columns = [f'{col}_iv' for col in df_iv_by_month.columns]+[f'{col}_psi' for col in df_psi_by_month.columns]+[f'{col}_na' for col in df_miss_month.columns]
df_iv_psi_miss = df_iv_psi_miss.reset_index()
df_iv_psi_miss.rename(columns={"index":"feature"},inplace=True)


# In[ ]:


df_iv_by_month.head()


# In[18]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance, df_iv_psi_miss, how='left',on='feature')
# df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[ ]:





# In[ ]:


# df_corr.loc[varsname,varsname]


# In[ ]:


# df_corr.loc[varsname,varsname].to_csv('df_corr.csv')


# # å››ã€æ¨¡å‹æ•ˆæœè¯„ä¼°

# In[19]:


df_ks_auc_month_v1 = file['df_ks_auc_month_v6'].copy()
df_ks_auc_month_v1.rename(columns={'Unnamed: 0':'apply_month'},inplace=True)
# df_ks_auc_month_v1.drop(index=df_ks_auc_month_v1.query("apply_month=='2025-04'").index,inplace=True)
df_ks_auc_month_v1.reset_index(drop=True,inplace=True)
df_ks_auc_month_v1


# In[ ]:


file.keys()


# In[20]:


df_ks_auc_set_v1 = file['df_ks_auc_set_v6'].copy()
df_ks_auc_set_v1.rename(columns={'Unnamed: 0':'data_set'},inplace=True)
# df_ks_auc_set_v1.drop(index=df_ks_auc_set_v1.query("data_set=='3_oot1'").index,inplace=True)
df_ks_auc_set_v1.insert(0,'time_windows',value=['20241101-20250228','20241101-20250228','20241001-20241031','20250301-20250331']*6)
df_ks_auc_set_v1.reset_index(drop=True, inplace=True)
df_ks_auc_set_v1


# In[ ]:


df_ks_auc_set_v12 = df_ks_auc_set_v1.copy()


# In[ ]:


df_ks_auc_set_v1.loc[ 3,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[ 5,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[ 7,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[11,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[11,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[17,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[15,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[23,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[19,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[29,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[23,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[35,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1


# ### 0. æ¨¡å‹ä¸å…¥æ¨¡å˜é‡é—´çš„ç›¸å…³æ€§

# In[23]:


df_sample['high_p_f30_2506'] = df_sample['y_prob_base_v6']


# In[24]:


df_sample[['high_p_f30_2506']+varsname].corr()


# ### 1.åœ¨å…¨æ¸ é“ä¸èåˆæ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[ ]:


file4path = './result/6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_20250612154707.xlsx'
file4 = pd.read_excel(file4path, sheet_name=None)
file4.keys()


# In[33]:


df_c1 = file4['fpd30_èåˆ'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c1.drop(columns=drop_cols,inplace=True)
# df_c1.drop(index=df_c1.query("apply_month=='2025-04'").index,inplace=True)
df_c1 = df_c1.reset_index(drop=True)
df_c1.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506","KS_m1a0030_g_p":"KS_high_p_f30_2504","AUC_m1a0030_g_p":"AUC_high_p_f30_2504"},inplace=True)
df_c1


# In[34]:


df_c2 = file4['fpd30_ä¸‰æ–¹'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c2.drop(columns=drop_cols,inplace=True)
# df_c2.drop(index=df_c2.query("apply_month=='2025-04'").index,inplace=True)
df_c2 = df_c2.reset_index(drop=True)
df_c2.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506"},inplace=True)
df_c2


# ### 2.åœ¨å¾ä¿¡æ¸ é“ä¸èåˆæ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[ ]:


#(227,213,231,233,240,245,241,246)


# In[ ]:


file5path = './result/6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_pboc_20250612154845.xlsx'
file5 = pd.read_excel(file5path, sheet_name=None)
file5.keys()


# In[36]:


df_c3 = file5['pboc_fpd30_èåˆ'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c3.drop(columns=drop_cols,inplace=True)
# df_c3.drop(index=df_c3.query("apply_month=='2025-04'").index,inplace=True)
df_c3 = df_c3.reset_index(drop=True)
df_c3.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506","KS_m1a0030_g_p":"KS_high_p_f30_2504","AUC_m1a0030_g_p":"AUC_high_p_f30_2504"},inplace=True)
df_c3


# In[37]:


df_c4 = file5['pboc_pd30_ä¸‰æ–¹'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c4.drop(columns=drop_cols,inplace=True)
# df_c4.drop(index=df_c4.query("apply_month=='2025-04'").index,inplace=True)
df_c4 = df_c4.reset_index(drop=True)
df_c4.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506"},inplace=True)
df_c4


# # äº”ã€è¯„åˆ†åˆ†å¸ƒ

# In[38]:


filepath_score = './result/6_è¯„åˆ†åˆ†å¸ƒ_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_20250612155850.xlsx'
file_score = pd.read_excel(filepath_score, sheet_name=None)


# In[39]:


df_psi_matrix = file_score['df_psi_matrix']
df_psi_matrix.rename(columns={'Unnamed: 0':'psi'},inplace=True)
# df_psi_matrix.drop(columns=['2025-04'],inplace=True)
df_psi_matrix.set_index('psi',inplace=True)
# df_psi_matrix.drop(index=['2025-04'],inplace=True)
df_psi_matrix


# In[42]:


score_group_by_dataset = file_score['score_group_by_dataset'].copy()
score_group_by_dataset.drop(columns=['Unnamed: 0'],inplace=True)
score_group_by_dataset = score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")
score_group_by_dataset = score_group_by_dataset.reset_index(drop=True)
score_group_by_dataset.drop(columns=['ks_bin'],inplace=True)
score_group_by_dataset


# # å…­ã€å˜é‡åˆ†å¸ƒ

# In[43]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname ä¸º ä½ ä¸‹è½½çš„å­—ä½“åº“è·¯å¾„ï¼Œæ³¨æ„ SourceHanSansSC-Bold.otf å­—ä½“çš„è·¯å¾„
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # ä½¿ç”¨æ›´æ·±çš„å¯¹è‰²ç›²å‹å¥½çš„é¢œè‰²
 color_train = '#004494'  # æ·±è“è‰²
 color_oot = '#D66100'    # æ·±æ©™è‰²

 # ç»˜åˆ¶æŸ±çŠ¶å›¾
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # æŸ±çŠ¶å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # æŠ˜çº¿å›¾ï¼Œä½¿ç”¨æ›´æ·±çš„é¢œè‰²å’Œæ ‡è®°
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # ä½¿ç”¨æ–¹å½¢æ ‡è®°
 ax2.set_ylabel('Bad Rate')

 # æŠ˜çº¿å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # æ·»åŠ IVå€¼
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # è°ƒæ•´å›¾ä¾‹ä½ç½®
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # è°ƒæ•´å›¾è¡¨å¸ƒå±€ï¼Œç»™é¡¶éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[44]:


df_importance.head()


# In[45]:


importance_dict = df_importance.set_index('feature')['å˜é‡åç§°'].to_dict()


# In[ ]:


# df_group_set = pd.read_excel('./result/3_å˜é‡åˆ†æ_dis_iv_psi_æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2_20250331205148.xlsx',sheet_name='df_group_set')
df_group_set = file2['df_group_set'].copy()
df_group_set.head()


# In[ ]:


df_group_month = file2['df_group_month'].copy()
df_group_month.head()


# In[48]:



for col in importance_dict.keys():
    var_des = importance_dict[col]
    print(f"----------{col}:{var_des}----------")
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot2'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    var_des = importance_dict[col]
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot2'],3)
    # è°ƒç”¨å‡½æ•°
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot',iv_train, iv_oot)


# In[ ]:


# è¾“å‡ºæ¨¡å‹æŠ¥å‘Š
jupyter nbconvert --to html --no-input ./æ¨¡å‹å¼€å‘/01æˆä¿¡æ¨¡å‹/05æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502/æŠ¥å‘Š_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502.ipynb




#==============================================================================
# File: æŠ½æ ·å‡½æ•°.py
#==============================================================================

# æ•°æ®æŠ½æ ·
def resample_negative_samples(X_train, y_train, date_column, amplification_factor, total_samples=200000):
   
    # è®¡ç®—æ¯ä¸ªæœˆçš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    df = pd.concat([X_train, y_train], axis=1)
    df['flag'] = y_train
    df[date_column] = pd.to_datetime(df[date_column])
    df['month'] = df[date_column].dt.to_period('M').astype(str) 
    df_month_ratios = df.pivot_table(index='month', values='flag', aggfunc='mean')
    month_ratios = df_month_ratios['flag'].to_dict()
    
    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
    pos_samples = df[df['flag'] == 1]
    neg_samples = df[df['flag'] == 0]
    
    # è®¡ç®—æ­£æ ·æœ¬æ€»æ•°
    pos_count = len(pos_samples)

    # è®¡ç®—è´Ÿæ ·æœ¬åº”è¯¥æœ‰çš„æ€»æ•°
    neg_count = total_samples - pos_count
       
    # è®¡ç®—æ¯ä¸ªæ—¶é—´çª—å£éœ€è¦çš„ç›®æ ‡è´Ÿæ ·æœ¬æ•°é‡
    target_counts = {}
    for month, ratio in month_ratios.items():
        # è·å–è¯¥æœˆä»½çš„æ­£æ ·æœ¬æ•°é‡
        pos_count = len(pos_samples[pos_samples['month'] == month])
        # è®¡ç®—ç›®æ ‡è´Ÿæ ·æœ¬æ•°é‡
        target_neg_count = pos_count / (amplification_factor * ratio / (1 - ratio))
        target_counts[month] = int(target_neg_count)
    
    # æ ¹æ®æ€»è´Ÿæ ·æœ¬æ•°é‡é‡æ–°åˆ†é…å„æœˆçš„è´Ÿæ ·æœ¬æ•°é‡
    total_target_neg_count = sum(target_counts.values())
    adjusted_counts = {}
    for month, count in target_counts.items():
        adjusted_counts[month] = int((count / total_target_neg_count) * neg_count)
    
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºDataFrameæ¥å­˜å‚¨é‡é‡‡æ ·çš„ç»“æœ
    resampled_neg_samples = pd.DataFrame()

    # å¯¹æ¯ä¸ªæ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬è¿›è¡Œé‡é‡‡æ ·
    for month, target_count in adjusted_counts.items():
        # è·å–å½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬
        current_neg_samples = neg_samples[neg_samples['month'] == month]
        # å¦‚æœå½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬æ•°é‡å°‘äºç›®æ ‡æ•°é‡ï¼Œåˆ™ç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­
        if len(current_neg_samples) <= target_count:
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples])
        else:
            # å¦åˆ™ï¼Œä»å½“å‰æ—¶é—´çª—å£çš„è´Ÿæ ·æœ¬ä¸­éšæœºæŠ½å–ç›®æ ‡æ•°é‡çš„æ ·æœ¬
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples.sample(n=target_count)])

    # åˆå¹¶æ­£æ ·æœ¬å’Œé‡é‡‡æ ·åçš„è´Ÿæ ·æœ¬
    X_resampled = pd.concat([pos_samples, resampled_neg_samples])[X_train.columns]
    y_resampled = y_train.loc[X_resampled.index]

    return (X_resampled, y_resampled)



#==============================================================================
# File: æˆä¿¡_ç¦»çº¿ç‰¹å¾å˜é‡å·¥ç¨‹åŒ–å¤„ç†.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import time 
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc
from jinja2 import Template
import os

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'


# # é…ç½®å‚æ•°

# In[2]:



# é…ç½®å‚æ•°
BATCH_SIZE = 1500
OUTPUT_DIR = "./generated_sql"
MAIN_SQL_FILE = "main_table.sql"  # ä½ å¯ä»¥æŠŠä¸»è¡¨ SQL å†™å…¥æ–‡ä»¶ä¸­å¤‡ç”¨

os.makedirs(OUTPUT_DIR, exist_ok=True)


# # ç”Ÿæˆsqlè„šæœ¬

# In[3]:




# ç¤ºä¾‹å˜é‡æ¸…å•æ ¼å¼ï¼š
# variable_name | table_name
variables_df = pd.read_excel("äººè¡Œå˜é‡æ¸…å•2506.xlsx",sheet_name='ç¦»çº¿å˜é‡æ¸…å•')
variables_df.rename(columns={"Table": "table_name","var":"variable_name"}, inplace=True)
variables_df.info(show_counts=True)
variables_df.head()


# In[4]:


# åˆ†æ‰¹å¤„ç†
batches = [variables_df[i:i+BATCH_SIZE] for i in range(0, len(variables_df), BATCH_SIZE)]
print(len(batches))


# In[5]:


# Jinja2 æ¨¡æ¿ï¼šç‰¹å¾å˜é‡éƒ¨åˆ†
feature_template = Template("""
left join 
(
select t.*,
   ROW_NUMBER() OVER (PARTITION BY id_no_des ORDER BY dt DESC) AS rk 
from {{ full_table_name }} as t 
where dt <= date_sub('$[last_day(yyyy-MM-dd)]', 1) 
  and dt >= date_sub('$[last_day(yyyy-MM-dd)]', 100)
) as {{ alias_name }} on t.id_no_des = {{ alias_name }}.id_no_des and {{ alias_name }}.rk = 1
""")


# In[6]:



# ä¸»è¡¨æ¨¡æ¿å ä½ç¬¦
MAIN_TEMPLATE = """
WITH main_base AS (
    -- ä¸»è¡¨ SQL æ”¾åœ¨è¿™é‡Œ
),
features AS (
    -- æ‰€æœ‰ left join çš„ç‰¹å¾è¡¨æ”¾åœ¨è¿™é‡Œ
)
SELECT *
FROM main_base
LEFT JOIN features USING (id_no_des)
"""


# In[7]:


# è¯»å–ä¸»è¡¨ SQLï¼ˆå‡è®¾ä½ å·²ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼‰
with open(MAIN_SQL_FILE, 'r') as f:
    main_sql = f.read()


# In[8]:


print(main_sql)


# In[9]:


for idx, batch in enumerate(batches):
    print(f"\nProcessing Batch {idx + 1} / {len(batches)}")
    
    # æŒ‰ table_name åˆ†ç»„ï¼Œå¾—åˆ° { 'tableA': ['var1', 'var2'], ... }
    grouped = batch.groupby('table_name')['variable_name'].apply(list).to_dict()
    
    feature_joins = []
    alias_counter = 1
    
    # æ–°å¢ï¼šè®°å½•æ¯ä¸ªè¡¨å¯¹åº”çš„åˆ«åå’Œå˜é‡åˆ—è¡¨
    batch_aliases = {}

    for table_name, vars_list in grouped.items():
        alias_name = f"t{alias_counter}"
        alias_counter += 1
        
        # è®°å½•å½“å‰è¡¨çš„åˆ«åå’Œå­—æ®µ
        batch_aliases[table_name] = (alias_name, vars_list)
        
        # å¡«å……æ¨¡æ¿ï¼Œæ„é€ æ¯ä¸ªç‰¹å¾è¡¨çš„ SELECT å­—æ®µåˆ—è¡¨
        sql_part = feature_template.render(
            full_table_name=table_name,
            variables=vars_list,
            alias_name=alias_name
        )
        feature_joins.append(sql_part)
    
    # åˆå¹¶æ‰€æœ‰ left join å­å¥
    all_features_sql = "\n".join(feature_joins)

    # æ„é€  SELECT åˆ—è¡¨ï¼št.*, t1.var1, t1.var2, t2.var101, ...
    feature_columns = [
        f"{alias}.{var}"
        for table_name, (alias, vars_list) in batch_aliases.items()
        for var in vars_list
    ]
    features_select_clause = ",\n        ".join(feature_columns)

    # å°†ä¸»è¡¨å’Œç‰¹å¾ join åˆå¹¶
    final_sql = f"""
    WITH main_base AS (
        {main_sql}
    ),
    features_with_vars AS (
        SELECT 
            t.*,
            {features_select_clause}
        FROM main_base AS t
        {all_features_sql}
    )
    SELECT * FROM features_with_vars
    """
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = os.path.join(OUTPUT_DIR, f"batch_{idx + 1}.sql")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_sql)
        
    print(f"âœ… å·²ç”Ÿæˆ SQL æ–‡ä»¶: {output_file}")


# # å¤„ç†æ•°æ®

# In[10]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
import glob
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[11]:



# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance

def model_trian(df,target,varsname_base):
    ### æ¨¡å‹å‚æ•°
    opt_params = {}
    opt_params['boosting'] = 'gbdt'
    opt_params['objective'] = 'binary'
    opt_params['metric'] = 'auc'
    opt_params['bagging_freq'] = 1
    opt_params['scale_pos_weight'] = 1 
    opt_params['seed'] = 1 
    opt_params['num_threads'] = -1 
    # è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
    opt_params['learning_rate'] = 0.1
    ## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    opt_params['bagging_fraction'] = 0.8628008772208227     
    opt_params['feature_fraction'] = 0.6177619614753441
    opt_params['lambda_l1'] = 0
    opt_params['lambda_l2'] = 300
    opt_params['early_stopping_rounds'] = 50

    # è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
    opt_params['num_leaves'] = 21
    opt_params['min_data_in_leaf'] = 103
    opt_params['max_depth'] = 2
    # è°ƒå‚åçš„å…¶ä»–å‚
    opt_params['min_gain_to_split'] = 10
    
    df_sample = df[df[target].notna()]
    df_sample[target] = df_sample[target].astype('int')
    df_sample = df_sample[df_sample[target]>=0].reset_index(drop=True)
    df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-10-31'").index, 'data_set']='1_train'
    df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot1'
    df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-12-14'").index, 'data_set']='3_oot2'

    # ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹
    for i, col in enumerate(varsname_base):
        if df_sample[col].dtype=='object':
            df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')
    
    X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
    y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[target]
    X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                        y_train_,
                                                        test_size=0.2, 
                                                        random_state=22, 
                                                        stratify=y_train_
                                                       )
    df_sample.loc[X_train.index, 'data_set']='1_train'
    df_sample.loc[X_test.index, 'data_set']='2_test'

    train_set = lgb.Dataset(X_train, label=y_train)
    valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
    lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)
    df_importance = feature_importance(lgb_model) 
    df_importance = df_importance.reset_index()
    return df_importance


def load_sql_from_file(filepath):
    """è¯»å– .sql æ–‡ä»¶å†…å®¹"""
    with open(filepath, 'r', encoding='utf-8') as f:
        sql_content = f.read()
    return sql_content



def run_all_sql_files(target, non_feature_cols, input_dir='./generated_sql/', output_file='all_features.csv'):
    """
    è¯»å–æ‰€æœ‰ SQL æ–‡ä»¶ï¼Œä¾æ¬¡æ‰§è¡Œå¹¶åˆå¹¶ç»“æœ
    """
    sql_files = sorted(glob.glob(os.path.join(input_dir, "*.sql")))
    all_data = []
    for idx, file in enumerate(sql_files):
        print(f"\nğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {idx+1} / {len(sql_files)} ä¸ª SQL æ–‡ä»¶: {file}")

        try:
            sql = load_sql_from_file(file)
            df_sample_dict = {}
            
            # è®¡ç®—ä»Šå¤©çš„æ—¶é—´
            from datetime import datetime, timedelta, date
            this_day =datetime.strptime('2024-12-14', '%Y-%m-%d')
            end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

            while this_day >= end_day:
                run_day = this_day.strftime('%Y-%m-%d')
                # æ›¿æ¢æ—¥æœŸå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                final_sql = sql.replace("$[last_day(yyyy-MM-dd)]", f"{run_day}")
                print(f'=========================={run_day}=============================')
                df_sample_dict[run_day] = get_data(final_sql)
                this_day = this_day - timedelta(days=1)
            df = pd.concat(df_sample_dict.values(), ignore_index=True)
            
            if not df.empty:
                # å¯é€‰ï¼šåˆ é™¤é‡å¤å­—æ®µï¼ˆå¦‚ id_no_des å·²å­˜åœ¨äºä¸»è¡¨ï¼‰
                cols_to_drop = [col for col in df.columns if df.columns.tolist().count(col) > 1]
                df = df.loc[:, ~df.columns.duplicated()]
                print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œè¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
                varsname_base = [col for col in df.columns if col not in non_feature_cols]
                df_importance = model_trian(df, target, varsname_base)
                all_data.append(df_importance)
            else:
                print(f"âš ï¸ è­¦å‘Šï¼š{file} è¿”å›ç©ºæ•°æ®")

        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {file}")
            print("é”™è¯¯è¯¦æƒ…:", str(e))
            continue

    if all_data:
        print("\nğŸ“Š æ­£åœ¨åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®...")
        final_df = pd.concat(all_data, axis=0)

        # ä¿å­˜æœ€ç»ˆç»“æœ
#         final_df.to_parquet(output_file)
        final_df.to_csv(output_file)
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {output_file}")
        return final_df
    else:
        print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®ã€‚")
        return None


# In[ ]:


non_feature_cols = ['order_no','user_id','id_no_des','channel_id','apply_date',
                    'target_fpd30','target_cpd30','target_mob4dpd30','target_mob6dpd30']
target = 'target_mob6dpd30'
final_data = run_all_sql_files(target, non_feature_cols, input_dir="./generated_sql", output_file="final_features.csv")
print("æœ€ç»ˆæ•°æ®é›†åˆ—æ•°:", len(final_data.columns))


# In[13]:


final_data


# In[14]:


final_data.to_csv('20000ä¸ªç¦»çº¿äººè¡Œå˜é‡ç‰¹å¾é‡è¦æ€§æ’åº.csv')


# In[15]:


final_data_v1 = final_data[['gain']]
final_data_v1.info(show_counts=True)
final_data_v1.head()


# In[21]:


final_data_v2 = final_data_v1[final_data_v1>0]
final_data_v2.info(show_counts=True)


# In[22]:


final_data_v2 = final_data_v2.dropna(how='all')
final_data_v2 =final_data_v2.reset_index()
final_data_v2.info(show_counts=True)


# In[23]:


final_data_v2.to_csv('13500ä¸ªç¦»çº¿äººè¡Œå˜é‡ç‰¹å¾é‡è¦æ€§æ’åºå¤§äº0.csv')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


# df_sample_dict = {}


# In[5]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[6]:


print(len(table_name_list))


# In[8]:


df_target = pd.read_csv(result_path + '00_model_target.csv')


# In[9]:


df_feature1 = pd.read_csv(result_path + '01_behave_features.csv')
df_feature1 = df_feature1.set_index('order_no')

df_feature2 = pd.read_csv(result_path + '02_behave_features.csv')
df_feature2 = df_feature2.set_index('order_no')

df_feature3 = pd.read_csv(result_path + '03_behave_features.csv')
df_feature3 = df_feature3.set_index('order_no')

df_feature4 = pd.read_csv(result_path + '04_behave_features.csv')
df_feature4 = df_feature4.set_index('order_no')

df_feature5 = pd.read_csv(result_path + '05_behave_features.csv')
df_feature5 = df_feature5.set_index('order_no')

df_feature6 = pd.read_csv(result_path + '06_behave_features.csv')
df_feature6 = df_feature6.set_index('order_no')


# In[15]:


df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5,df_feature6],axis=1)
df_feature = df_feature.reset_index()
df_feature.shape


# In[16]:


df_target = df_target.reset_index()
df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[24]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[27]:


varsname = df_sample.columns.to_list()[22:]


print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))

print(varsname[:5], varsname[-5:])


# In[28]:


pd.set_option('display.max_columns',None)


# In[30]:


df_sample.head(2)


# In[33]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.csv')


# In[36]:


df_sample[varsname].info()


# # 1. æ ·æœ¬æ¦‚å†µ

# In[150]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # è¿”å›ç»“æœ
    return result


# In[48]:


target = 'mob4dpd30'


# In[49]:


print(df_sample[target].value_counts())


# In[50]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[51]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[52]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[53]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[54]:


# for col in varsname:
#     if df_sample[col].min()<0:
#         print(f"--{col}--")
#         df_sample.loc[df_sample[col]<0, col] = np.nan
# gc.collect()


# In[55]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[250]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[251]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[57]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[58]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[59]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[60]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[61]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
#         df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[286]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.0001].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[287]:


df_iv.loc[to_drop_iv,:].head()


# In[290]:


df_vars_list = pd.read_excel(r'è¡Œä¸ºç‰¹å¾å˜é‡æ¸…å•.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[419]:


varsname_all = [col for col in varsname if col not in  [col for col in varsname if f'{col}'.startswith('uti')]]


# In[296]:


to_drop1 = [col for col in varsname if f'{col}'.startswith('uti')] + to_drop1


# In[297]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1)


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[298]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.0001, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[299]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[300]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(varsname_v2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[301]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[302]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[303]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[304]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[305]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}, éç©ºç®±å­ä¸ªæ•°ï¼š{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[306]:


new_bins_dict


# In[307]:


combiner.update(new_bins_dict)


# In[308]:


combiner.export()


# In[309]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[310]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[311]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[313]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)


# In[ ]:





# In[314]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[315]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[316]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[317]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname ä¸º ä½ ä¸‹è½½çš„å­—ä½“åº“è·¯å¾„ï¼Œæ³¨æ„ SourceHanSansSC-Bold.otf å­—ä½“çš„è·¯å¾„
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # ä½¿ç”¨æ›´æ·±çš„å¯¹è‰²ç›²å‹å¥½çš„é¢œè‰²
 color_train = '#004494'  # æ·±è“è‰²
 color_oot = '#D66100'    # æ·±æ©™è‰²

 # ç»˜åˆ¶æŸ±çŠ¶å›¾
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # æŸ±çŠ¶å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # æŠ˜çº¿å›¾ï¼Œä½¿ç”¨æ›´æ·±çš„é¢œè‰²å’Œæ ‡è®°
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # ä½¿ç”¨æ–¹å½¢æ ‡è®°
 ax2.set_ylabel('Bad Rate')

 # æŠ˜çº¿å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # æ·»åŠ IVå€¼
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # è°ƒæ•´å›¾ä¾‹ä½ç½®
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # è°ƒæ•´å›¾è¡¨å¸ƒå±€ï¼Œç»™é¡¶éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[318]:


df_vars_list = pd.read_excel(r'è¡Œä¸ºç‰¹å¾å˜é‡æ¸…å•.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[319]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[320]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # è°ƒç”¨å‡½æ•°
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[322]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.0001].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[323]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[324]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[325]:


to_drop3 = []


# In[326]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[327]:


target


# In[328]:


df_bins.shape
df_bins.head()


# In[329]:



def calculate_woe(df, col, target):
    """
    è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
    :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
    :param binned_col: åˆ†ç®±å˜é‡å
    :param target_col: ç›®æ ‡å˜é‡å
    :return: WOEå€¼çš„å­—å…¸
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[330]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[331]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[337]:


df_sample_woe[varsname_v3].head()


# In[338]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[340]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[341]:


c.head()


# In[346]:


table_drop_ = ['znzz_fintech_ads.dim_pub_user_fd_tal_vars','znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n']
table_drop_ = list(df_vars_list.query("è¡¨å in @table_drop_")['name'])


# In[348]:


len(table_drop_)


# In[349]:


table_drop = [col for col in df_corr_matrix if col in  table_drop]
table_drop


# In[350]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[351]:


df_high_corr


# In[352]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[353]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[355]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[356]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[357]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[358]:


to_drop5 


# In[ ]:





# In[359]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[360]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[361]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[362]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']
df_sample['mob4dpd30'].value_counts()


# In[363]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[364]:


df_sample['data_set'].value_counts()


# In[365]:


# # æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[366]:


df_sample['channel_types'].value_counts()


# In[367]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[403]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 0.60


# In[386]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[370]:


print(len(varsname_v5))
print(varsname_v5)


# In[371]:


# varsname_base = varsname_v5[:]


# In[399]:


xx = ['au00dn_f','au00dn_p','au12odc_f','r03odsa','rnns1rpddnl','ap03odcap12odcr_f','ap00dn','ap00dn_f','rnnt1tubal','r03pall','r00ol01sedn','rnns0nplam','ad06odc','au06chlcau12chlcr','rnntalubcam','ap03lml','ap06odcap12odcr_f','rnnt1tucaubarm','rnnt1tufacfara','rnnt1tuam','au01odc_f','rnnt1tt1c','rnns1selamdim','r01palm','r02rnrm','ap12chlodcap12chlodcup_f','r24o00pals','rnns0rpddnl','rnns0c','r06palslwwlammr','ap03lmlap12lmlr','ap06lmsap12lmsr_p','r24palsl24lammr','rnns1c','r02ol03cr06ol03cr','rnns1rwlnpls','r02prdtpsdfl','l03laml','r01palsrwwsplmr','r06o00palsr12o00palsr','ap06odcap06odcr_p','au12odpr','rnnt1tucal','ap06chlcap06chlcr','ap12lms_f','ap12mnc','rnnt1tt0c','r03o00palsr06o00palsr','r24odss','r01s1prnodsm','rnntalucaubcarl','r01s1prnodsdnm','ap06lmmap12lmmr','ap03chlodcap12chlodcup_p','r01nplsr02nplsr','r03palsl03lammr','ap06lmlap12lmlr','r02o00palsr03o00palsr','ap06chlodcap12chlodcup_f','r01pr6odsl','r03pr6c','ap03dchlmodc_p','r12s1prnodsdnm','ap03chlodcap12chlodcup_f','au06odcau12odcr_p','rnnt1tufacfarl','ap02mnoddcm','r03prdtpsdf0c','ap03dchlmodc_f']
xxx = [col for col in xx if col not in df_sample.columns]
print(len(xxx))


# In[420]:


df_sample['data_set'].value_counts()


# In[421]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_all]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[422]:


df_sample['data_set'].value_counts()


# In[392]:


# 2 å®šä¹‰è¶…å‚ç©ºé—´
# hp.quniform("å‚æ•°åç§°",ä¸‹ç•Œ,ä¸Šç•Œ,æ­¥é•¿)-é€‚ç”¨äºç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æµ®ç‚¹ç‚¹æ•°
# hp.uniform("å‚æ•°åç§°",ä¸‹ç•Œ, ä¸‹ç•Œ)-é€‚ç”¨äºè¿ç»­éšæœºåˆ†å¸ƒçš„æµ®ç‚¹æ•°
# hp.randint("å‚æ•°åç§°",ä¸Šç•Œ)-é€‚ç”¨äº[0,ä¸Šç•Œ)çš„æ•´æ•°,åŒºé—´ä¸ºå·¦é—­å³å¼€
# hp.choice("å‚æ•°åç§°",["å­—ç¬¦ä¸²1","å­—ç¬¦ä¸²2",...])-é€‚ç”¨äºå­—ç¬¦ä¸²ç±»å‹,æœ€ä¼˜å‚æ•°ç”±ç´¢å¼•è¡¨ç¤º
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# å¯ä»¥æ ¹æ®éœ€è¦ï¼Œæ³¨é‡Šæ‰ååçš„ä¸€äº›ä¸å¤ªé‡è¦çš„è¶…å‚

spaces = {
          # general parameters
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,5,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.6,1.0),
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.0,1.0),
          "lambda_l1": 0,
          "lambda_l2": 300,
          "early_stopping_rounds": 50
          }


# In[396]:


# 3ï¼Œæ‰§è¡Œè¶…å‚æœç´¢
# æœ‰äº†ç›®æ ‡å‡½æ•°å’Œå‚æ•°ç©ºé—´,æ¥ä¸‹æ¥è¦è¿›è¡Œä¼˜åŒ–,éœ€è¦äº†è§£ä»¥ä¸‹å‚æ•°:
# fmin:è‡ªå®šä¹‰ä½¿ç”¨çš„ä»£ç†æ¨¡å‹(å‚æ•°algo),hyperoptæ”¯æŒå¦‚ä¸‹æœç´¢ç®—æ³•ï¼š
#       éšæœºæœç´¢(hyperopt.rand.suggest)
#       æ¨¡æ‹Ÿé€€ç«(hyperopt.anneal.suggest)
#       TPEç®—æ³•ï¼ˆhyperopt.tpe.suggestï¼Œç®—æ³•å…¨ç§°ä¸ºTree-structured Parzen Estimator Approachï¼‰
# partial:ä¿®æ”¹ç®—æ³•æ¶‰åŠåˆ°çš„å…·ä½“å‚æ•°,åŒ…æ‹¬æ¨¡å‹å…·ä½“ä½¿ç”¨äº†å¤šå°‘å°‘ä¸ªåˆå§‹è§‚æµ‹å€¼(å‚æ•°n_start_jobs),
#         ä»¥åŠåœ¨è®¡ç®—é‡‡é›†å‡½æ•°å€¼æ—¶ç©¶ç«Ÿè€ƒè™‘å¤šå°‘ä¸ªæ ·æœ¬(å‚æ•°n_EI_candidates)
# trials:è®°å½•æ•´ä¸ªè¿­ä»£è¿‡ç¨‹,ä»hyperoptåº“ä¸­å¯¼å…¥çš„æ–¹æ³•Trials(),ä¼˜åŒ–å®Œæˆä¹‹å,
#        å¯ä»¥ä»ä¿å­˜å¥½çš„trialsä¸­æŸ¥çœ‹æŸå¤±ã€å‚æ•°ç­‰å„ç§ä¸­é—´ä¿¡æ¯
# early_stop_fn:æå‰åœæ­¢å‚æ•°,ä»hyperoptåº“å¯¼å…¥çš„æ–¹æ³•no_progresss_loss(),å¯ä»¥è¾“å…¥å…·ä½“çš„æ•°å­—n,
#               è¡¨ç¤ºå½“æŸå¤±è¿ç»­næ¬¡æ²¡æœ‰ä¸‹é™æ—¶,è®©ç®—æ³•æå‰åœæ­¢
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=100):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """å®šä¹‰ç›®æ ‡å‡½æ•°"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(50)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    best_params['boosting'] = 'gbdt'
    best_params['objective'] = 'binary'
    best_params['metric'] = 'auc'
    best_params['num_leaves'] = int(best_params['num_leaves'])
    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])
    best_params['max_depth'] = int(best_params['max_depth'])
    best_params['bagging_freq'] = 1 
    best_params['early_stopping_rounds'] = 50
    best_params['scale_pos_weight'] = 1 
    best_params['seed'] = 1 
    print("æœ€ä¼˜å‚æ•°", best_params)
    
    return (best_params, trials)


# In[415]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=10)


# In[416]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(best_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[423]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[417]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[382]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[401]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[418]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[410]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[424]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[402]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[391]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[285]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1


# In[196]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2.2 baseæ¨¡å‹_å‰”é™¤ç‰¹å¾å…¨ä¸ºç©ºå€¼çš„æ ·æœ¬

# In[191]:


df_sample_notna = df_sample.loc[~df_sample[varsname_base].isna().all(axis=1),:]
df_sample_notna.shape


# In[192]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_notna['data_set'].value_counts()


# In[193]:


# è®­ç»ƒæ•°æ®é›†
X_train = df_sample_notna.query("data_set in ('1_train')")[varsname_base]
y_train = df_sample_notna.query("data_set in ('1_train')")[modeltrian_target]

X_test = df_sample_notna.query("data_set in ('2_test')")[varsname_base]
y_test = df_sample_notna.query("data_set in ('2_test')")[modeltrian_target]


# In[194]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[195]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[197]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_notna['y_prob_v2'] = lgb_model.predict(df_sample_notna[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_notna['y_prob_v2'].head()


# In[ ]:





# In[198]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample_notna, modeltrian_target, target, 'y_prob_v2', 'apply_month')
df_ks_auc_month_v2


# In[200]:


df_ks_auc_set_v2 = model_ks_auc(df_sample_notna, modeltrian_target, 'y_prob_v2', 'data_set')
df_ks_auc_set_v2['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_notna, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
df_ks_auc_set_v2


# In[223]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model)
df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[224]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = pd.merge(df_vars_list.drop(columns='type'), df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[225]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# In[204]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v2'].head()


# In[206]:


df_ks_auc_month_v2_all = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v2', 'apply_month')
df_ks_auc_month_v2_all


# In[207]:


df_ks_auc_set_v2_all = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v2', 'data_set')
df_ks_auc_set_v2_all['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2_all = pd.concat([tmp, df_ks_auc_set_v2_all], axis=1)
df_ks_auc_set_v2_all


# In[256]:


with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_all_{timestamp}.xlsx') as writer:
    df_importance_month_v2_all.to_excel(writer, sheet_name='df_importance_month_v2_all')
    df_importance_set_v2_all.to_excel(writer, sheet_name='df_importance_set_v2_all')
    df_ks_auc_month_v2_all.to_excel(writer, sheet_name='df_ks_auc_month_v2_all')
    df_ks_auc_set_v2_all.to_excel(writer, sheet_name='df_ks_auc_set_v2_all')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_all_{timestamp}.xlsx')


# ### 5.2.3 baseæ¨¡å‹_è°ƒæ•´è®­ç»ƒæ ·æœ¬

# In[227]:


df_sample_09 = df_sample.query("apply_date>='2024-09-01'")
df_sample_09.shape


# In[228]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_09['data_set'].value_counts()


# In[230]:


toad.detect(df_sample_09[varsname_base])


# In[231]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_09.loc[df_sample_09.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample_09['data_set'].value_counts()


# In[232]:


# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†

X_train_ = df_sample_09.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample_09.query("data_set not in ('3_oot')")[modeltrian_target]


X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample_09.loc[X_train.index, 'data_set']='1_train'
df_sample_09.loc[X_test.index, 'data_set']='2_test'


# In[234]:


print(df_sample_09['data_set'].value_counts())


# In[235]:



### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[236]:



# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[240]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_09['y_prob_v3'] = lgb_model.predict(df_sample_09[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_09['y_prob_v3'].head()


# In[241]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample_09, modeltrian_target, target, 'y_prob_v3', 'apply_month')
df_ks_auc_month_v3


# In[242]:


df_ks_auc_set_v3 = model_ks_auc(df_sample_09, modeltrian_target, 'y_prob_v3', 'data_set')
df_ks_auc_set_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_09, target, 'data_set').set_index('bins')
df_ks_auc_set_v3 = pd.concat([tmp, df_ks_auc_set_v3], axis=1)
df_ks_auc_set_v3


# In[243]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[244]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = pd.merge(df_vars_list.drop(columns='type'), df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[245]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx')


# ### 5.2.3 åŠ å…¥ä¸‰æ–¹ç¼“å­˜æ•°æ®

# In[504]:


df_sample_30 = df_sample.query("channel_id!=1").reset_index(drop=True)
df_sample_30.shape


# In[281]:


print(len(varsname_v5))
print(varsname_v5)


# In[505]:


varsname_three = ['duxiaoman_6', 'hengpu_4', 'aliyun_5', 'pudao_34', 'feicuifen', 'pudao_20',
                  'pudao_68', 'ruizhi_6', 'pudao_21']
                   
varsname_base_v3 = varsname_base_v2 + varsname_three
print(len(varsname_base_v3), varsname_base_v3)


# In[506]:


varsname_base_v3.remove('feicuifen')
varsname_base_v3.remove('pudao_20')
varsname_base_v3.remove('score_fpd10_v2')


# In[489]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[507]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_30['data_set'].value_counts()


# In[52]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[508]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[509]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[53]:


# è®­ç»ƒæ•°æ®é›†
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]

# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[510]:



# è®­ç»ƒæ•°æ®é›†
X_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample_30.loc[X_train.index, 'data_set']='1_train'
df_sample_30.loc[X_test.index, 'data_set']='2_test'
print(df_sample_30['data_set'].value_counts())


# In[511]:



### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[512]:



# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[56]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head() 


# In[289]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base2_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v3'].head()   


# In[513]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base2_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v3'].head()   


# In[57]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v3 = pd.concat([tmp, df_ks_auc_month_base_v3], axis=1)
print(df_ks_auc_month_base_v3)


df_ks_auc_set_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_base_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3 = pd.concat([tmp, df_ks_auc_set_base_v3], axis=1)
print(df_ks_auc_set_base_v3)


# In[520]:


df_ks_auc_month_base2_v3 = calculate_ks_auc(df_sample, modeltrian_target,target,'y_prob_base2_v3','apply_month')


# In[515]:


modeltrian_target


# In[368]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2', 'apply_month')
df_tmp_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[584]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_tmp_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[502]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_tmp_base2_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_tmp_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[ ]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_tmp_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


df_ks_auc_set_base2_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_tmp_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v3 = pd.concat([tmp, df_ks_auc_set_base2_v3], axis=1)
print(df_ks_auc_set_base2_v3)


# In[585]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_ks_auc_month_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_ks_auc_month_base2_v3 = pd.concat([tmp, df_ks_auc_month_base2_v3], axis=1)
print(df_ks_auc_month_base2_v3)


df_ks_auc_set_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_ks_auc_set_base2_v3['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v3 = pd.concat([tmp, df_ks_auc_set_base2_v3], axis=1)
print(df_ks_auc_set_base2_v3)


# In[58]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v3_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v3', 'apply_month')
    tmp1['æ¸ é“'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v3_type = pd.concat([df_ks_auc_month_base_v3_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base_v3_type)


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v3_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v3', 'apply_month')
    tmp1['æ¸ é“'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v3_rate = pd.concat([df_ks_auc_month_base_v3_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base_v3_rate)


# In[291]:




# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v3_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
    tmp1['æ¸ é“'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v3_type = pd.concat([df_ks_auc_month_base2_v3_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base2_v3_type)


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v3_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
    tmp1['æ¸ é“'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v3_rate = pd.concat([df_ks_auc_month_base2_v3_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base2_v3_rate)


# In[59]:



# åˆå¹¶
df_ks_auc_month_base_v3 = pd.concat([df_ks_auc_month_base_v3, df_ks_auc_month_base_v3_type, df_ks_auc_month_base_v3_rate])
df_ks_auc_month_base_v3.head()


# In[292]:


# åˆå¹¶
df_ks_auc_month_base2_v3 = pd.concat([df_ks_auc_month_base2_v3, df_ks_auc_month_base2_v3_type, df_ks_auc_month_base2_v3_rate])
df_ks_auc_month_base2_v3.head()


# In[61]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_base_v3 = feature_importance(lgb_model) 
df_importance_base_v3 = pd.merge(df_importance_base_v3, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base_v3 = df_importance_base_v3.reset_index()
df_importance_base_v3 = df_importance_base_v3.rename(columns={'index':'varsname'})
df_importance_base_v3


# In[521]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_base2_v3 = feature_importance(lgb_model) 
df_importance_base2_v3 = pd.merge(df_importance_base2_v3, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base2_v3 = df_importance_base2_v3.reset_index()
df_importance_base2_v3 = df_importance_base2_v3.rename(columns={'index':'varsname'})
df_importance_base2_v3


# In[62]:


# æ¨¡å‹ç›¸å…³æ€§
df_corr_base_v3 = df_corr_matrix.loc[varsname_base_v3, varsname_base_v3]
df_iv_base_v3 = df_iv_by_set.loc[varsname_base_v3,:]

df_corr_base_v3


# In[522]:


# æ¨¡å‹ç›¸å…³æ€§
df_corr_base2_v3 = df_corr_matrix.loc[varsname_base_v3, varsname_base_v3]
df_iv_base2_v3 = df_iv_by_set.loc[varsname_base_v3,:]


# In[63]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base_v3_{timestamp}.xlsx') as writer:
    df_importance_base_v3.to_excel(writer, sheet_name='df_importance_base_v3')
    df_ks_auc_month_base_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_v3')
    df_ks_auc_set_base_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_v3')
    df_corr_base_v3.to_excel(writer, sheet_name='df_corr_base_v3')
    df_iv_base_v3.to_excel(writer, sheet_name='df_iv_base_v3')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base_v3_{timestamp}.xlsx')


# In[295]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v3_{timestamp}.xlsx') as writer:
    df_importance_base2_v3.to_excel(writer, sheet_name='df_importance_base2_v3')
    df_ks_auc_month_base2_v3.to_excel(writer, sheet_name='df_ks_auc_month_base2_v3')
    df_ks_auc_set_base2_v3.to_excel(writer, sheet_name='df_ks_auc_set_base2_v3')
    df_corr_base2_v3.to_excel(writer, sheet_name='df_corr_base2_v3')
    df_iv_base2_v3.to_excel(writer, sheet_name='df_iv_base2_v3')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v3_{timestamp}.xlsx')


# In[523]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v3_{timestamp}.xlsx') as writer:
    df_importance_base2_v3.to_excel(writer, sheet_name='df_importance_base2_v3')
    df_ks_auc_month_base2_v3.to_excel(writer, sheet_name='df_ks_auc_month_base2_v3')
    df_ks_auc_set_base2_v3.to_excel(writer, sheet_name='df_ks_auc_set_base2_v3')
    df_corr_base2_v3.to_excel(writer, sheet_name='df_corr_base2_v3')
    df_iv_base2_v3.to_excel(writer, sheet_name='df_iv_base2_v3')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v3_{timestamp}.xlsx')


# ### 5.2.4 åŠ å…¥èåˆæ¨¡å‹å­åˆ†

# In[296]:


print(len(varsname_v5))
print(varsname_v5)


# In[535]:


varsname_merge_score = ['all_a_app_free_fpd30_202502_s',
                   'hlv_d_holo_jk_certno_fpd1_score',
                   'hlv_d_holo_jk_certno_varcode_standard_bd0004',
                       'hlv_d_holo_jk_certno_score_fpd30_v1']
                   
varsname_base_v4 = varsname_base_v3 + varsname_merge_score
print(len(varsname_base_v4),varsname_base_v4)


# In[ ]:


score_fpd6_v1	0.000000	0	0.011333	0.018843	0.026104	0.02051	0.02822
24	score_fpd10_v1	0.000000	0	0.022598	0.020146	0.021562	0.0212	0.022973
25	hlv_d_holo_certno_variablecode_dpd30_6m_bd0001...	0.000000	0	0.12608	0.154373	0.154025	0.083876	0.079164
26	hlv_d_holo_certno_variablecode_dpd30_4m_bd0002...	0.000000	0	0.12311	0.167428	0.145912	0.098665	0.11505
27	hlv_d_holo_jk_certno_score_fpd30_v1	0.00


# In[536]:


varsname_base_v4.remove('score_fpd6_v1')
varsname_base_v4.remove('score_fpd10_v1')
varsname_base_v4.remove('hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard')
varsname_base_v4.remove('hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard')
varsname_base_v4.remove('hlv_d_holo_jk_certno_score_fpd30_v1')


# In[549]:



varsname_base_v4.remove('hlv_d_holo_certno_variablecode_standard_bd003')


# In[67]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[525]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_30['data_set'].value_counts()


# In[68]:




# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[550]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[69]:



# è®­ç»ƒæ•°æ®é›†
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[551]:



# è®­ç»ƒæ•°æ®é›†
X_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# ç¡®å®šå‚æ•°åï¼Œç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample_30.loc[X_train.index, 'data_set']='1_train'
df_sample_30.loc[X_test.index, 'data_set']='2_test'
print(df_sample_30['data_set'].value_counts())


# In[552]:



### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[553]:



# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# æœ€åˆè®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[72]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v4'].head()   


# In[554]:


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base2_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v4'].head()   


# In[73]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v4', 'apply_month')
df_ks_auc_month_base_v4['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v4 = pd.concat([tmp, df_ks_auc_month_base_v4], axis=1)
print(df_ks_auc_month_base_v4)


df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v4', 'data_set')
df_ks_auc_set_base_v4['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[304]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
df_ks_auc_month_base2_v4['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base2_v4 = pd.concat([tmp, df_ks_auc_month_base2_v4], axis=1)
print(df_ks_auc_month_base2_v4)


df_ks_auc_set_base2_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v4', 'data_set')
df_ks_auc_set_base2_v4['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v4 = pd.concat([tmp, df_ks_auc_set_base2_v4], axis=1)
print(df_ks_auc_set_base2_v4)


# In[74]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v4_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v4', 'apply_month')
    tmp1['æ¸ é“'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v4_type = pd.concat([df_ks_auc_month_base_v4_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base_v4_type)


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base_v4_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v4', 'apply_month')
    tmp1['æ¸ é“'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v4_rate = pd.concat([df_ks_auc_month_base_v4_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base_v4_rate)


# In[305]:



# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v4_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
    tmp1['æ¸ é“'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v4_type = pd.concat([df_ks_auc_month_base2_v4_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base2_v4_type)


# æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
df_ks_auc_month_base2_v4_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
    tmp1['æ¸ é“'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v4_rate = pd.concat([df_ks_auc_month_base2_v4_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base2_v4_rate)


# In[75]:


# åˆå¹¶
df_ks_auc_month_base_v4 = pd.concat([df_ks_auc_month_base_v4, df_ks_auc_month_base_v4_type, df_ks_auc_month_base_v4_rate])
df_ks_auc_month_base_v4.head()


# In[306]:


# åˆå¹¶
df_ks_auc_month_base2_v4 = pd.concat([df_ks_auc_month_base2_v4, df_ks_auc_month_base2_v4_type, df_ks_auc_month_base2_v4_rate])
df_ks_auc_month_base2_v4.head()


# In[555]:


df_ks_auc_month_base2_v4 = calculate_ks_auc(df_sample, modeltrian_target,target,'y_prob_base2_v4','apply_month')
df_ks_auc_month_base2_v4


# In[547]:


df_ks_auc_month_base2_v3


# In[544]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_base_v4 = feature_importance(lgb_model) 
df_importance_base_v4 = pd.merge(df_importance_base_v4, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base_v4 = df_importance_base_v4.reset_index()
df_importance_base_v4 = df_importance_base_v4.rename(columns={'index':'varsname'})
df_importance_base_v4


# In[545]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_base2_v4 = feature_importance(lgb_model) 
df_importance_base2_v4 = pd.merge(df_importance_base2_v4, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base2_v4 = df_importance_base2_v4.reset_index()
df_importance_base2_v4 = df_importance_base2_v4.rename(columns={'index':'varsname'})
df_importance_base2_v4


# In[546]:


# æ¨¡å‹ç›¸å…³æ€§
df_corr_base_v4 = df_corr_matrix.loc[varsname_base_v4, varsname_base_v4]
df_iv_base_v4 = df_iv_by_set.loc[varsname_base_v4,:]


# In[308]:


# æ¨¡å‹ç›¸å…³æ€§
df_corr_base2_v4 = df_corr_matrix.loc[varsname_base_v4, varsname_base_v4]
df_iv_base2_v4 = df_iv_by_set.loc[varsname_base_v4,:]


# In[78]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base_v4_{timestamp}.xlsx') as writer:
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v4')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v4')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v4')
    df_corr_base_v4.to_excel(writer, sheet_name='df_corr_base_v4')
    df_iv_base_v4.to_excel(writer, sheet_name='df_iv_base_v4')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base_v4_{timestamp}.xlsx')


# In[309]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v4_{timestamp}.xlsx') as writer:
    df_importance_base2_v4.to_excel(writer, sheet_name='df_importance_base2_v4')
    df_ks_auc_month_base2_v4.to_excel(writer, sheet_name='df_ks_auc_month_base2_v4')
    df_ks_auc_set_base2_v4.to_excel(writer, sheet_name='df_ks_auc_set_base2_v4')
    df_corr_base2_v4.to_excel(writer, sheet_name='df_corr_base2_v4')
    df_iv_base2_v4.to_excel(writer, sheet_name='df_iv_base2_v4')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v4_{timestamp}.xlsx')


# In[548]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v4_{timestamp}.xlsx') as writer:
    df_importance_base2_v4.to_excel(writer, sheet_name='df_importance_base2_v4')
    df_ks_auc_month_base2_v4.to_excel(writer, sheet_name='df_ks_auc_month_base2_v4')
    df_ks_auc_set_base2_v4.to_excel(writer, sheet_name='df_ks_auc_set_base2_v4')
    df_corr_base2_v4.to_excel(writer, sheet_name='df_corr_base2_v4')
    df_iv_base2_v4.to_excel(writer, sheet_name='df_iv_base2_v4')  

print(result_path + f'5_æ¨¡å‹ä¼˜åŒ–_{task_name}_base2_v4_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[310]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[311]:


# baseæ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base1_20250304142048.pkl')
print(lgb_model.feature_name()==varsname_base)
df_sample['y_prob_base'] = lgb_model.predict(df_sample[varsname_base],num_iteration=lgb_model.best_iteration)


# In[557]:


# base2æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_20250304144553.pkl')
print(lgb_model.feature_name()==varsname_base)
df_sample['y_prob_base2'] = lgb_model.predict(df_sample[lgb_model.feature_name()],num_iteration=lgb_model.best_iteration)


# In[313]:


# base_v1æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base_v1_20250304160800.pkl')
print(lgb_model.feature_name()==varsname_base_v1)
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[varsname_base_v1],num_iteration=lgb_model.best_iteration)


# In[314]:


df_sample.info(show_counts=True)


# In[92]:


df_sample.loc[df_sample[varsname_base_v4].isna().all(axis=1),:]


# In[95]:


df_sample.loc[df_sample[varsname_base].isna().all(axis=1),:]['apply_month'].value_counts()


# In[97]:


df_sample.loc[df_sample[varsname_base].isna().all(axis=1),:]['y_prob_base'].value_counts()


# In[99]:


df_sample_c = pd.read_csv('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411/è¡¥å……_å»ºæ¨¡æ•°æ®é›†_250305.csv')
df_sample_c.info()


# In[558]:


df_evalue = pd.merge(df_sample, df_sample_c,how='left',on='order_no')
df_evalue.info(show_counts=True)


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[572]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[562]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['t_br_fpd','t_br_mob4','t_br2_fpd','t_br2_mob4','t_beha3_fpd','t_beha3_mob4','t_gen_fpd',
               't_gen_mob4','t_gen3_fpd','t_gen3_mob4','t_mix_low_fpd','t_mix_low_mob4','t_mix_nopboc_fpd',
               't_mix_nopboc_mob4','hlv_d_holo_certno_variablecode_standard_bd003',
               'hlv_d_holo_jk_certno_fpd1_score','hlv_d_holo_jk_certno_varcode_standard_bd0004',
               'all_a_app_free_fpd30_202502_s']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[563]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p',
               'all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p',
               'ypy_bhxz_a_fpd30_v1_prob_good','xz_fpd']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[564]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['m1a0022_g_p','m1a0023_g_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)


df_ksauc_all_3 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[565]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd',
               'all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[566]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['score_fpd0_v1','score_fpd6_v1','score_fpd10_v1','score_fpd10_v2','score_fpd30_v1',
               't_mix_pboc2_dpd20','t_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[567]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[323]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[324]:


df_sample.to_csv(result_path + r'æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2409_2411.csv',index=False)


# ## 5.4 ç‰¹å¾å˜é‡è´¡çŒ®åº¦

# ### 5.4.1 ç‰¹å¾ç¼ºå¤±æ—¶ï¼ŒKSå€¼å˜åŒ–

# In[325]:


varsname_base_v3


# In[574]:


# base_v4æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****æ•°æ®é‡ï¼š{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='æ¡”å­å•†åŸ'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # æ‰¾åˆ°å½“å‰è¡Œçš„æœ€å°å€¼
        min_value = row.min()
        # æ‰¾å‡ºæ‰€æœ‰ç­‰äºæœ€å°å€¼çš„åˆ—å
        min_cols = row[row == min_value].index.tolist()
        # æ„å»ºå­—å…¸ï¼šåˆ—åä¸ºé”®ï¼Œæœ€å°å€¼ä¸ºå€¼
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# è¾“å‡ºç»“æœ
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx')


# ## 5.5 åœ¨æˆä¿¡åœºæ™¯çš„è¯„ä¼°

# In[223]:


df_sample_auth_dict={}


# In[232]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------ç¦»çº¿æ¨¡å‹å­åˆ†-----------------
--è´·ä¸­ç¦»çº¿å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--æˆä¿¡å…¨æ¸ é“è¡Œä¸ºç‰¹å¾æ¨¡å‹fpd1æ ‡ç­¾_æ ‡å‡†åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--è´·ä¸­æˆªé¢é£é™©dpd30_6mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--è´·ä¸­æç°é£é™©dpd30_4mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--è´·ä¸­è¡Œä¸ºæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- äººè¡Œç¦»çº¿å­åˆ†
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------ä¸‰æ–¹ç¼“å­˜æ•°æ®-----------------    
--è¿‘100å¤©ç¼“å­˜ä¸‰æ–¹è¯„åˆ†æ•°æ®
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------æ— æˆæœ¬æˆ–è€…ä½æˆæœ¬çš„å®æ—¶æ•°æ®-----------------   
--åŒ—äº¬å›¢é˜Ÿå­åˆ†
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--ç™¾èå­åˆ†
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--ç™¾èå­åˆ†v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--ç™¾èå­åˆ†v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--ç™¾èå­åˆ†v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--æˆä¿¡ç™¾è¡Œæ´è§fpd30æ ‡ç­¾202502_å¥½æ¦‚ç‡
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--ç»­ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--æˆä¿¡å…¨æ¸ é“æ— æˆæœ¬æ•°æ®èåˆæ¨¡å‹fp30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2æ¨¡å‹æ‰“åˆ† 
# lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1æ¨¡å‹æ‰“åˆ† 
lgb_model1= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[65]:


df_sample['apply_month'].value_counts()


# In[66]:


score = 'y_prob_base2'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample_30.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-08'"), 
                                                [score], 'apply_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v2'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result_v2'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[4]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[5]:


df_sample_dict = {}


# In[6]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[7]:


print(len(table_name_list))


# In[8]:


df_target = pd.read_csv('./result/00_model_target.csv')
df_target.info(show_counts=True)


# In[9]:


df_target['mob4dpd30'].value_counts()


# In[10]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv')
df_feature1.info()


# In[13]:


target = 'mob4dpd30'


# In[20]:


df_target['data_set'].value_counts()


# In[23]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv')
df1 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature1, how='left', on='order_no')
print(df1.shape)

df1_selected, df1_dropped = toad.selection.select(df1,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.85, 
                                                    return_drop=True, exclude=['order_no'])
print(df1_selected.shape)
df1_selected.drop(columns=['mob4dpd30'],inplace=True)
df1_drop_cols = []
for k, v in df1_dropped.items():
    print(k, ":", len(v))
    df1_drop_cols.extend(list(v))
df1_drop_cols = list(set(df1_drop_cols))
print(len(df1_drop_cols))


# In[17]:


df1_drop_cols = []
for k, v in df1_dropped.items():
    print(k, ":", len(v))
    df1_drop_cols.extend(list(v))
df1_drop_cols = list(set(df1_drop_cols))
print(len(df1_drop_cols))


# In[26]:


df_feature2 = pd.read_csv('./result/02_behave_features_v2.csv')
df2 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature2, how='left', on='order_no')
print(df2.shape)

df2_selected, df2_dropped = toad.selection.select(df2,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df2_selected.shape)
df2_selected.drop(columns=['mob4dpd30'],inplace=True)
df2_drop_cols = []
for k, v in df2_dropped.items():
    print(k, ":", len(v))
    df2_drop_cols.extend(list(v))
df2_drop_cols = list(set(df2_drop_cols))
print(len(df2_drop_cols))


# In[27]:


df_feature3 = pd.read_csv('./result/03_behave_features_v2.csv')
df3 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature3, how='left', on='order_no')
print(df3.shape)

df3_selected, df3_dropped = toad.selection.select(df3,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df3_selected.shape)
df3_selected.drop(columns=['mob4dpd30'],inplace=True)
df3_drop_cols = []
for k, v in df3_dropped.items():
    print(k, ":", len(v))
    df3_drop_cols.extend(list(v))
df3_drop_cols = list(set(df3_drop_cols))
print(len(df3_drop_cols))


# In[28]:


df_feature4 = pd.read_csv('./result/04_behave_features_v2.csv')
df4 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature4, how='left', on='order_no')
print(df4.shape)

df4_selected, df4_dropped = toad.selection.select(df4,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df4_selected.shape)
df4_selected.drop(columns=['mob4dpd30'],inplace=True)
df4_drop_cols = []
for k, v in df4_dropped.items():
    print(k, ":", len(v))
    df4_drop_cols.extend(list(v))
df4_drop_cols = list(set(df4_drop_cols))
print(len(df4_drop_cols))


# In[29]:


del df_feature1,df_feature2,df_feature3,df_feature4
gc.collect()


# In[32]:


usecols = list(set(df1_selected.columns) | set(df2_selected.columns) | set(df3_selected.columns) | set(df4_selected.columns))
print(usecols)


# In[33]:


usecols.remove('order_no')
print(len(usecols))


# In[34]:


df_feature5 = pd.read_csv('./result/05_behave_features_v3.csv')
df5 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature5, how='left', on='order_no')
print(df5.shape)

df5_selected, df5_dropped = toad.selection.select(df5,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df5_selected.shape)
df5_selected.drop(columns=['mob4dpd30'],inplace=True)
df5_drop_cols = []
for k, v in df5_dropped.items():
    print(k, ":", len(v))
    df5_drop_cols.extend(list(v))
df5_drop_cols = list(set(df5_drop_cols))
print(len(df5_drop_cols))


# In[46]:


usecols = usecols+df5_selected.columns.to_list()
usecols.remove('order_no')
print(len(usecols))
print(usecols)


# In[49]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv',usecols=df1_selected.columns.to_list())
df_feature1 = df_feature1.set_index('order_no')
print(df_feature1.shape)

df_feature2 = pd.read_csv('./result/02_behave_features_v2.csv',usecols=df2_selected.columns.to_list())
df_feature2 = df_feature2.set_index('order_no')
print(df_feature2.shape)

df_feature3 = pd.read_csv('./result/03_behave_features_v2.csv',usecols=df3_selected.columns.to_list())
df_feature3 = df_feature3.set_index('order_no')
print(df_feature3.shape)

df_feature4 = pd.read_csv('./result/04_behave_features_v2.csv',usecols=df4_selected.columns.to_list())
df_feature4 = df_feature4.set_index('order_no')
print(df_feature4.shape)

df_feature5 = pd.read_csv('./result/05_behave_features_v3.csv',usecols=df5_selected.columns.to_list())
df_feature5 = df_feature5.set_index('order_no')
print(df_feature5.shape)


# In[50]:


df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5],axis=1)
df_feature = df_feature.reset_index()
df_feature.shape


# In[51]:


df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
df_sample.info(show_counts=True)
df_sample.head()


# In[52]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[53]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[54]:


df_sample.loc[df_sample[usecols].isna().all(axis=1), :].shape


# In[55]:


df_sample_copy = df_sample.copy()


# In[56]:


df_sample.drop(index=df_sample[df_sample[usecols].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# In[59]:


df_sample[target].value_counts(normalize=True)


# In[60]:


df_sample[target].value_counts()


# In[61]:


varsname = df_sample.columns.to_list()[22:]

print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))

print(varsname[:5], varsname[-5:])


# In[63]:


df_sample.info(show_counts=True)


# In[64]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾_250325_notna.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾_250325_notna.csv')


# # 1. æ ·æœ¬æ¦‚å†µ

# In[65]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # è¿”å›ç»“æœ
    return result


# In[66]:


target = 'mob4dpd30'


# In[67]:


print(df_sample[target].value_counts())


# In[68]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[69]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[70]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[73]:


df_vars_list = pd.read_excel(r'è¡Œä¸ºç‰¹å¾å˜é‡æ¸…å•.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[74]:


[col for col in varsname if col in df_vars_list.query("è¡¨å=='znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n'")['name']]


# In[75]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# In[76]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[77]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[78]:


df_explor.head()


# In[79]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[80]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[81]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[82]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[83]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[84]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[88]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[df_miss_set['1_train']>=0.90].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[90]:


df_iv.loc[to_drop_iv,:]


# In[91]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1)


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[92]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[93]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[94]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(varsname_v2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[95]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[96]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[97]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[98]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[99]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}, éç©ºç®±å­ä¸ªæ•°ï¼š{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[100]:


new_bins_dict


# In[101]:


combiner.update(new_bins_dict)


# In[102]:


combiner.export()


# In[103]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[104]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[105]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[106]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)


# In[ ]:





# In[107]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[108]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[109]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[110]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname ä¸º ä½ ä¸‹è½½çš„å­—ä½“åº“è·¯å¾„ï¼Œæ³¨æ„ SourceHanSansSC-Bold.otf å­—ä½“çš„è·¯å¾„
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # ä½¿ç”¨æ›´æ·±çš„å¯¹è‰²ç›²å‹å¥½çš„é¢œè‰²
 color_train = '#004494'  # æ·±è“è‰²
 color_oot = '#D66100'    # æ·±æ©™è‰²

 # ç»˜åˆ¶æŸ±çŠ¶å›¾
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # æŸ±çŠ¶å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # æŠ˜çº¿å›¾ï¼Œä½¿ç”¨æ›´æ·±çš„é¢œè‰²å’Œæ ‡è®°
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # ä½¿ç”¨æ–¹å½¢æ ‡è®°
 ax2.set_ylabel('Bad Rate')

 # æŠ˜çº¿å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # æ·»åŠ IVå€¼
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # è°ƒæ•´å›¾ä¾‹ä½ç½®
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # è°ƒæ•´å›¾è¡¨å¸ƒå±€ï¼Œç»™é¡¶éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[111]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[112]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # è°ƒç”¨å‡½æ•°
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[115]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))

drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[118]:


df_psi_by_month.loc[drop_by_psi_month,:]


# In[120]:


df_iv_by_month.loc[drop_by_iv_month,:]


# In[123]:


to_drop3 = list(df_iv_by_month[df_iv_by_month["2024-11"]<0.01].index)


# In[124]:


to_drop3.remove('acccagel')
to_drop3.append('r06spll')
to_drop3.append('r02odsmr06odsmr')


# In[126]:


[col for col in varsname_v2 if col in list(df_vars_list.query("è¡¨å=='znzz_fintech_ads.dim_pub_user_fd_t1f_vars'")['name'])]


# In[127]:


print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[128]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[129]:


target


# In[130]:


df_bins.shape
df_bins.head()


# In[131]:



def calculate_woe(df, col, target):
    """
    è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
    :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
    :param binned_col: åˆ†ç®±å˜é‡å
    :param target_col: ç›®æ ‡å˜é‡å
    :return: WOEå€¼çš„å­—å…¸
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[132]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[133]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[134]:


df_sample_woe.head()


# In[135]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[136]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[137]:


df_corr_matrix.head()


# In[138]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[139]:


df_high_corr


# In[140]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[141]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[142]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[143]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[144]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[145]:


to_drop5


# In[146]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[147]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[148]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[149]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']


# In[150]:


df_sample['mob4dpd30'].value_counts()


# In[151]:


df_sample['mob4dpd30_1'].value_counts()


# In[152]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[153]:


df_sample['data_set'].value_counts()


# In[154]:


# # æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[155]:


df_sample['channel_types'].value_counts()


# In[156]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[157]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 0.60


# In[158]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[159]:


print(len(varsname_v5))
print(varsname_v5)


# In[160]:


varsname_base = varsname_v5[:]


# In[161]:


df_sample['data_set'].value_counts()


# In[162]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[163]:


df_sample['data_set'].value_counts()


# In[164]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[165]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[166]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[167]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[168]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[169]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1


# In[170]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# In[171]:


df_sample_copy.shape


# In[172]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_copy['y_prob_all'] = lgb_model.predict(df_sample_copy[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_copy['y_prob_all'].head()


# In[175]:


df_sample_copy['mob4dpd30_1'] = 1 - df_sample_copy['mob4dpd30']


# In[176]:


df_ks_auc_month_all = calculate_ks_auc(df_sample_copy, modeltrian_target, target, 'y_prob_all', 'apply_month')
df_ks_auc_month_all


# In[177]:


df_ks_auc_set_all = model_ks_auc(df_sample_copy, modeltrian_target, 'y_prob_all', 'data_set')
df_ks_auc_set_all['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_copy, target, 'data_set').set_index('bins')
df_ks_auc_set_all = pd.concat([tmp, df_ks_auc_set_all], axis=1)
df_ks_auc_set_all


# In[178]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_all_{timestamp}.xlsx') as writer:
    df_ks_auc_month_all.to_excel(writer, sheet_name='df_ks_auc_month_all')
    df_ks_auc_set_all.to_excel(writer, sheet_name='df_ks_auc_set_all')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_all_{timestamp}.xlsx')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")


# In[ ]:





# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[179]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[182]:


data_dict = {}


# In[186]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-16', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
    where dt = '2025-03-24'
      and apply_date= '{run_day}'
    ) as t 
------------------ç¦»çº¿æ¨¡å‹å­åˆ†-----------------
--è´·ä¸­æˆªé¢é£é™©dpd30_6mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--è´·ä¸­æç°é£é™©dpd30_4mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--è´·ä¸­è¡Œä¸ºæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

'''
    print(f'=========================={run_day}=============================')
    data_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[187]:


df_compare = pd.concat(data_dict.values(), ignore_index=True)
df_compare.to_csv(result_path + 'df_compare_data.csv')


# In[188]:


df_compare.info(show_counts=True)


# In[ ]:


df_compare.drop(columns=['hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard'],inplace=True)


# In[215]:


data_dict.keys()


# In[196]:


df_evalue = pd.merge(df_sample_copy, df_compare, how='left', on='order_no')
df_evalue.info(show_counts=True)


# In[197]:


df_evalue['fpd30_1'] = 1 - df_evalue['fpd30']
print(df_evalue['fpd30'].value_counts(),df_evalue['fpd30_1'].value_counts())


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[206]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[ ]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30', 'mob4dpd30']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

for label_ in target_list:
    label_1 = f'{label_}_'
    for score_ in score_list:
        df1 = calculate_ks_auc(tmp_df_evalue, label_1, label_, score_, 'apply_month')


# In[207]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list))
df_evalue[score_list].info(show_counts=True)


# In[208]:


# df_evalue['hlv_d_holo_certno_variablecode_standard_bd003'] = pd.to_numeric(df_evalue['hlv_d_holo_certno_variablecode_standard_bd003'])


# In[214]:


tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
print(tmp_df_evalue.shape)
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)


# In[209]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys1 = ['apply_month']
df_ksauc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_1 = pd.concat([df_ksauc_v1, df_ksauc_v2,df_ksauc_v4], axis=0)
df_ksauc_1


# In[210]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[211]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_1.to_excel(writer, sheet_name='df_ksauc_notna')
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[213]:


df_sample_copy.to_csv(result_path + r'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v2.csv',index=False)
print(result_path + r'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v2.csv')


# ## 5.4 ç‰¹å¾å˜é‡è´¡çŒ®åº¦

# ### 5.4.1 ç‰¹å¾ç¼ºå¤±æ—¶ï¼ŒKSå€¼å˜åŒ–

# In[325]:


varsname_base_v3


# In[574]:


# base_v4æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****æ•°æ®é‡ï¼š{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='æ¡”å­å•†åŸ'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # æ‰¾åˆ°å½“å‰è¡Œçš„æœ€å°å€¼
        min_value = row.min()
        # æ‰¾å‡ºæ‰€æœ‰ç­‰äºæœ€å°å€¼çš„åˆ—å
        min_cols = row[row == min_value].index.tolist()
        # æ„å»ºå­—å…¸ï¼šåˆ—åä¸ºé”®ï¼Œæœ€å°å€¼ä¸ºå€¼
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# è¾“å‡ºç»“æœ
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx')


# ## 5.5 åœ¨æˆä¿¡åœºæ™¯çš„è¯„ä¼°

# In[223]:


df_sample_auth_dict={}


# In[232]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------ç¦»çº¿æ¨¡å‹å­åˆ†-----------------
--è´·ä¸­ç¦»çº¿å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--æˆä¿¡å…¨æ¸ é“è¡Œä¸ºç‰¹å¾æ¨¡å‹fpd1æ ‡ç­¾_æ ‡å‡†åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--è´·ä¸­æˆªé¢é£é™©dpd30_6mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--è´·ä¸­æç°é£é™©dpd30_4mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--è´·ä¸­è¡Œä¸ºæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- äººè¡Œç¦»çº¿å­åˆ†
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------ä¸‰æ–¹ç¼“å­˜æ•°æ®-----------------    
--è¿‘100å¤©ç¼“å­˜ä¸‰æ–¹è¯„åˆ†æ•°æ®
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------æ— æˆæœ¬æˆ–è€…ä½æˆæœ¬çš„å®æ—¶æ•°æ®-----------------   
--åŒ—äº¬å›¢é˜Ÿå­åˆ†
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--ç™¾èå­åˆ†
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--ç™¾èå­åˆ†v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--ç™¾èå­åˆ†v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--ç™¾èå­åˆ†v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--æˆä¿¡ç™¾è¡Œæ´è§fpd30æ ‡ç­¾202502_å¥½æ¦‚ç‡
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--ç»­ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--æˆä¿¡å…¨æ¸ é“æ— æˆæœ¬æ•°æ®èåˆæ¨¡å‹fp30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2æ¨¡å‹æ‰“åˆ† 
# lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1æ¨¡å‹æ‰“åˆ† 
lgb_model1= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[216]:


df_sample['apply_month'].value_counts()


# In[218]:


score = 'y_prob_v1'
flag = 'mob4dpd30_1'


# In[219]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, flag]], y=flag, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[220]:


df_sample['score_bins'].head()


# In[221]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[222]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[223]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[224]:


score_group_by_dataset.head()


# In[225]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v3.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


# df_sample_dict = {}


# In[5]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[6]:


print(len(table_name_list))


# In[7]:


# df_target = pd.read_csv(result_path + '00_model_target.csv')
# df_target.info()


# In[8]:


# df_feature1 = pd.read_csv(result_path + '01_behave_features.csv')
# df_feature1 = df_feature1.set_index('order_no')

# df_feature2 = pd.read_csv(result_path + '02_behave_features.csv')
# df_feature2 = df_feature2.set_index('order_no')

# df_feature3 = pd.read_csv(result_path + '03_behave_features.csv')
# df_feature3 = df_feature3.set_index('order_no')

# df_feature4 = pd.read_csv(result_path + '04_behave_features.csv')
# df_feature4 = df_feature4.set_index('order_no')

# df_feature5 = pd.read_csv(result_path + '05_behave_features.csv')
# df_feature5 = df_feature5.set_index('order_no')

# df_feature6 = pd.read_csv(result_path + '06_behave_features.csv')
# df_feature6 = df_feature6.set_index('order_no')


# In[9]:


# df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5,df_feature6],axis=1)
# df_feature = df_feature.reset_index()
# df_feature.shape


# In[10]:


# df_target = df_target.reset_index()
# df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[11]:


df_sample = pd.read_csv(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[12]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[13]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[14]:


varsname = df_sample.columns.to_list()[22:]
varsname = list(filter(lambda col: not col.startswith('uti'), varsname))

print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))

print(varsname[:5], varsname[-5:])


# In[15]:


pd.set_option('display.max_columns',None)


# In[16]:


# df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.csv',index=False)
# print(result_path + 'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾.csv')


# In[17]:


df_sample[varsname].info()


# In[18]:


df_sample.loc[df_sample[varsname].isna().all(axis=1), :].shape


# In[19]:


df_sample_copy = df_sample.copy()


# In[20]:


df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# # 1. æ ·æœ¬æ¦‚å†µ

# In[21]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # è®¡ç®—æ•´ä¸ª DataFrame çš„èšåˆç»Ÿè®¡é‡
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # è¿”å›ç»“æœ
    return result


# In[22]:


target = 'mob4dpd30'


# In[23]:


print(df_sample[target].value_counts())


# In[24]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[25]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[26]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[27]:


df_vars_list = pd.read_excel('è¡Œä¸ºç‰¹å¾å˜é‡æ¸…å•.xlsx')
df_vars_list.drop(columns=['type'],inplace=True)


# In[28]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# In[29]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[30]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[31]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[32]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[33]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[34]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[35]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢ç»Ÿè®¡åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[37]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[df_miss_set['1_train']>=0.90].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[38]:


df_iv.loc[to_drop_iv,:].head()


# In[39]:


# varsname_all = [col for col in varsname if col not in  [col for col in varsname if f'{col}'.startswith('uti')]]


# In[40]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1)


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[42]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[43]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[44]:


df_vars_list[df_vars_list['name'].isin(to_drop2)]


# In[45]:


to_drop2=[]


# In[46]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(varsname_v2)


# In[47]:


df_sample[df_sample[varsname_v2].isna().all(axis=1)].shape


# In[48]:


df_sample.drop(index=df_sample[df_sample[varsname_v2].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[49]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[50]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[51]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[52]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[53]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}, éç©ºç®±å­ä¸ªæ•°ï¼š{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[54]:


new_bins_dict


# In[55]:


combiner.update(new_bins_dict)


# In[56]:


combiner.export()


# In[57]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[58]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[59]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[60]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')


# In[ ]:





# In[61]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[62]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[63]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[64]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname ä¸º ä½ ä¸‹è½½çš„å­—ä½“åº“è·¯å¾„ï¼Œæ³¨æ„ SourceHanSansSC-Bold.otf å­—ä½“çš„è·¯å¾„
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # ä½¿ç”¨æ›´æ·±çš„å¯¹è‰²ç›²å‹å¥½çš„é¢œè‰²
 color_train = '#004494'  # æ·±è“è‰²
 color_oot = '#D66100'    # æ·±æ©™è‰²

 # ç»˜åˆ¶æŸ±çŠ¶å›¾
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # æŸ±çŠ¶å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # æŠ˜çº¿å›¾ï¼Œä½¿ç”¨æ›´æ·±çš„é¢œè‰²å’Œæ ‡è®°
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # ä½¿ç”¨æ–¹å½¢æ ‡è®°
 ax2.set_ylabel('Bad Rate')

 # æŠ˜çº¿å›¾æ•°æ®æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²è®¾ä¸ºé»‘è‰²
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # æ·»åŠ IVå€¼
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # è°ƒæ•´å›¾ä¾‹ä½ç½®
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # è°ƒæ•´å›¾è¡¨å¸ƒå±€ï¼Œç»™é¡¶éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[65]:


# df_vars_list = pd.read_excel(r'è¡Œä¸ºç‰¹å¾å˜é‡æ¸…å•.xlsx')
# df_vars_list.drop(columns=['type'],inplace=True)
# df_vars_list.info()
# df_vars_list.head(2)


# In[66]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[67]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # è°ƒç”¨å‡½æ•°
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[68]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))

drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[69]:


df_psi_by_month.loc[drop_by_psi_month,:]


# In[70]:


df_iv_by_month.loc[drop_by_iv_month,:]


# In[71]:


to_drop3 = list(df_iv_by_month[df_iv_by_month["2024-11"]<0.01].index)


# In[72]:


to_drop3.remove('acccagel')


# In[75]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[76]:


target


# In[77]:


df_bins.shape
df_bins.head()


# In[78]:



def calculate_woe(df, col, target):
    """
    è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
    :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
    :param binned_col: åˆ†ç®±å˜é‡å
    :param target_col: ç›®æ ‡å˜é‡å
    :return: WOEå€¼çš„å­—å…¸
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[79]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[80]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[81]:


df_sample_woe[varsname_v3].head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[118]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[119]:


df_corr_matrix.head()


# In[120]:


table_drop_ = ['znzz_fintech_ads.dim_pub_user_fd_t1f_vars','znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n']
table_drop_ = list(df_vars_list.query("è¡¨å in @table_drop_")['name'])


# In[121]:


len(table_drop_)


# In[122]:


table_drop = [col for col in df_corr_matrix.index if col in  table_drop_]
table_drop


# In[123]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[124]:


df_high_corr


# In[125]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[126]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[127]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[128]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[129]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[130]:


to_drop5 


# In[131]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[132]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[133]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[134]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']
df_sample['mob4dpd30'].value_counts()


# In[135]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[136]:


df_sample['data_set'].value_counts()


# In[137]:


# # æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[138]:


df_sample['channel_types'].value_counts()


# In[139]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[140]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 0.60


# In[141]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[142]:


print(len(varsname_v5))
print(varsname_v5)


# In[143]:


varsname_base = varsname_v5[:]


# In[144]:


df_sample['data_set'].value_counts()


# In[145]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[146]:


df_sample['data_set'].value_counts()


# In[392]:


# 2 å®šä¹‰è¶…å‚ç©ºé—´
# hp.quniform("å‚æ•°åç§°",ä¸‹ç•Œ,ä¸Šç•Œ,æ­¥é•¿)-é€‚ç”¨äºç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æµ®ç‚¹ç‚¹æ•°
# hp.uniform("å‚æ•°åç§°",ä¸‹ç•Œ, ä¸‹ç•Œ)-é€‚ç”¨äºè¿ç»­éšæœºåˆ†å¸ƒçš„æµ®ç‚¹æ•°
# hp.randint("å‚æ•°åç§°",ä¸Šç•Œ)-é€‚ç”¨äº[0,ä¸Šç•Œ)çš„æ•´æ•°,åŒºé—´ä¸ºå·¦é—­å³å¼€
# hp.choice("å‚æ•°åç§°",["å­—ç¬¦ä¸²1","å­—ç¬¦ä¸²2",...])-é€‚ç”¨äºå­—ç¬¦ä¸²ç±»å‹,æœ€ä¼˜å‚æ•°ç”±ç´¢å¼•è¡¨ç¤º
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# å¯ä»¥æ ¹æ®éœ€è¦ï¼Œæ³¨é‡Šæ‰ååçš„ä¸€äº›ä¸å¤ªé‡è¦çš„è¶…å‚

spaces = {
          # general parameters
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,5,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.6,1.0),
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.0,1.0),
          "lambda_l1": 0,
          "lambda_l2": 300,
          "early_stopping_rounds": 50
          }


# In[396]:


# 3ï¼Œæ‰§è¡Œè¶…å‚æœç´¢
# æœ‰äº†ç›®æ ‡å‡½æ•°å’Œå‚æ•°ç©ºé—´,æ¥ä¸‹æ¥è¦è¿›è¡Œä¼˜åŒ–,éœ€è¦äº†è§£ä»¥ä¸‹å‚æ•°:
# fmin:è‡ªå®šä¹‰ä½¿ç”¨çš„ä»£ç†æ¨¡å‹(å‚æ•°algo),hyperoptæ”¯æŒå¦‚ä¸‹æœç´¢ç®—æ³•ï¼š
#       éšæœºæœç´¢(hyperopt.rand.suggest)
#       æ¨¡æ‹Ÿé€€ç«(hyperopt.anneal.suggest)
#       TPEç®—æ³•ï¼ˆhyperopt.tpe.suggestï¼Œç®—æ³•å…¨ç§°ä¸ºTree-structured Parzen Estimator Approachï¼‰
# partial:ä¿®æ”¹ç®—æ³•æ¶‰åŠåˆ°çš„å…·ä½“å‚æ•°,åŒ…æ‹¬æ¨¡å‹å…·ä½“ä½¿ç”¨äº†å¤šå°‘å°‘ä¸ªåˆå§‹è§‚æµ‹å€¼(å‚æ•°n_start_jobs),
#         ä»¥åŠåœ¨è®¡ç®—é‡‡é›†å‡½æ•°å€¼æ—¶ç©¶ç«Ÿè€ƒè™‘å¤šå°‘ä¸ªæ ·æœ¬(å‚æ•°n_EI_candidates)
# trials:è®°å½•æ•´ä¸ªè¿­ä»£è¿‡ç¨‹,ä»hyperoptåº“ä¸­å¯¼å…¥çš„æ–¹æ³•Trials(),ä¼˜åŒ–å®Œæˆä¹‹å,
#        å¯ä»¥ä»ä¿å­˜å¥½çš„trialsä¸­æŸ¥çœ‹æŸå¤±ã€å‚æ•°ç­‰å„ç§ä¸­é—´ä¿¡æ¯
# early_stop_fn:æå‰åœæ­¢å‚æ•°,ä»hyperoptåº“å¯¼å…¥çš„æ–¹æ³•no_progresss_loss(),å¯ä»¥è¾“å…¥å…·ä½“çš„æ•°å­—n,
#               è¡¨ç¤ºå½“æŸå¤±è¿ç»­næ¬¡æ²¡æœ‰ä¸‹é™æ—¶,è®©ç®—æ³•æå‰åœæ­¢
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=100):
    """
    è´å¶æ–¯è°ƒå‚, ç¡®å®šå…¶ä»–å‚æ•°
    """
    
    # 1 å®šä¹‰ç›®æ ‡å‡½æ•°
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """å®šä¹‰ç›®æ ‡å‡½æ•°"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #ä¿å­˜è¿­ä»£è¿‡ç¨‹
    trials = Trials()
    #è®¾ç½®æå‰åœæ­¢
    early_stop_fn = no_progress_loss(50)
    #å®šä¹‰ä»£ç†æ¨¡å‹
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #ç›®æ ‡å‡½æ•°
                      ,space=param_spaces  #å‚æ•°ç©ºé—´
                      ,algo = tpe.suggest  #ä»£ç†æ¨¡å‹
                      ,max_evals=max_evals #å…è®¸çš„è¿­ä»£æ¬¡æ•°
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    best_params['boosting'] = 'gbdt'
    best_params['objective'] = 'binary'
    best_params['metric'] = 'auc'
    best_params['num_leaves'] = int(best_params['num_leaves'])
    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])
    best_params['max_depth'] = int(best_params['max_depth'])
    best_params['bagging_freq'] = 1 
    best_params['early_stopping_rounds'] = 50
    best_params['scale_pos_weight'] = 1 
    best_params['seed'] = 1 
    print("æœ€ä¼˜å‚æ•°", best_params)
    
    return (best_params, trials)


# In[415]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=10)


# In[116]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(best_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[147]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[148]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[149]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[150]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[151]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[152]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[153]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# In[154]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample_copy['y_prob_all'] = lgb_model.predict(df_sample_copy[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_copy['y_prob_all'].head()


# In[156]:


df_sample_copy['mob4dpd30_1'] = 1 - df_sample_copy['mob4dpd30']
print(df_sample_copy['mob4dpd30'].value_counts(), df_sample_copy['mob4dpd30_1'].value_counts())


# In[157]:


df_ks_auc_month_all = calculate_ks_auc(df_sample_copy, modeltrian_target, target, 'y_prob_all', 'apply_month')
df_ks_auc_month_all


# In[158]:


df_ks_auc_set_all = model_ks_auc(df_sample_copy, modeltrian_target, 'y_prob_all', 'data_set')
df_ks_auc_set_all['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample_copy, target, 'data_set').set_index('bins')
df_ks_auc_set_all = pd.concat([tmp, df_ks_auc_set_all], axis=1)
df_ks_auc_set_all


# In[159]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_all_{timestamp}.xlsx') as writer:
    df_ks_auc_month_all.to_excel(writer, sheet_name='df_ks_auc_month_all')
    df_ks_auc_set_all.to_excel(writer, sheet_name='df_ks_auc_set_all')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_all_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[160]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[162]:


df_compare = pd.read_csv('./result_v2/df_compare_data.csv')
df_compare.info(show_counts=True)


# In[163]:


df_compare.drop(columns=['Unnamed: 0','hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard'],inplace=True)


# In[164]:


df_evalue = pd.merge(df_sample_copy, df_compare, how='left', on='order_no')
df_evalue.info(show_counts=True)


# In[165]:


df_evalue['fpd30_1'] = 1 - df_evalue['fpd30']


# In[166]:


print(df_evalue['fpd30'].value_counts(),df_evalue['fpd30_1'].value_counts())


# In[167]:


print(df_evalue['mob4dpd30'].value_counts(),df_evalue['mob4dpd30_1'].value_counts())


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[168]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[169]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list))
df_evalue[score_list].info(show_counts=True)


# In[170]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
print(tmp_df_evalue.shape)
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)

groupkeys1 = ['apply_month']
df_ksauc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_1 = pd.concat([df_ksauc_v1, df_ksauc_v2,df_ksauc_v4], axis=0)
df_ksauc_1


# In[171]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[172]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_1.to_excel(writer, sheet_name='df_ksauc_notna')
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[173]:


df_sample_copy.to_csv(result_path + r'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v3.csv',index=False)


# ## 5.4 ç‰¹å¾å˜é‡è´¡çŒ®åº¦

# ### 5.4.1 ç‰¹å¾ç¼ºå¤±æ—¶ï¼ŒKSå€¼å˜åŒ–

# In[325]:


varsname_base_v3


# In[574]:


# base_v4æ¨¡å‹æ‰“åˆ† 
lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****æ•°æ®é‡ï¼š{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='æ¡”å­å•†åŸ'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='å…¨æ¸ é“')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # æ‰¾åˆ°å½“å‰è¡Œçš„æœ€å°å€¼
        min_value = row.min()
        # æ‰¾å‡ºæ‰€æœ‰ç­‰äºæœ€å°å€¼çš„åˆ—å
        min_cols = row[row == min_value].index.tolist()
        # æ„å»ºå­—å…¸ï¼šåˆ—åä¸ºé”®ï¼Œæœ€å°å€¼ä¸ºå€¼
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# è¾“å‡ºç»“æœ
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_ç‰¹å¾å˜é‡è´¡çŒ®åº¦_ç‰¹å¾ç¼ºå¤±æ—¶.xlsx')


# ## 5.5 åœ¨æˆä¿¡åœºæ™¯çš„è¯„ä¼°

# In[223]:


df_sample_auth_dict={}


# In[232]:



# è®¡ç®—ä»Šå¤©çš„æ—¶é—´
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------ç¦»çº¿æ¨¡å‹å­åˆ†-----------------
--è´·ä¸­ç¦»çº¿å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7ç¦»çº¿å­åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--æˆä¿¡å…¨æ¸ é“è¡Œä¸ºç‰¹å¾æ¨¡å‹fpd1æ ‡ç­¾_æ ‡å‡†åˆ†
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--è´·ä¸­æˆªé¢é£é™©dpd30_6mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--è´·ä¸­æç°é£é™©dpd30_4mæ¨¡å‹
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--è´·ä¸­è¡Œä¸ºæ¨¡å‹fpd30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- äººè¡Œç¦»çº¿å­åˆ†
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------ä¸‰æ–¹ç¼“å­˜æ•°æ®-----------------    
--è¿‘100å¤©ç¼“å­˜ä¸‰æ–¹è¯„åˆ†æ•°æ®
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------æ— æˆæœ¬æˆ–è€…ä½æˆæœ¬çš„å®æ—¶æ•°æ®-----------------   
--åŒ—äº¬å›¢é˜Ÿå­åˆ†
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--ç™¾èå­åˆ†
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--ç™¾èå­åˆ†v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--ç™¾èå­åˆ†v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--ç™¾èå­åˆ†v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--æ´ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--æˆä¿¡ç™¾è¡Œæ´è§fpd30æ ‡ç­¾202502_å¥½æ¦‚ç‡
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--ç»­ä¾¦å­åˆ†
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--æˆä¿¡å…¨æ¸ é“æ— æˆæœ¬æ•°æ®èåˆæ¨¡å‹fp30æ ‡ç­¾_åˆ†æ•°
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2æ¨¡å‹æ‰“åˆ† 
# lgb_model= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1æ¨¡å‹æ‰“åˆ† 
lgb_model1= load_model_from_pkl('./result/06_æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_æˆä¿¡åœºæ™¯è¯„ä¼°_{task_name}_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[174]:


df_sample['apply_month'].value_counts()


# In[177]:


score = 'y_prob_v1'
flag = 'mob4dpd30_1'


# In[178]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, flag]], y=flag, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)
df_sample['score_bins'].head()


# In[179]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[180]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[181]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
score_group_by_dataset.head()


# In[182]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[183]:



df_sample_copy['score_bins'] = c.transform(df_sample_copy['y_prob_all'], labels=True)
df_sample_copy['score_bins'].head()


# In[184]:


df_sample_copy.to_csv(result_path + r'æˆä¿¡å…¨æ¸ åˆ°è¡Œä¸ºæ•°æ®æ¨¡å‹å››æœŸæ ‡ç­¾v3.csv',index=False)




#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”Ÿmob6dpd30ç¦»çº¿æ¨¡å‹'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[4]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[5]:


sql = """
select * from znzz_fintech_ads.lxl_tmp_pboc_model_sample
"""
df_sample_ = get_data(sql)


# In[6]:


df_sample_.info(show_counts=True)
df_sample_.head()


# In[7]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[8]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[9]:


df_sample_['order_no'].value_counts()


# In[10]:


df_sample_.drop_duplicates(subset=None,inplace=True)


# In[12]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[13]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:10], varsname[-10:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[15]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col],errors='coerce')


# In[17]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[ ]:





# In[18]:


df_sample = df_sample_.query("target_mob6dpd30>=0").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[19]:


df_sample.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[20]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-17'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-11-18' & apply_date<='2024-11-30'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-17'").index, 'data_set']='3_oot2'


# In[ ]:





# In[21]:


target = 'target_mob6dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[22]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[23]:


print(df_sample[target].value_counts())


# In[24]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[25]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[26]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# In[27]:


df_sample.dropna(how='all',axis=1,inplace=True)
df_sample.shape


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[28]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[ ]:





# In[29]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[30]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[31]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[32]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[33]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[34]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[35]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[ ]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(to_drop2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[37]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[38]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[40]:


varsname_v2 = varsname[:]


# In[41]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[42]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[44]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}, {len(existing_bins_dict[col])}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[45]:


new_bins_dict


# In[46]:


combiner.load(new_bins_dict)


# In[ ]:


combiner.export()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[48]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[49]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[ ]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[300]:


varsname_v2 = varsname_base_v2[:]


# In[314]:


df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")

# df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
# print("-------")


# In[316]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[315]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[317]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24åˆ©ç‡'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[56]:


target


# In[57]:


df_sample[target].value_counts()


# In[58]:


modeltrian_target = 'target_mob6dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[ ]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[134]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[135]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[136]:


varsname_base = varsname[:]
print(len(varsname_base))
# print(varsname_base)


# In[137]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[138]:


df_sample['data_set'].value_counts()


# In[139]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[140]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[141]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[142]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[143]:


df_vars_desc = pd.read_excel('äººè¡Œå˜é‡æ¸…å•2506.xlsx',sheet_name='ç¦»çº¿å˜é‡æ¸…å•')
df_vars_desc.info(show_counts=True)


# In[144]:


df_vars_desc.rename(columns={'var':'feature'},inplace=True)
df_vars_desc['feature']=df_vars_desc['feature'].str.lower()

df_vars_desc = df_vars_desc[['feature','comment']]


# In[145]:


df_iv_psi_miss = pd.concat([df_iv_by_month, df_psi_by_month, df_explor.loc[:,'missing']], axis=1)
df_iv_psi_miss.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + ['total_na']
df_iv_psi_miss.head()


# In[146]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_desc, df_importance_month_v1, how='right',on='feature')
df_importance_month_v1


# In[148]:


# df_iv_by_set.columns = [f'{col}_iv' for col in df_iv_by_set.columns]
# df_psi_by_set.columns = [f'{col}_psi' for col in df_psi_by_set.columns]
# df_miss = df_explor.loc[:,'missing']

# df_set_iv_psi_miss = pd.concat([df_iv_by_set, df_psi_by_set, df_miss], axis=1)
df_set_iv_psi_miss.columns=['1_train_iv','3_oot1_iv','3_oot2_iv','1_train_psi','3_oot1_psi','3_oot2_psi','missing']
df_set_iv_psi_miss.head()


# In[149]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = pd.merge(df_vars_desc, df_importance_set_v1, how='right',on='feature')
df_importance_set_v1


# In[150]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# In[88]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 ç‰¹å¾å˜é‡ä¼˜åŒ–1

# In[151]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[152]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[92]:


# df_corr_matric_person = df_sample[varsname_base].corr()
# df_corr_matric_person.to_csv('df_corr.csv')


# In[ ]:


df_corr_matric_person = df_sample[varsname_base].corr()

# è°ƒç”¨å‡½æ•°
df_corr_drop, to_drop_vars = find_high_correlation_pairs(df_corr_matric_person,
                                                     df_iv_by_month['2025-03'],
                                                     threshold=0.75)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop_vars))


# In[ ]:


print(to_drop_vars)


# In[153]:


varsname_base_v2 = df_importance_month_v1[df_importance_month_v1['gain']>0]['feature'].to_list()
print(len(varsname_base_v2))
# print(varsname_base_v2)


# In[154]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[155]:


df_sample['data_set'].value_counts()


# In[156]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[157]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v2'].head()


# In[158]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'apply_month')
df_ks_auc_month_v2


# In[159]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'data_set')
df_ks_auc_set_v2


# In[160]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = pd.merge(df_vars_desc, df_importance_month_v2, how='right',on='feature')
df_importance_month_v2


# In[161]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = pd.merge(df_vars_desc, df_importance_set_v2, how='right',on='feature')
df_importance_set_v2


# In[162]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–2

# In[273]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[191]:


# varsname_base_v3 = df_importance_month_v2[df_importance_month_v2['gain']>0]['feature'].to_list()
# print(len(varsname_base_v3))


# In[259]:


varsname_base_v3 = df_importance_set_v3[df_importance_set_v3['gain']>15]['feature'].to_list()
print(len(varsname_base_v3))


# In[274]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )

print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[275]:


df_sample['data_set'].value_counts()


# In[276]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[277]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v4'].head()


# In[278]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v4', 'apply_month')
df_ks_auc_month_v3


# In[279]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v4', 'data_set')
df_ks_auc_set_v3


# In[280]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = pd.merge(df_vars_desc, df_importance_month_v3, how='right',on='feature')
df_importance_month_v3


# In[281]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
df_importance_set_v3 = feature_importance(lgb_model) 
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = pd.merge(df_vars_desc, df_importance_set_v3, how='right',on='feature')
df_importance_set_v3


# In[282]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx')


# In[205]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–3

# In[303]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[304]:


df_vars_desc = pd.read_excel('äººè¡Œå˜é‡æ¸…å•2506.xlsx',sheet_name='ç¦»çº¿å˜é‡æ¸…å•')
df_vars_desc.rename(columns={'var':'feature'},inplace=True)
df_vars_desc['feature']=df_vars_desc['feature'].str.lower()
df_vars_desc.info(show_counts=True)

# df_vars_desc = df_vars_desc[['feature','comment']]


# In[305]:


to_drop_cols4 = df_vars_desc[df_vars_desc['Source']=='äººè¡Œå®æ—¶è°ƒç”¨å˜é‡']['feature'].to_list()
print(len(to_drop_cols4))


# In[307]:


varsname_base_v4 = [col for col in varsname if col not in to_drop_cols4]
print(len(varsname_base_v4))
print(varsname_base_v4) 


# In[308]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )

print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'

df_sample['data_set'].value_counts()


# In[309]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[310]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_pred_v5'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v5'].head()


# In[311]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v5', 'apply_month')
df_ks_auc_month_v4


# In[312]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v5', 'data_set')
df_ks_auc_set_v4


# In[313]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v4 = pd.merge(df_importance_month_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v4 = pd.merge(df_vars_list, df_importance_month_v4, how='right',left_on='name',right_on='feature')
df_importance_month_v4


# In[ ]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v4 = pd.merge(df_importance_set_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v4 = pd.merge(df_vars_list, df_importance_set_v4, how='right',left_on='name',right_on='feature')
df_importance_set_v4


# In[ ]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx')


# In[ ]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v5_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v5_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v5')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v5')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v5')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v5')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v5_{timestamp}.xlsx')


# In[ ]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx')


# In[ ]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[283]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[284]:


target


# In[285]:


usecols = ['order_no','channel_id','apply_date','apply_month', 'data_set', 'target_mob6dpd30', 'target_mob6dpd30_1','channel_types', 'channel_rates', 'y_pred_v1', 'y_pred_v2', 'y_pred_v3','y_pred_v4']
print(len(usecols))


# In[286]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[287]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)       
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[288]:


score_list = ['y_pred_v1','y_pred_v2','y_pred_v3','y_pred_v4']
print(len(score_list))
print(score_list)

target_list = ['target_mob6dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all1


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2


# In[318]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_1_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='allchannel')
#     df_ksauc_all2.to_excel(writer, sheet_name='pboc')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_1_{timestamp}.xlsx')


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[ ]:



# df_evalue_fpd30 = df_evalue.query("target_mob4dpd30>=0")
# df_evalue_fpd30.info(show_counts=True)


# In[ ]:


df_evalue_fpd30['target_mob4dpd30_1'] = 1 - df_evalue_fpd30['target_mob4dpd30']
df_evalue_fpd30['target_mob4dpd30_1'].value_counts()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_3.head()


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_all_2.to_excel(writer, sheet_name='fpd30_èåˆ')
    df_ksauc_all_3.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
    df_ksauc_all_4.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='mob4_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# ##### è°ƒç”¨å¾ä¿¡çš„æ¸ é“

# In[ ]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_month_pboc


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[ ]:


df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc_fpd30.info(show_counts=True)


# In[ ]:


model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_2.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_3.head()


# In[ ]:





# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_fpd30_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_pd30_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_fpd30_ç¦»çº¿')
#     df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
#     df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
#     df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx')


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[289]:


df_sample['apply_month'].value_counts()


# In[326]:


score = 'y_pred_v4'


# In[327]:


c = toad.transform.Combiner()
c.fit(df_sample[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[328]:


df_sample['score_bins'].head()


# In[329]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[330]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[331]:


df_psi_matrix = df_psi_matrix.loc['1_train',:]
df_psi_matrix


# In[332]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[333]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[298]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[299]:


df_evalue.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“äººè¡Œè¡ç”ŸM6D30ç¦»çº¿æ¨¡å‹_2409_2412.csv')


# In[ ]:







#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[ ]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[ ]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--èåˆæ¨¡å‹
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- ä¸‰æ–¹æ•°æ®å­åˆ†
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--è¡Œä¸ºæ¨¡å‹å­åˆ†
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    -- è¡Œä¸ºæ¨¡å‹å­åˆ†
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- å­åˆ†å®æ—¶æ¨¡å‹
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- æˆä¿¡èåˆæ¨¡å‹
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------ä¸‰æ–¹æ•°æ®-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[3]:


df_sample_1 = pd.read_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv')
df_merge_off= pd.read_csv(result_path + 'df_off_merge2.csv')
df_sample_ = pd.merge(df_sample_1, df_merge_off, how='left', on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[4]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[5]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[ ]:


# df_sample_.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv',index=False)
# print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv')


# In[10]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
df_sample_.drop(columns=drop_cols, inplace=True)
df_sample_.shape


# In[11]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:10], varsname[-10:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[12]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[13]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[ ]:





# In[15]:


df_sample = df_sample_.query("target_fpd30>=0 & channel_id > 1 & apply_date>='2024-10-01'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[18]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2025-02-29'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-04-02'").index, 'data_set']='3_oot2'


# In[19]:


df_sample.to_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fd30èåˆæ¨¡å‹_2411_2502.csv',index=False)
print(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fd30èåˆæ¨¡å‹_2411_2502.csv')


# In[22]:


target = 'target_fpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[23]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[24]:


print(df_sample[target].value_counts())


# In[25]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[26]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[27]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# In[29]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[31]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# In[35]:


df_m1a0044 = pd.read_csv(result_path + 'df_m1a0044.csv')
df_m1a0044.info(show_counts=True)


# In[36]:


df_sample = pd.merge(df_sample, df_m1a0044, how='left',on='order_no')
df_sample.shape


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[37]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[38]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[39]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[40]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[41]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[42]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[43]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[44]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[45]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[46]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[48]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[49]:


print(len(to_drop_iv))
to_drop_iv


# In[50]:


print(len(to_drop_missing))
to_drop_missing


# In[51]:


df_iv.loc[to_drop_iv,:]


# In[52]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[53]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[54]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[55]:


df_iv.loc[c,:]


# In[56]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")
print(to_drop2)


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[57]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[58]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[59]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[60]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[61]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[62]:


new_bins_dict


# In[63]:


combiner.load(new_bins_dict)


# In[64]:


combiner.export()


# In[65]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[66]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[67]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[68]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[69]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[70]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[71]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[89]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[90]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[91]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[92]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[93]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[77]:


target


# In[78]:


df_bins.shape
df_bins.head()


# In[79]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[80]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[81]:


df_sample_woe.head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[83]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[84]:


df_corr_matrix.head()


# In[85]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[86]:


df_high_corr


# In[87]:


print(to_drop4)


# In[94]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[95]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[96]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[97]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[98]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[99]:


print(to_drop5)


# In[100]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[101]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[208]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24åˆ©ç‡'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[209]:


df_sample = pd.read_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_report.csv')
df_sample = df_sample.query("apply_date<'2025-04-01'").reset_index(drop=True)


# In[210]:


target


# In[211]:


modeltrian_target = 'target_fpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[212]:


df_sample[target].value_counts()


# In[213]:


df_sample[modeltrian_target].value_counts()


# In[214]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[215]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[216]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[217]:


df_sample['channel_types'].value_counts()


# In[218]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[219]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[220]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[221]:


print(len(varsname_v5))
print(varsname_v5)


# In[222]:


to_drop_all1 = ['feicuifen','pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = [col for col in varsname_v5 if 'm1b' in col]
to_drop_all3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
to_drop_all4 = ['pboc_dpd20']


to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[223]:


varsname_base = [col for col in varsname_v5 if col not in to_drop_all]
print(len(varsname_base))
print(varsname_base)


# In[224]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[225]:


df_sample['data_set'].value_counts()


# In[226]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[227]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[228]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[229]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[230]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[231]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[232]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 ç‰¹å¾å˜é‡ä¼˜åŒ–1

# In[245]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[246]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[247]:


df_corr_matric_person = df_sample[varsname_base].corr()
df_corr_matric_person.to_csv('df_corr.csv')


# In[248]:


df_corr_matric_person = df_sample[varsname_base].corr()

# è°ƒç”¨å‡½æ•°
df_corr_drop, to_drop_vars = find_high_correlation_pairs(df_corr_matric_person,
                                                     df_iv_by_month['2025-03'],
                                                     threshold=0.75)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop_vars))


# In[249]:


print(to_drop_vars)


# In[250]:


df_corr_drop


# In[281]:


to_drop_cols1 = ['dz_v1_mob4', 'xz_fpd', 'm1a0037_g_p', 'a_pboc_fpd10_v2', 'a_pboc_fpd0_v1', 'm1a0026_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0031_g_p', 'm1a0021_g_p', 'dz_fpd', 'hengpu_7', 'm1a0020_g_p', 'tengxun_1', 'br_fpd_2', 'm1a0016_g_p']
to_drop_cols2 = ['br_v3_fpd', 'dz_fpd', 'br_fpd_2', 'a_pboc_fpd10_v2', 'm1a0011_g_p', 'm1a0034_g_p', 'm1a0009_g_p', 'a_pboc_fpd10_v1', 'm1a0027_g_p', 'a_pboc_fpd30_v1', 'm1a0021_g_p', 'xz_fpd', 'm1a0016_g_p', 'tengxun_1', 'm1a0022_g_p', 'a_pboc_fpd6_v1', 'hengpu_7', 'br_v3_mob4', 'm1a0023_g_p', 'm1a0026_g_p', 'br_mob4']

to_drol_cols = to_drop_cols1 + to_drop_cols2
print(to_drol_cols)
print(len(set(to_drol_cols)))


# In[282]:


varsname_base_v2 = [col for col in varsname_base if col not in to_drol_cols]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[ ]:


varsname_base_v2 = ['dz_v2_fpd', 'm1a0033_g_p', 'm1a0043_g_p', 'xz_v2_fpd', 'hengpu_4', 'm1a0041_g_p', 'pudao_34', 'ruizhi_6', 'm1a0038_g_p', 'pd_fpd', 'hengpu_5', 'baihang_13', 'pudao_54', 'pboc_dpd20', 'pudao_20', 'm1a0028_g_p', 'aliyun_5', 'pudao_82', 'baihang_28', 'a_bhdj_fpd10_v1', 'zhirongfen', 'duxiaoman_6', 'tianchuang_7', 'm1a0040_g_p', 'rong360_4', 'm1a0035_g_p', 'xz_v1_mob4', 'ali_fraud_score3', 'm1a0020_g_p', 'pudao_87', 'm1a0037_g_p', 'm1a0044_g_p', 'bileizhenv1', 'a_pboc_fpd0_v1', 'pudao_68', 'pudao_84', 'br_fpd']
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[283]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[284]:


df_sample['data_set'].value_counts()


# In[285]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[286]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[287]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[288]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[289]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[290]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[291]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–2

# In[355]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[295]:


to_drop_cols3 = ['m1a0044_g_p','m1a0041_g_p']


# In[296]:



varsname_base_v3 = [col for col in varsname_base_v2 if col not in to_drop_cols3]
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[297]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[298]:


df_sample['data_set'].value_counts()


# In[299]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[300]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[301]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[302]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[303]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[304]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[305]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–3

# In[402]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[403]:


df_importance_set_v2


# In[416]:


# to_drop_cols4 = ['ali_fraud_score9','ali_fraud_score3','ppcm_behav_score','umeng_score_v5','tianchuang_7','duxiaoman_6']
# to_drop_cols4 = list(df_importance_set_v2[df_importance_set_v2['gain']<400]['feature'])
to_drop_cols4 = ['ali_fraud_score3','ali_fraud_score9']
print(len(to_drop_cols4))
print(to_drop_cols4)


# In[ ]:


# to_drop_cols4.append('m1a0041_g_p')


# In[417]:


varsname_base_v4 = [col for col in varsname_base_v2 if col not in to_drop_cols4]
print(len(varsname_base_v4))
print(varsname_base_v4) 


# In[418]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'

df_sample['data_set'].value_counts()


# In[419]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[420]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v7'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v7'].head()


# In[409]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'apply_month')
df_ks_auc_month_v4


# In[421]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'apply_month')
df_ks_auc_month_v4


# In[412]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v6', 'data_set')
df_ks_auc_set_v4


# In[422]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'data_set')
df_ks_auc_set_v4


# In[423]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v4 = pd.merge(df_importance_month_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v4 = pd.merge(df_vars_list, df_importance_month_v4, how='right',left_on='name',right_on='feature')
df_importance_month_v4


# In[424]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v4 = pd.merge(df_importance_set_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v4 = pd.merge(df_vars_list, df_importance_set_v4, how='right',left_on='name',right_on='feature')
df_importance_set_v4


# In[332]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx')


# In[343]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v5_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v5_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v5')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v5')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v5')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v5')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v5_{timestamp}.xlsx')


# In[415]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx')


# In[425]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[426]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[427]:


print(df_sample.columns[-17:].to_list())


# In[429]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'channel_types', 'channel_rates', 'y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7','t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506']
print(len(usecols))
print(usecols)


# In[430]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[431]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[432]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc.info(show_counts=True)


# In[433]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all1


# In[434]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2


# In[435]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_1_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='allchannel')
    df_ksauc_all2.to_excel(writer, sheet_name='pboc')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_1_{timestamp}.xlsx')


# In[437]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[438]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[440]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[ ]:



# df_evalue_fpd30 = df_evalue.query("target_mob4dpd30>=0")
# df_evalue_fpd30.info(show_counts=True)


# In[ ]:


df_evalue_fpd30['target_mob4dpd30_1'] = 1 - df_evalue_fpd30['target_mob4dpd30']
df_evalue_fpd30['target_mob4dpd30_1'].value_counts()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_3.head()


# In[441]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_all_2.to_excel(writer, sheet_name='fpd30_èåˆ')
    df_ksauc_all_3.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
    df_ksauc_all_4.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='mob4_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# ##### è°ƒç”¨å¾ä¿¡çš„æ¸ é“

# In[442]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_month_pboc


# In[443]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[444]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[445]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[ ]:


df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc_fpd30.info(show_counts=True)


# In[ ]:


model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_2.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_3.head()


# In[ ]:





# In[446]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_fpd30_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_pd30_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_fpd30_ç¦»çº¿')
#     df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
#     df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
#     df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx')


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[447]:


df_sample['apply_month'].value_counts()


# In[448]:


score = 'y_prob_base_v6'


# In[449]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[450]:


df_sample['score_bins'].head()


# In[451]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[452]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[453]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[454]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[455]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_report.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬fpd30èåˆæ¨¡å‹_2411_2502_report.csv')


# In[ ]:







#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--èåˆæ¨¡å‹
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- ä¸‰æ–¹æ•°æ®å­åˆ†
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--è¡Œä¸ºæ¨¡å‹å­åˆ†
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    -- è¡Œä¸ºæ¨¡å‹å­åˆ†
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- å­åˆ†å®æ—¶æ¨¡å‹
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- æˆä¿¡èåˆæ¨¡å‹
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------ä¸‰æ–¹æ•°æ®-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


df_sample_['order_no'].value_counts().head(3)


# In[9]:


df_sample_.query("order_no=='auth_122980696020240825115424'")


# In[13]:


df_sample_.drop_duplicates(subset=['order_no'],inplace=True)


# In[14]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[15]:


df_sample_.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv')


# In[16]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:5], varsname[-5:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[17]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[18]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[23]:


df_sample = df_sample_.query("target_mob4dpd30>=0 & channel_id > 1 & apply_date<='2025-01-05'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[24]:


df_sample.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[25]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-01-05'").index, 'data_set']='3_oot2'


# In[26]:


df_sample.to_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411.csv',index=False)
print(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411.csv')


# In[27]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[28]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[29]:


print(df_sample[target].value_counts())


# In[30]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[31]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[32]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# In[246]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[247]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[248]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[249]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[250]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[251]:


df_explor_v1['missing'].value_counts()


# In[252]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[253]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[254]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[255]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[256]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[257]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# In[44]:


sql = """
    select 
     order_no
    ,max(case when variable_code = 'm1a0044' then good_score else null end) as M1A0044_g_p 
    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
"""
df_m1a0044 = get_data(sql)


# In[45]:


df_m1a0044.info()


# In[46]:


df_sample.drop(columns=['m1a0044_g_p'],inplace=True)


# In[47]:


df_sample = pd.merge(df_sample, df_m1a0044, how='inner',on='order_no')


# In[48]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[54]:


varsname_v1 = [col for col in varsname if col in df_sample.columns ]
len(varsname_v1)


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[259]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:





# In[261]:


print(len(to_drop_iv))
to_drop_iv


# In[262]:


print(len(to_drop_missing))
to_drop_missing


# In[263]:


df_iv.loc[to_drop_iv,:]


# In[266]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[ ]:


# train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
#                                                 target=target, 
#                                                 empty=0.90, iv=0.01, corr=0.85, 
#                                                 return_drop=True, exclude=None)
# train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:]


# In[270]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[271]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[274]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[275]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[276]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[277]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[278]:


new_bins_dict


# In[279]:


combiner.load(new_bins_dict)


# In[280]:


combiner.export()


# In[281]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[282]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[283]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[284]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[285]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[286]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[287]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[288]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[289]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[290]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[291]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[294]:


print(len(to_drop3))
print(to_drop3)


# In[295]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[296]:


target


# In[297]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[298]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[299]:


df_sample_woe.head()


# In[300]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[301]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[302]:


df_corr_matrix.head()


# In[303]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[304]:


df_high_corr


# In[306]:


print(to_drop4)


# In[307]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[308]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[309]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[310]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[311]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[312]:


print(to_drop5)


# In[313]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[314]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[56]:


target


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[target].value_counts()


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[ ]:


# # æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# In[ ]:





# ### 5.2.1 baseæ¨¡å‹

# In[64]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[65]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[67]:


varsname_v5 = varsname_v1[:]
print(len(varsname_v5))
print(varsname_v5)


# In[82]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
print(len(drop_cols))


# In[83]:


varsname_base = [col for col in varsname_v5 if col not in drop_cols]


# In[84]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[85]:


df_sample['data_set'].value_counts()


# In[86]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[87]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[88]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[89]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[90]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[91]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[92]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 ç‰¹å¾å˜é‡ä¼˜åŒ–1

# In[93]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[94]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[97]:


drop_cols_v2 = [col for col in varsname_base if 'm1b' in col]


# In[98]:


drop_cols_v2


# In[99]:


varsname_base_v2 = [col for col in varsname_base if col not in drop_cols_v2]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[100]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[101]:


df_sample['data_set'].value_counts()


# In[102]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[103]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[104]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[106]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[107]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[108]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[109]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–2

# In[120]:


# sql = '''
# select 
#  t.order_no
# ,t_off_m4d30_2504
# ,t_off_f30_2504
# ,t_off_f30_2506
# ,t_off_m3d30_2506
# ,off_m4d30_2504
# ,off_f30_2504

# from 
#     (
#     select distinct t2.order_no
#     from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
#     inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
#     on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
#     where t1.dt = date_sub(current_date(), 1) 
#       and t2.dt = date_sub(current_date(), 1) 
#       and t2.auth_status = 6
#       and t2.apply_date >= '2024-08-01'
#       and t2.apply_date <= '2025-01-05'
#     ) as t 

# -- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
# left join 
#     (
#     select 
#      order_no
#     ,max(case when variable_code = 't_off_m4d30_2504' then standard_score else null end) as t_off_m4d30_2504 
#     ,max(case when variable_code = 't_off_f30_2504' then standard_score else null end) as t_off_f30_2504 
#     ,max(case when variable_code = 't_off_f30_2506' then standard_score else null end) as t_off_f30_2506 
#     ,max(case when variable_code = 't_off_m3d30_2506' then standard_score else null end) as t_off_m3d30_2506 
#     ,max(case when variable_code = 'off_m4d30_2504' then standard_score else null end) as off_m4d30_2504 
#     ,max(case when variable_code = 'off_f30_2504' then standard_score else null end) as off_f30_2504 
#     from znzz_fintech_ads.apply_model01_scores_off 
#     where apply_time >= '2024-08-01'
#       and apply_time <= '2025-01-05'
#     group by order_no
#     ) as t2 on t.order_no=t2.order_no 
# ;
# '''

# df_off_merge = get_data(sql)


# In[122]:


df_off = pd.read_csv(result_path + 'df_off_merge.csv')
df_off.info(show_counts=True)


# In[124]:


df_off.drop(columns=['Unnamed: 0'],inplace=True)


# In[126]:


df_m4d30 = pd.read_csv(result_path + 't_off_m4d30_2504.csv')
df_m4d30.drop(columns=['Unnamed: 0'],inplace=True)
df_m4d30.info(show_counts=True)


# In[127]:


df_off_merge = pd.merge(df_m4d30, df_off,how='inner',on='order_no')
df_off_merge.info(show_counts=True)


# In[164]:


drop_cols3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']


# In[165]:


df_off_merge2.info(show_counts=True)


# In[166]:


df_sample = pd.merge(df_sample.drop(columns=drop_cols3), df_off_merge2,how='left',on='order_no')
df_sample.info(show_counts=True)


# In[167]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[168]:


print(df_off_merge.columns.to_list())


# In[169]:



varsname_base_v3 = varsname_base + ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[170]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[171]:


df_sample['data_set'].value_counts()


# In[172]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[173]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[174]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[175]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[176]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[177]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[178]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx')


# In[212]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[213]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[214]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[151]:


# df_off_merge2 = pd.read_csv(result_path + 'df_off_merge2.csv')
# df_off_merge2.info(show_counts=True)


# In[152]:


# df_sample_ = pd.merge(df_sample_, df_off_merge2, how='left', on='order_no')


# In[418]:


df_evalue_all = df_sample_.query("target_fpd30>=0 & channel_id>1 & apply_date<='2025-03-31'")
df_evalue_all['target_fpd30_1'] = 1 - df_evalue_all['target_fpd30']


# In[419]:



lgb_model_v1 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v1_20250604190522.pkl')
varsnamev1 = lgb_model_v1.feature_name()
df_evalue_all['y_prob_base_v1'] = lgb_model_v1.predict(df_evalue_all[varsnamev1], num_iteration=lgb_model_v1.best_iteration)


# In[420]:


lgb_model_v2 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v2_20250604191712.pkl')
varsnamev2 = lgb_model_v2.feature_name()
df_evalue_all['y_prob_base_v2'] = lgb_model_v2.predict(df_evalue_all[varsnamev2], num_iteration=lgb_model_v2.best_iteration)


# In[421]:


lgb_model_v3 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v3_20250604210408.pkl')
varsnamev3 = lgb_model_v3.feature_name()
df_evalue_all['y_prob_base_v3'] = lgb_model_v3.predict(df_evalue_all[varsnamev3], num_iteration=lgb_model_v3.best_iteration)


# In[423]:


df_evalue_all['apply_month'] = df_evalue_all['apply_date'].str[0:7]


# In[422]:


df_evalue_all['channel_types'] = df_evalue_all['channel_id'].apply(channel_type)
df_evalue_all['channel_rates'] = df_evalue_all['channel_id'].apply(channel_rate)


# In[ ]:


# dfks_v3 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v3', 'apply_month')
# dfks_v3


# In[ ]:


dfks_v1 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v1', 'apply_month')
dfks_v1


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# ### 5.4 ç‰¹å¾å˜é‡ä¼˜åŒ–3

# In[736]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[737]:


varsname_base_v4 = ['dz_v1_mob4', 'xz_v1_mob4', 'hengpu_4', 'm1a0038_g_p', 'm1a0028_g_p', 'm1a0033_g_p', 'pudao_34', 'm1a0027_g_p', 'duxiaoman_6', 'pudao_87', 'hengpu_5', 'baihang_28', 'pudao_68', 'm1a0031_g_p', 'aliyun_5', 'pboc_dpd20', 'pudao_54', 'ruizhi_6', 'pudao_20', 'm1a0040_g_p', 'pudao_82', 'm1a0043_g_p', 'pudao_84', 'baihang_13', 'tianchuang_7', 'm1a0035_g_p', 'br_v3_mob4', 'a_pboc_fpd10_v2', 'ali_fraud_score3', 'a_bhdj_fpd10_v1', 'rong360_4', 'br_fpd_2', 'br_mob4_2', 'bileizhenv1', 'm1a0021_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0011_g_p', 'pudao_91', 'm1a0044_g_p', 'baihang_31', 'm1a0026_g_p', 'a_pboc_fpd30_v1', 'xz_v2_fpd', 'umeng_score_v5', 'dz_v2_fpd', 'ali_fraud_score9', 'zhirongfen', 'm1a0023_g_p', 'br_fpd']
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[738]:


to_drop_all1 = ['pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = ['tianchuang_7','ali_fraud_score3','umeng_score_v5','dz_v2_fpd','m1a0044_g_p','a_pboc_fpd30_v1','ali_fraud_score9','m1a0023_g_p','br_fpd']
to_drop_all3 = ['m1a0027_g_p','dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[739]:


varsname_base_v5 = [col for col in varsname_base_v4 if col not in to_drop_all]
print(len(varsname_base_v5))
print(varsname_base_v5)


# In[740]:


varsname_base_v6 = varsname_base_v5[:] + ['dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
varsname_base_v6.remove('xz_v1_mob4')
print(len(varsname_base_v6))
print(varsname_base_v6)


# In[741]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v6]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[742]:



# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[743]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v13'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v13'].head()


# In[ ]:


# df_sample = df_sample.query("apply_date>='2024-09-01'")
# df_sample = df_sample.reset_index(drop=True)


# In[744]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'apply_month')
df_ks_auc_month_v4


# In[745]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'data_set')
df_ks_auc_set_v4


# In[746]:


vars_des = {
 'dz_v1_mob4':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦å¤šå¤´æ¨¡å‹v1å››æœŸæ ‡ç­¾202505'
,'xz_v1_mob4':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v1å››æœŸæ ‡ç­¾202505'
,'hengpu_4':'æ’æ™®-åæ¬ºè¯ˆåˆ†M3'
,'m1a0038_g_p':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡åˆ†'
,'m1a0028_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œmob4dpd30æ¨¡å‹V1_2408_2410_å¥½æ¦‚ç‡'
,'m1a0033_g_p':'æˆä¿¡å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿfpd30æ¨¡å‹2408_2411_å¥½æ¦‚ç‡åˆ†'
,'pudao_34':'æœ´é“-é¿é›·é’ˆå®šåˆ¶åˆ†V1'
,'m1a0027_g_p':'æç°å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2407_2409_å¥½æ¦‚ç‡åˆ†'
,'duxiaoman_6':'åº¦å°æ»¡-æ¬ºè¯ˆå› å­V4'
,'pudao_87':'æœ´é“-å­—èŠ‚-äº’è”ç½‘è¡Œä¸ºè¯„åˆ†25128'
,'hengpu_5':'æ’æ™®-å®šåˆ¶ä¿¡ç”¨åˆ†Y'
,'baihang_28':'ficoåæ¬ºè¯ˆæ´è§3.0'
,'aliyun_5':'æœ´é“-é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆV5'
,'pudao_68':'æœ´é“-é“¶å•†é“¶æå®šåˆ¶åˆ†'
,'m1a0031_g_p':'æç°å…¨æ¸ é“äººè¡Œmob5dpd30æ¨¡å‹v1_2406_2409_å¥½æ¦‚ç‡'
,'m1a0026_g_p':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2407_2409_å¥½æ¦‚ç‡åˆ†'
,'pudao_54':'æœ´é“-å“ˆå•°-hl-ç«çœ¼åˆ†v7'
,'pudao_20':'æœ´é“-è…¾è®¯å¤©å¾¡åæ¬ºè¯ˆV7é€šç”¨ç‰ˆ'
,'pboc_dpd20':'æˆä¿¡é€šç”¨äººè¡Œæ¨¡å‹dpd20æ ‡ç­¾202410'
,'m1a0043_g_p':'æç°å…¨æ¸ é“äººè¡Œfpd30æ¨¡å‹v1_2411_2502_å¥½æ¦‚ç‡'
,'m1a0040_g_p':'æˆä¿¡å…¨æ¸ é“æœ´é“å¤šå¤´å››æœŸæ¨¡å‹V1_2409_2411_å¥½æ¦‚ç‡'
,'ruizhi_6':'FICOè”åˆå»ºæ¨¡å®šåˆ¶åˆ†2'
,'pudao_82':'æœ´é“-å¤©åˆ›-ç„è¾°ä¿¡ç”¨åˆ†_AC1501'
,'br_v3_mob4':'æˆä¿¡å…¨æ¸ é“ç™¾èå¤šå¤´æ¨¡å‹v3å››æœŸæ ‡ç­¾202502'
,'pudao_84':'æœ´é“-å‹ç›Ÿ-å»ºæ¨¡åˆ†score_v3'
,'a_pboc_fpd10_v2':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v2fpd10æ ‡ç­¾202410'
,'rong360_4':'æ•°æ®æº_rong360'
,'a_bhdj_fpd10_v1':'æˆä¿¡å…¨æ¸ é“ç™¾è¡Œæ´è§æ¨¡å‹v1fpd10æ ‡ç­¾'
,'tianchuang_7':'å¤©åˆ›-è”åˆåˆ†A1502'
,'br_fpd_2':'æˆä¿¡ç™¾èå¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202407'
,'baihang_13':'ç™¾è¡Œ-çµçŠ€äº§å“-å­šä¸´-107'
,'m1a0011_g_p':'æˆä¿¡ç™¾èå¤šå¤´è¡ç”Ÿv3é¦–æœŸæ ‡ç­¾æ¨¡å‹202412'
,'a_pboc_fpd6_v1':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v1fpd6æ ‡ç­¾202409'
,'m1a0035_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œfpd7æ¨¡å‹202408_2411'
,'br_mob4_2':'æˆä¿¡ç™¾èå¤šå¤´v2å››æœŸæ ‡ç­¾202407'
,'ali_fraud_score3':'é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆå­åˆ†score3'
,'m1a0021_g_p':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿfpd30æ¨¡å‹202409_2411'
,'baihang_31':'ficoæ™®æƒ å°ç‰ˆåˆ†'
,'pudao_91':'æœ´é“-åº¦å°æ»¡-å°æ»¡åˆ†æ¡”å­æ•°ç§‘å®šåˆ¶v2'
,'br_v3_fpd':'æˆä¿¡å…¨æ¸ é“ç™¾èå¤šå¤´æ¨¡å‹v3é¦–æœŸæ ‡ç­¾202502'
,'bileizhenv1':'é¿é›·é’ˆv1h1'
,'xz_v2_fpd':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'dz_v2_fpd':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'m1a0044_g_p':'æç°å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡'
,'umeng_score_v5':'å‹ç›Ÿ-å°é¢åˆ†V5.0'
,'a_pboc_fpd30_v1':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v1fpd30æ ‡ç­¾'
,'m1a0023_g_p':'æç°é‡‘ç§‘æ¸ é“ç»­ä¾¦åŠ è¡ç”Ÿfpd30æ¨¡å‹202407_2411_å¥½æ¦‚ç‡åˆ†'
,'zhirongfen':'åŒç›¾æ™ºèåˆ†'
,'ali_fraud_score9':'é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆå­åˆ†score9'
,'br_fpd':'æˆä¿¡ç™¾èå¤šå¤´é¦–æœŸæ ‡ç­¾æ¨¡å‹202404'
}


# In[747]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v4['å˜é‡åç§°'] = df_importance_month_v4['feature'].map(vars_des)
df_importance_month_v4


# In[748]:




# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v4['å˜é‡åç§°'] = df_importance_set_v4['feature'].map(vars_des)
df_importance_set_v4


# In[401]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx')


# In[517]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx')


# In[563]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx')


# In[573]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v8_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v8_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v8_{timestamp}.pkl')
print(result_path + f'{task_name}_v8_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v8_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v8')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v8')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v8')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v8')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v8_{timestamp}.xlsx')


# In[604]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v11_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v11_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v11_{timestamp}.pkl')
print(result_path + f'{task_name}_v11_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v11_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v11')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v11')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v11')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v11')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v11_{timestamp}.xlsx')


# In[707]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v12_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v12_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v12_{timestamp}.pkl')
print(result_path + f'{task_name}_v12_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v12')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v12')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v12')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v12')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v12_{timestamp}.xlsx')


# In[749]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v13_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v13_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v13_{timestamp}.pkl')
print(result_path + f'{task_name}_v13_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v13')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v13')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v13')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v13')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v13_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[750]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[709]:


# lgb_model_v5 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v4_20250605181337.pkl')
# varsnamev5 = lgb_model_v5.feature_name()
# df_sample['y_prob_base_v5'] = lgb_model_v5.predict(df_sample[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[751]:


print(df_sample.columns[-17:].to_list())


# In[752]:


df_sample['high_p_m4d30_2506_v12'] = df_sample['y_prob_base_v12']
df_sample['high_p_m4d30_2506_v13'] = df_sample['y_prob_base_v13']


# In[754]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'target_mob4dpd30_1', 'channel_types', 'channel_rates', 'high_p_m4d30_2506_v12','high_p_m4d30_2506_v13','t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
print(len(usecols))
print(usecols)


# In[755]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[756]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[778]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5


# In[757]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# In[758]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_m3d30_2504', 'low_m3d30_2504','free_v1_fpd', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[759]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new', 'high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[760]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[761]:


# # lgb_model_v5 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v4_20250605181337.pkl')
# # varsnamev5 = lgb_model_v5.feature_name()
# df_evalue_all['y_prob_base_v5'] = lgb_model_v5.predict(df_evalue_all[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[762]:


# varsnamev4 = lgb_model.feature_name()
# df_evalue_all['y_prob_base_v4'] = lgb_model.predict(df_evalue_all[varsnamev4], num_iteration=lgb_model.best_iteration)


# In[763]:


# usecols_fpd30 = [col for col in usecols if col not in ['data_set', 'target_mob4dpd30_1']]
# df_evalue_fpd30 = df_evalue_all[usecols_fpd30]
# df_evalue_fpd30.info(show_counts=True)


# In[764]:


# df_evalue_fpd30['target_fpd30_1'].value_counts()


# In[765]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30.head()


# In[766]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_2.head()


# In[767]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_3.head()


# In[768]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_èåˆ')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_v13_{timestamp}.xlsx')


# In[482]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_èåˆ')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# ##### è°ƒç”¨å¾ä¿¡çš„æ¸ é“

# In[769]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[770]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p',  'low_m3d30_2504','free_m3d30_2504', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[771]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new','high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[772]:



model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[773]:


# df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc_fpd30.info(show_counts=True)


# In[774]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30.head()


# In[775]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_2.head()


# In[776]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_3.head()


# In[442]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
    df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_fpd30_èåˆ')
    df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_pd30_ä¸‰æ–¹')
    df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx')


# In[777]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_v13_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[779]:


df_sample['apply_month'].value_counts()


# In[780]:


score = 'y_prob_base_v12'


# In[781]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[782]:


df_sample['score_bins'].head()


# In[783]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[784]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[785]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[787]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'").drop(columns=['ks_bin'])


# In[788]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_v12_{timestamp}.xlsx')


# In[496]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report.csv')


# In[789]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report_0610.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report_0610.csv')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2411_2501.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--èåˆæ¨¡å‹
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- ä¸‰æ–¹æ•°æ®å­åˆ†
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--è¡Œä¸ºæ¨¡å‹å­åˆ†
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    -- è¡Œä¸ºæ¨¡å‹å­åˆ†
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- å­åˆ†å®æ—¶æ¨¡å‹
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- æˆä¿¡èåˆæ¨¡å‹
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------ä¸‰æ–¹æ•°æ®-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


df_sample_['order_no'].value_counts().head(3)


# In[9]:


df_sample_.query("order_no=='auth_122980696020240825115424'")


# In[13]:


df_sample_.drop_duplicates(subset=['order_no'],inplace=True)


# In[14]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[15]:


df_sample_.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬èåˆæ¨¡å‹250604.csv')


# In[16]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:5], varsname[-5:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[17]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[18]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[23]:


df_sample = df_sample_.query("target_mob4dpd30>=0 & channel_id > 1 & apply_date<='2025-01-05'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[24]:


df_sample.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[25]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-01-05'").index, 'data_set']='3_oot2'


# In[26]:


df_sample.to_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411.csv',index=False)
print(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411.csv')


# In[27]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[28]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[29]:


print(df_sample[target].value_counts())


# In[30]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[31]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[32]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# In[246]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[247]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[248]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[249]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[250]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[251]:


df_explor_v1['missing'].value_counts()


# In[252]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[253]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[254]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[255]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[256]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[257]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# In[44]:


sql = """
    select 
     order_no
    ,max(case when variable_code = 'm1a0044' then good_score else null end) as M1A0044_g_p 
    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
"""
df_m1a0044 = get_data(sql)


# In[45]:


df_m1a0044.info()


# In[46]:


df_sample.drop(columns=['m1a0044_g_p'],inplace=True)


# In[47]:


df_sample = pd.merge(df_sample, df_m1a0044, how='inner',on='order_no')


# In[48]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[54]:


varsname_v1 = [col for col in varsname if col in df_sample.columns ]
len(varsname_v1)


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[259]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:





# In[261]:


print(len(to_drop_iv))
to_drop_iv


# In[262]:


print(len(to_drop_missing))
to_drop_missing


# In[263]:


df_iv.loc[to_drop_iv,:]


# In[266]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[ ]:


# train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
#                                                 target=target, 
#                                                 empty=0.90, iv=0.01, corr=0.85, 
#                                                 return_drop=True, exclude=None)
# train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:]


# In[270]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[271]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[274]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[275]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[276]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[277]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty


# In[278]:


new_bins_dict


# In[279]:


combiner.load(new_bins_dict)


# In[280]:


combiner.export()


# In[281]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[282]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[283]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[284]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[285]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[286]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[287]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[288]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[289]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[290]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[291]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[294]:


print(len(to_drop3))
print(to_drop3)


# In[295]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[296]:


target


# In[297]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[298]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[299]:


df_sample_woe.head()


# In[300]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[301]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[302]:


df_corr_matrix.head()


# In[303]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[304]:


df_high_corr


# In[306]:


print(to_drop4)


# In[307]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[308]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[309]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[310]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[311]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[312]:


print(to_drop5)


# In[313]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[314]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[56]:


target


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[target].value_counts()


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[ ]:


# # æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# In[ ]:





# ### 5.2.1 baseæ¨¡å‹

# In[64]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[65]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[67]:


varsname_v5 = varsname_v1[:]
print(len(varsname_v5))
print(varsname_v5)


# In[82]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
print(len(drop_cols))


# In[83]:


varsname_base = [col for col in varsname_v5 if col not in drop_cols]


# In[84]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[85]:


df_sample['data_set'].value_counts()


# In[86]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[87]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[88]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[89]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[90]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[91]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[92]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 ç‰¹å¾å˜é‡ä¼˜åŒ–1

# In[93]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[94]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[97]:


drop_cols_v2 = [col for col in varsname_base if 'm1b' in col]


# In[98]:


drop_cols_v2


# In[99]:


varsname_base_v2 = [col for col in varsname_base if col not in drop_cols_v2]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[100]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[101]:


df_sample['data_set'].value_counts()


# In[102]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[103]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[104]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[106]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[107]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[108]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[109]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 ç‰¹å¾å˜é‡ä¼˜åŒ–2

# In[120]:


# sql = '''
# select 
#  t.order_no
# ,t_off_m4d30_2504
# ,t_off_f30_2504
# ,t_off_f30_2506
# ,t_off_m3d30_2506
# ,off_m4d30_2504
# ,off_f30_2504

# from 
#     (
#     select distinct t2.order_no
#     from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
#     inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
#     on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
#     where t1.dt = date_sub(current_date(), 1) 
#       and t2.dt = date_sub(current_date(), 1) 
#       and t2.auth_status = 6
#       and t2.apply_date >= '2024-08-01'
#       and t2.apply_date <= '2025-01-05'
#     ) as t 

# -- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
# left join 
#     (
#     select 
#      order_no
#     ,max(case when variable_code = 't_off_m4d30_2504' then standard_score else null end) as t_off_m4d30_2504 
#     ,max(case when variable_code = 't_off_f30_2504' then standard_score else null end) as t_off_f30_2504 
#     ,max(case when variable_code = 't_off_f30_2506' then standard_score else null end) as t_off_f30_2506 
#     ,max(case when variable_code = 't_off_m3d30_2506' then standard_score else null end) as t_off_m3d30_2506 
#     ,max(case when variable_code = 'off_m4d30_2504' then standard_score else null end) as off_m4d30_2504 
#     ,max(case when variable_code = 'off_f30_2504' then standard_score else null end) as off_f30_2504 
#     from znzz_fintech_ads.apply_model01_scores_off 
#     where apply_time >= '2024-08-01'
#       and apply_time <= '2025-01-05'
#     group by order_no
#     ) as t2 on t.order_no=t2.order_no 
# ;
# '''

# df_off_merge = get_data(sql)


# In[122]:


df_off = pd.read_csv(result_path + 'df_off_merge.csv')
df_off.info(show_counts=True)


# In[124]:


df_off.drop(columns=['Unnamed: 0'],inplace=True)


# In[126]:


df_m4d30 = pd.read_csv(result_path + 't_off_m4d30_2504.csv')
df_m4d30.drop(columns=['Unnamed: 0'],inplace=True)
df_m4d30.info(show_counts=True)


# In[127]:


df_off_merge = pd.merge(df_m4d30, df_off,how='inner',on='order_no')
df_off_merge.info(show_counts=True)


# In[164]:


drop_cols3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']


# In[165]:


df_off_merge2.info(show_counts=True)


# In[166]:


df_sample = pd.merge(df_sample.drop(columns=drop_cols3), df_off_merge2,how='left',on='order_no')
df_sample.info(show_counts=True)


# In[167]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[168]:


print(df_off_merge.columns.to_list())


# In[169]:



varsname_base_v3 = varsname_base + ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[170]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[171]:


df_sample['data_set'].value_counts()


# In[172]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[173]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[174]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[175]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[176]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[177]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[178]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v3_{timestamp}.xlsx')


# In[212]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[213]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[214]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[151]:


# df_off_merge2 = pd.read_csv(result_path + 'df_off_merge2.csv')
# df_off_merge2.info(show_counts=True)


# In[152]:


# df_sample_ = pd.merge(df_sample_, df_off_merge2, how='left', on='order_no')


# In[418]:


df_evalue_all = df_sample_.query("target_fpd30>=0 & channel_id>1 & apply_date<='2025-03-31'")
df_evalue_all['target_fpd30_1'] = 1 - df_evalue_all['target_fpd30']


# In[419]:



lgb_model_v1 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v1_20250604190522.pkl')
varsnamev1 = lgb_model_v1.feature_name()
df_evalue_all['y_prob_base_v1'] = lgb_model_v1.predict(df_evalue_all[varsnamev1], num_iteration=lgb_model_v1.best_iteration)


# In[420]:


lgb_model_v2 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v2_20250604191712.pkl')
varsnamev2 = lgb_model_v2.feature_name()
df_evalue_all['y_prob_base_v2'] = lgb_model_v2.predict(df_evalue_all[varsnamev2], num_iteration=lgb_model_v2.best_iteration)


# In[421]:


lgb_model_v3 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v3_20250604210408.pkl')
varsnamev3 = lgb_model_v3.feature_name()
df_evalue_all['y_prob_base_v3'] = lgb_model_v3.predict(df_evalue_all[varsnamev3], num_iteration=lgb_model_v3.best_iteration)


# In[423]:


df_evalue_all['apply_month'] = df_evalue_all['apply_date'].str[0:7]


# In[422]:


df_evalue_all['channel_types'] = df_evalue_all['channel_id'].apply(channel_type)
df_evalue_all['channel_rates'] = df_evalue_all['channel_id'].apply(channel_rate)


# In[ ]:


# dfks_v3 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v3', 'apply_month')
# dfks_v3


# In[ ]:


dfks_v1 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v1', 'apply_month')
dfks_v1


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# ### 5.4 ç‰¹å¾å˜é‡ä¼˜åŒ–3

# In[736]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[737]:


varsname_base_v4 = ['dz_v1_mob4', 'xz_v1_mob4', 'hengpu_4', 'm1a0038_g_p', 'm1a0028_g_p', 'm1a0033_g_p', 'pudao_34', 'm1a0027_g_p', 'duxiaoman_6', 'pudao_87', 'hengpu_5', 'baihang_28', 'pudao_68', 'm1a0031_g_p', 'aliyun_5', 'pboc_dpd20', 'pudao_54', 'ruizhi_6', 'pudao_20', 'm1a0040_g_p', 'pudao_82', 'm1a0043_g_p', 'pudao_84', 'baihang_13', 'tianchuang_7', 'm1a0035_g_p', 'br_v3_mob4', 'a_pboc_fpd10_v2', 'ali_fraud_score3', 'a_bhdj_fpd10_v1', 'rong360_4', 'br_fpd_2', 'br_mob4_2', 'bileizhenv1', 'm1a0021_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0011_g_p', 'pudao_91', 'm1a0044_g_p', 'baihang_31', 'm1a0026_g_p', 'a_pboc_fpd30_v1', 'xz_v2_fpd', 'umeng_score_v5', 'dz_v2_fpd', 'ali_fraud_score9', 'zhirongfen', 'm1a0023_g_p', 'br_fpd']
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[738]:


to_drop_all1 = ['pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = ['tianchuang_7','ali_fraud_score3','umeng_score_v5','dz_v2_fpd','m1a0044_g_p','a_pboc_fpd30_v1','ali_fraud_score9','m1a0023_g_p','br_fpd']
to_drop_all3 = ['m1a0027_g_p','dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[739]:


varsname_base_v5 = [col for col in varsname_base_v4 if col not in to_drop_all]
print(len(varsname_base_v5))
print(varsname_base_v5)


# In[740]:


varsname_base_v6 = varsname_base_v5[:] + ['dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
varsname_base_v6.remove('xz_v1_mob4')
print(len(varsname_base_v6))
print(varsname_base_v6)


# In[741]:



# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v6]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[742]:



# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[743]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v13'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v13'].head()


# In[ ]:


# df_sample = df_sample.query("apply_date>='2024-09-01'")
# df_sample = df_sample.reset_index(drop=True)


# In[744]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'apply_month')
df_ks_auc_month_v4


# In[745]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'data_set')
df_ks_auc_set_v4


# In[746]:


vars_des = {
 'dz_v1_mob4':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦å¤šå¤´æ¨¡å‹v1å››æœŸæ ‡ç­¾202505'
,'xz_v1_mob4':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v1å››æœŸæ ‡ç­¾202505'
,'hengpu_4':'æ’æ™®-åæ¬ºè¯ˆåˆ†M3'
,'m1a0038_g_p':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡åˆ†'
,'m1a0028_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œmob4dpd30æ¨¡å‹V1_2408_2410_å¥½æ¦‚ç‡'
,'m1a0033_g_p':'æˆä¿¡å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿfpd30æ¨¡å‹2408_2411_å¥½æ¦‚ç‡åˆ†'
,'pudao_34':'æœ´é“-é¿é›·é’ˆå®šåˆ¶åˆ†V1'
,'m1a0027_g_p':'æç°å…¨æ¸ é“ç™¾èåŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2407_2409_å¥½æ¦‚ç‡åˆ†'
,'duxiaoman_6':'åº¦å°æ»¡-æ¬ºè¯ˆå› å­V4'
,'pudao_87':'æœ´é“-å­—èŠ‚-äº’è”ç½‘è¡Œä¸ºè¯„åˆ†25128'
,'hengpu_5':'æ’æ™®-å®šåˆ¶ä¿¡ç”¨åˆ†Y'
,'baihang_28':'ficoåæ¬ºè¯ˆæ´è§3.0'
,'aliyun_5':'æœ´é“-é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆV5'
,'pudao_68':'æœ´é“-é“¶å•†é“¶æå®šåˆ¶åˆ†'
,'m1a0031_g_p':'æç°å…¨æ¸ é“äººè¡Œmob5dpd30æ¨¡å‹v1_2406_2409_å¥½æ¦‚ç‡'
,'m1a0026_g_p':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2407_2409_å¥½æ¦‚ç‡åˆ†'
,'pudao_54':'æœ´é“-å“ˆå•°-hl-ç«çœ¼åˆ†v7'
,'pudao_20':'æœ´é“-è…¾è®¯å¤©å¾¡åæ¬ºè¯ˆV7é€šç”¨ç‰ˆ'
,'pboc_dpd20':'æˆä¿¡é€šç”¨äººè¡Œæ¨¡å‹dpd20æ ‡ç­¾202410'
,'m1a0043_g_p':'æç°å…¨æ¸ é“äººè¡Œfpd30æ¨¡å‹v1_2411_2502_å¥½æ¦‚ç‡'
,'m1a0040_g_p':'æˆä¿¡å…¨æ¸ é“æœ´é“å¤šå¤´å››æœŸæ¨¡å‹V1_2409_2411_å¥½æ¦‚ç‡'
,'ruizhi_6':'FICOè”åˆå»ºæ¨¡å®šåˆ¶åˆ†2'
,'pudao_82':'æœ´é“-å¤©åˆ›-ç„è¾°ä¿¡ç”¨åˆ†_AC1501'
,'br_v3_mob4':'æˆä¿¡å…¨æ¸ é“ç™¾èå¤šå¤´æ¨¡å‹v3å››æœŸæ ‡ç­¾202502'
,'pudao_84':'æœ´é“-å‹ç›Ÿ-å»ºæ¨¡åˆ†score_v3'
,'a_pboc_fpd10_v2':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v2fpd10æ ‡ç­¾202410'
,'rong360_4':'æ•°æ®æº_rong360'
,'a_bhdj_fpd10_v1':'æˆä¿¡å…¨æ¸ é“ç™¾è¡Œæ´è§æ¨¡å‹v1fpd10æ ‡ç­¾'
,'tianchuang_7':'å¤©åˆ›-è”åˆåˆ†A1502'
,'br_fpd_2':'æˆä¿¡ç™¾èå¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202407'
,'baihang_13':'ç™¾è¡Œ-çµçŠ€äº§å“-å­šä¸´-107'
,'m1a0011_g_p':'æˆä¿¡ç™¾èå¤šå¤´è¡ç”Ÿv3é¦–æœŸæ ‡ç­¾æ¨¡å‹202412'
,'a_pboc_fpd6_v1':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v1fpd6æ ‡ç­¾202409'
,'m1a0035_g_p':'æˆä¿¡å…¨æ¸ é“äººè¡Œfpd7æ¨¡å‹202408_2411'
,'br_mob4_2':'æˆä¿¡ç™¾èå¤šå¤´v2å››æœŸæ ‡ç­¾202407'
,'ali_fraud_score3':'é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆå­åˆ†score3'
,'m1a0021_g_p':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿfpd30æ¨¡å‹202409_2411'
,'baihang_31':'ficoæ™®æƒ å°ç‰ˆåˆ†'
,'pudao_91':'æœ´é“-åº¦å°æ»¡-å°æ»¡åˆ†æ¡”å­æ•°ç§‘å®šåˆ¶v2'
,'br_v3_fpd':'æˆä¿¡å…¨æ¸ é“ç™¾èå¤šå¤´æ¨¡å‹v3é¦–æœŸæ ‡ç­¾202502'
,'bileizhenv1':'é¿é›·é’ˆv1h1'
,'xz_v2_fpd':'æˆä¿¡å…¨æ¸ é“ç»­ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'dz_v2_fpd':'æˆä¿¡å…¨æ¸ é“æ´ä¾¦å¤šå¤´æ¨¡å‹v2é¦–æœŸæ ‡ç­¾202505'
,'m1a0044_g_p':'æç°å…¨æ¸ é“æ´ä¾¦åŠ è¡ç”Ÿmob4dpd30æ¨¡å‹2409_2411_å¥½æ¦‚ç‡'
,'umeng_score_v5':'å‹ç›Ÿ-å°é¢åˆ†V5.0'
,'a_pboc_fpd30_v1':'æˆä¿¡å…¨æ¸ é“äººè¡Œæ¨¡å‹v1fpd30æ ‡ç­¾'
,'m1a0023_g_p':'æç°é‡‘ç§‘æ¸ é“ç»­ä¾¦åŠ è¡ç”Ÿfpd30æ¨¡å‹202407_2411_å¥½æ¦‚ç‡åˆ†'
,'zhirongfen':'åŒç›¾æ™ºèåˆ†'
,'ali_fraud_score9':'é˜¿é‡Œç”³è¯·åæ¬ºè¯ˆå­åˆ†score9'
,'br_fpd':'æˆä¿¡ç™¾èå¤šå¤´é¦–æœŸæ ‡ç­¾æ¨¡å‹202404'
}


# In[747]:



# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v4['å˜é‡åç§°'] = df_importance_month_v4['feature'].map(vars_des)
df_importance_month_v4


# In[748]:




# æ¨¡å‹å˜é‡é‡è¦æ€§
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v4['å˜é‡åç§°'] = df_importance_set_v4['feature'].map(vars_des)
df_importance_set_v4


# In[401]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v4_{timestamp}.xlsx')


# In[517]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v6_{timestamp}.xlsx')


# In[563]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v7_{timestamp}.xlsx')


# In[573]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v8_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v8_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v8_{timestamp}.pkl')
print(result_path + f'{task_name}_v8_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v8_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v8')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v8')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v8')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v8')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v8_{timestamp}.xlsx')


# In[604]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v11_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v11_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v11_{timestamp}.pkl')
print(result_path + f'{task_name}_v11_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v11_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v11')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v11')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v11')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v11')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v11_{timestamp}.xlsx')


# In[707]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v12_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v12_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v12_{timestamp}.pkl')
print(result_path + f'{task_name}_v12_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v12')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v12')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v12')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v12')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v12_{timestamp}.xlsx')


# In[749]:



# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v13_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v13_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v13_{timestamp}.pkl')
print(result_path + f'{task_name}_v13_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v13')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v13')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v13')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v13')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v13_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[750]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[709]:


# lgb_model_v5 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v4_20250605181337.pkl')
# varsnamev5 = lgb_model_v5.feature_name()
# df_sample['y_prob_base_v5'] = lgb_model_v5.predict(df_sample[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[751]:


print(df_sample.columns[-17:].to_list())


# In[752]:


df_sample['high_p_m4d30_2506_v12'] = df_sample['y_prob_base_v12']
df_sample['high_p_m4d30_2506_v13'] = df_sample['y_prob_base_v13']


# In[754]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'target_mob4dpd30_1', 'channel_types', 'channel_rates', 'high_p_m4d30_2506_v12','high_p_m4d30_2506_v13','t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
print(len(usecols))
print(usecols)


# In[755]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[756]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[778]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5


# In[757]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# In[758]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_m3d30_2504', 'low_m3d30_2504','free_v1_fpd', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[759]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new', 'high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[760]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[761]:


# # lgb_model_v5 = load_model_from_pkl('./result/æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_v4_20250605181337.pkl')
# # varsnamev5 = lgb_model_v5.feature_name()
# df_evalue_all['y_prob_base_v5'] = lgb_model_v5.predict(df_evalue_all[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[762]:


# varsnamev4 = lgb_model.feature_name()
# df_evalue_all['y_prob_base_v4'] = lgb_model.predict(df_evalue_all[varsnamev4], num_iteration=lgb_model.best_iteration)


# In[763]:


# usecols_fpd30 = [col for col in usecols if col not in ['data_set', 'target_mob4dpd30_1']]
# df_evalue_fpd30 = df_evalue_all[usecols_fpd30]
# df_evalue_fpd30.info(show_counts=True)


# In[764]:


# df_evalue_fpd30['target_fpd30_1'].value_counts()


# In[765]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30.head()


# In[766]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_2.head()


# In[767]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_3.head()


# In[768]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_èåˆ')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_v13_{timestamp}.xlsx')


# In[482]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_èåˆ')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_ä¸‰æ–¹')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_ç¦»çº¿')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_èåˆ')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_ä¸‰æ–¹')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# ##### è°ƒç”¨å¾ä¿¡çš„æ¸ é“

# In[769]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[770]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p',  'low_m3d30_2504','free_m3d30_2504', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[771]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new','high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[772]:



model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[773]:


# df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc_fpd30.info(show_counts=True)


# In[774]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30.head()


# In[775]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_2.head()


# In[776]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_3.head()


# In[442]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
    df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_fpd30_èåˆ')
    df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_pd30_ä¸‰æ–¹')
    df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_fpd30_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx')


# In[777]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_èåˆ')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_ä¸‰æ–¹')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_ç¦»çº¿')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_v13_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[779]:


df_sample['apply_month'].value_counts()


# In[780]:


score = 'y_prob_base_v12'


# In[781]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[782]:


df_sample['score_bins'].head()


# In[783]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[784]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[785]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[787]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'").drop(columns=['ks_bin'])


# In[788]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_v12_{timestamp}.xlsx')


# In[496]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report.csv')


# In[789]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report_0610.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬mob4dpd30èåˆæ¨¡å‹_2409_2411_report_0610.csv')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾-Copy1.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,all_a_third_pdv3_fpd30_v_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1B0011_st_score
,M1B0012_st_score
,M1B0013_st_score
,M1B0014_st_score
,M1B0015_st_score
,ypy_bhxz_a_fpd30_v1_prob_good
,ypy_pboc_a_fpd7_v1_prob_good

-- ä¸‰æ–¹æ•°æ®å­åˆ†
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pd_unif_numberrisk_level_new
,ali_fraud_score3
,ali_fraud_score9
,tengxun_cash_score
,ppcm_behav_score
,umeng_score_v5
,dianhuabang_score
,duxiaoman_credit_score
,duxiaoman_cash_score
,haluo_cto_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,pudao_54
,tianchuang36
,tianchuang24
,baihang_13
,hengpu_7
,pudao_35
,pudao_78
,pudao_82
,pudao_84
,pudao_85
,rong360_4
,tengxun_1
,zhirongfen
,bh_lx_115
,baihang_31
,pudao_91
,bileizhenv1
,tianchuang_7
,hangliezhi_1
-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,pboc_dpd20
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,dz_fpd
,xz_fpd
,free_v1_fpd

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-07-01'
      and t2.apply_date <= '2024-11-30'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-07-01'
      and apply_date <= '2024-11-30'
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'all_a_bhdj_fpd10_v1_p' then variable_value else null end) as all_a_bhdj_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_br_derived_fpd30_202408_g_p' then variable_value else null end) as all_a_br_derived_fpd30_202408_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p' then variable_value else null end) as all_a_br_derived_v1_mob4dpd30_202502_st_p 
    ,max(case when variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p' then variable_value else null end) as all_a_br_derived_v2_fpd30_202411_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p' then variable_value else null end) as all_a_br_derived_v3_fpd30_202412_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v1_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v2_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
    ,max(case when variable_code = 'all_a_third_pdv3_fpd30_v_p' then variable_value else null end) as all_a_third_pdv3_fpd30_v_p 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_standard_BD003 
    ,max(case when variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1' then variable_value else null end) as HLV_D_HOLO_jk_certNo_score_fpd7_v1 
    ,max(case when variable_code = 'M1A0022_g_p' then variable_value else null end) as M1A0022_g_p 
    ,max(case when variable_code = 'M1A0023_g_p' then variable_value else null end) as M1A0023_g_p 
    ,max(case when variable_code = 'M1A0026_g_p' then variable_value else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027_g_p' then variable_value else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028_g_p' then variable_value else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1B0011_st_score' then variable_value else null end) as M1B0011_st_score 
    ,max(case when variable_code = 'M1B0012_st_score' then variable_value else null end) as M1B0012_st_score 
    ,max(case when variable_code = 'M1B0013_st_score' then variable_value else null end) as M1B0013_st_score 
    ,max(case when variable_code = 'M1B0014_st_score' then variable_value else null end) as M1B0014_st_score 
    ,max(case when variable_code = 'M1B0015_st_score' then variable_value else null end) as M1B0015_st_score 
    ,max(case when variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good' then variable_value else null end) as ypy_bhxz_a_fpd30_v1_prob_good 
    ,max(case when variable_code = 'ypy_pboc_a_fpd7_v1_prob_good' then variable_value else null end) as ypy_pboc_a_fpd7_v1_prob_good 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-07-01'
      and apply_time <= '2024-11-30'
      and variable_value is not null 
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------ä¸‰æ–¹æ•°æ®-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-07-01'
      and dt <= '2024-11-30'
    ) as t3 on t.order_no=t3.order_no

;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:5], varsname[-5:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[9]:


print(result_path)


# In[10]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[11]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob3dpd30'])['order_no'].count().unstack()


# In[12]:


df_sample_.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv')


# In[13]:


df_sample = df_sample_.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[14]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]

df_sample.loc[df_sample.query("apply_date>='2024-07-01' & apply_date<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot'


# In[15]:


df_sample.to_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv',index=False)
print(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv')


# In[16]:


target = 'target_mob3dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[24]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[25]:


print(df_sample[target].value_counts())


# In[26]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[27]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[28]:


task_name


# In[29]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[30]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[31]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1


# In[33]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[34]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[35]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[36]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[37]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[38]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[39]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[40]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:





# In[41]:


to_drop_iv


# In[42]:


to_drop_missing


# In[43]:


df_iv.loc[to_drop_iv,:]


# In[44]:


varsname_v1 = ['all_a_bhdj_fpd10_v1_p','all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p','all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p','all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','all_a_third_pdv3_fpd30_v_p','M1A0022_g_p','M1A0023_g_p','M1A0026_g_p','M1A0027_g_p','M1A0028_g_p','ypy_bhxz_a_fpd30_v1_prob_good','ypy_pboc_a_fpd7_v1_prob_good','duxiaoman_6','hengpu_4','aliyun_5','feicuifen','hengpu_5','rong360_4','tengxun_1','zhirongfen','bileizhenv1','tianchuang_7','hangliezhi_1','br_fpd','br_mob4','br_fpd_2','br_mob4_2','br_v3_fpd','br_v3_mob4','pboc_dpd20','dz_fpd','xz_fpd']


# In[47]:


varsname_v1 = [f'{col}'.lower() for col in varsname_v1]


# In[48]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[49]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[50]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[51]:


df_iv.loc[to_drop2,:]


# In[53]:


to_drop2 = ['all_a_third_pdv3_fpd30_v_p']
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[54]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[55]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[56]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[57]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[59]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[60]:


new_bins_dict


# In[61]:


combiner.update(new_bins_dict)


# In[62]:


combiner.export()


# In[63]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[64]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[67]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[68]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[69]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[70]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[71]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[72]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[73]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[74]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[75]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[76]:


to_drop3 = []
varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[77]:


target


# In[78]:


df_bins.shape
df_bins.head()


# In[79]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[80]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[81]:


df_sample_woe.head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[83]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[84]:


df_corr_matrix.head()


# In[85]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[86]:


df_high_corr


# In[87]:


print(to_drop4)


# In[88]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[89]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[91]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[92]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[93]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[94]:


to_drop5


# In[95]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[96]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[97]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[98]:


# df_sample = pd.read_csv(r'æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2409_2411.csv')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[99]:


target


# In[100]:


modeltrian_target = 'target_mob3dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[101]:


df_sample[target].value_counts()


# In[102]:


df_sample[modeltrian_target].value_counts()


# In[103]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[104]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[105]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[106]:


df_sample['channel_types'].value_counts()


# In[107]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[108]:


### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.1
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 10


# In[109]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[110]:


print(len(varsname_v5))
print(varsname_v5)


# In[111]:


varsname_base = varsname_v5[:]


# In[112]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[113]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[114]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_all'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_all'].head()


# In[115]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_all', 'apply_month')
df_ks_auc_month_v1


# In[117]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_all', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[119]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[120]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[122]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[123]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[124]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[129]:


df_ksauc_all = pd.DataFrame()
for col in varsname_v1:
    model_score = ['y_prob_base_all']
    vars_score = [col]

    score_list = model_score + vars_score
    target_list = ['target_mob3dpd30_1']
    labels_models_dict = {target: score_list for target in target_list}
    
    tmp_df_evalue = df_sample.loc[df_sample[score_list].notna().all(axis=1),:]

    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    df_ksauc_all = pd.concat([df_ksauc_all, df_ksauc_all_1], axis=0)


# In[97]:


df_ksauc_all_1.to_excel(r'baseæ¨¡å‹åŠ å…¥ä¸åŒå­åˆ†æ—¶èåˆæ¨¡å‹çš„æ•ˆæœå¯¹æ¯”_cpd30.xlsx')


# In[95]:


df_sample_new['fpd30_1'].value_counts()


# In[96]:


df_sample_new['fpd30_2'].value_counts()


# In[563]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p',
               'all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p',
               'ypy_bhxz_a_fpd30_v1_prob_good','xz_fpd']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[564]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['m1a0022_g_p','m1a0023_g_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)


df_ksauc_all_3 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[565]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd',
               'all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[566]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['score_fpd0_v1','score_fpd6_v1','score_fpd10_v1','score_fpd10_v2','score_fpd30_v1',
               't_mix_pboc2_dpd20','t_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[567]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[323]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# In[324]:


df_sample.to_csv(result_path + r'æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2409_2411.csv',index=False)


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[65]:


df_sample['apply_month'].value_counts()


# In[66]:


score = 'y_prob_base2'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample_30.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-08'"), 
                                                [score], 'apply_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# è®¾ç½®æ•°æ®å­˜å‚¨
task_name = 'æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # å‡½æ•°å®šä¹‰

# In[3]:


# è·å–æ•°æ®
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # è·å–cpuæ ¸çš„æ•°é‡
    n_process = multiprocessing.cpu_count()
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('å¼€å§‹è·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    instance = conn.execute_sql(sql)
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    with instance.open_reader() as reader:
        print('-------æ•°æ®å¼€å§‹è½¬æ¢ä¸ºDataFrame--------')
        data = reader.to_pandas(n_process=n_process) # å¤šæ ¸å¤„ç†ï¼Œé¿å…å•æ ¸å¤„ç†

    end = time.time()
    print('ç»“æŸè·‘æ•°ï¼š' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   

    return data

# æ’å…¥æ•°æ®
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # è¾“å…¥è´¦å·å¯†ç 
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('å¼€å§‹è·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # æ‰§è¡Œè„šæœ¬
    conn.execute_sql(sql)
    end = time.time()
    print('ç»“æŸè·‘æ•°' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("è¿è¡Œäº‹ä»¶ï¼š{}ç§’".format(end-start))   


# # 0. æ•°æ®è¯»å–

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,all_a_third_pdv3_fpd30_v_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1B0011_st_score
,M1B0012_st_score
,M1B0013_st_score
,M1B0014_st_score
,M1B0015_st_score
,ypy_bhxz_a_fpd30_v1_prob_good
,ypy_pboc_a_fpd7_v1_prob_good

-- ä¸‰æ–¹æ•°æ®å­åˆ†
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pd_unif_numberrisk_level_new
,ali_fraud_score3
,ali_fraud_score9
,tengxun_cash_score
,ppcm_behav_score
,umeng_score_v5
,dianhuabang_score
,duxiaoman_credit_score
,duxiaoman_cash_score
,haluo_cto_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,pudao_54
,tianchuang36
,tianchuang24
,baihang_13
,hengpu_7
,pudao_35
,pudao_78
,pudao_82
,pudao_84
,pudao_85
,rong360_4
,tengxun_1
,zhirongfen
,bh_lx_115
,baihang_31
,pudao_91
,bileizhenv1
,tianchuang_7
,hangliezhi_1
-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,pboc_dpd20
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,dz_fpd
,xz_fpd
,free_v1_fpd

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-07-01'
      and t2.apply_date <= '2024-11-30'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-07-01'
      and apply_date <= '2024-11-30'
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'all_a_bhdj_fpd10_v1_p' then variable_value else null end) as all_a_bhdj_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_br_derived_fpd30_202408_g_p' then variable_value else null end) as all_a_br_derived_fpd30_202408_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p' then variable_value else null end) as all_a_br_derived_v1_mob4dpd30_202502_st_p 
    ,max(case when variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p' then variable_value else null end) as all_a_br_derived_v2_fpd30_202411_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p' then variable_value else null end) as all_a_br_derived_v3_fpd30_202412_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v1_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v2_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
    ,max(case when variable_code = 'all_a_third_pdv3_fpd30_v_p' then variable_value else null end) as all_a_third_pdv3_fpd30_v_p 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_standard_BD003 
    ,max(case when variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1' then variable_value else null end) as HLV_D_HOLO_jk_certNo_score_fpd7_v1 
    ,max(case when variable_code = 'M1A0022_g_p' then variable_value else null end) as M1A0022_g_p 
    ,max(case when variable_code = 'M1A0023_g_p' then variable_value else null end) as M1A0023_g_p 
    ,max(case when variable_code = 'M1A0026_g_p' then variable_value else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027_g_p' then variable_value else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028_g_p' then variable_value else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1B0011_st_score' then variable_value else null end) as M1B0011_st_score 
    ,max(case when variable_code = 'M1B0012_st_score' then variable_value else null end) as M1B0012_st_score 
    ,max(case when variable_code = 'M1B0013_st_score' then variable_value else null end) as M1B0013_st_score 
    ,max(case when variable_code = 'M1B0014_st_score' then variable_value else null end) as M1B0014_st_score 
    ,max(case when variable_code = 'M1B0015_st_score' then variable_value else null end) as M1B0015_st_score 
    ,max(case when variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good' then variable_value else null end) as ypy_bhxz_a_fpd30_v1_prob_good 
    ,max(case when variable_code = 'ypy_pboc_a_fpd7_v1_prob_good' then variable_value else null end) as ypy_pboc_a_fpd7_v1_prob_good 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-07-01'
      and apply_time <= '2024-11-30'
      and variable_value is not null 
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------ä¸‰æ–¹æ•°æ®-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-07-01'
      and dt <= '2024-11-30'
    ) as t3 on t.order_no=t3.order_no

;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:5], varsname[-5:])
print("åˆå§‹ç‰¹å¾å˜é‡ä¸ªæ•°ï¼š",len(varsname))


# In[9]:


print(result_path)


# In[10]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======ç¬¬{i}ä¸ªå˜é‡ï¼š{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[11]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob3dpd30'])['order_no'].count().unstack()


# In[12]:


df_sample_.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv')


# In[13]:


df_sample = df_sample_.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[25]:


df_sample = pd.read_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv')


# In[26]:


varsname = ['all_a_bhdj_fpd10_v1_p','all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p','all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p','all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','all_a_third_pdv3_fpd30_v_p','M1A0022_g_p','M1A0023_g_p','M1A0026_g_p','M1A0027_g_p','M1A0028_g_p','ypy_bhxz_a_fpd30_v1_prob_good','ypy_pboc_a_fpd7_v1_prob_good','duxiaoman_6','hengpu_4','aliyun_5','feicuifen','hengpu_5','rong360_4','tengxun_1','zhirongfen','bileizhenv1','tianchuang_7','hangliezhi_1','br_fpd','br_mob4','br_fpd_2','br_mob4_2','br_v3_fpd','br_v3_mob4','pboc_dpd20','dz_fpd','xz_fpd']
varsname = [f'{col}'.lower() for col in varsname]
print(len(varsname))


# In[27]:


df_sample.columns.to_list()[0:9]


# In[28]:


df_sample = df_sample[df_sample.columns.to_list()[0:9] + varsname]
df_sample = df_sample.reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[29]:


df_sample = df_sample.query("apply_date>='2024-08-01'")
df_sample = df_sample.reset_index(drop=True)


# In[30]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-10-31'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot'


# In[31]:


df_sample.to_csv(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2.csv',index=False)
print(result_path + 'model_æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2.csv')


# In[32]:


df_sample.info(show_counts=True)


# In[33]:


target = 'target_mob3dpd30'


# In[ ]:





# # 1. æ ·æœ¬æ¦‚å†µ

# In[34]:


def get_target_summary(df, target, groupby_col):
    """
    å¯¹ DataFrame è¿›è¡Œåˆ†ç»„èšåˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæ±‡æ€»è¡Œã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: ç”¨äºåˆ†ç»„çš„åˆ—å
    - agg_cols: å­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯èšåˆå‡½æ•°åç§°ï¼ˆå¦‚ 'count', 'sum', 'mean'ï¼‰
    - new_col_name: å­—å…¸,é”®æ˜¯æ—§åˆ—çš„åç§°ï¼Œå€¼æ˜¯æ–°åˆ—çš„åç§°
    
    è¿”å›:
    - åŒ…å«åˆ†ç»„èšåˆç»“æœå’Œæ±‡æ€»è¡Œçš„æ–° DataFrame
    """
    # ä½¿ç”¨ groupby å’Œ agg è¿›è¡Œåˆ†ç»„å’Œèšåˆ
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # å°†æ±‡æ€»è¡Œæ·»åŠ åˆ°åˆ†ç»„ç»“æœä¸­
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # è¿”å›ç»“æœ
    return result


# In[35]:


print(df_sample[target].value_counts())


# In[36]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[37]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[40]:


task_name = 'æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2'


# In[41]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"1_æ ·æœ¬æ¦‚å†µ_{task_name}_{timestamp}.xlsx")


# # 2.æ•°æ®æ¢ç´¢æ€§åˆ†æ

# In[42]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1ç¼ºå¤±å€¼å¤„ç†

# In[43]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[44]:


# 2.1 å˜é‡åˆ†å¸ƒ
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1


# In[45]:


# 2.2 æ·»åŠ æœ€é«˜å æ¯”
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[46]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")

print(f"æ•°æ®å­˜å‚¨å®Œæˆ: {timestamp}")
print(result_path + f"2_å˜é‡åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx")


# ## 2.2 æ•°æ®æ¢ç´¢

# In[47]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯åˆ—çš„ç¼ºå¤±ç‡ã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame
    - groupby_col: åˆ†ç»„çš„åˆ—å
    - columns: éœ€è¦è®¡ç®—ç¼ºå¤±ç‡çš„åˆ—ååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ç¼ºå¤±ç‡çš„æ–° DataFrame
    """
    # åˆ†ç»„å¹¶è®¡ç®—ç¼ºå¤±ç‡
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[48]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æœˆåˆ†å¸ƒ
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[49]:


# 2.2 ç¼ºå¤±ç‡æŒ‰æ•°æ®é›†åˆ†å¸ƒ
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[50]:


# 2.3 å¿«é€ŸæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[51]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}")
print(result_path + f'2_æ•°æ®æ¢ç´¢æ€§åˆ†æ_{task_name}_{timestamp}.xlsx')


# # 3.ç‰¹å¾ç²—ç­›é€‰

# ## 3.1 åŸºäºè‡ªèº«å±æ€§åˆ é™¤å˜é‡

# In[52]:


# åˆ é™¤è¿‘æœŸä¸å¯ä½¿ç”¨çš„ç‰¹å¾(æœ€è¿‘æœˆä»½çš„ç¼ºå¤±ç‡å¤§äºç­‰äº0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# åˆ é™¤ç¼ºå¤±ç‡å¤§äº0.95/åˆ é™¤æšä¸¾å€¼åªæœ‰ä¸€ä¸ª/åˆ é™¤æ–¹å·®ç­‰äº0/åˆ é™¤é›†ä¸­åº¦å¤§äº0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"åˆ é™¤çš„å˜é‡æœ‰{len(to_drop1)}ä¸ª")


# In[ ]:





# In[53]:


to_drop_iv


# In[54]:


to_drop_missing


# In[55]:


df_iv.loc[to_drop_iv,:]


# In[56]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v1)}ä¸ª")
print(varsname_v1[:10])


# ## 3.2 åŸºäºç›¸å…³æ€§åˆ é™¤å˜é‡
# 

# In[57]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[58]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[59]:


df_iv.loc[to_drop2,:]


# In[60]:



varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v2)}ä¸ª")


# # 4.ç‰¹å¾ç»†ç­›é€‰

# ## 4.1 åŸºäºå˜é‡ç¨³å®šæ€§ç­›é€‰

# In[61]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    è®¡ç®—æ¯ä¸ªæœˆæ¯çš„psiã€‚
    
    å‚æ•°:
    - df_actual: æµ‹è¯•é›†
    - df_expect: è®­ç»ƒé›†
    - cols: éœ€è¦è®¡ç®—ç¨³å®šæ€§çš„åˆ—ååˆ—è¡¨
    - month_col: åˆ†ç»„çš„åˆ—å

    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆçš„æ–° DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # åˆå¹¶æ‰€æœ‰ç»“æœ DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    è®¡ç®—æ¯ä¸ªå˜é‡æ¯ä¸ªæœˆçš„ivã€‚
    
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦è®¡ç®—ivçš„åˆ—ååˆ—è¡¨
    - month_colï¼šæœˆä»½åˆ—å
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    å‚æ•°:
    - df: å¾…å¤„ç†çš„ DataFrame,å·²åˆ†ç®±
    - target: Yæ ‡ç­¾
    - cols: éœ€è¦åˆ†ç®±çš„åˆ—ååˆ—è¡¨
    - group_colï¼šåˆ†ç»„åˆ—åï¼Œå¦‚æœˆä»½ã€æ¸ é“ã€æ•°æ®ç±»å‹
    
    è¿”å›:
    - åŒ…å«æ¯ä¸ªæœˆæ¯åˆ—ivçš„æ–° DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[62]:



# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œçš„åä¸€è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#åå®¢æˆ·
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# åˆ é™¤å½“å‰ç´¢å¼•å€¼æ‰€åœ¨è¡Œ(æŒ‰ä»å°åˆ°å¤§æ’åº,åˆå¹¶éƒ½æ˜¯ä¿ç•™è¾ƒå°å€¼)ï¼Œé…åˆå·¦é—­å³å¼€
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#åå®¢æˆ·
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#å¥½å®¢æˆ·
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# åˆ é™¤/åˆå¹¶å®¢æˆ·æ•°ä¸º0çš„ç®±å­
def MergeZero(np_regroup):
#åˆå¹¶å¥½åå®¢æˆ·æ•°è¿ç»­éƒ½ä¸º0çš„ç®±å­
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#åˆå¹¶åå®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#åˆå¹¶å¥½å®¢æˆ·æ•°ä¸º0çš„ç®±å­
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#ç®±å­æœ€å°å æ¯”
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª,æœ€å°ç®±å­å æ¯”{min_pct}")
        break
    if min_pct>=threshold:
        print(f"ç®±å­æœ€å°å æ¯”ï¼šå„ç®±å­çš„æ ·æœ¬å æ¯”æœ€å°å€¼: {threshold}ï¼Œå·²æ»¡è¶³è¦æ±‚")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# ç®±å­çš„å•è°ƒæ€§
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("ç®±å­å•è°ƒæ€§ï¼šç®±å­æ•°è¾¾åˆ°æœ€å°å€¼2ä¸ª")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #ç¡®å®šæ˜¯å¦å•è°ƒ
    if_Montone = len(set(BadRateMonetone))
    #åˆ¤æ–­è·³å‡ºå¾ªç¯
    if if_Montone==1:
        print("ç®±å­å•è°ƒæ€§ï¼šå„ç®±å­çš„åæ ·æœ¬ç‡å•è°ƒ")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# å˜é‡åˆ†ç®±ï¼Œè¿”å›åˆ†å‰²ç‚¹ï¼Œç‰¹æ®Šå€¼ä¸å‚ä¸åˆ†ç®±
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#åŒºé—´å·¦é—­å³å¼€
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# åˆ¤æ–­é‡æ–°åˆ†ç®±åæœ€é«˜é›†ä¸­åº¦å æ¯”
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# åˆ¤æ–­ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹æ˜¯å¦æœ€å°å€¼
if df[col].min()==cutoffpoints[0]:
    print(f"å˜é‡{col}ï¼šæœ€å°å€¼æ‰€åœ¨ç®±å­æ²¡æœ‰è¢«åˆå¹¶è¿‡")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[63]:


# è®¡ç®—åˆ†å¸ƒå‰å…ˆå˜é‡åˆ†ç®±
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[64]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[65]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======ç¬¬{i+1}ä¸ªå˜é‡ï¼š{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # ç¡®ä¿åˆ†ç®±æ— 0å€¼ï¼Œå•è°ƒï¼Œæœ€å°å æ¯”ç¬¦åˆè¦æ±‚
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # æ–°çš„åˆ†ç®±åˆ†å‰²ç‚¹ï¼Œç¬¦åˆtoadåŒ…è¦æ±‚
    new_bins_dict[col] = cutbins + empty
    # åˆ é™¤é‡æ–°åˆ†ç®±åï¼Œé«˜åº¦é›†ä¸­çš„å˜é‡
    if is_drop_mode:
        print(f"{col}é‡æ–°åˆ†ç®±åï¼Œé›†ä¸­åº¦å æ¯”è¶…95%")
        to_drop_mode.append(col)


# In[66]:


new_bins_dict


# In[67]:


combiner.update(new_bins_dict)


# In[68]:


combiner.export()


# In[69]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'å˜é‡åˆ†ç®±å­—å…¸_{timestamp}.pkl')


# In[70]:


# è®¡ç®—psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[71]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[72]:


# è®¡ç®—iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[73]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[74]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[75]:


# è®¡ç®—total_pct å’Œ # bad_rate ä»¥åŠivçš„æ—¶é—´åˆ†å¸ƒ
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### åˆ é™¤ä¸ç¨³å®šç‰¹å¾

# In[76]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[77]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("å‰”é™¤çš„å˜é‡æœ‰: ", len(to_drop3))


# In[78]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[79]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[80]:


to_drop3 = ['ypy_pboc_a_fpd7_v1_prob_good']
varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"ä¿ç•™çš„å˜é‡æœ‰{len(varsname_v3)}ä¸ª: ")


# ## 4.2 Yæ ‡ç­¾ç›¸å…³æ€§åˆ é™¤

# In[81]:


target


# In[82]:


df_bins.shape
df_bins.head()


# In[83]:



# def calculate_woe(df, col, target):
#     """
#     è®¡ç®—ç»™å®šåˆ†ç®±åˆ—çš„WOEå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºåç»­æ˜ å°„ã€‚
#     :param df: DataFrame åŒ…å«åˆ†ç®±å’Œç›®æ ‡å˜é‡
#     :param binned_col: åˆ†ç®±å˜é‡å
#     :param target_col: ç›®æ ‡å˜é‡å
#     :return: WOEå€¼çš„å­—å…¸
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[84]:


# è®¡ç®—ç›¸å…³æ€§
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[85]:


df_sample_woe.head()


# In[86]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºæŒ‡å®šé˜ˆå€¼çš„å˜é‡å¯¹ï¼Œå¹¶æ’é™¤å¯¹è§’çº¿ã€‚ä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ã€‚

    :param df: è¾“å…¥çš„DataFrame
    :param iv_series: åŒ…å«æ¯ä¸ªå˜é‡çš„IVå€¼çš„Seriesï¼Œå˜é‡åä¸ºè¡Œç´¢å¼•
    :param threshold: ç›¸å…³ç³»æ•°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.80
    :return: åŒ…å«é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°çš„DataFrameï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    """
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df.copy()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨é«˜ç›¸å…³æ€§å˜é‡å¯¹
    high_corr_pairs = []
    # éå†ç›¸å…³ç³»æ•°çŸ©é˜µ
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨éœ€è¦åˆ é™¤çš„å˜é‡
    to_remove = set()
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè®°å½•è¢«å‰”é™¤çš„å˜é‡ï¼ˆå¯¹åº”æ¯ä¸€è¡Œï¼‰
    removed_vars = []
    # éå†é«˜ç›¸å…³æ€§å˜é‡å¯¹ï¼Œä¿ç•™IVå€¼è¾ƒå¤§çš„å˜é‡ï¼Œåˆ é™¤IVå€¼è¾ƒå°çš„å˜é‡
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # è¿”å›é«˜ç›¸å…³æ€§å˜é‡å¯¹åŠå…¶ç›¸å…³ç³»æ•°ï¼Œä»¥åŠä¿ç•™çš„å˜é‡
    return high_corr_df, list(to_remove)


# In[87]:


# param method: è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æ˜¯'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[88]:


df_corr_matrix.head()


# In[89]:


# è°ƒç”¨å‡½æ•°

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.70)

# æŸ¥çœ‹ç»“æœ
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop4))


# In[90]:


df_high_corr


# In[91]:


print(to_drop4)


# In[92]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"ä¿ç•™å˜é‡{len(varsname_v4)}ä¸ª")
print(varsname_v4)


# In[93]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # æŒ‰æŒ‡å®šçš„åˆ†ç»„åˆ—åˆ†ç»„
    grouped = df.groupby(group_col)
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„ç›¸å…³ç³»æ•°
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # éå†æ¯ä¸ªåˆ†ç»„
    for name, group in grouped:      
        # è®¡ç®—æ¯ä¸ªå˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨ç»“æœ
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # è®¡ç®—æ¯ä¸ªè¿ç»­å˜é‡ä¸äºŒåˆ†ç±»å˜é‡ä¹‹é—´çš„ç‚¹äºŒåˆ—ç›¸å…³ç³»æ•°
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # å°†ç»“æœæ·»åŠ åˆ°æ€»çš„DataFrameä¸­ï¼Œå¹¶æ·»åŠ åˆ†ç»„æ ‡è¯†
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # è¿”å›åŒ…å«æ‰€æœ‰åˆ†ç»„ç›¸å…³ç³»æ•°çš„DataFrame
    return (all_corrs, all_pvalue)


# In[94]:


# è°ƒç”¨å‡½æ•°
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# æŸ¥çœ‹å‰å‡ è¡Œ
# df_corr_vars_target


# In[95]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[96]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("åˆ é™¤çš„å˜é‡æœ‰ï¼š", len(to_drop5))


# In[97]:


to_drop5


# In[98]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"ä¿ç•™çš„å˜é‡{len(varsname_v5)}ä¸ª")


# In[99]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"æ•°æ®å­˜å‚¨å®Œæˆæ—¶é—´ï¼š{timestamp}ï¼")        
print(result_path + f'3_å˜é‡åˆ†æ_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.æ¨¡å‹è®­ç»ƒ

# ## 5.0 å‡½æ•°å®šä¹‰

# In[100]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): å«æœ‰Yæ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°çš„æ•°æ®é›†
        target (string): Yæ ‡ç­¾åˆ—å
        y_pred (string): åæ¦‚ç‡åˆ†æ•°åˆ—å
        group_col (string): åˆ†ç»„åˆ—åå¦‚æœˆä»½ï¼Œæ•°æ®é›†

    Returns:
        dataframe: AUCå’ŒKSå€¼çš„æ•°æ®æ¡†
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # è®¡ç®— AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}ï¼šKSå€¼{ks_}ï¼ŒAUCå€¼{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("è¿™æ˜¯åŸç”Ÿæ¥å£çš„æ¨¡å‹ (Booster)")
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # è·å–ç‰¹å¾åç§°
        feature_names = model.feature_name()
        # å°†ç‰¹å¾é‡è¦æ€§è½¬æ¢ä¸ºæ•°æ®æ¡†
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("è¿™æ˜¯ sklearn æ¥å£çš„æ¨¡å‹")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("æœªçŸ¥æ¨¡å‹ç±»å‹")
        df_importance = None
    
    return df_importance


# Pickleæ–¹å¼ä¿å­˜å’Œè¯»å–æ¨¡å‹
def save_model_as_pkl(model, path):
    """
    ä¿å­˜æ¨¡å‹åˆ°è·¯å¾„path
    :param model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgbæ¨¡å‹ä¿å­˜.bin æ ¼å¼
def save_model_as_bin(model, save_file_path):
    #ä¿å­˜lgbæ¨¡å‹ä¸ºbinæ ¼å¼
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    ä»è·¯å¾„pathåŠ è½½æ¨¡å‹
    :param path: ä¿å­˜çš„ç›®æ ‡è·¯å¾„
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='é‡‘ç§‘æ¸ é“'
    elif x==1:
        channel='æ¡”å­å•†åŸ'
    else:
        channel='apiæ¸ é“'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227æ¸ é“'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24åˆ©ç‡'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36åˆ©ç‡'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # æœ€åˆè¯„ä¼°æ¨¡å‹æ•ˆæœ 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['æ¸ é“'] = 'å…¨æ¸ é“'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['æ¸ é“'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # åˆå¹¶
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 æ•°æ®é¢„å¤„ç†

# In[101]:


# df_sample = pd.read_csv(r'æç°å…¨æ¸ é“æ— æˆæœ¬å­åˆ†èåˆæ¨¡å‹fpd30æ ‡ç­¾_2409_2411.csv')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[102]:


target


# In[103]:


modeltrian_target = 'target_mob3dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[104]:


df_sample[target].value_counts()


# In[105]:


df_sample[modeltrian_target].value_counts()


# In[106]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample['data_set'].value_counts()


# In[107]:


# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†
df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[108]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[109]:


df_sample['channel_types'].value_counts()


# In[110]:


df_sample['channel_rates'].value_counts()


# ## 5.2 æ¨¡å‹è®­ç»ƒ

# ### 5.2.1 baseæ¨¡å‹

# In[111]:


# ### æ¨¡å‹å‚æ•°
# opt_params = {}
# opt_params['boosting'] = 'gbdt'
# opt_params['objective'] = 'binary'
# opt_params['metric'] = 'auc'
# opt_params['bagging_freq'] = 1
# opt_params['scale_pos_weight'] = 1 
# opt_params['seed'] = 1 
# opt_params['num_threads'] = -1 
# # è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
# opt_params['learning_rate'] = 0.1
# ## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
# opt_params['bagging_fraction'] = 0.8628008772208227     
# opt_params['feature_fraction'] = 0.6177619614753441
# opt_params['lambda_l1'] = 0
# opt_params['lambda_l2'] = 300
# opt_params['early_stopping_rounds'] = 50

# # è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
# opt_params['num_leaves'] = 21
# opt_params['min_data_in_leaf'] = 103
# opt_params['max_depth'] = 2
# # è°ƒå‚åçš„å…¶ä»–å‚
# opt_params['min_gain_to_split'] = 10

### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 0.60


# In[112]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[113]:


print(len(varsname_v5))
print(varsname_v5)


# In[114]:


varsname_base = varsname_v5[:]


# In[115]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[116]:


df_sample['data_set'].value_counts()


# In[117]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[118]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[119]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[120]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[121]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[122]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[123]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 å‚æ•°ä¼˜åŒ–

# In[124]:


# ### æ¨¡å‹å‚æ•°
# opt_params = {}
# opt_params['boosting'] = 'gbdt'
# opt_params['objective'] = 'binary'
# opt_params['metric'] = 'auc'
# opt_params['bagging_freq'] = 1
# opt_params['scale_pos_weight'] = 1 
# opt_params['seed'] = 1 
# opt_params['num_threads'] = -1 
# # è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
# opt_params['learning_rate'] = 0.1
# ## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
# opt_params['bagging_fraction'] = 0.8628008772208227     
# opt_params['feature_fraction'] = 0.6177619614753441
# opt_params['lambda_l1'] = 0
# opt_params['lambda_l2'] = 300
# opt_params['early_stopping_rounds'] = 50

# # è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
# opt_params['num_leaves'] = 21
# opt_params['min_data_in_leaf'] = 103
# opt_params['max_depth'] = 2
# # è°ƒå‚åçš„å…¶ä»–å‚
# opt_params['min_gain_to_split'] = 10

### æ¨¡å‹å‚æ•°
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# è°ƒå‚æ—¶è®¾ç½®æˆä¸ç”¨è°ƒå‚çš„å‚æ•°
opt_params['learning_rate'] = 0.02
## æ­£åˆ™å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# è°ƒå‚åçš„å‚æ•°éœ€è¦å˜æˆæ•´æ•°å‹
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 2
# è°ƒå‚åçš„å…¶ä»–å‚
opt_params['min_gain_to_split'] = 0.60


# In[125]:


print("æœ€ä¼˜å‚æ•°opt_params: ", opt_params)


# In[126]:


print(len(varsname_v5))
print(varsname_v5)


# In[127]:


varsname_base = varsname_v5[:]


# In[128]:


# ç¡®å®šæ•°æ®é›†å‚æ•°åï¼Œè®­ç»ƒæ¨¡å‹

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[129]:


df_sample['data_set'].value_counts()


# In[130]:


# 6ï¼Œè®­ç»ƒ/ä¿å­˜/è¯„ä¼°æ¨¡å‹
# ä¼˜åŒ–è®­ç»ƒæ¨¡å‹
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[131]:


# ä¼˜åŒ–åè¯„ä¼°æ¨¡å‹æ•ˆæœ
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[132]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[133]:


df_ks_auc_set_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
df_ks_auc_set_v2


# In[137]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[138]:


# æ¨¡å‹å˜é‡é‡è¦æ€§
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[139]:


# æ•ˆæœè¯„ä¼°åä¿å­˜æ¨¡å‹
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"æ¨¡å‹ä¿å­˜å®Œæˆï¼ï¼š{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_æ¨¡å‹è®­ç»ƒ_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 æ¨¡å‹æ•ˆæœå¯¹æ¯”

# In[140]:


df_sample.info(show_counts=True)


# ### 5.3.1æ•°æ®å¤„ç†

# In[142]:


df_sample_all = pd.read_csv(result_path + 'æˆä¿¡å…¨æ¸ é“é«˜æˆæœ¬å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾.csv')
df_sample_all = df_sample_all.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)


# In[143]:


usecols = ['order_no','sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'mix_pboc_dpd20'
,'gen7_fpd'
,'gen7_mob4'
,'free_v1_fpd']


# In[193]:


sql = """
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- æ·±åœ³å›¢é˜Ÿå­åˆ†
,good_score as M1A0030_g_p
,M1A0029_g_p

-- åŒ—äº¬å›¢é˜Ÿæ¨¡å‹å­åˆ†
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,free_v1_fpd
,low_v2_fpd30

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-02-10'
    ) as t 
-- åŒ—äº¬å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-02-10'
    ) as t1 on t.order_no=t1.order_no

-- æ·±åœ³å›¢é˜Ÿçš„å­åˆ†
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'M1A0029_g_p' then variable_value else null end) as M1A0029_g_p 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-02-10'
      and variable_value is not null 
      and variable_code = 'M1A0029_g_p'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
left join 
    (
    select 
     order_no
    ,good_score
    from znzz_fintech_ads.lxl_a_pboc_submodel_merge_model_mob3pdp30_score_auth_v2 as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-02-10'
    ) as t3 on t.order_no=t3.order_no    
;
"""
df_1 = get_data(sql)


# In[194]:


df_1.info(show_counts=True)


# In[185]:


df_evalue_copy = df_evalue.copy()


# In[186]:


df_evalue.shape


# In[187]:


df_evalue = pd.merge(df_evalue_copy, df_1[['order_no','m1a0029_g_p']],  how='inner', on='order_no')
df_evalue.shape


# In[202]:


df_evalue = df_1.query("channel_id!=1")
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)


# In[144]:


df_evalue = pd.merge(df_sample, df_sample_all[usecols],  how='inner', on='order_no')
df_evalue.shape


# ### 5.3.2 æ•ˆæœå¯¹æ¯”

# In[146]:


# å°æ•°è½¬æ¢ç™¾åˆ†æ•°
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#æ¦‚ç‡åˆ†æ•°æ˜¯ååˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°ï¼Œè®¡ç®—æœ€å5%å®¢ç¾¤çš„lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("è¯·æ ¹æ®æ¦‚ç‡åˆ†æ•°æ˜¯å¥½åˆ†æ•°è¿˜æ˜¯ååˆ†æ•°ï¼Œå†³å®šåˆ†ä½æ•°çš„ä½ç½®")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# è®¡ç®—KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: åˆ†ç»„å­—æ®µ
    # model_score_label_dict: value: score_list: å¾—åˆ†å­—æ®µåˆ—è¡¨, key: label_list: æ ‡ç­¾å­—æ®µåˆ—è¡¨
    # df: æœ‰æ ‡ç­¾å’Œå¾—åˆ†çš„æ•°æ®æ¡†
    # è¾“å‡ºKSã€AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============å®Œæˆæ ‡ç­¾ï¼š{label_}===============')
    
    return ks_auc_result


# In[ ]:


df_ksauc_all = pd.DataFrame()
for col in varsname_base:
    model_score = ['y_prob_base_all']
    vars_score = [col]

    score_list = model_score + vars_score
    target_list = ['target_mob3dpd30_1']
    labels_models_dict = {target: score_list for target in target_list}
    
    tmp_df_evalue = df_sample.loc[df_sample[score_list].notna().all(axis=1),:]

    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    df_ksauc_all = pd.concat([df_ksauc_all, df_ksauc_all_1], axis=0)


# In[147]:


model_score = ['y_prob_base_v2']
vars_score = ['br_fpd'
,'br_mob4'
,'br_fpd_2'
,'br_mob4_2'
,'br_v3_fpd'
,'br_v3_mob4'
,'sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'gen7_fpd'
,'gen7_mob4'
,'xz_fpd'
,'all_a_br_derived_fpd30_202408_g_p'
,'all_a_br_derived_v1_mob4dpd30_202502_st_p'
,'all_a_br_derived_v2_fpd30_202411_g_p'
,'all_a_br_derived_v3_fpd30_202412_g_p'
,'ypy_bhxz_a_fpd30_v1_prob_good'
,'m1a0026_g_p'
,'m1a0027_g_p'
,'m1a0029_g_p']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[215]:


df_evalue.info()


# In[218]:


df_evalue['channel_types'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_rates'] = df_evalue['channel_id'].apply(channel_rate)


# In[219]:


model_score = ['m1a0030_g_p']
vars_score = ['m1a0029_g_p','gen7_fpd','gen7_mob4','low_v2_fpd30']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[220]:


df_ksauc_all_3.to_csv(r'æ•ˆæœå¯¹æ¯”_è¡¥å……æ•°æ®.csv',encoding='gbk')


# In[203]:


df_evalue['target_mob3dpd30'].value_counts()


# In[204]:


df_evalue['target_fpd30'].value_counts()


# In[205]:


df_evalue['apply_month'] = df_evalue['apply_date'].str[0:7]
df_evalue.loc[df_evalue['target_fpd30']==-1, 'target_fpd30']=np.nan
df_evalue.loc[df_evalue['target_mob3dpd30']==-1, 'target_mob3dpd30']=np.nan


# In[206]:


df_evalue['target_mob3dpd30'].value_counts()


# In[211]:


df_evalue['target_mob3dpd30_1'].value_counts()


# In[212]:


df_evalue['target_fpd30_1'].value_counts()


# In[207]:


df_evalue['target_fpd30'].value_counts()


# In[208]:


df_evalue['target_fpd30_1'] = 1-df_evalue['target_fpd30']
df_evalue['target_mob3dpd30_1'] = 1-df_evalue['target_mob3dpd30']


# In[213]:


model_score = ['y_prob_base_v2']
vars_score = ['m1a0022_g_p','m1a0023_g_p','m1a0026_g_p',',gen7_fpd','gen7_mob4','low_v2_fpd30']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[ ]:





# In[151]:


model_score = ['y_prob_base_v2']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[153]:


model_score = ['y_prob_base_v2']
vars_score = ['pboc_dpd20','mix_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()


# In[154]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_{timestamp}.xlsx')


# ##### è°ƒç”¨å¾ä¿¡çš„æ¸ é“

# In[221]:


tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue.info(show_counts=True)


# In[156]:


df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_pboc


# In[158]:


df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_pboc['æ¸ é“'] = 'å…¨æ¸ é“'
tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
df_ks_auc_set_pboc


# In[222]:


model_score = ['m1a0030_g_p']
vars_score = ['m1a0029_g_p','low_v2_fpd30','gen7_fpd','gen7_mob4']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2.head()


# In[225]:


df_ksauc_pboc_2_v2.to_csv('æ•ˆæœå¯¹æ¯”_å¾ä¿¡æ¸ é“_è¡¥å……æ•°æ®.csv',encoding='gbk')


# In[161]:


model_score = ['y_prob_base_v2']
vars_score = ['br_fpd'
,'br_mob4'
,'br_fpd_2'
,'br_mob4_2'
,'br_v3_fpd'
,'br_v3_mob4'
,'sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'gen7_fpd'
,'gen7_mob4'
,'xz_fpd'
,'all_a_br_derived_fpd30_202408_g_p'
,'all_a_br_derived_v1_mob4dpd30_202502_st_p'
,'all_a_br_derived_v2_fpd30_202411_g_p'
,'all_a_br_derived_v3_fpd30_202412_g_p'
,'ypy_bhxz_a_fpd30_v1_prob_good'
,'m1a0026_g_p'
,'m1a0027_g_p'
,'m1a0028_g_p'
,'all_a_rh_fpd0_v1_p'
,'all_a_rh_fpd10_v1_p'
,'all_a_rh_fpd10_v2_p'
,'all_a_rh_fpd30_v1_p'
,'all_a_rh_fpd6_v1_p'       
,'pboc_dpd20'
,'mix_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_pboc_2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2.head()


# In[162]:


model_score = ['y_prob_base_v2']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd','all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='å…¨æ¸ é“', allow_duplicates=False)

df_ksauc_pboc_4 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_4.head()


# In[177]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_pboc_2.to_excel(writer, sheet_name='df_ksauc_pboc_2')
    df_ksauc_pboc_4.to_excel(writer, sheet_name='df_ksauc_pboc_4')
    df_ks_auc_month_pboc.to_excel(writer, sheet_name='df_ks_auc_month_pboc')
    df_ks_auc_set_pboc.to_excel(writer, sheet_name='df_ks_auc_set_pboc')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼{timestamp}")
print(result_path + f'6_æ¨¡å‹æ•ˆæœå¯¹æ¯”_{task_name}_pboc_{timestamp}.xlsx')


# # 6. è¯„åˆ†åˆ†å¸ƒ

# In[ ]:





# In[163]:


df_sample['apply_month'].value_counts()


# In[164]:


score = 'y_prob_base_v2'


# In[165]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[166]:


df_sample['score_bins'].head()


# In[167]:


def get_model_psi(df, cols, month_col, combiner):
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æœˆä»½
    months = sorted(list(set(df[month_col])))
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame æ¥å­˜å‚¨ PSI å€¼
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæœˆä»½ä¸å…¶ä»–æœˆä»½ä¹‹é—´çš„PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # ä»åŸå§‹æ•°æ®é›†ä¸­æå–ç‰¹å®šæœˆä»½çš„æ•°æ®
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # è°ƒç”¨å‡½æ•°è®¡ç®—PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # å°†ç»“æœå­˜å…¥çŸ©é˜µ
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # å¯¹è§’çº¿ä¸Šçš„å€¼è®¾ä¸º NaN æˆ– 0ï¼Œè¡¨ç¤ºåŒä¸€æœˆä»½çš„ PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[169]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# æ‰“å°æœ€ç»ˆçš„ PSI çŸ©é˜µ
print(df_psi_matrix)


# In[170]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[173]:


score_group_by_dataset.query("groupvars=='3_oot' & bins!='Total'")


# In[174]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"æ•°æ®å­˜å‚¨å®Œæˆï¼:{timestamp}")
print(result_path + f'6_è¯„åˆ†åˆ†å¸ƒ_{task_name}_{timestamp}.xlsx')


# In[176]:


df_sample.to_csv(result_path + 'æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2.csv',index=False)
print(result_path + 'æˆä¿¡å…¨æ¸ é“å­åˆ†èåˆæ¨¡å‹ä¸‰æœŸæ ‡ç­¾v2.csv')


