# Auto-merged batch 4/4
# Total files in this batch: 57



#==============================================================================
# File: 人行特征变量一致性监控0905.py
#==============================================================================

# -*- coding: utf-8 -*-
"""
特征一致性监控脚本（支持从 txt 读取监控变量）
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import smtplib
import requests
import logging
import os
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

# =============================
# 1. 配置区
# =============================

# 网易企业邮箱配置
EMAIL_CONFIG = {
    'host': 'smtphz.qiye.163.com',
    'port': 465,
    'user': 'liaoxilin@hulianshuzhi.com',
    'password': 'Life2010.',
    'recipients': ['lianghuiyi@juzishuke.com','chendonggen@juzishuke.com','chenshengwen@juzishuke.com','tanxing@hulianshuzhi.com','jileilei@hulianshuzhi.com','liyi@hulianshuzhi.com','youpengyu@hulianshuzhi.com','liaoxilin@hulianshuzhi.com'],
    'title': '【人行特征一致性监控】{date}'
}

# 飞书机器人 Webhook
FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/8ac2ce64-33fb-45b6-9cad-11a530761ef9"

# 变量名文件路径（每行一个变量名）

VARIABLES_FILE = "/data/home/liaoxilin/数据监控/人行特征变量监控/pboc_variable_file_v2.txt"  # 支持相对路径或绝对路径
SCRIPT_DIR = Path(VARIABLES_FILE).parent.resolve()

VARS_FILE   = SCRIPT_DIR / "pboc_variable_file_v2.txt"
VARS_DESC_FILE   = SCRIPT_DIR / "人行变量清单250820.xlsx"
LOG_FILE    = SCRIPT_DIR / "consistency_monitor_v2.log"
OUTPUT_FILE = SCRIPT_DIR / "不一致性明细_v2.csv"

# 日期
EXECUTION_DATE = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
DS = EXECUTION_DATE.replace('-','')


# SQL：线上特征（含 value_006）
SQL_ONLINE = f"""
        select 
         t.order_no
        ,t.encrypt_id_card as id_no_des
        ,t.channel_id
        ,t.ds
        ,t.result_json as value_006
        ,t.create_time
        ,t.update_time
        from znzz_fintech_ods.ods_risk_jk_third_hl_jk_yhx_credit_authorization_pbc_dd  as t
        where t.ds="{DS}" 
          and result_json is not null
          and SUBSTR(create_time, 12, 8) < '23:59:40'
"""


# SQL：线下特征表（请根据实际修改）
SQL_OFFLINE = f"""

select
 t.order_no
,t.id_no_des
,acan_zbva_xavc_bbvf_n5
,acao_bbvb_ldve_n12
,acao_bbvb_ldve_n3
,acbg_zbva_bbvf_levg_n12
,acbg_zbva_bbvf_levg_n9
,acdb_zbve_n4
,agaa_zbva_xavh_bbvh_md
,agaa_zbva_xawd_bbvf_mf
,agaa_zbvg_xava_bbvh_mf
,agaa_zbvg_xavh_bbvf_mf
,agaa_zbvg_xawh_bbvh_md
,agaa_zbvg_xawi_bbvf_md
,acda_zbva_n1
,acda_zbvg_n5
,crab_zbva_beva_lavh_n11
,crab_zbva_beva_lavh_n9
,crad_lbvc_n9
,crad_lbvh_n9
,cral_zbva_xava_bbvf_men11
,cral_zbva_xava_bbvf_mfn3
,cral_zbva_xavc_bbvf_mdn10
,cral_zbva_xavc_bbvf_mdn11
,cral_zbva_xavc_bbvf_mdn5
,cral_zbva_xavc_bbvf_men3
,cral_zbva_xavc_bbvf_mfn2
,cral_zbva_xavh_bbvf_mfn10
,cral_zbva_xavh_bbvf_mfn3
,cral_zbva_xavh_bbvf_mfn5
,cral_zbva_xawd_bbvf_mcn10
,cral_zbva_xawd_bbvf_mcn11
,cral_zbva_xawd_bbvf_men4
,cral_zbva_xawd_bbvf_men5
,cral_zbva_xawe_bbvf_mcn9
,cral_zbva_xawe_bbvf_mfn11
,cral_zbva_xawh_bbvf_mcn11
,cral_zbva_xawh_bbvf_mfn3
,cral_zbvg_xava_bbvf_mdn11
,cral_zbvg_xava_bbvf_mfn11
,cral_zbvg_xava_bbvf_mfn2
,cral_zbvg_xavc_bbvf_mcn10
,cral_zbvg_xave_bbvf_mcn2
,cral_zbvg_xave_bbvf_mcn9
,cral_zbvg_xave_bbvf_mdn2
,cral_zbvg_xave_bbvf_mdn3
,cral_zbvg_xave_bbvf_mdn4
,cral_zbvg_xave_bbvf_men4
,cral_zbvg_xave_bbvf_mfn5
,cral_zbvg_xawd_bbvf_mcn10
,cral_zbvg_xawd_bbvf_mcn11
,cral_zbvg_xawd_bbvf_mdn3
,cral_zbvg_xawd_bbvf_mfn11
,cral_zbvg_xawe_bbvf_mcn5
,cral_zbvg_xawe_bbvf_men4
,cral_zbvg_xawh_bbvf_mcn10
,cral_zbvg_xawh_bbvf_mdn11
,cral_zbvg_xawh_bbvf_mdn9
,acaf_zbvb_bevg_n4
,acaf_zbvb_bevg_n5
,cram_bbvb_ldvn_mfn11
,cram_bbvf_ldvn_mcn11
,cram_bbvf_ldvn_mfn11
,cram_bbvh_ldve_mcn9
,cram_bbvh_ldvn_mfn11
,crbf_zcva_xeve
,crbf_zcvc_xeve
,deac_zbvg_n5
,deaa_zbva_xavc_bbvf_mco3
,deaa_zbva_xawd_bbvf_mdo2
,deaa_zbvg_xawd_bbvb_mco3
,deaa_zbvg_xawd_bbvf_mdo2
,redg_zbva_xavc_mc
,redg_zbva_xavc_md
,candlted12m10cnyua_cam
,candlted24m10cnycal
,candlted24m10cnyuas
,candlted24mbcnycal
,candlted24mnbcnyua_uuar
,candlted2m10ua_a9r
,candlted2mnbcnycal
,candlted3mnbcnycaa
,candltedallm30ua_a75r
,candltedallmnbcnyuam
,candltedallmnbcnyuual
,candnlted1m10cnyuual
,candnlted1mbcnycal
,candnlted2mnbcnycal
,candltdd_ed31dfl
,candlted3_6mnbmor
,candltrt_ed10dfl
,candltrt_ed10dfm
,candnlted12_24m10cnyua_ar
,candnlted12m10cnyuuas
,candnlted12m20cnyua_cam
,candnlted12mbcnyua_uuam_l
,candnlted12mbcnyuaa
,candnlted12mnbcnyuas
,candnlted12mnbeddfm
,candnlted1mnbeddfm
,candnlted24_allmbcnyuuar
,candnlted24_allmnbcnyuar
,candnlted24m10cnycla
,candnlted24m10cnycls
,candnlted24mbcnyua_uuam_l
,candnlted24mnbcnycla
,candnlted3m10cnycas
,candnlted3m10cnyua_clr
,candnlted3m10cnyuas
,candnlted3mbcnycal
,candnlted3mnbcnycal
,candnlted6_12mbcnyuar
,candnlted6m10cnycll
,candnltedallm10cnycla
,candnltedallm10cnyuuas
,candnltedallmnbcnyua_clr
,candnltedallmnbcnyuas
,candnltedallmnbua_a75c
,s02jstoas
,s06jhstoam
,s24jhstoam
,swwjstoam
,trrba11r
,dsdlldn
,tisr1anc
,q01_ncfc_qryorg_c
,q24_rfi_c
,d07_q03_fgcqc_oorg_r
,d07_q06_oorgqc_r
,d07_q09_bfiqc_r
,d07_q09_rfigea_nqi_r
,d15_q03_mfcqc_r
,d15_q09_rfigea_nqi_r
,d15_q24_fl_nqi_r
,q01_q03_nbfiqc_r
,q01_q06_nbfiqc_r
,q01_q24_fl_nqi_r
,q01_rfila_nqi_rrgr_r
,q03_q24_rficca_qc_r
,q03_tqc_ncfcqc_r
,q06_q24_cfcqc_r
,q06_q24_rficca_qc_r
,q06_tqi_cfc_nqi_r
,q06_tqi_rfila_nqi_r
,q09_tqi_rficca_nqi_r
,q12_bfiqc_rrgr_r
,q12_fgcqc_oorg_rrgr_r
,q12_q06_mfcqc_s
,q12_q24_nbfiqc_r
,q12_rfigea_qc_rrgr_r
,q24_q12_mfcqc_s
,mdal
,o01cno03cnr
,t03cnt12cnr
,t06cnt12cnr
,c03crchr
,c24crchr
,o12cwwxcdr
,owwcwwncdr
,o01callcho03callchr
,o12cwwchr
,t03crcht12crchr
,t06callcht12callchr
,pbocch01id2mcb
,pbocch01id4mcb
,pbocch02id5mc
,pbocch02id7mc
,pbocch03id20mc
,pbocch03id4mc
,pbocch03id6mc
,pbocch03id7mc
,pbocch06id10mc
,pbocch06id15mc
,pbocch06id20mc
,pbocch06id5mc
,pbocch12id10mc
,pbocch12id15mc
,pbocch12id20mc
,pbocch12id2mc
,pbocch12id2mcb
,pbocch12id3mc
,pbocch12id3mcb
,pbocch12id4mc
,pbocch12id7mc
,pbocchidc0102r
,pbocchidc0103r
,pbocchidc0203r
,pbocchidc0206r
,pbocchidc0306r
,pbocchidc0312r
,pbocchidc0612r
,pbocchidcb0103r
,pbocchidcb0206r
,pbocchidcb0306r
,pbocchidcb0312r
,pbocchidcb0612r


from 
    (
        select order_no,encrypt_id_card as id_no_des
        from znzz_fintech_ods.ods_risk_jk_third_hl_jk_yhx_credit_authorization_pbc_dd as t
        where t.ds="{DS}"
          and SUBSTR(create_time, 12, 8) < '23:59:40'
          and result_json is not null 
        group by order_no,encrypt_id_card
    ) as t

left join 
(
select t.*
from znzz_fintech_ads.dm_f_lxl_test_pboc_query_vars_01 as t 
where dt = "{EXECUTION_DATE}"
  
) as t1 on t.order_no = t1.order_no

left join 
(
select t.*
from znzz_fintech_ads.dm_f_lxl_test_pboc_query_vars_02 as t 
where dt = "{EXECUTION_DATE}"
  
) as t2 on t.order_no = t2.order_no 

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_insurance_housing_fund_info_detail_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t7 on t.order_no = t7.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_large_installments_detail_df_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t8 on t.order_no = t8.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_large_installments_detail_dt_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t9 on t.order_no = t9.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_mobile_phone_number_info_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t10 on t.order_no = t10.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_df as t 
where dt = "{EXECUTION_DATE}"
  
) as t11 on t.order_no = t11.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_ds as t 
where dt = "{EXECUTION_DATE}"
  
) as t12 on t.order_no = t12.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_repay_responsibility_info_vars_dt as t 
where dt = "{EXECUTION_DATE}"
  
) as t13 on t.order_no = t13.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_special_trans_info_detail_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t14 on t.order_no = t14.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_special_trans_info_detail_view_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t15 on t.order_no = t15.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t16 on t.order_no = t16.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_Rh as t 
where dt = "{EXECUTION_DATE}"
  
) as t17 on t.order_no = t17.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R0 as t 
where dt = "{EXECUTION_DATE}"
  
) as t18 on t.order_no = t18.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R3 as t 
where dt = "{EXECUTION_DATE}"
  
) as t19 on t.order_no = t19.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_R4 as t 
where dt = "{EXECUTION_DATE}"
  
) as t20 on t.order_no = t20.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_repay_status_l60m_view_vars_Rhn as t 
where dt = "{EXECUTION_DATE}"
  
) as t21 on t.order_no = t21.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_ac_01_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t22 on t.order_no = t22.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_ac_07_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t23 on t.order_no = t23.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_co_02_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t24 on t.order_no = t24.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_03_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t25 on t.order_no = t25.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_04_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t26 on t.order_no = t26.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_cr_06_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t27 on t.order_no = t27.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_detail_vars_de_05_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t28 on t.order_no = t28.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_alert_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t29 on t.order_no = t29.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_overdue_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t30 on t.order_no = t30.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_recovered_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t31 on t.order_no = t31.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_trans_repay_responsibility_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t32 on t.order_no = t32.order_no

left join 
(
select t.*
from znzz_fintech_ads.dwd_beforeloan_pboc_trans_info_summary_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t33 on t.order_no = t33.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_1_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t34 on t.order_no = t34.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_2_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t35 on t.order_no = t35.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_3_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t36 on t.order_no = t36.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_credit_agreement_info_4_vars_v2 as t 
where dt = "{EXECUTION_DATE}"
  
) as t37 on t.order_no = t37.order_no

left join 
(
select t.*
from znzz_fintech_ads.beforeloan_pboc_trans_info_detail_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t39 on t.order_no = t39.order_no

left join 
(
select t.*
from znzz_fintech_ads.pboc_trans_info_detail_repay_status_del_month_vars as t 
where dt = "{EXECUTION_DATE}"
  
) as t40 on t.order_no = t40.order_no
;

"""

# 一致率阈值
THRESHOLD = 0.9999

# 日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# =============================
# 2. 读取变量名 txt 文件
# =============================
def read_variables_from_txt(file_path):
    """
    读取监控变量名文件
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            variables = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        logging.info(f"✅ 从 {file_path} 读取到 {len(variables)} 个监控变量：{variables}")
        return variables
    except Exception as e:
        logging.error(f"❌ 读取变量文件失败: {e}")
        raise

# =============================
# 3. 获取数据函数（ODPS）
# =============================
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    logging.info('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()

    try:
        instance = conn.execute_sql(sql)
        if not instance.is_successful():
            raise Exception(f"SQL 执行失败: {instance.get_logview_address()}")

        with instance.open_reader() as reader:
            data = reader.to_pandas(n_process=n_process)
        end = time.time()
        logging.info(f'结束跑数：{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        logging.info(f"运行时间：{end - start:.2f}秒")
        return data

    except Exception as e:
        logging.error(f"数据获取失败: {e}")
        raise

# =============================
# 4. 解析 value_006 JSON
# =============================
def parse_value_006(df):
    def safe_parse(x):
        try:
            return json.loads(x) if pd.notna(x) else {}
        except:
            return {}
    
    parsed = df['value_006'].apply(safe_parse)
    features_df = pd.json_normalize(parsed)
    return pd.concat([df.drop(columns=['value_006']), features_df], axis=1)


# =============================
# 5. 计算一致性（只对比 txt 中的变量）
# =============================
def calculate_consistency(online_df, offline_df, monitor_vars):

    # 只保留要监控的变量（且存在于两个 DataFrame 中）
    available_online = set(online_df.columns) & set(monitor_vars)
    available_offline = set(offline_df.columns) & set(monitor_vars)
    valid_vars = list(available_online & available_offline)

    if not valid_vars:
        logging.warning("⚠️ 未在数据中找到任何要监控的变量")
        return pd.DataFrame(), pd.DataFrame()

    logging.info(f"📊 正在对比以下 {len(valid_vars)} 个变量: {valid_vars}")

    ## 4.1数据类型转换
    for col in valid_vars:
        # 判断是否为 object 类型
        if online_df[col].dtype == 'object':
            online_df[col] = pd.to_numeric(online_df[col], errors='coerce')               

        if offline_df[col].dtype == 'object':
            offline_df[col] = pd.to_numeric(offline_df[col])

    ## 4.2数据处理缺失值
    # online_df.replace([-997],np.nan,inplace=True) 
    offline_df.fillna(-999, inplace=True)

    ## 4.3线下数据和线上数据精度保持一致
    online_df[valid_vars]= online_df[valid_vars].round(9) 
    s_vars = ['candnlted12_24m10cnyua_ar','crbf_zcva_xeve','acdb_zbve_n4','crbf_zcvc_xeve','crad_lbvc_n9','crad_lbvh_n9','acda_zbva_n1','acda_zbvg_n5']
    online_df[s_vars]= online_df[s_vars].round(4) 
    offline_df[valid_vars]= offline_df[valid_vars].round(9)
    offline_df[s_vars]= offline_df[s_vars].round(4)
    
    merged = pd.merge(
        online_df[['order_no','id_no_des'] + valid_vars],
        offline_df[['order_no','id_no_des'] + valid_vars],
        on=['order_no','id_no_des'],
        suffixes=('_online', '_offline'),
        how='inner'
    )

    if merged.empty:
        logging.warning("⚠️ 合并后数据为空")
        return pd.DataFrame(), pd.DataFrame()

    results = []
    inconsistent_orders = []

    
    for var in valid_vars:
        online_col = merged[f'{var}_online']
        offline_col = merged[f'{var}_offline']
        both_numeric = online_col.notna() & offline_col.notna()
        within_tolerance = (online_col - offline_col).abs() < 1e-7
        is_equal = (online_col == offline_col) | (online_col.isna() & offline_col.isna()) | (both_numeric & within_tolerance)  
        rate = is_equal.mean()
        count = (~is_equal).sum()

        results.append({
            'variable': var,
            'consistency_rate': round(rate * 100, 4),
            'inconsistent_count': int(count)
        })
        

        # 重点：记录不一致的订单及其线上/线下值
        if rate < THRESHOLD or count > 0:  # 即使 rate 高但有少量不一致也记录
            bad = merged.loc[~is_equal, ['id_no_des','order_no']].copy()
            bad['variable'] = var
            bad['consistency_rate'] = rate
            bad['online_value'] = merged.loc[~is_equal, f'{var}_online']
            bad['offline_value'] = merged.loc[~is_equal, f'{var}_offline']

            inconsistent_orders.append(bad)
    
    # 读取变量清单,描述变量
    df_vars_des = pd.read_excel(VARS_DESC_FILE, sheet_name='Sheet1')
    df_vars_des.drop(columns=['type'],inplace=True)
    df_vars_des.rename(columns={'var':'variable'},inplace=True)
    df_vars_des['variable']=df_vars_des['variable'].str.lower()
    # 生成最终结果
    consistency_df = pd.DataFrame(results).sort_values('consistency_rate', ascending=False).reset_index(drop=True)
    consistency_df['线上线下合并后的记录数']=merged.shape[0]
    consistency_df['线上数据记录数']=online_df.shape[0]
    consistency_df['线下数据记录数']=offline_df.shape[0]
    consistency_df=pd.merge(consistency_df, df_vars_des, how='left',on=['variable'])
    
    all_inconsistent_df = pd.concat(inconsistent_orders, ignore_index=True) if inconsistent_orders \
        else pd.DataFrame(columns=['id_no_des','order_no', 'variable', 'consistency_rate',
            'online_value', 'offline_value'])
    all_inconsistent_df=pd.merge(all_inconsistent_df,online_df[['id_no_des','order_no','ds','create_time','update_time']],\
                             how='left',on=['id_no_des','order_no'])
    all_inconsistent_df=pd.merge(all_inconsistent_df, df_vars_des, how='left',on=['variable'])

    # 保存数据为csv
    if len(all_inconsistent_df) > 0:
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            # 写入总览 Sheet
            consistency_df.to_excel(writer, sheet_name='一致率总览', index=False)
            all_inconsistent_df.to_excel(writer, sheet_name='不一致明细', index=False)
#             all_inconsistent_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        logging.info(f"💾 不一致记录已保存至: {OUTPUT_FILE}")
    else:
        logging.info("✅ 所有变量在容忍度范围内完全一致，未生成不一致记录文件")
    
    # 去重逻辑：同一个身份证号只保留一个订单，同一个特征变量也只保留一个
    inconsistent_df = all_inconsistent_df.drop_duplicates(subset=['id_no_des', 'variable'], keep='first')
    inconsistent_df = inconsistent_df.reset_index(drop=True)
    logging.info(f"📊 不一致样本去重后记录数：{len(inconsistent_df)} 条（原 {len(all_inconsistent_df)} 条）")

    # ✅ 在 send_email 前进行采样
    if len(inconsistent_df) > 6000:
        # 按 variable 分组，每组最多取 10 条
        inconsistent_df_sample = inconsistent_df.groupby('variable').head(10).reset_index(drop=True)
        logging.info(f"📊 不一致样本已采样：{len(inconsistent_df_sample)} 条（原 {len(inconsistent_df)} 条）")
    else:
        inconsistent_df_sample = inconsistent_df.copy()    
    
    return consistency_df, inconsistent_df_sample


# =============================
# 6. 发送邮件（Excel 附件版）
# =============================
def send_email(consistency_df, inconsistent_df):
    cfg = EMAIL_CONFIG
    title = cfg['title'].format(date=datetime.now().strftime('%Y-%m-%d'))

    msg = MIMEMultipart()
    msg['Subject'] = title
    msg['From'] = cfg['user']
    msg['To'] = ", ".join(cfg['recipients'])

    low_count = (consistency_df['consistency_rate'] < 99.99).sum() if len(consistency_df) > 0 else 0

    html = f"""
    <h3>人行特征一致性监控报告</h3>
    <p><strong>执行时间：</strong>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    <p><strong>数据日期：</strong>{EXECUTION_DATE}</p>
    <p><strong>监控变量数：</strong>{len(consistency_df)}</p>
    <p><strong>一致率低于99.99%的变量数：</strong><span style="color:red">{low_count}</span></p>
    """

    msg.attach(MIMEText(html, 'html', 'utf-8'))

    # ========== 生成 Excel 附件 ==========
    if len(consistency_df) > 0 or len(inconsistent_df) > 0:
        import io
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils.dataframe import dataframe_to_rows

        # 创建内存中的 Excel
        excel_buffer = io.BytesIO()

        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            # 写入总览 Sheet
            if len(consistency_df) > 0:
                consistency_df.to_excel(writer, sheet_name='一致率总览', index=False)
                ws1 = writer.sheets['一致率总览']
                # 自动列宽
                for col in ws1.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws1.column_dimensions[column].width = adjusted_width

                # 表头加粗 + 背景色
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                for cell in ws1[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center")

            # 写入不一致明细 Sheet
            if len(inconsistent_df) > 0:
                inconsistent_df.to_excel(writer, sheet_name='部分不一致明细', index=False)
                ws2 = writer.sheets['部分不一致明细']
                for col in ws2.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws2.column_dimensions[column].width = adjusted_width

                # 表头格式
                for cell in ws2[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center")

        # 获取二进制数据
        excel_data = excel_buffer.getvalue()

        # 添加附件
        attachment = MIMEApplication(excel_data, _subtype="xlsx")
        attachment.add_header('Content-Disposition', 'attachment', filename=f'人行特征一致性报告_{EXECUTION_DATE}.xlsx')
        msg.attach(attachment)

        logging.info("📎 已生成并附加 Excel 报告")
    else:
        logging.warning("⚠️ 无数据可生成 Excel 附件")

    # ========== 发送邮件 ==========
    try:
        server = smtplib.SMTP_SSL(cfg['host'], cfg['port'])
        server.login(cfg['user'], cfg['password'])
        server.sendmail(cfg['user'], cfg['recipients'], msg.as_string())
        server.quit()
        logging.info("📧 邮件发送成功")
    except Exception as e:
        logging.error(f"❌ 邮件发送失败: {e}")


# =============================
# 7. 飞书预警
# =============================
def send_feishu_alert(consistency_df):
    low_count = (consistency_df['consistency_rate'] < 99.99).sum() if len(consistency_df) > 0 else 0

    if low_count == 0:
        logging.info("✅ 所有变量一致率 ≥ 99.99%，跳过飞书预警")
        return

    content = f"""
⚠️【人行特征一致性预警】⚠️
👤 负责人：廖禧林
📅 时间：{datetime.now().strftime('%Y-%m-%d %H:%M')}
📉 一致率低于99.99%的变量数量：{low_count} 个
📌 请立即检查！

详情见邮件。
    """.strip()

    payload = {"msg_type": "text", "content": {"text": content}}
    try:
        resp = requests.post(FEISHU_WEBHOOK, data=json.dumps(payload), headers={'Content-Type': 'application/json'}, timeout=10)
        if resp.status_code == 200 and resp.json().get('code') == 0:
            logging.info("🚀 飞书预警发送成功")
        else:
            logging.error(f"❌ 飞书发送失败: {resp.text}")
    except Exception as e:
        logging.error(f"❌ 飞书发送异常: {e}")


# =============================
# 8. 主函数
# =============================
def main():
    logging.info("========== 人行特征一致性监控任务启动 ==========")

    try:
        # 1. 读取监控变量名
        monitor_vars = read_variables_from_txt(VARS_FILE)

        # 2. 获取数据
        df_online_raw = get_data(SQL_ONLINE)
        
        df_offline = get_data(SQL_OFFLINE)

        logging.info(f"线上数据行数: {len(df_online_raw)}")
        logging.info(f"线下数据行数: {len(df_offline)}")

        # 3. 解析 value_006
        df_online_parsed = parse_value_006(df_online_raw)

        # 4. 计算一致性       
        consistency_df, inconsistent_df = calculate_consistency(df_online_parsed, df_offline, monitor_vars)
        
        # 5. 发送通知
        send_email(consistency_df, inconsistent_df)
        send_feishu_alert(consistency_df)

        logging.info("✅ 人行特征一致性监控任务完成")

    except Exception as e:
        logging.critical(f"❌ 任务执行失败: {e}")
        raise

# =============================
# 9. 运行
# =============================
if __name__ == '__main__':
    main()


#==============================================================================
# File: 分箱后画图.py
#==============================================================================


# 调用函数绘制堆叠柱状图
def plot_stacked_bar(df, var, month_col, bins, values, filename=None):
    # 假设df是一个DataFrame，包含您的数据
    # month_col 是月份列
    # bins 是分箱列
    # values 是要绘制的值列
    # 创建一个透视表
    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values, fill_value=0)

    # 初始化图形
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # 获取所有的分箱类别
    bins_list = pivot_df.columns.tolist()
    
    # 计算每个柱子的宽度
    bar_width = 0.8
    
    # 计算x轴上的位置
    x_pos = range(len(pivot_df.index))
    
    # 初始化底部
    bottom = [0] * len(x_pos)
    
    # 遍历每个分箱，并绘制柱状图
    for bin in bins_list:
        # 使用fillna(0)处理NaN值
        pivot_df[bin] = pivot_df[bin].fillna(0)
        
        ax.bar(x_pos,
               pivot_df[bin],
               width=bar_width,
               label=bin,
               bottom=bottom,
               align='center',
               alpha=0.8)
        
        # 更新底部的值
        bottom = [b + v for b, v in zip(bottom, pivot_df[bin])]
    
    # 设置图形属性
    ax.set_title(f'{values}——{var}')
    # ax.set_xlabel('Month')
    ax.set_ylabel(f'{values}')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(pivot_df.index)
    ax.legend()

    plt.grid(axis='y', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show() 
    
# 调用函数绘制时间序列图
def draw_time_series(df, var, month_col, bins, values, filename=None):

    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values)
    
    # 绘制时间序列折线图
    plt.figure(figsize=(14, 7))
    for bin in pivot_df.columns:
        i = list(pivot_df.index)
        j = list(pivot_df[bin])
        plt.plot(i, j, label=bin, marker='o')

    plt.title(f'{values}——{var}')
    # plt.xlabel('Month')
    plt.ylabel(f'{values}')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show()


def draw_line_bar(df, col, bin, var1, var2, var3, filename=None):
    # 处理数据
    bins = df[bin]
    varsname = df[col].unique()[0]
    values1 = list(df[var1].fillna(0))
    values2 = list(df[var2].fillna(0))
    values3 = df[var3].unique()[0]

    # 创建图形和主轴
    fig, ax1 = plt.subplots()

    # 主纵轴 - 柱状图 (分箱占比)
    color = 'tab:blue'
    ax1.set_xlabel(f'{varsname}')
    ax1.set_ylabel(f'{var1}', color=color)
    bars = ax1.bar(bins, values1, color=color)
    # 在柱状图上添加数值标签
    for bar in bars:
        yval = bar.get_height()
        # va='bottom' to place label below the bar
        ax1.text(bar.get_x() + bar.get_width()/2.0, yval, f'{round(yval, 3)}', ha='center', va='top') 
    
    ax1.tick_params(axis='y', labelcolor=color, rotation=45)

    # 副纵轴 - 折线图 (坏占比)
    ax2 = ax1.twinx()  # 创建第二个纵坐标轴
    color = 'tab:red'
    ax2.set_ylabel(f'{var2}', color=color)  # 设置标签颜色
    _ = ax2.plot(values2, color=color, marker='o')  # 绘制折线图
    # 在折线图上添加数值标签
    for xtick, txt in zip(ax1.get_xticks(), values2):
        ax2.text(xtick, txt, f'{round(txt, 3)}', ha='center', va='top', rotation=45)
    ax2.tick_params(axis='y', labelcolor=color)

    # 获取x轴的刻度位置
    xtick_positions = range(len(bins))
    # 设置X轴标签自动调整
    ax1.set_xticks(xtick_positions)
    ax1.set_xticklabels(bins, rotation=45, ha='right')
    
    # 设置标题和网格
    ax1.set_title(f'{varsname}')
    ax1.grid(False)

    # 在左上角添加文本
    ax1.text(0.05, 0.95, f'IV value:{round(values3,3)}', transform=ax1.transAxes, verticalalignment='top')
    # 调整布局
    # fig.tight_layout()
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    # 显示图表
    plt.show()


#==============================================================================
# File: 变量woe离散化.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 变量woe离散化

# 变量woe结果表
def woe_df_concat(bin_df):
    """
    bin_df:list形式，里面存储每个变量的分箱结果
    
    return :woe结果表
    """
    woe_df_list =[]
    for df in bin_df:
        woe_df = df.reset_index().assign(col=df.index.name).rename(columns={df.index.name:'bin'})
        woe_df_list.append(woe_df)
    woe_result = pd.concat(woe_df_list,axis=0)
    # 为了便于查看，将字段名列移到第一列的位置上
    woe_result1 = woe_result['col']
    woe_result2 = woe_result.iloc[:,:-1]
    woe_result_df = pd.concat([woe_result1,woe_result2],axis=1)
    woe_result_df = woe_result_df.reset_index(drop=True)
    return woe_result_df

# woe转换
def woe_transform(df,target,df_woe):
    """
    df:数据集
    target:目标变量的字段名
    df_woe:woe结果表
    
    return:woe转化之后的数据集
    """
    df2 = df.copy()
    for col in df2.drop([target],axis=1).columns:
        x = df2[col]
        bin_map = df_woe[df_woe.col==col]
        bin_res = np.array([0]*x.shape[0],dtype=float)
        for i in bin_map.index:
            lower = bin_map['min_bin'][i]
            upper = bin_map['max_bin'][i]
            if lower == upper:
                x1 = x[np.where(x == lower)[0]]
            else:
                x1 = x[np.where((x>=lower)&(x<=upper))[0]]
            mask = np.in1d(x,x1)
            bin_res[mask] = bin_map['woe'][i]
        bin_res = pd.Series(bin_res,index=x.index)
        bin_res.name = x.name
        df2[col] = bin_res
    return df2




#==============================================================================
# File: 变量分箱.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 变量分箱

# 类别性变量的分箱 
def binning_cate(df,col_list,target):
    """
    df:数据集
    col_list:变量list集合
    target:目标变量的字段名
    
    return: 
    bin_df :list形式，里面存储每个变量的分箱结果
    iv_value:list形式，里面存储每个变量的IV值
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total-bad
    all_odds = good*1.0/bad
    bin_df =[]
    iv_value=[]
    for col in col_list:
        d1 = df.groupby([col],as_index=True)
        d2 = pd.DataFrame()
        d2['min_bin'] = d1[col].min()
        d2['max_bin'] = d1[col].max()
        d2['total'] = d1[target].count()
        d2['totalrate'] = d2['total']/total
        d2['bad'] = d1[target].sum()
        d2['badrate'] = d2['bad']/d2['total']
        d2['good'] = d2['total'] - d2['bad']
        d2['goodrate'] = d2['good']/d2['total']
        d2['badattr'] = d2['bad']/bad
        d2['goodattr'] = (d2['total']-d2['bad'])/good
        d2['odds'] = d2['good']/d2['bad']
        GB_list=[]
        for i in d2.odds:
            if i>=all_odds:
                GB_index = str(round((i/all_odds)*100,0))+str('G')
            else:
                GB_index = str(round((all_odds/i)*100,0))+str('B')
            GB_list.append(GB_index)
        d2['GB_index'] = GB_list
        d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
        d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
        d2['IV'] = d2['bin_iv'].sum()
        iv = d2['bin_iv'].sum().round(3)
        print('变量名:{}'.format(col))
        print('IV:{}'.format(iv))
        print('\t')
        bin_df.append(d2)
        iv_value.append(iv)
    return bin_df,iv_value


# 类别性变量iv的明细表
def iv_cate(df,col_list,target):
    """
    df:数据集
    col_list:变量list集合
    target:目标变量的字段名
    
    return:变量的iv明细表
    """
    bin_df,iv_value = binning_cate(df,col_list,target)
    iv_df = pd.DataFrame({'col':col_list,
                          'iv':iv_value})
    iv_df = iv_df.sort_values('iv',ascending=False)
    return iv_df


# 数值型变量的分箱 

# 先用卡方分箱输出变量的分割点
def split_data(df,col,split_num):
    """
    df: 原始数据集
    col:需要分箱的变量
    split_num:分割点的数量
    """
    df2 = df.copy()
    count = df2.shape[0] # 总样本数
    n = math.floor(count/split_num) # 按照分割点数目等分后每组的样本数
    split_index = [i*n for i in range(1,split_num)] # 分割点的索引
    values = sorted(list(df2[col])) # 对变量的值从小到大进行排序
    split_value = [values[i] for i in split_index] # 分割点对应的value
    split_value = sorted(list(set(split_value))) # 分割点的value去重排序
    return split_value

def assign_group(x,split_bin):
    """
    x:变量的value
    split_bin:split_data得出的分割点list
    """
    n = len(split_bin)
    if x<=min(split_bin):   
        return min(split_bin) # 如果x小于分割点的最小值，则x映射为分割点的最小值
    elif x>max(split_bin): # 如果x大于分割点的最大值，则x映射为分割点的最大值
        return 10e10
    else:
        for i in range(n-1):
            if split_bin[i]<x<=split_bin[i+1]:# 如果x在两个分割点之间，则x映射为分割点较大的值
                return split_bin[i+1]

def bin_bad_rate(df,col,target,grantRateIndicator=0):
    """
    df:原始数据集
    col:原始变量/变量映射后的字段
    target:目标变量的字段
    grantRateIndicator:是否输出总体的违约率
    """
    total = df.groupby([col])[target].count()
    bad = df.groupby([col])[target].sum()
    total_df = pd.DataFrame({'total':total})
    bad_df = pd.DataFrame({'bad':bad})
    regroup = pd.merge(total_df,bad_df,left_index=True,right_index=True,how='left')
    regroup = regroup.reset_index()
    regroup['bad_rate'] = regroup['bad']/regroup['total']  # 计算根据col分组后每组的违约率
    dict_bad = dict(zip(regroup[col],regroup['bad_rate'])) # 转为字典形式
    if grantRateIndicator==0:
        return (dict_bad,regroup)
    total_all= df.shape[0]
    bad_all = df[target].sum()
    all_bad_rate = bad_all/total_all # 计算总体的违约率
    return (dict_bad,regroup,all_bad_rate)

def cal_chi2(df,all_bad_rate):
    """
    df:bin_bad_rate得出的regroup
    all_bad_rate:bin_bad_rate得出的总体违约率
    """
    df2 = df.copy()
    df2['expected'] = df2['total']*all_bad_rate # 计算每组的坏用户期望数量
    combined = zip(df2['expected'],df2['bad']) # 遍历每组的坏用户期望数量和实际数量
    chi = [(i[0]-i[1])**2/i[0] for i in combined] # 计算每组的卡方值
    chi2 = sum(chi) # 计算总的卡方值
    return chi2

def assign_bin(x,cutoffpoints):
    """
    x:变量的value
    cutoffpoints:分箱的切割点
    """
    bin_num = len(cutoffpoints)+1 # 箱体个数
    if x<=cutoffpoints[0]:  # 如果x小于最小的cutoff点，则映射为Bin 0
        return 'Bin 0'
    elif x>cutoffpoints[-1]: # 如果x大于最大的cutoff点，则映射为Bin(bin_num-1)
        return 'Bin {}'.format(bin_num-1)
    else:
        for i in range(0,bin_num-1):
            if cutoffpoints[i]<x<=cutoffpoints[i+1]: # 如果x在两个cutoff点之间，则x映射为Bin(i+1)
                return 'Bin {}'.format(i+1)

def ChiMerge(df,col,target,max_bin=5,min_binpct=0):
    col_unique = sorted(list(set(df[col]))) # 变量的唯一值并排序
    n = len(col_unique) # 变量唯一值得个数
    df2 = df.copy()
    if n>100:  # 如果变量的唯一值数目超过100，则将通过split_data和assign_group将x映射为split对应的value
        split_col = split_data(df2,col,100)  # 通过这个目的将变量的唯一值数目人为设定为100
        df2['col_map'] = df2[col].map(lambda x:assign_group(x,split_col))
    else:
        df2['col_map'] = df2[col]  # 变量的唯一值数目没有超过100，则不用做映射
    # 生成dict_bad,regroup,all_bad_rate的元组
    (dict_bad,regroup,all_bad_rate) = bin_bad_rate(df2,'col_map',target,grantRateIndicator=1)
    col_map_unique = sorted(list(set(df2['col_map'])))  # 对变量映射后的value进行去重排序
    group_interval = [[i] for i in col_map_unique]  # 对col_map_unique中每个值创建list并存储在group_interval中
    
    while (len(group_interval)>max_bin): # 当group_interval的长度大于max_bin时，执行while循环
        chi_list=[]
        for i in range(len(group_interval)-1):
            temp_group = group_interval[i]+group_interval[i+1] # temp_group 为生成的区间,list形式，例如[1,3]
            chi_df = regroup[regroup['col_map'].isin(temp_group)]
            chi_value = cal_chi2(chi_df,all_bad_rate) # 计算每一对相邻区间的卡方值
            chi_list.append(chi_value)
        best_combined = chi_list.index(min(chi_list)) # 最小的卡方值的索引
        # 将卡方值最小的一对区间进行合并
        group_interval[best_combined] = group_interval[best_combined]+group_interval[best_combined+1]
        # 删除合并前的右区间
        group_interval.remove(group_interval[best_combined+1])
        # 对合并后每个区间进行排序
    group_interval = [sorted(i) for i in group_interval]
    # cutoff点为每个区间的最大值
    cutoffpoints = [max(i) for i in group_interval[:-1]]
    
    # 检查是否有箱只有好样本或者只有坏样本
    df2['col_map_bin'] = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints)) # 将col_map映射为对应的区间Bin
    # 计算每个区间的违约率
    (dict_bad,regroup) = bin_bad_rate(df2,'col_map_bin',target)
    # 计算最小和最大的违约率
    [min_bad_rate,max_bad_rate] = [min(dict_bad.values()),max(dict_bad.values())]
    # 当最小的违约率等于0，说明区间内只有好样本，当最大的违约率等于1，说明区间内只有坏样本
    while min_bad_rate==0 or max_bad_rate==1:
        bad01_index = regroup[regroup['bad_rate'].isin([0,1])].col_map_bin.tolist()# 违约率为1或0的区间
        bad01_bin = bad01_index[0]
        if bad01_bin==max(regroup.col_map_bin):
            cutoffpoints = cutoffpoints[:-1] # 当bad01_bin是最大的区间时，删除最大的cutoff点
        elif bad01_bin==min(regroup.col_map_bin):
            cutoffpoints = cutoffpoints[1:] # 当bad01_bin是最小的区间时，删除最小的cutoff点
        else:
            bad01_bin_index = list(regroup.col_map_bin).index(bad01_bin) # 找出bad01_bin的索引
            prev_bin = list(regroup.col_map_bin)[bad01_bin_index-1] # bad01_bin前一个区间
            df3 = df2[df2.col_map_bin.isin([prev_bin,bad01_bin])] 
            (dict_bad,regroup1) = bin_bad_rate(df3,'col_map_bin',target)
            chi1 = cal_chi2(regroup1,all_bad_rate)  # 计算前一个区间和bad01_bin的卡方值
            later_bin = list(regroup.col_map_bin)[bad01_bin_index+1] # bin01_bin的后一个区间
            df4 = df2[df2.col_map_bin.isin([later_bin,bad01_bin])] 
            (dict_bad,regroup2) = bin_bad_rate(df4,'col_map_bin',target)
            chi2 = cal_chi2(regroup2,all_bad_rate) # 计算后一个区间和bad01_bin的卡方值
            if chi1<chi2:  # 当chi1<chi2时,删除前一个区间对应的cutoff点
                cutoffpoints.remove(cutoffpoints[bad01_bin_index-1])
            else:  # 当chi1>=chi2时,删除bin01对应的cutoff点
                cutoffpoints.remove(cutoffpoints[bad01_bin_index])
        df2['col_map_bin'] = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints))
        (dict_bad,regroup) = bin_bad_rate(df2,'col_map_bin',target)
        # 重新将col_map映射至区间，并计算最小和最大的违约率，直达不再出现违约率为0或1的情况，循环停止
        [min_bad_rate,max_bad_rate] = [min(dict_bad.values()),max(dict_bad.values())]
    
    # 检查分箱后的最小占比
    if min_binpct>0:
        group_values = df2['col_map'].apply(lambda x:assign_bin(x,cutoffpoints))
        df2['col_map_bin'] = group_values # 将col_map映射为对应的区间Bin
        group_df = group_values.value_counts().to_frame() 
        group_df['bin_pct'] = group_df['col_map']/n # 计算每个区间的占比
        min_pct = group_df.bin_pct.min() # 得出最小的区间占比
        while min_pct<min_binpct and len(cutoffpoints)>2: # 当最小的区间占比小于min_pct且cutoff点的个数大于2，执行循环
            # 下面的逻辑基本与“检验是否有箱体只有好/坏样本”的一致
            min_pct_index = group_df[group_df.bin_pct==min_pct].index.tolist()
            min_pct_bin = min_pct_index[0]
            if min_pct_bin == max(group_df.index):
                cutoffpoints=cutoffpoints[:-1]
            elif min_pct_bin == min(group_df.index):
                cutoffpoints=cutoffpoints[1:]
            else:
                minpct_bin_index = list(group_df.index).index(min_pct_bin)
                prev_pct_bin = list(group_df.index)[minpct_bin_index-1]
                df5 = df2[df2['col_map_bin'].isin([min_pct_bin,prev_pct_bin])]
                (dict_bad,regroup3) = bin_bad_rate(df5,'col_map_bin',target)
                chi3 = cal_chi2(regroup3,all_bad_rate)
                later_pct_bin = list(group_df.index)[minpct_bin_index+1]
                df6 = df2[df2['col_map_bin'].isin([min_pct_bin,later_pct_bin])]
                (dict_bad,regroup4) = bin_bad_rate(df6,'col_map_bin',target)
                chi4 = cal_chi2(regroup4,all_bad_rate)
                if chi3<chi4:
                    cutoffpoints.remove(cutoffpoints[minpct_bin_index-1])
                else:
                    cutoffpoints.remove(cutoffpoints[minpct_bin_index])
    return cutoffpoints

# 数值型变量的分箱（卡方分箱）
def binning_num(df,target,col_list,max_bin=None,min_binpct=None):
    """
    df:数据集
    target:目标变量的字段名
    col_list:变量list集合
    max_bin:最大的分箱个数
    min_binpct:区间内样本所占总体的最小比
    
    return:
    bin_df :list形式，里面存储每个变量的分箱结果
    iv_value:list形式，里面存储每个变量的IV值
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total-bad
    all_odds = good/bad
    inf = float('inf')
    ninf = float('-inf')
    bin_df=[]
    iv_value=[]
    for col in col_list:
        cut = ChiMerge(df,col,target,max_bin=max_bin,min_binpct=min_binpct)
        cut.insert(0,ninf)
        cut.append(inf)
        bucket = pd.cut(df[col],cut)
        d1 = df.groupby(bucket)
        d2 = pd.DataFrame()
        d2['min_bin'] = d1[col].min()
        d2['max_bin'] = d1[col].max()
        d2['total'] = d1[target].count()
        d2['totalrate'] = d2['total']/total
        d2['bad'] = d1[target].sum()
        d2['badrate'] = d2['bad']/d2['total']
        d2['good'] = d2['total'] - d2['bad']
        d2['goodrate'] = d2['good']/d2['total']
        d2['badattr'] = d2['bad']/bad
        d2['goodattr'] = (d2['total']-d2['bad'])/good
        d2['odds'] = d2['good']/d2['bad']
        GB_list=[]
        for i in d2.odds:
            if i>=all_odds:
                GB_index = str(round((i/all_odds)*100,0))+str('G')
            else:
                GB_index = str(round((all_odds/i)*100,0))+str('B')
            GB_list.append(GB_index)
        d2['GB_index'] = GB_list
        d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
        d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
        d2['IV'] = d2['bin_iv'].sum()
        iv = d2['bin_iv'].sum().round(3)
        print('变量名:{}'.format(col))
        print('IV:{}'.format(iv))
        print('\t')
        bin_df.append(d2)
        iv_value.append(iv)
    return bin_df,iv_value


# 数值型变量的iv明细表
def iv_num(df,target,col_list,max_bin=None,min_binpct=None):
    """
    df:数据集
    target:目标变量的字段名
    col_list:变量list集合
    max_bin:最大的分箱个数
    min_binpct:区间内样本所占总体的最小比
    
    return :变量的iv明细表
    """
    bin_df,iv_value = binning_num(df,target,col_list,max_bin=max_bin,min_binpct=min_binpct)
    iv_df = pd.DataFrame({'col':col_list,
                          'iv':iv_value})
    iv_df = iv_df.sort_values('iv',ascending=False)
    return iv_df


# 自定义分箱
def binning_self(df,col,target,cut=None,right_border=True):
    """
    df: 数据集
    col:分箱的单个变量名
    cut:划分区间的list
    right_border：设定左开右闭、左闭右开
    
    return: 
    bin_df: df形式，单个变量的分箱结果
    iv_value: 单个变量的iv
    """
    total = df[target].count()
    bad = df[target].sum()
    good = total - bad
    all_odds = good/bad
    bucket = pd.cut(df[col],cut,right=right_border)
    d1 = df.groupby(bucket)
    d2 = pd.DataFrame()
    d2['min_bin'] = d1[col].min()
    d2['max_bin'] = d1[col].max()
    d2['total'] = d1[target].count()
    d2['totalrate'] = d2['total']/total
    d2['bad'] = d1[target].sum()
    d2['badrate'] = d2['bad']/d2['total']
    d2['good'] = d2['total'] - d2['bad']
    d2['goodrate'] = d2['good']/d2['total']
    d2['badattr'] = d2['bad']/bad
    d2['goodattr'] = (d2['total']-d2['bad'])/good
    d2['odds'] = d2['good']/d2['bad']
    GB_list=[]
    for i in d2.odds:
        if i>=all_odds:
            GB_index = str(round((i/all_odds)*100,0))+str('G')
        else:
            GB_index = str(round((all_odds/i)*100,0))+str('B')
        GB_list.append(GB_index)
    d2['GB_index'] = GB_list
    d2['woe'] = np.log(d2['badattr']/d2['goodattr'])
    d2['bin_iv'] = (d2['badattr']-d2['goodattr'])*d2['woe']
    d2['IV'] = d2['bin_iv'].sum()
    iv_value = d2['bin_iv'].sum().round(3)
    print('变量名:{}'.format(col))
    print('IV:{}'.format(iv_value))
    bin_df = d2.copy()
    return bin_df,iv_value


# 变量分箱结果的检查

# woe的可视化
def plot_woe(bin_df,hspace=0.4,wspace=0.4,plt_size=None,plt_num=None,x=None,y=None):
    """
    bin_df:list形式，里面存储每个变量的分箱结果
    hspace :子图之间的间隔(y轴方向)
    wspace :子图之间的间隔(x轴方向)
    plt_size :图纸的尺寸
    plt_num :子图的数量
    x :子图矩阵中一行子图的数量
    y :子图矩阵中一列子图的数量
    
    return :每个变量的woe变化趋势图
    """
    plt.figure(figsize=plt_size)
    plt.subplots_adjust(hspace=hspace,wspace=wspace)
    for i,df in zip(range(1,plt_num+1,1),bin_df):
        col_name = df.index.name
        df = df.reset_index()
        plt.subplot(x,y,i)
        plt.title(col_name)
        sns.barplot(data=df,x=col_name,y='woe')
        plt.xlabel('')
        plt.xticks(rotation=30)
    return plt.show()


# 检验woe是否单调 
def woe_monoton(bin_df):
    """
    bin_df:list形式，里面存储每个变量的分箱结果
    
    return :
    woe_notmonoton_col :woe没有呈单调变化的变量，list形式
    woe_judge_df :df形式，每个变量的检验结果
    """
    woe_notmonoton_col =[]
    col_list = []
    woe_judge=[]
    for woe_df in bin_df:
        col_name = woe_df.index.name
        woe_list = list(woe_df.woe)
        if woe_df.shape[0]==2:
            #print('{}是否单调: True'.format(col_name))
            col_list.append(col_name)
            woe_judge.append('True')
        else:
            woe_not_monoton = [(woe_list[i]<woe_list[i+1] and woe_list[i]<woe_list[i-1])                                or (woe_list[i]>woe_list[i+1] and woe_list[i]>woe_list[i-1])                                for i in range(1,len(woe_list)-1,1)]
            if True in woe_not_monoton:
                #print('{}是否单调: False'.format(col_name))
                woe_notmonoton_col.append(col_name)
                col_list.append(col_name)
                woe_judge.append('False')
            else:
                #print('{}是否单调: True'.format(col_name))
                col_list.append(col_name)
                woe_judge.append('True')
    woe_judge_df = pd.DataFrame({'col':col_list,
                                 'judge_monoton':woe_judge})
    return woe_notmonoton_col,woe_judge_df


# 检查某个区间的woe是否大于1
def woe_large(bin_df):
    """
    bin_df:list形式，里面存储每个变量的分箱结果
    
    return:
    woe_large_col: 某个区间woe大于1的变量，list集合
    woe_judge_df :df形式，每个变量的检验结果
    """
    woe_large_col=[]
    col_list =[]
    woe_judge =[]
    for woe_df in bin_df:
        col_name = woe_df.index.name
        woe_list = list(woe_df.woe)
        woe_large = list(filter(lambda x:x>=1,woe_list))
        if len(woe_large)>0:
            col_list.append(col_name)
            woe_judge.append('True')
            woe_large_col.append(col_name)
        else:
            col_list.append(col_name)
            woe_judge.append('False')
    woe_judge_df = pd.DataFrame({'col':col_list,
                                 'judge_large':woe_judge})
    return woe_large_col,woe_judge_df




#==============================================================================
# File: 变量筛选.py
#==============================================================================


# coding: utf-8

# In[ ]:


# 变量筛选 

# xgboost筛选变量 
def select_xgboost(df,target,imp_num=None):
    """
    df:数据集
    target:目标变量的字段名
    imp_num:筛选变量的个数
    
    return:
    xg_fea_imp:变量的特征重要性
    xg_select_col:筛选出的变量
    """
    x = df.drop([target],axis=1)
    y = df[target]
    xgmodel = XGBClassifier(random_state=0)
    xgmodel = xgmodel.fit(x,y,eval_metric='auc')
    xg_fea_imp = pd.DataFrame({'col':list(x.columns),
                               'imp':xgmodel.feature_importances_})
    xg_fea_imp = xg_fea_imp.sort_values('imp',ascending=False).reset_index(drop=True).iloc[:imp_num,:]
    xg_select_col = list(xg_fea_imp.col)
    return xg_fea_imp,xg_select_col


# 随机森林筛选变量 
def select_rf(df,target,imp_num=None):
    """
    df:数据集
    target:目标变量的字段名
    imp_num:筛选变量的个数
    
    return:
    rf_fea_imp:变量的特征重要性
    rf_select_col:筛选出的变量
    """
    x = df.drop([target],axis=1)
    y = df[target]
    rfmodel = RandomForestClassifier(random_state=0)
    rfmodel = rfmodel.fit(x,y)
    rf_fea_imp = pd.DataFrame({'col':list(x.columns),
                               'imp':rfmodel.feature_importances_})
    rf_fea_imp = rf_fea_imp.sort_values('imp',ascending=False).reset_index(drop=True).iloc[:imp_num,:]
    rf_select_col = list(rf_fea_imp.col)
    return rf_fea_imp,rf_select_col


# 相关性可视化
def plot_corr(df,col_list,threshold=None,plt_size=None,is_annot=True):
    """
    df:数据集
    col_list:变量list集合
    threshold: 相关性设定的阈值
    plt_size:图纸尺寸
    is_annot:是否显示相关系数值
    
    return :相关性热力图
    """
    corr_df = df.loc[:,col_list].corr()
    plt.figure(figsize=plt_size)
    sns.heatmap(corr_df,annot=is_annot,cmap='rainbow',vmax=1,vmin=-1,mask=np.abs(corr_df)<=threshold)
    return plt.show()


# 相关性剔除
def forward_delete_corr(df,col_list,threshold=None):
    """
    df:数据集
    col_list:变量list集合
    threshold: 相关性设定的阈值
    
    return:相关性剔除后的变量
    """
    list_corr = col_list[:]
    for col in list_corr:
        corr = df.loc[:,list_corr].corr()[col]
        corr_index= [x for x in corr.index if x!=col]
        corr_values  = [x for x in corr.values if x!=1]
        for i,j in zip(corr_index,corr_values):
            if abs(j)>=threshold:
                list_corr.remove(i)
    return list_corr


# 相关性变量映射关系 
def corr_mapping(df,col_list,threshold=None):
    """
    df:数据集
    col_list:变量list集合
    threshold: 相关性设定的阈值
    
    return:强相关性变量之间的映射关系表
    """
    corr_df = df.loc[:,col_list].corr()
    col_a = []
    col_b = []
    corr_value = []
    for col,i in zip(col_list[:-1],range(1,len(col_list),1)):
        high_corr_col=[]
        high_corr_value=[]
        corr_series = corr_df[col][i:]
        for i,j in zip(corr_series.index,corr_series.values):
            if abs(j)>=threshold:
                high_corr_col.append(i)
                high_corr_value.append(j)
        col_a.extend([col]*len(high_corr_col))
        col_b.extend(high_corr_col)
        corr_value.extend(high_corr_value)

    corr_map_df = pd.DataFrame({'col_A':col_a,
                                'col_B':col_b,
                                'corr':corr_value})
    return corr_map_df


# 显著性筛选,在筛选前需要做woe转换
def forward_delete_pvalue(x_train,y_train):
    """
    x_train -- x训练集
    y_train -- y训练集
    
    return :显著性筛选后的变量
    """
    col_list = list(x_train.columns)
    pvalues_col=[]
    for col in col_list:
        pvalues_col.append(col)
        x_train2 = sm.add_constant(x_train.loc[:,pvalues_col])
        sm_lr = sm.Logit(y_train,x_train2)
        sm_lr = sm_lr.fit()
        for i,j in zip(sm_lr.pvalues.index[1:],sm_lr.pvalues.values[1:]): 
            if j>=0.05:
                pvalues_col.remove(i)
    
    x_new_train = x_train.loc[:,pvalues_col]
    x_new_train2 = sm.add_constant(x_new_train)
    lr = sm.Logit(y_train,x_new_train2)
    lr = lr.fit()
    print(lr.summary2())
    return pvalues_col


# 逻辑回归系数符号筛选,在筛选前需要做woe转换
def forward_delete_coef(x_train,y_train):
    """
    x_train -- x训练集
    y_train -- y训练集
    
    return :
    coef_col回归系数符号筛选后的变量
    lr_coe：每个变量的系数值
    """
    col_list = list(x_train.columns)
    coef_col = []
    for col in col_list:
        coef_col.append(col)
        x_train2 = x_train.loc[:,coef_col]
        sk_lr = LogisticRegression(random_state=0).fit(x_train2,y_train)
        coef_df = pd.DataFrame({'col':coef_col,'coef':sk_lr.coef_[0]})
        if coef_df[coef_df.coef<0].shape[0]>0:
            coef_col.remove(col)
    
    x_new_train = x_train.loc[:,coef_col]
    lr = LogisticRegression(random_state=0).fit(x_new_train,y_train)
    lr_coe = pd.DataFrame({'col':coef_col,
                           'coef':lr.coef_[0]})
    return coef_col,lr_coe




#==============================================================================
# File: 增益评估.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# 设置数据存储
task_name = '增益评估'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./增益评估'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. 数据读取

# In[ ]:


print(result_path)


# In[ ]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# In[4]:


df1 = pd.read_csv('前筛实时模型250919_ys.csv')
df1.info(show_counts=True)
df1.head()


# In[5]:


df2 = pd.read_csv('前筛实时模型250906.csv')
df2.info(show_counts=True)
df2.head()


# In[6]:


df_sample_ = pd.merge(df2, df1[['order_no','m1b0070','m1b0077']], how='left',on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[7]:


varsname = ['id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2', 'm1b0070', 'm1b0071', 'm1b0074', 'm1b0075', 'm1b0077', 'umeng_sdk_score', 'tianchuang_score', 'fico_model', 'haina_model']
print(len(varsname))
print(varsname)


# In[8]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[9]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[10]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[11]:


df_sample = df_sample_.copy()
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[12]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[13]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-14'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-02-15' & apply_date<='2025-02-28'").index, 'data_set']='2_test'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-09'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-10' & apply_date<='2025-04-15'").index, 'data_set']='3_oot2'


# In[14]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. 样本概况

# In[87]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[ ]:


print(df_sample[target].value_counts())


# In[ ]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[ ]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[ ]:


# # 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[ ]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[ ]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[ ]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set)


# In[ ]:



def calculate_iv_by_group(df, 
                          group_col, 
                          target, 
                          variables,
                          is_return_unique=False,
                          method='quantile', 
                          n_bins=10):
    """
    按分组字段计算每个变量在各组中的 IV 和 unique 值
    输出格式：所有 IV 列在前，所有 UNIQUE 列在后
    
    Parameters:
    -----------
    df : pd.DataFrame
        原始数据
    group_col : str
        分组列名（如 'sample_type', 'year_month' 等）
    target : str
        目标变量列名
    variables : list
        要分析的变量名列表
    method : str
        toad.quality 的分箱方法
    n_bins : int
        分箱数量
    
    Returns:
    --------
    pd.DataFrame
        索引: variable
        列:  {group1}_iv, {group2}_iv, ..., {group1}_unique, {group2}_unique, ...
    """
    # 检查列是否存在
    required_cols = [group_col, target] + variables
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame.")
    
    # 存储每组的结果
    iv_data = {}   # 存储所有 iv 列
    unique_data = {}  # 存储所有 unique 列
    
    # 按 group_col 分组
    for group_name, group_df in df.groupby(group_col):
        data = group_df[variables + [target]].copy()
        
        if data.empty:
            print(f"Warning: No data in group '{group_name}'")
            continue
        
        try:
            # 使用 toad 计算质量指标
            quality = toad.quality(
                data,
                target=target,
                iv_only=True,
                method=method,
                n_bins=n_bins
            )
            
            # 只保留 iv 和 unique
            cols_to_keep = ['iv', 'unique']
            existing_cols = [c for c in cols_to_keep if c in quality.columns]
            quality = quality[existing_cols]
            
            # 分开存储
            if 'iv' in quality.columns:
                iv_data[f"{group_name}_iv"] = quality['iv']
            
            if 'unique' in quality.columns:
                unique_data[f"{group_name}_unique"] = quality['unique']
                
        except Exception as e:
            print(f"Error processing group '{group_name}': {str(e)}")
            continue
    
    # 检查是否有数据
    if not iv_data and not unique_data:
        raise ValueError("No valid group data processed.")
    
    # 转换为 DataFrame
    df_iv = pd.DataFrame(iv_data)
    df_unique = pd.DataFrame(unique_data)
    
    # 合并：IV 在前，Unique 在后
    if is_return_unique:
        result = pd.concat([df_iv, df_unique], axis=1)
    else:
        result = df_iv.copy()
    
    # 填充缺失值
    result = result.fillna(value=pd.NA)
    
    # 设置索引名
    result.index.name = 'variable'
    
    return result


# In[ ]:


# 2.3 快速查看特征重要性
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='quantile', n_bins=10)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[ ]:



df_iv = calculate_iv_by_group(
    df=df_sample,
    group_col='data_set',
    target=target,
    variables=varsname
)
df_iv


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[ ]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[ ]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[ ]:


target


# In[ ]:


varsname_v2 = varsname[:]


# In[ ]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[ ]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[ ]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[ ]:


new_bins_dict


# In[ ]:


combiner.load(new_bins_dict)


# In[ ]:


combiner.export()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[ ]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[ ]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[ ]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[ ]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[ ]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[ ]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[15]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[16]:


from itertools import combinations

# 定义一个函数来进行递归特征消除
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # 使用当前最佳特征集训练模型
        # ✅ 构建当前特征子集的数据
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # 如果是 numpy array，用位置索引
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # 更新最佳特征集
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'友盟联合建模_v6_20250717140214.bin')  # 自动识别 .txt/.bin/.json


# ## 5.1 数据预处理

# In[ ]:


df_sample[target] = pd.to_numeric(df_sample[target])


# In[17]:


df_sample[target].value_counts()


# In[18]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[19]:


df_sample[modeltrian_target].value_counts()


# In[20]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[21]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[22]:


df_sample['channel_types'].value_counts()


# In[23]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.07
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 80


# In[ ]:



### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[ ]:


print("最优参数opt_params: ", opt_params)


# In[ ]:


varsname


# In[33]:


varsname_base = ['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0074',
 'm1b0075',
 'm1b0077']


# In[34]:


print(len(varsname_base))
print(varsname_base)


# In[ ]:


def model_train(data, selected_vars, target, opt_params):
    X_train = data[data['data_set']=='1_train'][selected_vars]
    y_train = data[data['data_set']=='1_train'][target]
    X_test = data[data['data_set']=='2_test'][selected_vars]
    y_test = data[data['data_set']=='2_test'][target]      
    
    best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, opt_params)
    
    dtrain = lgb.Dataset(X_train[best_features], label=y_train)
    dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)
    lgb_model = lgb.train(opt_params, dtrain, valid_sets=dtest, num_boost_round=10000)
    
    return lgb_model

from itertools import combinations

# 定义一个函数来进行递归特征消除
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # 使用当前最佳特征集训练模型
        # ✅ 构建当前特征子集的数据
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # 如果是 numpy array，用位置索引
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # 更新最佳特征集
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


vars_combiner_list = []
vars_combiner_dict = {}


# In[ ]:



for three_score in ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']:
    print(f"===========开始变量：{three_score}===========")
    selected_vars = varsname_base + [three_score]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([three_score, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()

    gc.collect()


# In[ ]:



for score_a, score_b in combinations(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'], 2):
    print(f"===========开始变量组合：{score_a} vs {score_b}===========")
       
    selected_vars = varsname_base + [score_a, score_b]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([score_a, score_b, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()
    gc.collect()




# In[ ]:



for score_a, score_b, score_c in combinations(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'], 3):
    print(f"===========开始变量组合：{score_a} vs {score_b} vs {score_c}===========")
       
    selected_vars = varsname_base + [score_a, score_b, score_c]
    lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

    varname = "@".join([score_a, score_b, score_c, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname]=lgb_model.feature_name()
    gc.collect()




# In[ ]:



selected_vars = varsname_base + ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
lgb_model = model_train(df_sample, selected_vars, modeltrian_target, opt_params)

varname = "@".join(['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model', '_main'])        
df_sample[varname] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
vars_combiner_list.append(varname)
vars_combiner_dict[varname]=lgb_model.feature_name()
gc.collect()


# In[ ]:





# In[ ]:





# In[ ]:


# # 确定数据集参数后，训练模型
# X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
# y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
# print(df_sample.groupby(['data_set'])['order_no'].count())
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(X_train.shape)
# print(df_sample.groupby(['data_set'])['order_no'].count())


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# 优化后评估模型效果
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[ ]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[ ]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[ ]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[ ]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[ ]:


df_sample["customer_tags"].value_counts()


# In[ ]:


df_sample['fico数据是否缺失']=df_sample['fico_model'].apply(lambda x: '1_不缺失' if pd.notna(x) else '2_有缺失' )
df_sample['fico数据是否缺失'].value_counts()


# In[ ]:


# # 按 flag 分组计算
# df_ks_auc_set_all = (
#     df_sample[df_sample['fico_model'].notna()]
#     .groupby(['customer_tags'])
#     .apply(
#         lambda g: calculate_ks_auc(g, modeltrian_target, target, 'm1b0074', 'data_set')
#     )
#     .reset_index()
# )
# df_ks_auc_set_all


# In[ ]:


(
    df_sample[df_sample['m1b0072'].notna()]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'm1b0072', 'data_set')
    )
    .reset_index()
)


# In[ ]:


df_sample[df_sample['fico_model'].notna()]['apply_date'].max()


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 参数优化

# In[27]:


from itertools import combinations
import pandas as pd
import numpy as np
import lightgbm as lgb
from hyperopt import fmin, tpe, hp, Trials
from hyperopt.early_stop import no_progress_loss
from sklearn.metrics import roc_auc_score
import gc
import pickle
import os
from typing import List, Tuple, Dict, Any


# In[28]:



# ================================
# 1. 工具函数定义
# ================================

def feature_importance(model: lgb.Booster) -> pd.DataFrame:
    """获取特征重要性"""
    importance = model.feature_importance(importance_type='gain')
    features = model.feature_name()
    return pd.DataFrame({'feature': features, 'gain': importance}).sort_values(by='gain', ascending=False)

def rfe_with_lgb(
    X_train: pd.DataFrame, y_train: pd.Series,
    X_test: pd.DataFrame, y_test: pd.Series,
    params: Dict[str, Any]
) -> List[str]:
    """递归特征消除（RFE）"""
    feature_names = list(X_train.columns)
    best_features = feature_names[:]

    while len(best_features) > 0:
        train_set = lgb.Dataset(X_train[best_features], label=y_train)
        valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)

        try:
            model = lgb.train(params, train_set, valid_sets=[valid_set], num_boost_round=10000, verbose_eval=False)
            df_importance = feature_importance(model).reset_index(drop=True)
        except Exception as e:
            print(f"  ⚠️ RFE 训练失败: {e}")
            break

        if all(df_importance['gain'] > 0):
            break

        best_features = df_importance[df_importance['gain'] > 0]['feature'].tolist()
        gc.collect()

    return best_features

def param_hyperopt(
    param_spaces: Dict, X_train, y_train, X_test, y_test,
    num_boost_round=10000, max_evals=50
) -> Tuple[Dict, Trials]:
    """贝叶斯调参主函数（兼容老版 hyperopt）"""
    def lgb_hyperopt_object(params):
        max_depth = int(params['max_depth'])
        num_leaves = int(params['num_leaves'])
        
        # ⛔️ 约束检查：num_leaves <= 2^max_depth
        if num_leaves >= 2 ** max_depth:
            return 1.0  # 惩罚，拒绝非法组合

        param = {
            'objective': 'binary',
            'boosting': 'gbdt',
            'metric': 'auc',
            'learning_rate': params['learning_rate'],
            'num_leaves': num_leaves,
            'max_depth': max_depth,
            'min_data_in_leaf': int(params['min_data_in_leaf']),
            'feature_fraction': params['feature_fraction'],
            'bagging_fraction': params['bagging_fraction'],
            'lambda_l1': params['lambda_l1'],
            'lambda_l2': params['lambda_l2'],
            'min_gain_to_split': 10,
            'early_stopping_rounds': 30,
            'scale_pos_weight': 1,
            'seed': 1,
            'verbose': -1
        }

        train_set = lgb.Dataset(X_train, label=y_train)
        valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)

        try:
            clf_obj = lgb.train(param, train_set, valid_sets=[valid_set], num_boost_round=num_boost_round, verbose_eval=False)
            pred = clf_obj.predict(X_test)
            loss = 1 - roc_auc_score(y_test, pred)
        except Exception as e:
            loss = 1.0  # 异常也返回高损失

        return loss

    trials = Trials()
    early_stop_fn = no_progress_loss(50)

    best_params = fmin(
        fn=lgb_hyperopt_object,
        space=param_spaces,
        algo=tpe.suggest,
        max_evals=max_evals,
        trials=trials,
        early_stop_fn=early_stop_fn,
        show_progressbar=True
    )

    # 转换整数参数
    for key in ['num_leaves', 'max_depth', 'min_data_in_leaf']:
        if key in best_params:
            best_params[key] = int(best_params[key])

    return best_params, trials


# In[30]:



# ================================
# 2. 定义搜索空间（兼容老版 hyperopt）
# ================================

spaces = {
    "learning_rate": hp.uniform('learning_rate', 0.05, 0.1),
    'max_depth': hp.quniform('max_depth', 2, 6, 1),
    "num_leaves": hp.quniform("num_leaves", 4, 64, 1),  # 用约束检查保证合法性
    "min_data_in_leaf": hp.quniform("min_data_in_leaf", 100, 1000, 50),
    "feature_fraction": hp.uniform("feature_fraction", 0.5, 0.9),
    "bagging_fraction": hp.uniform("bagging_fraction", 0.5, 0.9),
    "lambda_l1": hp.uniform("lambda_l1", 0, 10),
    "lambda_l2": hp.uniform("lambda_l2", 1, 300)
}

# 基础参数
base_params = {
    'objective': 'binary',
    'boosting': 'gbdt',
    'metric': 'auc',
    'min_gain_to_split': 80,
    'early_stopping_rounds': 30,
    'scale_pos_weight': 1,
    'seed': 1,
    'verbose': -1
}


# In[31]:



# ================================
# 3. 初始化存储
# ================================

score_vars = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
vars_combiner_list = []          # 存储新变量名
vars_combiner_dict = {}          # {varname: feature_list}
model_results = []               # 存储每次训练的元信息


# In[45]:



# ================================
# 4. 遍历所有非空子集（1~4个变量）
# ================================

print("🚀 开始遍历所有评分变量组合（1~4个）...")

for r in range(1, 5):  # 1, 2, 3, 4
    for combo in combinations(score_vars, r):
        score_names = list(combo)
        combo_name = "@".join(score_names + ['_main'])  # 如 fico_model@umeng_sdk_score_main
        print(f"\n=========== 开始组合：{combo_name} ===========")

        # 选择变量
        selected_vars = varsname_base + score_names  # varsname_base 是你的基础变量列表

        # 划分数据
        X_train = df_sample[df_sample['data_set'] == '1_train'][selected_vars]
        y_train = df_sample[df_sample['data_set'] == '1_train'][modeltrian_target]
        X_test = df_sample[df_sample['data_set'] == '2_test'][selected_vars]
        y_test = df_sample[df_sample['data_set'] == '2_test'][modeltrian_target]

        if len(X_train) == 0 or len(X_test) == 0:
            print("  ⚠️ 训练/测试集为空，跳过...")
            continue

        # 步骤1：贝叶斯调参
        print("  → 开始贝叶斯调参...")
        opt_params, _ = param_hyperopt(spaces, X_train, y_train, X_test, y_test, max_evals=20)

        # 合并参数
        final_params = {**base_params, **opt_params}

        # 步骤2：RFE 特征选择
        print("  → 开始递归特征消除（RFE）...")
        best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, final_params)

        # 步骤3：训练最终模型
        dtrain = lgb.Dataset(X_train[best_features], label=y_train)
        dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)

        lgb_model = lgb.train(
            final_params,
            dtrain,
            valid_sets=[dtest],
            num_boost_round=10000,
            verbose_eval=50
        )

        # 步骤4：生成预测变量
        df_sample[combo_name] = lgb_model.predict(df_sample[best_features], num_iteration=lgb_model.best_iteration)

        # 步骤5：记录
        vars_combiner_list.append(combo_name)
        vars_combiner_dict[combo_name] = best_features

        model_results.append({
            'combination': combo_name,
            'n_scores': len(combo),
            'scores': list(combo),
            'params': final_params,
            'selected_features': best_features,
            'n_features': len(best_features),
            'best_iteration': lgb_model.best_iteration
        })

        gc.collect()

print(f"\n✅ 所有 {len(model_results)} 个组合训练完成！")


# In[48]:



# ================================
# 5. 保存结果
# ================================

# 保存结果字典
with open(result_path + 'vars_combiner_list.pkl', 'wb') as f:
    pickle.dump(vars_combiner_list, f)

with open(result_path + 'vars_combiner_dict.pkl', 'wb') as f:
    pickle.dump(vars_combiner_dict, f)

with open(result_path + 'model_results_all.pkl', 'wb') as f:
    pickle.dump(model_results, f)

# 导出摘要 CSV
results_df = pd.DataFrame([
    {
        'combination': r['combination'],
        'n_scores': r['n_scores'],
        'n_features': r['n_features'],
        'learning_rate': r['params']['learning_rate'],
        'num_leaves': r['params']['num_leaves'],
        'max_depth': r['params']['max_depth'],
        'lambda_l2': r['params']['lambda_l2'],
        'feature_fraction': r['params']['feature_fraction'],
        'bagging_fraction': r['params']['bagging_fraction']
    }
    for r in model_results
])

results_df.to_csv(result_path + 'hyperopt_summary_all_combinations.csv', index=False)
print("📊 结果已保存：")
print("   - vars_combiner_list.pkl")
print("   - vars_combiner_dict.pkl")
print("   - model_results_all.pkl")
print("   - hyperopt_summary_all_combinations.csv")


# In[71]:


results_df


# In[79]:


model_results[6]


# In[85]:


model_results[0]


# In[ ]:



for three_score in ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']:
    print(f"=========== 开始变量：{three_score} ===========")
    selected_vars = varsname_base + [three_score]
    
    # 调用带调参的训练函数
    result = model_train_with_hyperopt(
        data=df_sample,
        selected_vars=selected_vars,
        target=modeltrian_target,
        base_params=base_params,
        num_boost_round=10000,
        max_evals=30  # 可调整
    )
    
    lgb_model = result['model']
    final_params = result['params']
    best_features = result['features']
    
    # 生成新变量名
    varname = "@".join([three_score, '_main'])        
    df_sample[varname] = lgb_model.predict(df_sample[best_features], num_iteration=lgb_model.best_iteration)
    
    # 保存结果
    vars_combiner_list.append(varname)
    vars_combiner_dict[varname] = best_features
    
    # 保存模型的参数和变量（可用于后续分析或导出）
    model_results.append({
        'score_name': three_score,
        'model_params': final_params,
        'selected_features': best_features,
        'num_features': len(best_features),
        'best_iteration': lgb_model.best_iteration,
        'auc_train': lgb_model.best_score['training']['auc'],
        'auc_valid': lgb_model.best_score['valid_0']['auc']
    })
    
    gc.collect()


# In[ ]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("最优参数opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# 优化后评估模型效果
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'umeng_sdk_score@_main', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[88]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'umeng_sdk_score@_main', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['fico数据是否缺失','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='分客群')  
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[49]:


df_sample.info(show_counts=True)


# In[ ]:


vars_combiner_dict


# In[ ]:


vars_combiner_list


# In[50]:


without_fico= load_model_from_pkl('pre_selection_20250919_without_fico.pkl')
without_fico = lgb.Booster(model_str = without_fico._handle)
print(without_fico.feature_name())
df_sample['tianchuang_id5@_main'] = without_fico.predict(df_sample[without_fico.feature_name()], num_iteration=without_fico.best_iteration)
df_sample['tianchuang_id5@_main'].head()


# In[51]:


df_sample['tianchuang_id5@_main'] = 1 - df_sample['tianchuang_id5@_main']


# In[52]:


fico_= load_model_from_pkl('pre_selection_20250919.pkl')
fico_ = lgb.Booster(model_str = fico_._handle)
print(fico_.feature_name())
df_sample['fico_model@tianchuang_score_id@_main'] = fico_.predict(df_sample[fico_.feature_name()], num_iteration=fico_.best_iteration)
df_sample['fico_model@tianchuang_score_id@_main'].head()


# In[53]:


df_sample['fico_model@tianchuang_score_id@_main'] = 1 - df_sample['fico_model@tianchuang_score_id@_main']


# In[56]:


filepath = '/home/liaoxilin/联合建模/友盟sdk&百行多头/'
umeng_model= load_model_from_pkl(filepath + 'result_友盟联合分融合模型/友盟联合分融合模型_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_sample['id5_off_cpd30_2508'] = umeng_model.predict(df_sample[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_sample['id5_off_cpd30_2508'].head()


# ### 5.3.1数据处理

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[ ]:


df_evalue = df_sample.copy()
df_evalue.info(show_counts=True)
df_evalue.head()


# In[ ]:


list(vars_combiner_dict.values())


# In[57]:


# 获取所有值列表
lists = list(vars_combiner_dict.values())

# 将第一个列表转换为集合，作为初始交集
common_elements = set(lists[0])  # 使用第一个列表

# 与其余每个列表求交集
for lst in lists[1:]:
    common_elements &= set(lst)  # 等同于 common_elements = common_elements.intersection(set(lst))

print("所有列表共有的元素:", common_elements)


# In[58]:


varsname_base = ['md5_off_m4d30_2509v2', 'id5_off_m4d30_2509v2']


# In[59]:


print(len(vars_combiner_list))
print(vars_combiner_list)


# In[61]:


list1 = [item.replace('@_main', '').split('@') for item in vars_combiner_list[1:]]
print(len(list1), list1)


# In[62]:


list2 = vars_combiner_list[1:]
print(len(list2), list2)

list3 = [['id5_off_m3d30_2507','id5_off_m4d30_2509v2','md5_off_m3d30_2507','md5_off_m4d30_2509v2']] * 15
print(len(list3), list3)


# In[63]:


# 生成三元组：(list1[i], list3[i], list2[i])
triplets = [(list2[i], list3[i], list1[i]) for i in range(len(list2))]

# 打印结果
for triplet in triplets:
    print(triplet)
    score_1, score_2, score_3 = triplet
    print(score_1, score_2, score_3)


# In[ ]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[ ]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[ ]:


filepath = '/home/liaoxilin/联合建模/友盟sdk&百行多头/'
umeng_model= load_model_from_pkl(filepath + 'result_友盟联合分融合模型/友盟联合分融合模型_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[ ]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_友盟Fico/友盟Fico离线融合_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[ ]:


fico_model_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型/fico联合分融合模型_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[ ]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v2/fico联合分融合模型v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl


# In[ ]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[ ]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[ ]:


# 方法2：使用向量化操作（更高效）
a_has_data = df_evalue['fico_model'].notna()  # 等价于 ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score' umeng_sdk_score].notna()

df_evalue['友盟fico是否缺失'] = np.where(
    a_has_data & b_has_data & c_has_data,  # 都有数据（都不是NaN）
    '1_都不缺失',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # 都无数据（都是NaN）
        '2_都有缺失',
        None  # 其他情况（一个有数据一个无数据）
    )
)


# In[ ]:


df_evalue['友盟fico是否缺失'].value_counts(dropna=False)


# ### 5.3.2 效果对比

# In[64]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    data = pd.Series({'KS': ks_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
#         df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[65]:


df_evalue = df_sample.dropna(subset=['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'],how='all')
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)
df_evalue.head()


# In[66]:


map_dict = {'3_oot2':'20250310-20250415','3_oot1':'20250301-20250309','2_test':'20250215-20250228','1_train':'20250101-20250214'}


# In[67]:


def rename_models(original_list):
    # 定义替换映射（按长度降序排列，避免短名称干扰长名称，比如 umeng 被 fico_model 包含）
    replacements = {
        'umeng_sdk_score': '友盟',
        'fico_model': 'fico',
        'haina_model': '海纳',
        'tianchuang_score': '天创'
    }
    
    result = []
    for name in original_list:
        new_name = name
        for old, new in replacements.items():
            new_name = new_name.replace(old, new)
        result.append(new_name)
    
    return result


# In[70]:



with pd.ExcelWriter(result_path + '授信场景m4d30_增益评估250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list[1:] + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[72]:



with pd.ExcelWriter(result_path + '授信场景m4d30_增益评估250922_v3.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list[1:] + varsname_base + ['m1b0077','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[74]:



with pd.ExcelWriter(result_path + '授信场景m4d30_贝叶斯调参_无fico_增益评估250922_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' not in col]
    e_score3 = ['md5_off_m4d30_2509v2','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[76]:



with pd.ExcelWriter(result_path + '授信场景m4d30_贝叶斯调参_有fico_增益评估250922_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model', 'fico_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' in col]
    e_score3 = ['md5_off_m4d30_2509v2','id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[83]:



with pd.ExcelWriter(result_path + '授信场景m4d30_3个子分_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' not in col]
    e_score3 = ['id5_off_cpd30_2508','tianchuang_id5@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[84]:



with pd.ExcelWriter(result_path + '授信场景m4d30_fico子分_v1.xlsx') as writer:

    e_score1 = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model']
    e_score2 = [col for col in vars_combiner_list[1:] if 'fico' in col]
    e_score3 = ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    score_list = e_score1 + e_score2 + e_score3
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[ ]:



with pd.ExcelWriter('授信场景m4d30_增益评估250922_v3.xlsx') as writer:

    for i, triplet in enumerate(triplets):
        score_1, score_2, score_3 = triplet
        score_list = [score_1] + score_2 + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
        print(len(score_list),score_list)
        
        target_list = ['target_mob4dpd30_1'] 
        labels_models_dict = {target: score_list for target in target_list}
        print(labels_models_dict)

        print(df_evalue.shape[0])
        tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
        tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_3].notna().any(axis=1),:]
        print(tmp_df_evalue.shape[0])
        # 整体客群
        groupkeys2 = ['channel_types', 'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

        df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        # 分客群
        groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'customer_tags', 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

        df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        
        # 合并数据
        df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
        df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)
        score_2_new = [f"KS_{col}" for col in score_2]
        df_auc_ks_all['KS最大值'] = df_auc_ks_all[score_2_new].max(axis=1)
        ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
        df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
        df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)
        original_list = [score_1]
        new_original_list = rename_models(original_list)
        score_2_new = new_original_list[0]
        df_auc_ks_all.to_excel(writer, sheet_name=f'{score_2_new}')
        
        gc.collect()


# In[ ]:


(df_auc_ks_all[f"KS_{score_1}"] - df_auc_ks_all[score_2_new]).min(axis=1)


# In[ ]:


df_auc_ks_all[[f"KS_{score_1}"]].subtract(df_auc_ks_all[score_2_new], axis=0)


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2.insert(0, 'time_windowns', value=df_ksauc_all2['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all2


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[ ]:


score_list = com_list
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','友盟fico是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','友盟fico是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','友盟fico是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all4.insert(1, 'time_windowns', value=df_ksauc_all4['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all4


# In[ ]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','fico数据是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','fico数据是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','fico数据是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("fico数据是否缺失=='1_不缺失' & flag=='1_新客' & channel=='金科渠道'").reset_index(drop=True)


# # 6. 评分分布

# In[ ]:





# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


score = 'y_pred_v3'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[ ]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[ ]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix_month)


# In[ ]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[ ]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[ ]:


df_evalue.to_csv(result_path + 'fico离线融合_report.csv',index=False)
print(result_path + 'fico离线融合_report.csv')


# # 7.模型部署和回溯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## 字段名称
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## 本地文件，注意：只写文件名即可，参数是 list 类型
        csv_filename_list=['天创模型分数v2.csv'],
        ## 本地文件路径，注意：需要本地的绝对路径
        input_path='/data/home/liaoxilin/联合建模/友盟sdk&百行多头/',                    
        ## 上传的数据库
        database='znzz_fintech_ads',        
        ## 上传的表名
        table_name='lxl_model_',
        # 分区字段
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # 自定义分隔符
        delimiter='|'
       ) 




#==============================================================================
# File: 实时模型_授信fpd30_洞侦续侦.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '洞侦续侦模型fpd30'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result/{task_name}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('===================')
        data = reader.to_pandas()

    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


df_sample_dict = {}


# In[7]:



# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-08', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
--离线fpd30融合模型子分
,t1.standard_score
,t1.bad_score
,t2.bh_a005
,t2.bh_a007
,t2.bh_a019
,t2.bh_a043
,t2.bh_a053
,t2.bh_a057
,t2.bh_a060
,t2.bh_a062
,t2.bh_a070
,t2.bh_a072
,t2.bh_a076
,t2.bh_a094
,t2.bh_a101
,t2.bh_a103
,t2.bh_a111
,t2.bh_a114
,t2.bh_a118
,t2.bh_a120
,t2.bh_a126
,t2.bh_a132
,t2.bh_a139
,t2.bh_a152
,t2.bh_a154
,t2.bh_a163
,t2.bh_a165
,t2.bh_a166
,t2.bh_a168
,t2.bh_a170
,t2.bh_a179
,t2.bh_a187
,t2.bh_a190
,t2.bh_a191
,t2.bh_a207
,t2.bh_a212
,t2.bh_a217
,t2.bh_a218
,t2.bh_a288
,t2.bh_a292
,t2.bh_a293
,t2.bh_a295
,t2.bh_a297
,t2.bh_a299
,t2.bh_a330
,t2.bh_a332
,t2.bh_a335
,t2.bh_a354
,t2.bh_a366
,t2.bh_a380
,t2.bh_a389
,t2.bh_a392
,t2.bh_a393
,t2.bh_b226
,t2.bh_b236
,t2.bh_b245
,t2.bh_b264
,t2.bh_b267
,t2.bh_b275
,t2.bh_b279
,t2.bh_b290
,t2.bh_b292
,t2.bh_b300
,t2.bh_c013
,t2.bh_c014
,t2.bh_c031
,t2.bh_c034
,t2.bh_c053
,t2.bh_c057
,t2.bh_c067
,t2.bh_c077
,t2.bh_c079
,t2.bh_d016
,t2.bh_d028
,t2.bh_d034
,t2.bh_e031
,t2.bh_e044
,t2.bh_e048
,t2.bh_e049
,t2.bh_e050
,t2.bh_f018
,t2.bh_f021
,t2.bh_f028
,t2.bh_g022
,t2.bh_g036
,t2.bh_g042
,t2.bh_g044
,t2.bh_h068
,t2.bh_h074
,t2.bh_h079
,t2.bh_h084
,t2.bh_h098
,t2.bh_h099
,t2.bh_h104
,t2.bh_h111
,t2.bh_h119
,t2.bh_h121
,t2.bh_h123
,t2.bh_h124
,t2.bh_h131
,t2.bh_h135
,t2.bh_h136
,t2.bh_h138
,t2.bh_h150
,t2.bh_h151
,t2.bh_h153
,t2.bh_h155
,t2.bh_h158
,t2.bh_h159
,t2.bh_h161
,t2.bh_h167
,t2.bh_h168
,t2.bh_h170
,t2.bh_h171
,t2.bh_h173
,t2.bh_qu005
,t2.bh_qu006
,t2.bh_qu009
,t2.bh_qu011
,t2.bh_qu015
,t2.bh_qu016
,t2.bh_qu018
,t2.bh_qu019
,t2.bh_qu022
,t2.bh_x075
,t2.bh_x090
,t2.bh_x113
,t3.value_016
,t3.value_048
,t3.value_077
,t3.value_082
,t3.value_108
,t3.value_110
,t3.value_115
,t3.value_123
,t3.value_125
,t3.value_132
,t3.value_145
,t3.value_151
,t3.value_178
,t3.value_181
,t3.value_193
,t3.value_202
,t3.value_210
,t3.value_220
,t3.value_223
,t3.value_232
,t3.value_236
,t3.value_240
,t3.value_241
,t3.value_251
,t3.value_266
,t3.value_276
,t3.value_285
,t3.value_299
,t3.value_305
,t3.value_306
,t3.value_313
,t3.value_329
,t3.value_335
,t3.value_341
,t3.value_342
,t3.value_351
,t3.value_368
,t3.value_377
,t3.value_380
,t3.value_384
,t3.value_392
,t3.value_398
,t3.value_408
,t3.value_414
,t3.value_415
,t3.value_416

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 2) 
      and lending_time='{run_day}'
    ) as t 
--离线风险模型子分
left join 
    (
    select t.* 
    from znzz_fintech_ads.dm_f_lxl_test_behave_model_merge_fpd30_score as t 
    where dt=date_sub('{run_day}',1)
    ) as t1 on t.id_no_des=t1.id_no_des

--洞侦变量
left join
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk 
    from znzz_fintech_dwd.dwd_beforeloan_data_source_bh_fqz_djv3_id as t
    where dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t2 on t.id_no_des=t2.id_no_des and t2.rk=1

--续侦变量
left join 
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk
    from znzz_fintech_dwd.dwd_beforeloan_third_combine_sub_id as t  
    where ds='jzhl_thirds_platform_intf_bh_nfacq932_20240529'
      and dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t3 on t.id_no_des=t3.id_no_des and t3.rk=1
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[8]:


data_time = pd.DataFrame({'run_day':list(df_sample_dict.keys())})
data_time['run_day'].value_counts()


# In[9]:


df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[10]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['id_no_des'].nunique())


# In[11]:


print(df_sample_['lending_time'].min(), df_sample_['lending_time'].max())


# In[ ]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[ ]:


df_sample['channel_type'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rate'] = df_sample['channel_id'].apply(channel_rate)


# In[12]:


df_sample_.to_csv(result_path + '桔子商城fpd30授信实时模型_原始数据集_250110_part1.csv',index=False)
print(result_path + '桔子商城fpd30授信实时模型_原始数据集_250110_part1.csv')


# In[13]:


df_sample = df_sample_.query("diff_days<=30")
df_sample.info()


# In[14]:


df_sample.to_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110_part1.csv',index=False)
print(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110_part1.csv')


# In[18]:


del df_sample
gc.collect()


# In[19]:


df_sample_part1 = pd.read_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110_part1.csv')
df_sample_part1.info()


# In[15]:


df_sample_part2 = pd.read_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110_part2.csv')
df_sample_part2.info()


# In[20]:


df_sample = pd.concat([df_sample_part1, df_sample_part2.query("lending_time>='2024-11-09'")], ignore_index=True)
df_sample.info()
df_sample.head()


# In[21]:


print(df_sample.shape, df_sample['order_no'].nunique(), df_sample['id_no_des'].nunique())


# In[22]:


df_sample.columns.to_list()[:12]


# In[23]:


varsname = [col for col in df_sample.columns.to_list()[12:]]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[24]:


print(result_path)


# In[25]:


# for i, col in enumerate(varsname[773:]):
#     if df_sample[col].dtype=='object':
#         print(f"======第{i}个变量：{col}========")
#         df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')


# In[34]:


pd.set_option('display.max_row',None)
df_sample.groupby(['lending_time','fpd20'])['order_no'].count().unstack()


# In[33]:


df_sample['fpd20'] = df_sample['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[35]:


df_sample.loc[df_sample.query("lending_time>='2024-11-10'").index, 'fpd30']= -1
df_sample.loc[df_sample.query("lending_time>='2024-11-20'").index, 'fpd20']= -1


# In[28]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-10-01' & lending_time<='2024-11-09'").index, 'data_set']='3_oot'


# In[29]:


target = 'fpd30'


# In[30]:


df_sample[[target]+varsname].info(show_counts=True)
df_sample[[target]+varsname].head()


# In[36]:


df_sample.to_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110.csv',index=False)
print(result_path + '桔子商城fpd30授信实时模型_建模数据集_250110.csv')


# In[37]:


df_model = df_sample.copy()
df_sample = df_sample.query("lending_time<='2024-11-09'")


# # 1. 样本概况

# In[38]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[39]:


print(df_sample[target].value_counts())


# In[40]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[41]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[42]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[43]:


task_name


# In[44]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[45]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])


# In[46]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.1缺失值处理

# In[48]:


# df_sample = df_model.copy()df_sample[varsname]
df_sample = df_sample.replace(-1, np.nan)
gc.collect()


# ## 2.2 数据探索

# In[50]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[51]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[53]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[55]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[56]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[57]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[58]:


df_iv.loc[to_drop_iv,:].head()


# In[59]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[62]:


# df_sample[varsname_v1+[target]].info()
df_sample = df_sample.reset_index(drop=True)


# In[63]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]].drop('standard_score',axis=1),
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[65]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[66]:


df_iv.loc[to_drop2,:].head()


# In[67]:


tmp = df_iv.loc[to_drop2,:].sort_values(by='iv', ascending=False)
tmp.head(20)


# In[68]:


# to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# In[70]:


# df_sample[varsname_v2].info()


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[71]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            if any(x == 0 for x in regroup['bad']) or any(x == 0 for x in regroup['good']):
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
                regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10 
            else:
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
                regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            if any(x == 0 for x in regroup['bad']) or any(x == 0 for x in regroup['good']):
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
                regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10 
            else:
                regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
                regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[72]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[73]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[74]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[75]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[77]:


new_bins_dict


# In[78]:


combiner.load(new_bins_dict)


# In[79]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[80]:


to_drop_mode


# In[81]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), varsname_v2,                                    'lending_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[82]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[83]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[84]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[85]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[86]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# In[ ]:





# ### 删除不稳定特征

# In[87]:


len(list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index))


# In[88]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[89]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[90]:


df_psi_by_set.loc[drop_by_psi_set,:].head()


# In[91]:


df_miss_set.info()


# In[92]:


to_drop3 = [col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1]
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[93]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[94]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[95]:


df_sample_woe.head()


# In[96]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
    :param threshold: 相关系数的阈值，默认为0.85
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.corr(method=method)
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[97]:


gc.collect()


# In[98]:


df_iv_by_set.info()
df_iv_by_set.head()


# In[108]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe[varsname_v3],
                                                     df_iv_by_set['3_oot'],
                                                     method='kendall',
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[109]:


df_high_corr.info()
df_high_corr.head()


# In[111]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[112]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[113]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'lending_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='kendalltau'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[114]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[115]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[116]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[ ]:





# ## 4.3 逐步回归筛选

# In[78]:


# # 将woe转化后的数据做逐步回归
# train_woe = df_sample_woe.query("data_set=='1_train'")[varsname_v3+[target]]
# final_data, to_drop6 = toad.selection.stepwise(train_woe, target=target, estimator='ols', direction = 'both', \
#                                      criterion = 'aic', exclude = None, return_drop=True)

# print(final_data.shape) # 逐步回归从31个变量中选出了10个


# In[118]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[119]:


gc.collect()


# # 5.模型训练

# ## 5.1 模型训练

# In[120]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[122]:


# 2 定义超参空间
# hp.quniform("参数名称",下界,上界,步长)-适用于离散均匀分布的浮点点数
# hp.uniform("参数名称",下界, 下界)-适用于连续随机分布的浮点数
# hp.randint("参数名称",上界)-适用于[0,上界)的整数,区间为左闭右开
# hp.choice("参数名称",["字符串1","字符串2",...])-适用于字符串类型,最优参数由索引表示
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# 可以根据需要，注释掉偏后的一些不太重要的超参

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",21,200,1),
          "max_depth":2,
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",30,200,1),
          "feature_fraction":hp.uniform("feature_fraction",0.5,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.5,1.0),
#           "feature_fraction":1.0,
#           "bagging_fraction":1.0,
          "min_gain_to_split":10,
#           "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 10.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 100, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 50
          }
spaces


# In[123]:


# 3，执行超参搜索
# 有了目标函数和参数空间,接下来要进行优化,需要了解以下参数:
# fmin:自定义使用的代理模型(参数algo),hyperopt支持如下搜索算法：
#       随机搜索(hyperopt.rand.suggest)
#       模拟退火(hyperopt.anneal.suggest)
#       TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）
# partial:修改算法涉及到的具体参数,包括模型具体使用了多少少个初始观测值(参数n_start_jobs),
#         以及在计算采集函数值时究竟考虑多少个样本(参数n_EI_candidates)
# trials:记录整个迭代过程,从hyperopt库中导入的方法Trials(),优化完成之后,
#        可以从保存好的trials中查看损失、参数等各种中间信息
# early_stop_fn:提前停止参数,从hyperopt库导入的方法no_progresss_loss(),可以输入具体的数字n,
#               表示当损失连续n次没有下降时,让算法提前停止
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds, init_model=None):

        """定义目标函数"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1,
                'num_threads': -1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                init_model=init_model,
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set,                                 num_boost_round=num_boost_round, init_model=init_model)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(30)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[124]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[125]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[128]:


len(varsname_v5)


# In[127]:


varsname_v5.remove('bad_score')


# In[129]:


# 训练数据集
X_train = df_sample.query("data_set!='3_oot'")[varsname_v5]
y_train = df_sample.query("data_set!='3_oot'")[target]
print(X_train.shape)


# In[ ]:


# 4，获取最优参数，调参过程
# 确定一个较高的学习率
# 对决策树基本参数调参
# 正则化参数调参
# 降低学习率
best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=10)


# In[131]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[132]:


print("最优参数best_params: ", best_params)


# In[384]:


### 添加无需调参的通用参数
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
bst_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
bst_params['learning_rate'] = spaces['learning_rate']
## 正则参数，防止过拟合
bst_params['bagging_fraction'] = best_params['bagging_fraction']    
bst_params['feature_fraction'] = best_params['feature_fraction'] 
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = spaces['lambda_l2']
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# 调参后的参数需要变成整数型
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = spaces['max_depth']
# 调参后的其他参
bst_params['min_gain_to_split'] = spaces['min_gain_to_split']


# In[385]:


print("最优参数bst_params: ", bst_params)


# In[386]:


# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[387]:


# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[389]:


gc.collect()


# In[390]:


# 最初评估模型效果
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[391]:


# 评估模型效果
ks_auc_dict_month = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month = pd.concat(ks_auc_dict_month.values(), keys=ks_auc_dict_month.keys())
df_ks_auc_month = df_ks_auc_month.reset_index()
df_ks_auc_month.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[392]:


df_ks_auc_month


# In[394]:


# 评估模型效果
ks_auc_dict_set = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set = pd.concat(ks_auc_dict_set.values(), keys=ks_auc_dict_set.keys())
df_ks_auc_set = df_ks_auc_set.reset_index()
df_ks_auc_set.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[395]:


df_ks_auc_set


# In[396]:


# 模型变量重要性
df_importance_month = feature_importance(lgb_model) 
df_importance_month = pd.merge(df_importance_month, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month = df_importance_month.reset_index()
df_importance_month = df_importance_month.rename(columns={'index':'varsname'})
df_importance_month.info()
df_importance_month.head()


# In[397]:



# 效果评估后模型变量重要性
df_importance_set = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set = pd.merge(df_importance_set, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set['iv的变化幅度'] = df_importance_set['3_oot_iv']/df_importance_set['1_train_iv'] - 1
df_importance_set.drop(columns=['1_train_psi'], inplace=True)
df_importance_set = df_importance_set.reset_index()
df_importance_set = df_importance_set.rename(columns={'index':'varsname'})
df_importance_set.info()
df_importance_set.head()


# In[399]:


# # 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
# print(f"模型保存完成！：{timestamp}")
# print(result_path + f'{task_name}_{timestamp}.pkl')
# print(result_path + f'{task_name}_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型结果_{task_name}_{timestamp}.xlsx') as writer:
    df_importance_month.to_excel(writer, sheet_name='df_importance_month')
    df_importance_set.to_excel(writer, sheet_name='df_importance_set')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
#     df_ks_auc_month_30.to_excel(writer, sheet_name='df_ks_auc_month_30')
#     df_ks_auc_set_30.to_excel(writer, sheet_name='df_ks_auc_set_30')
print(f"数据存储完成！{timestamp}")
print(result_path + f'4_模型结果_{task_name}_{timestamp}.xlsx')


# ## 5.2 模型优化

# ### 5.2.1参数优化

# In[ ]:


best_params:  {'bagging_fraction': 0.6138533923759146, 'feature_fraction': 0.6011434207908619, 'min_data_in_leaf': 75.0, 'num_leaves': 198.0}


# In[ ]:


bst_params:  {'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'bagging_freq': 1, 'scale_pos_weight': 1, 'seed': 1, 'num_threads': -1, 'learning_rate': 0.1, 'bagging_fraction': 0.6138533923759146, 'feature_fraction': 0.6011434207908619, 'lambda_l1': 0, 'lambda_l2': 300, 'early_stopping_rounds': 50, 'num_leaves': 198, 'min_data_in_leaf': 75, 'max_depth': 2, 'min_gain_to_split': 10}


# In[227]:


### 优化调参1
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6138533923759146     
opt_params['feature_fraction'] = 0.6011434207908619
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 75
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[228]:


print("最优参数opt_params: ", opt_params)


# In[229]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[230]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[231]:


# 确定数据集参数后，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[232]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[233]:


# 优化后评估模型效果
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[258]:


# 优化后评估模型效果
ks_auc_dict_month_v1 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v1', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v1[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v1[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v1[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v1 = pd.concat(ks_auc_dict_month_v1.values(), keys=ks_auc_dict_month_v1.keys())
df_ks_auc_month_v1 = df_ks_auc_month_v1.reset_index()
df_ks_auc_month_v1.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[259]:


df_ks_auc_month_v1


# In[261]:


# 优化后评估模型效果
ks_auc_dict_set_v1 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v1', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v1[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v1[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v1', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v1[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v1 = pd.concat(ks_auc_dict_set_v1.values(), keys=ks_auc_dict_set_v1.keys())
df_ks_auc_set_v1 = df_ks_auc_set_v1.reset_index()
df_ks_auc_set_v1.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[262]:


df_ks_auc_set_v1


# In[263]:


# 模型变量重要性
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = df_importance_month_v1.rename(columns={'index':'varsname'})
df_importance_month_v1.head()


# In[265]:


df_importance_set_v1 = feature_importance(lgb_model) 
tmp = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True, right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='inner', left_index=True, right_index=True)
df_importance_set_v1['iv的变化幅度'] = df_importance_set_v1['3_oot_iv']/df_importance_set_v1['1_train_iv'] - 1
df_importance_set_v1.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = df_importance_set_v1.rename(columns={'index':'varsname'})
df_importance_set_v1.head()


# In[238]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_paramsopt_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_paramsopt_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_paramsopt_{timestamp}.pkl')
print(result_path + f'{task_name}_paramsopt_{timestamp}.bin')


# In[266]:


# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_paramsopt_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')
print(f"数据存储完成！{timestamp}")
print(result_path + f'5_模型优化_{task_name}_paramsopt_{timestamp}.xlsx')


# ### 5.2.2 特征优化

# In[460]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[461]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[462]:


to_drop6 = ['standard_score']
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[463]:


# 使用的数据，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v6],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[469]:


varsname_v7 = ['y_prob_v2','standard_score']


# In[470]:


# 使用的数据，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v7],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[476]:


### 优化调参2
opt_params_v2 = {}
opt_params_v2['boosting'] = 'gbdt'
opt_params_v2['objective'] = 'binary'
opt_params_v2['metric'] = 'auc'
opt_params_v2['bagging_freq'] = 1
opt_params_v2['scale_pos_weight'] = 1 
opt_params_v2['seed'] = 1 
opt_params_v2['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params_v2['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params_v2['bagging_fraction'] = 0.6138533923759146     
opt_params_v2['feature_fraction'] = 0.6011434207908619
opt_params_v2['lambda_l1'] = 0
opt_params_v2['lambda_l2'] = 300
opt_params_v2['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params_v2['num_leaves'] = 21
opt_params_v2['min_data_in_leaf'] = 75
opt_params_v2['max_depth'] = 2
# 调参后的其他参
opt_params_v2['min_gain_to_split'] = 10
print(opt_params_v2)


# In[477]:


# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params_v2, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[466]:


# 最初评估模型效果
df_sample['y_prob_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[478]:


# 最初评估模型效果
df_sample['y_prob_v2_merge'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[479]:


# 优化后评估模型效果
ks_auc_dict_month_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v2[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v2 = pd.concat(ks_auc_dict_month_v2.values(), keys=ks_auc_dict_month_v2.keys())
df_ks_auc_month_v2 = df_ks_auc_month_v2.reset_index()
df_ks_auc_month_v2.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[480]:


df_ks_auc_month_v2


# In[481]:


# 优化后评估模型效果
ks_auc_dict_month_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2_merge', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v2[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2_merge', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2_merge', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v2 = pd.concat(ks_auc_dict_month_v2.values(), keys=ks_auc_dict_month_v2.keys())
df_ks_auc_month_v2 = df_ks_auc_month_v2.reset_index()
df_ks_auc_month_v2.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[482]:


df_ks_auc_month_v2


# In[284]:


df_ks_auc_month_v2


# In[285]:



# 优化后评估模型效果
ks_auc_dict_set_v2 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v2', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v2[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v2[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v2', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v2[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v2 = pd.concat(ks_auc_dict_set_v2.values(), keys=ks_auc_dict_set_v2.keys())
df_ks_auc_set_v2 = df_ks_auc_set_v2.reset_index()
df_ks_auc_set_v2.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[286]:


df_ks_auc_set_v2


# In[287]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = df_importance_month_v2.rename(columns={'index':'varsname'})
df_importance_month_v2


# In[294]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = df_importance_month_v2.rename(columns={'index':'varsname'})
df_importance_month_v2


# In[288]:


# 效果评估后模型变量重要性
df_importance_set_v2 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v2 = pd.merge(df_importance_set_v2, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v2['iv的变化幅度'] = df_importance_set_v2['3_oot_iv']/df_importance_set_v2['1_train_iv'] - 1
df_importance_set_v2.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = df_importance_set_v2.rename(columns={'index':'varsname'})
df_importance_set_v2


# In[311]:



# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_featureopt_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_featureopt_{timestamp}.bin')
# print(f"模型保存完成！：{timestamp}")
# print(result_path + f'{task_name}_featureopt_{timestamp}.pkl')
# print(result_path + f'{task_name}_featureopt_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_featureopt_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')     
print(f"数据存储完成！{timestamp}")
print(result_path + f'5_模型优化_{task_name}_featureopt_{timestamp}.xlsx')


# ### 5.2.3增量学习

# In[428]:


### 优化调参3
opt_params_v3 = {}
opt_params_v3['boosting'] = 'gbdt'
opt_params_v3['objective'] = 'binary'
opt_params_v3['metric'] = 'auc'
opt_params_v3['bagging_freq'] = 1
opt_params_v3['scale_pos_weight'] = 1 
opt_params_v3['seed'] = 1 
opt_params_v3['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params_v3['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params_v3['bagging_fraction'] = 0.6138533923759146     
opt_params_v3['feature_fraction'] = 0.6011434207908619
opt_params_v3['lambda_l1'] = 0
opt_params_v3['lambda_l2'] = 300
opt_params_v3['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params_v3['num_leaves'] = 198
opt_params_v3['min_data_in_leaf'] = 75
opt_params_v3['max_depth'] = 2
# 调参后的其他参
opt_params_v3['min_gain_to_split'] = 10
print(opt_params_v3)


# In[429]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[430]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[431]:


# 确定数据集参数后，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v5],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[432]:


init_model = load_model_from_pkl('./result/洞侦续侦模型fpd30/洞侦续侦模型fpd30_20250113103851.pkl')


# In[433]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=init_model)


# In[434]:


# 优化后评估模型效果
df_sample['y_prob_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[435]:


# 优化后评估模型效果
ks_auc_dict_month_v3 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v3', 'lending_month')
tmp2 = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
ks_auc_dict_month_v3[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v3[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'lending_month')
    tmp2 = get_target_summary(grouped_df, target, 'lending_month').set_index('bins')
    ks_auc_dict_month_v3[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_month_v3 = pd.concat(ks_auc_dict_month_v3.values(), keys=ks_auc_dict_month_v3.keys())
df_ks_auc_month_v3 = df_ks_auc_month_v3.reset_index()
df_ks_auc_month_v3.rename(columns={'level_0':'channel', 'level_1':'month'},inplace=True)


# In[436]:


df_ks_auc_month_v3


# In[437]:


# 优化后评估模型效果
ks_auc_dict_set_v3 = {}

tmp1 = model_ks_auc(df_sample, target, 'y_prob_v3', 'data_set')
tmp2 = get_target_summary(df_sample, target, 'data_set').set_index('bins')
ks_auc_dict_set_v3[('全渠道',)] = pd.concat([tmp2, tmp1], axis=1)

for channel_type, grouped_df in df_sample.groupby(['channel_type']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v3[channel_type] = pd.concat([tmp2, tmp1], axis=1)

for channel_rate, grouped_df in df_sample.groupby(['channel_rate']):  
    tmp1 = model_ks_auc(grouped_df, target, 'y_prob_v3', 'data_set')
    tmp2 = get_target_summary(grouped_df, target, 'data_set').set_index('bins')
    ks_auc_dict_set_v3[channel_rate] = pd.concat([tmp2, tmp1], axis=1)

df_ks_auc_set_v3 = pd.concat(ks_auc_dict_set_v3.values(), keys=ks_auc_dict_set_v3.keys())
df_ks_auc_set_v3 = df_ks_auc_set_v3.reset_index()
df_ks_auc_set_v3.rename(columns={'level_0':'channel', 'level_1':'dataset'},inplace=True)


# In[438]:


df_ks_auc_set_v3


# In[439]:


# 模型变量重要性
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_by_month, how='inner', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = df_importance_month_v3.rename(columns={'index':'varsname'})
df_importance_month_v3.info()
df_importance_month_v3.head()


# In[440]:



# 效果评估后模型变量重要性
df_importance_set_v3 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v3['iv的变化幅度'] = df_importance_set_v3['3_oot_iv']/df_importance_set_v3['1_train_iv'] - 1
df_importance_set_v3.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = df_importance_set_v3.rename(columns={'index':'varsname'})
df_importance_set_v3.info()
df_importance_set_v3.head()


# In[441]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_initmodel_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_initmodel_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_initmodel_{timestamp}.pkl')
print(result_path + f'{task_name}_initmodel_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_initmodel_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')     
print(f"数据存储完成！{timestamp}")
print(result_path + f'5_模型优化_{task_name}_initmodel_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# ### 5.3.1数据处理

# In[483]:


usecols= ['order_no','channel_id', 'lending_time','lending_month', 'mob',          'maxdpd', 'fpd', 'fpd10', 'fpd30', 'mob4dpd30', 'diff_days'] + varsname_v5
df1 = pd.read_csv(result_path+'桔子商城fpd30授信实时模型_原始数据集_250110_part1.csv',usecols=usecols)
df2 = pd.read_csv(result_path+'桔子商城fpd30授信实时模型_原始数据集_250117_part2.csv',usecols=usecols)
df_model_vars = pd.concat([df1, df2.query("lending_time>='2024-11-09'")],ignore_index=True)
df_model_vars.info(show_counts=True)
df_model_vars.head()


# In[486]:


print(df_model_vars.shape[0], df_model_vars['order_no'].nunique())


# In[487]:


df_model_vars[varsname_v5].describe().T


# In[488]:


df_model_vars[varsname_v5] = df_model_vars[varsname_v5].replace(-1, np.nan)
gc.collect()


# In[489]:


df_model_vars[varsname_v5].describe().T


# In[490]:


# 衍生Y标签
print(df_model_vars['fpd'].min(), df_model_vars['fpd'].max())
print(df_model_vars['fpd30'].min(), df_model_vars['fpd30'].max())


# In[491]:


# 衍生Y标签
# df_model_vars['fpd10'] = df_model_vars['fpd'].apply(lambda x: 1 if x>10 else 0)
df_model_vars['fpd20'] = df_model_vars['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[494]:


df_model_vars.groupby(["lending_time",'fpd10'])['order_no'].count().unstack()


# In[495]:


df_model_vars = df_model_vars.query("lending_time<='2024-12-05'").reset_index(drop=True)
df_model_vars.loc[df_model_vars.query("lending_time>='2024-11-17'").index, 'fpd30'] = -1
df_model_vars.loc[df_model_vars.query("lending_time>='2024-11-27'").index, 'fpd20'] = -1


# In[496]:


# 添加客群标签
def diff_days_(x):
    if x<=30:
        days = 'T30-'
    elif x>30:
        days = 'T30+'
    else:
        days = np.nan
    return days

df_model_vars['客群'] = df_model_vars['diff_days'].apply(diff_days_)


# In[497]:


# 基础训练模型打分 
lgb_model= load_model_from_pkl(result_path + '洞侦续侦模型fpd30_20250113103851.pkl')
df_model_vars['y_prob'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[498]:


# 第一次优化：参数优化模型打分 
lgb_model= load_model_from_pkl(result_path + '洞侦续侦模型fpd30_paramsopt_20250113115717.pkl')
df_model_vars['y_prob_v1'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[499]:


# 第三次优化：增量学习模型打分 
lgb_model= load_model_from_pkl(result_path + '洞侦续侦模型fpd30_initmodel_20250117103621.pkl')
df_model_vars['y_prob_v3'] = lgb_model.predict(df_model_vars[varsname_v5],
                                               num_iteration=lgb_model.best_iteration)


# In[357]:


# # 最新模型数据表现现模型数据 
# sql="""
# select t2.*
# from
# (
#     select order_no 
#     from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
#     where dt=date_sub(current_date(), 2) 
#       and lending_time>='2024-07-21'
#       and lending_time<='2024-11-05'
# ) as t1 
# inner join znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t2 on t1.order_no=t2.order_no
# ;
# """
# df_tx_bj = get_data(sql)
# df_tx_bj.info(show_counts=True)
# df_tx_bj.head() 


# In[358]:


# df_tx_bj['t_beha3_fpd'] = pd.to_numeric(df_tx_bj['t_beha3_fpd'])
# df_tx_bj['t_beha3_mob4'] = pd.to_numeric(df_tx_bj['t_beha3_mob4'])


# In[359]:


# selected_cols = df_tx_bj.columns.to_list()[3:]
# df_tx_bj[selected_cols].describe().T


# In[360]:


# df_tx_bj.to_csv(result_path + '全渠道其他提现模型分数_241218.csv')
# print(result_path + '全渠道其他提现模型分数_241218.csv')


# In[361]:


# # 好分数转为坏分数
# for i, col in enumerate(selected_cols):
#     print(f'第{i}个变量：{col}')
#     df_tx_bj[col] = 1 - df_tx_bj[col]


# ### 5.3.2 效果对比

# In[500]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col, percentile=0.95):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    badrate = df[label_col].mean()
    
    if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
    elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
    else:
        print("请根据概率分数是好分数还是坏分数，决定分位数的位置")

    if badrate>0 and pct_n_badrate>0:
        lift_n = pct_n_badrate/badrate
    else:
        lift_n = np.nan
    return pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}',
                                                    'top5lift':f'top5lift_{score_}'})
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[511]:


# df_evalue = df_model_vars.copy()
df_evalue = df_evalue.query("standard_score==standard_score")
df_evalue.info(show_counts=True)


# In[502]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[503]:


df_evalue['channel_type'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_rate'] = df_evalue['channel_id'].apply(channel_rate)


# In[504]:


df_evalue['bad_score'] = 1000 - df_evalue['standard_score'] 


# In[505]:


colsname = ['y_prob', 'y_prob_v1', 'y_prob_v3', 'bad_score']

print(colsname)
target_list = ['fpd10', 'fpd20', 'fpd30']
labels_models_dict = {target: colsname for target in target_list}
print(labels_models_dict)


# In[512]:


df_evalue['渠道'] = '全渠道'
groupkeys1 = ['客群', '渠道', 'lending_month']
df_ksauc_all_v1 = cal_ks_auc(df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.head()


# In[513]:



groupkeys2 = ['客群', 'channel_type', 'lending_month']
df_ksauc_all_v2 = cal_ks_auc(df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_type':'渠道'}, inplace=True)
df_ksauc_all_v2.head()


# In[518]:


df_ksauc_all_v2


# In[514]:



groupkeys4 = ['客群', 'channel_rate', 'lending_month']
df_ksauc_all_v4 = cal_ks_auc(df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rate':'渠道'}, inplace=True)
df_ksauc_all_v4.head()


# In[515]:


df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0, ignore_index=True)
df_ksauc_all_1.rename(columns={'lending_month':'月份', 'target_type':'标签'},inplace=True)
df_ksauc_all_1.head()


# In[516]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_模型对比分析_{task_name}_{timestamp}_部分覆盖.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}_部分覆盖.xlsx')


# In[ ]:





# # 6. 评分分布

# In[517]:


result_path


# In[484]:


df_sample.to_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250117.csv',index=False)
print(result_path + '桔子商城fpd30授信实时模型_建模数据集_250117.csv')


# In[376]:


score = 'y_prob_v3'


# In[377]:


df_sample['lending_month'].value_counts()


# In[378]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[379]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), 
                                                [score], 'lending_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[381]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: 实时模型_授信_fpd30_桔子商城.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '桔子商城授信模型fpd30'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result/{task_name}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('===================')
        data = reader.to_pandas()

    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


df_sample_dict = {}


# In[8]:



# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-15', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
--离线fpd30融合模型子分
,t1.standard_score
--洞侦变量
,t2.bh_a001
,t2.bh_a002
,t2.bh_a003
,t2.bh_a004
,t2.bh_a005
,t2.bh_a006
,t2.bh_a007
,t2.bh_a008
,t2.bh_a009
,t2.bh_a010
,t2.bh_a011
,t2.bh_a012
,t2.bh_a013
,t2.bh_a014
,t2.bh_a015
,t2.bh_a016
,t2.bh_a017
,t2.bh_a018
,t2.bh_a019
,t2.bh_a020
,t2.bh_a021
,t2.bh_a022
,t2.bh_a023
,t2.bh_a024
,t2.bh_a025
,t2.bh_a026
,t2.bh_a027
,t2.bh_a028
,t2.bh_a029
,t2.bh_a030
,t2.bh_a031
,t2.bh_a032
,t2.bh_a033
,t2.bh_a034
,t2.bh_a035
,t2.bh_a036
,t2.bh_a037
,t2.bh_a038
,t2.bh_a039
,t2.bh_a040
,t2.bh_a041
,t2.bh_a042
,t2.bh_a043
,t2.bh_a044
,t2.bh_a045
,t2.bh_a046
,t2.bh_a047
,t2.bh_a048
,t2.bh_a049
,t2.bh_a050
,t2.bh_a051
,t2.bh_a052
,t2.bh_a053
,t2.bh_a054
,t2.bh_a055
,t2.bh_a056
,t2.bh_a057
,t2.bh_a058
,t2.bh_a059
,t2.bh_a060
,t2.bh_a061
,t2.bh_a062
,t2.bh_a063
,t2.bh_a064
,t2.bh_a065
,t2.bh_a066
,t2.bh_a067
,t2.bh_a068
,t2.bh_a069
,t2.bh_a070
,t2.bh_a071
,t2.bh_a072
,t2.bh_a073
,t2.bh_a074
,t2.bh_a075
,t2.bh_a076
,t2.bh_a077
,t2.bh_a078
,t2.bh_a079
,t2.bh_a080
,t2.bh_a081
,t2.bh_a082
,t2.bh_a083
,t2.bh_a084
,t2.bh_a085
,t2.bh_a086
,t2.bh_a087
,t2.bh_a088
,t2.bh_a089
,t2.bh_a090
,t2.bh_a091
,t2.bh_a092
,t2.bh_a093
,t2.bh_a094
,t2.bh_a095
,t2.bh_a096
,t2.bh_a097
,t2.bh_a098
,t2.bh_a099
,t2.bh_a100
,t2.bh_a101
,t2.bh_a102
,t2.bh_a103
,t2.bh_a104
,t2.bh_a105
,t2.bh_a106
,t2.bh_a107
,t2.bh_a108
,t2.bh_a109
,t2.bh_a110
,t2.bh_a111
,t2.bh_a112
,t2.bh_a113
,t2.bh_a114
,t2.bh_a115
,t2.bh_a116
,t2.bh_a117
,t2.bh_a118
,t2.bh_a119
,t2.bh_a120
,t2.bh_a121
,t2.bh_a122
,t2.bh_a123
,t2.bh_a124
,t2.bh_a125
,t2.bh_a126
,t2.bh_a127
,t2.bh_a128
,t2.bh_a129
,t2.bh_a130
,t2.bh_a131
,t2.bh_a132
,t2.bh_a133
,t2.bh_a134
,t2.bh_a135
,t2.bh_a136
,t2.bh_a137
,t2.bh_a138
,t2.bh_a139
,t2.bh_a140
,t2.bh_a141
,t2.bh_a142
,t2.bh_a143
,t2.bh_a144
,t2.bh_a145
,t2.bh_a146
,t2.bh_a147
,t2.bh_a148
,t2.bh_a149
,t2.bh_a150
,t2.bh_a151
,t2.bh_a152
,t2.bh_a153
,t2.bh_a154
,t2.bh_a155
,t2.bh_a156
,t2.bh_a157
,t2.bh_a158
,t2.bh_a159
,t2.bh_a160
,t2.bh_a161
,t2.bh_a162
,t2.bh_a163
,t2.bh_a164
,t2.bh_a165
,t2.bh_a166
,t2.bh_a167
,t2.bh_a168
,t2.bh_a169
,t2.bh_a170
,t2.bh_a171
,t2.bh_a172
,t2.bh_a173
,t2.bh_a174
,t2.bh_a175
,t2.bh_a176
,t2.bh_a177
,t2.bh_a178
,t2.bh_a179
,t2.bh_a180
,t2.bh_a181
,t2.bh_a182
,t2.bh_a183
,t2.bh_a184
,t2.bh_a185
,t2.bh_a186
,t2.bh_a187
,t2.bh_a188
,t2.bh_a189
,t2.bh_a190
,t2.bh_a191
,t2.bh_a192
,t2.bh_a193
,t2.bh_a194
,t2.bh_a195
,t2.bh_a196
,t2.bh_a197
,t2.bh_a198
,t2.bh_a199
,t2.bh_a200
,t2.bh_a201
,t2.bh_a202
,t2.bh_a203
,t2.bh_a204
,t2.bh_a205
,t2.bh_a206
,t2.bh_a207
,t2.bh_a208
,t2.bh_a209
,t2.bh_a210
,t2.bh_a211
,t2.bh_a212
,t2.bh_a213
,t2.bh_a214
,t2.bh_a215
,t2.bh_a216
,t2.bh_a217
,t2.bh_a218
,t2.bh_a219
,t2.bh_a220
,t2.bh_b001
,t2.bh_b002
,t2.bh_b003
,t2.bh_b004
,t2.bh_b005
,t2.bh_b006
,t2.bh_b007
,t2.bh_b008
,t2.bh_b009
,t2.bh_b010
,t2.bh_b011
,t2.bh_b012
,t2.bh_b013
,t2.bh_b014
,t2.bh_b015
,t2.bh_b016
,t2.bh_b017
,t2.bh_b018
,t2.bh_b019
,t2.bh_b020
,t2.bh_b021
,t2.bh_b022
,t2.bh_b023
,t2.bh_b024
,t2.bh_b025
,t2.bh_b026
,t2.bh_b027
,t2.bh_b028
,t2.bh_b029
,t2.bh_b030
,t2.bh_b031
,t2.bh_b032
,t2.bh_b033
,t2.bh_b034
,t2.bh_b035
,t2.bh_b036
,t2.bh_b037
,t2.bh_b038
,t2.bh_b039
,t2.bh_b040
,t2.bh_b041
,t2.bh_b042
,t2.bh_b043
,t2.bh_b044
,t2.bh_b045
,t2.bh_b046
,t2.bh_b047
,t2.bh_b048
,t2.bh_b049
,t2.bh_b050
,t2.bh_b051
,t2.bh_b052
,t2.bh_b053
,t2.bh_b054
,t2.bh_b055
,t2.bh_b056
,t2.bh_b057
,t2.bh_b058
,t2.bh_b059
,t2.bh_b060
,t2.bh_b061
,t2.bh_b062
,t2.bh_b063
,t2.bh_b064
,t2.bh_b065
,t2.bh_b066
,t2.bh_b067
,t2.bh_b068
,t2.bh_b069
,t2.bh_b070
,t2.bh_b071
,t2.bh_b072
,t2.bh_b073
,t2.bh_b074
,t2.bh_b075
,t2.bh_b076
,t2.bh_b077
,t2.bh_b078
,t2.bh_b079
,t2.bh_b080
,t2.bh_b081
,t2.bh_b082
,t2.bh_b083
,t2.bh_b084
,t2.bh_b085
,t2.bh_b086
,t2.bh_b087
,t2.bh_b088
,t2.bh_b089
,t2.bh_b090
,t2.bh_b091
,t2.bh_b092
,t2.bh_b093
,t2.bh_b094
,t2.bh_b095
,t2.bh_b096
,t2.bh_b097
,t2.bh_b098
,t2.bh_b099
,t2.bh_b100
,t2.bh_b101
,t2.bh_b102
,t2.bh_b103
,t2.bh_b104
,t2.bh_b105
,t2.bh_b106
,t2.bh_b107
,t2.bh_b108
,t2.bh_b109
,t2.bh_b110
,t2.bh_b111
,t2.bh_b112
,t2.bh_b113
,t2.bh_b114
,t2.bh_b115
,t2.bh_b116
,t2.bh_b117
,t2.bh_b118
,t2.bh_b119
,t2.bh_b120
,t2.bh_b121
,t2.bh_b122
,t2.bh_b123
,t2.bh_b124
,t2.bh_b125
,t2.bh_b126
,t2.bh_b127
,t2.bh_b128
,t2.bh_b129
,t2.bh_b130
,t2.bh_b131
,t2.bh_b132
,t2.bh_b133
,t2.bh_b134
,t2.bh_b135
,t2.bh_b136
,t2.bh_b137
,t2.bh_b138
,t2.bh_b139
,t2.bh_b140
,t2.bh_b141
,t2.bh_b142
,t2.bh_b143
,t2.bh_b144
,t2.bh_b145
,t2.bh_b146
,t2.bh_b147
,t2.bh_b148
,t2.bh_b149
,t2.bh_b150
,t2.bh_b151
,t2.bh_b152
,t2.bh_b153
,t2.bh_b154
,t2.bh_b155
,t2.bh_b156
,t2.bh_b157
,t2.bh_b158
,t2.bh_b159
,t2.bh_b160
,t2.bh_b161
,t2.bh_b162
,t2.bh_b163
,t2.bh_b164
,t2.bh_b165
,t2.bh_b166
,t2.bh_b167
,t2.bh_b168
,t2.bh_b169
,t2.bh_b170
,t2.bh_b171
,t2.bh_b172
,t2.bh_b173
,t2.bh_b174
,t2.bh_b175
,t2.bh_b176
,t2.bh_b177
,t2.bh_b178
,t2.bh_b179
,t2.bh_b180
,t2.bh_b181
,t2.bh_b182
,t2.bh_b183
,t2.bh_b184
,t2.bh_b185
,t2.bh_b186
,t2.bh_b187
,t2.bh_b188
,t2.bh_b189
,t2.bh_b190
,t2.bh_b191
,t2.bh_b192
,t2.bh_b193
,t2.bh_b194
,t2.bh_b195
,t2.bh_b196
,t2.bh_b197
,t2.bh_b198
,t2.bh_b199
,t2.bh_b200
,t2.bh_b201
,t2.bh_b202
,t2.bh_b203
,t2.bh_b204
,t2.bh_b205
,t2.bh_b206
,t2.bh_b207
,t2.bh_b208
,t2.bh_b209
,t2.bh_b210
,t2.bh_b211
,t2.bh_b212
,t2.bh_b213
,t2.bh_b214
,t2.bh_b215
,t2.bh_b216
,t2.bh_b217
,t2.bh_b218
,t2.bh_b219
,t2.bh_b220
,t2.bh_b221
,t2.bh_b222
,t2.bh_b223
,t2.bh_b224
,t2.bh_b225
,t2.bh_b226
,t2.bh_b227
,t2.bh_b228
,t2.bh_b229
,t2.bh_b230
,t2.bh_b231
,t2.bh_b232
,t2.bh_b233
,t2.bh_b234
,t2.bh_b235
,t2.bh_b236
,t2.bh_b237
,t2.bh_b238
,t2.bh_b239
,t2.bh_b240
,t2.bh_b241
,t2.bh_b242
,t2.bh_b243
,t2.bh_b244
,t2.bh_b245
,t2.bh_b246
,t2.bh_b247
,t2.bh_b248
,t2.bh_b249
,t2.bh_b250
,t2.bh_b251
,t2.bh_b252
,t2.bh_b253
,t2.bh_b254
,t2.bh_b255
,t2.bh_b256
,t2.bh_b257
,t2.bh_b258
,t2.bh_b259
,t2.bh_b260
,t2.bh_b261
,t2.bh_b262
,t2.bh_b263
,t2.bh_b264
,t2.bh_b265
,t2.bh_b266
,t2.bh_b267
,t2.bh_b268
,t2.bh_b269
,t2.bh_b270
,t2.bh_b271
,t2.bh_b272
,t2.bh_b273
,t2.bh_b274
,t2.bh_b275
,t2.bh_b276
,t2.bh_b277
,t2.bh_b278
,t2.bh_b279
,t2.bh_b280
,t2.bh_b281
,t2.bh_b282
,t2.bh_b283
,t2.bh_b284
,t2.bh_b285
,t2.bh_b286
,t2.bh_b287
,t2.bh_b288
,t2.bh_b289
,t2.bh_b290
,t2.bh_b291
,t2.bh_b292
,t2.bh_b293
,t2.bh_b294
,t2.bh_b295
,t2.bh_b296
,t2.bh_b297
,t2.bh_b298
,t2.bh_b299
,t2.bh_b300
,t2.bh_b301
,t2.bh_b302
,t2.bh_b303
,t2.bh_b304
,t2.bh_b305
,t2.bh_b306
,t2.bh_b307
,t2.bh_b308
,t2.bh_b309
,t2.bh_b310
,t2.bh_b311
,t2.bh_b312
,t2.bh_b313
,t2.bh_b314
,t2.bh_b315
,t2.bh_b316
,t2.bh_b317
,t2.bh_c001
,t2.bh_c002
,t2.bh_c003
,t2.bh_c004
,t2.bh_c005
,t2.bh_c006
,t2.bh_c007
,t2.bh_c008
,t2.bh_c009
,t2.bh_c010
,t2.bh_c011
,t2.bh_c012
,t2.bh_c013
,t2.bh_c014
,t2.bh_c015
,t2.bh_c016
,t2.bh_c017
,t2.bh_c018
,t2.bh_c019
,t2.bh_c020
,t2.bh_c021
,t2.bh_c022
,t2.bh_c023
,t2.bh_c024
,t2.bh_c025
,t2.bh_c026
,t2.bh_c027
,t2.bh_c028
,t2.bh_c029
,t2.bh_c030
,t2.bh_c031
,t2.bh_c032
,t2.bh_c033
,t2.bh_c034
,t2.bh_c035
,t2.bh_c036
,t2.bh_c037
,t2.bh_c038
,t2.bh_c039
,t2.bh_c040
,t2.bh_c041
,t2.bh_c042
,t2.bh_c043
,t2.bh_c044
,t2.bh_c045
,t2.bh_c046
,t2.bh_c047
,t2.bh_c048
,t2.bh_c049
,t2.bh_c050
,t2.bh_c051
,t2.bh_c052
,t2.bh_c053
,t2.bh_c054
,t2.bh_c055
,t2.bh_c056
,t2.bh_c057
,t2.bh_c058
,t2.bh_c059
,t2.bh_c060
,t2.bh_c061
,t2.bh_c062
,t2.bh_c063
,t2.bh_c064
,t2.bh_c065
,t2.bh_c066
,t2.bh_c067
,t2.bh_c068
,t2.bh_c069
,t2.bh_c070
,t2.bh_c071
,t2.bh_c072
,t2.bh_c073
,t2.bh_c074
,t2.bh_c075
,t2.bh_c076
,t2.bh_c077
,t2.bh_c078
,t2.bh_c079
,t2.bh_c080
,t2.bh_d001
,t2.bh_d002
,t2.bh_d003
,t2.bh_d004
,t2.bh_d005
,t2.bh_d006
,t2.bh_d007
,t2.bh_d008
,t2.bh_d009
,t2.bh_d010
,t2.bh_d011
,t2.bh_d012
,t2.bh_d013
,t2.bh_d014
,t2.bh_d015
,t2.bh_d016
,t2.bh_d017
,t2.bh_d018
,t2.bh_d019
,t2.bh_d020
,t2.bh_d021
,t2.bh_d022
,t2.bh_d023
,t2.bh_d024
,t2.bh_d025
,t2.bh_d026
,t2.bh_d027
,t2.bh_d028
,t2.bh_d029
,t2.bh_d030
,t2.bh_d031
,t2.bh_d032
,t2.bh_d033
,t2.bh_d034
,t2.bh_d035
,t2.bh_d036
,t2.bh_d037
,t2.bh_d038
,t2.bh_d039
,t2.bh_d040
,t2.bh_d041
,t2.bh_d042
,t2.bh_d043
,t2.bh_d044
,t2.bh_d045
,t2.bh_e001
,t2.bh_e002
,t2.bh_e003
,t2.bh_e004
,t2.bh_e005
,t2.bh_e006
,t2.bh_e007
,t2.bh_e008
,t2.bh_e009
,t2.bh_e010
,t2.bh_e011
,t2.bh_e012
,t2.bh_e013
,t2.bh_e014
,t2.bh_e015
,t2.bh_e016
,t2.bh_e017
,t2.bh_e018
,t2.bh_e019
,t2.bh_e020
,t2.bh_e021
,t2.bh_e022
,t2.bh_e023
,t2.bh_e024
,t2.bh_e025
,t2.bh_e026
,t2.bh_e027
,t2.bh_e028
,t2.bh_e029
,t2.bh_e030
,t2.bh_e031
,t2.bh_e032
,t2.bh_e033
,t2.bh_e034
,t2.bh_e035
,t2.bh_e036
,t2.bh_e037
,t2.bh_e038
,t2.bh_e039
,t2.bh_e040
,t2.bh_e041
,t2.bh_e042
,t2.bh_e043
,t2.bh_e044
,t2.bh_e045
,t2.bh_e046
,t2.bh_e047
,t2.bh_e048
,t2.bh_e049
,t2.bh_e050
,t2.bh_e051
,t2.bh_e052
,t2.bh_e053
,t2.bh_e054
,t2.bh_e055
,t2.bh_e056
,t2.bh_e057
,t2.bh_e058
,t2.bh_e059
,t2.bh_e060
,t2.bh_e061
,t2.bh_e062
,t2.bh_e063
,t2.bh_e064
,t2.bh_e065
,t2.bh_e066
,t2.bh_f001
,t2.bh_f002
,t2.bh_f003
,t2.bh_f004
,t2.bh_f005
,t2.bh_f006
,t2.bh_f007
,t2.bh_f008
,t2.bh_f009
,t2.bh_f010
,t2.bh_f011
,t2.bh_f012
,t2.bh_f013
,t2.bh_f014
,t2.bh_f015
,t2.bh_f016
,t2.bh_f017
,t2.bh_f018
,t2.bh_f019
,t2.bh_f020
,t2.bh_f021
,t2.bh_f022
,t2.bh_f023
,t2.bh_f024
,t2.bh_f025
,t2.bh_f026
,t2.bh_f027
,t2.bh_f028
,t2.bh_f029
,t2.bh_f030
,t2.bh_g001
,t2.bh_g002
,t2.bh_g003
,t2.bh_g004
,t2.bh_g005
,t2.bh_g006
,t2.bh_g007
,t2.bh_g008
,t2.bh_g009
,t2.bh_g010
,t2.bh_g011
,t2.bh_g012
,t2.bh_g013
,t2.bh_g014
,t2.bh_g015
,t2.bh_g016
,t2.bh_g017
,t2.bh_g018
,t2.bh_g019
,t2.bh_g020
,t2.bh_g021
,t2.bh_g022
,t2.bh_g023
,t2.bh_g024
,t2.bh_g025
,t2.bh_g026
,t2.bh_g027
,t2.bh_g028
,t2.bh_g029
,t2.bh_g030
,t2.bh_g031
,t2.bh_g032
,t2.bh_g033
,t2.bh_g034
,t2.bh_g035
,t2.bh_g036
,t2.bh_g037
,t2.bh_g038
,t2.bh_g039
,t2.bh_g040
,t2.bh_g041
,t2.bh_g042
,t2.bh_g043
,t2.bh_g044
,t2.bh_g045
,t2.bh_g046
,t2.bh_g047
,t2.bh_g048
,t2.bh_g049
,t2.bh_g050
,t2.bh_g051
,t2.bh_g052
,t2.bh_g053
,t2.bh_g054
,t2.bh_g055
,t2.bh_g056
,t2.bh_g057
,t2.bh_g058
,t2.bh_g059
,t2.bh_g060
,t2.bh_h001
,t2.bh_h002
,t2.bh_h003
,t2.bh_h004
,t2.bh_h005
,t2.bh_h006
,t2.bh_h007
,t2.bh_h008
,t2.bh_h009
,t2.bh_h010
,t2.bh_h011
,t2.bh_h012
,t2.bh_h013
,t2.bh_h014
,t2.bh_h015
,t2.bh_h016
,t2.bh_h017
,t2.bh_h018
,t2.bh_h019
,t2.bh_h020
,t2.bh_h021
,t2.bh_h022
,t2.bh_h023
,t2.bh_h024
,t2.bh_h025
,t2.bh_h026
,t2.bh_h027
,t2.bh_h028
,t2.bh_h029
,t2.bh_h030
,t2.bh_h031
,t2.bh_h032
,t2.bh_h033
,t2.bh_h034
,t2.bh_h035
,t2.bh_h036
,t2.bh_h037
,t2.bh_h038
,t2.bh_h039
,t2.bh_h040
,t2.bh_h041
,t2.bh_h042
,t2.bh_h043
,t2.bh_h044
,t2.bh_h045
,t2.bh_h046
,t2.bh_h047
,t2.bh_h048
,t2.bh_h049
,t2.bh_h050
,t2.bh_h051
,t2.bh_h052
,t2.bh_h053
,t2.bh_h054
,t2.bh_h055
,t2.bh_h056
,t2.bh_h057
,t2.bh_h058
,t2.bh_h059
,t2.bh_h060
,t2.bh_h061
,t2.bh_h062
,t2.bh_h063
,t2.bh_h064
,t2.bh_h065
,t2.bh_h066
,t2.bh_h067
,t2.bh_h068
,t2.bh_h069
,t2.bh_h070
,t2.bh_h071
,t2.bh_h072
,t2.bh_h073
,t2.bh_h074
,t2.bh_h075
,t2.bh_h076
,t2.bh_h077
,t2.bh_h078
,t2.bh_h079
,t2.bh_h080
,t2.bh_h081
,t2.bh_h082
,t2.bh_h083
,t2.bh_h084
,t2.bh_h085
,t2.bh_h086
,t2.bh_h087
,t2.bh_h088
,t2.bh_h089
,t2.bh_h090
,t2.bh_h091
,t2.bh_h092
,t2.bh_h093
,t2.bh_h094
,t2.bh_h095
,t2.bh_h096
,t2.bh_h097
,t2.bh_h098
,t2.bh_h099
,t2.bh_h100
,t2.bh_h101
,t2.bh_h102
,t2.bh_h103
,t2.bh_h104
,t2.bh_h105
,t2.bh_h106
,t2.bh_h107
,t2.bh_h108
,t2.bh_h109
,t2.bh_h110
,t2.bh_h111
,t2.bh_h112
,t2.bh_h113
,t2.bh_h114
,t2.bh_h115
,t2.bh_h116
,t2.bh_h117
,t2.bh_h118
,t2.bh_h119
,t2.bh_h120
,t2.bh_h121
,t2.bh_h122
,t2.bh_h123
,t2.bh_h124
,t2.bh_h125
,t2.bh_h126
,t2.bh_h127
,t2.bh_h128
,t2.bh_h129
,t2.bh_h130
,t2.bh_h131
,t2.bh_h132
,t2.bh_h133
,t2.bh_h134
,t2.bh_h135
,t2.bh_h136
,t2.bh_h137
,t2.bh_h138
,t2.bh_h139
,t2.bh_h140
,t2.bh_h141
,t2.bh_h142
,t2.bh_h143
,t2.bh_h144
,t2.bh_h145
,t2.bh_h146
,t2.bh_h147
,t2.bh_h148
,t2.bh_h149
,t2.bh_h150
,t2.bh_h151
,t2.bh_h152
,t2.bh_h153
,t2.bh_h154
,t2.bh_h155
,t2.bh_h156
,t2.bh_h157
,t2.bh_h158
,t2.bh_h159
,t2.bh_h160
,t2.bh_h161
,t2.bh_h162
,t2.bh_h163
,t2.bh_h164
,t2.bh_h165
,t2.bh_h166
,t2.bh_h167
,t2.bh_h168
,t2.bh_h169
,t2.bh_h170
,t2.bh_h171
,t2.bh_h172
,t2.bh_h173
,t2.bh_a221
,t2.bh_a222
,t2.bh_a223
,t2.bh_a224
,t2.bh_a225
,t2.bh_a226
,t2.bh_a227
,t2.bh_a228
,t2.bh_a229
,t2.bh_a230
,t2.bh_a231
,t2.bh_a232
,t2.bh_a233
,t2.bh_a234
,t2.bh_a235
,t2.bh_a236
,t2.bh_a237
,t2.bh_a238
,t2.bh_a239
,t2.bh_a240
,t2.bh_a241
,t2.bh_a242
,t2.bh_a243
,t2.bh_a244
,t2.bh_a245
,t2.bh_a246
,t2.bh_a247
,t2.bh_a248
,t2.bh_a249
,t2.bh_a250
,t2.bh_a251
,t2.bh_a252
,t2.bh_a253
,t2.bh_a254
,t2.bh_a255
,t2.bh_a256
,t2.bh_a257
,t2.bh_a258
,t2.bh_a259
,t2.bh_a260
,t2.bh_a261
,t2.bh_a262
,t2.bh_a263
,t2.bh_a264
,t2.bh_a265
,t2.bh_a266
,t2.bh_a267
,t2.bh_a268
,t2.bh_a269
,t2.bh_a270
,t2.bh_a271
,t2.bh_a272
,t2.bh_a273
,t2.bh_a274
,t2.bh_a275
,t2.bh_a276
,t2.bh_a277
,t2.bh_a278
,t2.bh_a279
,t2.bh_a280
,t2.bh_a281
,t2.bh_a282
,t2.bh_a283
,t2.bh_a284
,t2.bh_a285
,t2.bh_a286
,t2.bh_a287
,t2.bh_a288
,t2.bh_a289
,t2.bh_a290
,t2.bh_a291
,t2.bh_a292
,t2.bh_a293
,t2.bh_a294
,t2.bh_a295
,t2.bh_a296
,t2.bh_a297
,t2.bh_a298
,t2.bh_a299
,t2.bh_a300
,t2.bh_a301
,t2.bh_a302
,t2.bh_a303
,t2.bh_a304
,t2.bh_a305
,t2.bh_a306
,t2.bh_a307
,t2.bh_a308
,t2.bh_a309
,t2.bh_a310
,t2.bh_a311
,t2.bh_a312
,t2.bh_a313
,t2.bh_a314
,t2.bh_a315
,t2.bh_a316
,t2.bh_a317
,t2.bh_a318
,t2.bh_a319
,t2.bh_a320
,t2.bh_a321
,t2.bh_a322
,t2.bh_a323
,t2.bh_a324
,t2.bh_a325
,t2.bh_a326
,t2.bh_a327
,t2.bh_a328
,t2.bh_a329
,t2.bh_a330
,t2.bh_a331
,t2.bh_a332
,t2.bh_a333
,t2.bh_a334
,t2.bh_a335
,t2.bh_a336
,t2.bh_a337
,t2.bh_a338
,t2.bh_a339
,t2.bh_a340
,t2.bh_a341
,t2.bh_a342
,t2.bh_a343
,t2.bh_a344
,t2.bh_a345
,t2.bh_a346
,t2.bh_a347
,t2.bh_a348
,t2.bh_a349
,t2.bh_a350
,t2.bh_a351
,t2.bh_a352
,t2.bh_a353
,t2.bh_a354
,t2.bh_a355
,t2.bh_a356
,t2.bh_a357
,t2.bh_a358
,t2.bh_a359
,t2.bh_a360
,t2.bh_a361
,t2.bh_a362
,t2.bh_a363
,t2.bh_a364
,t2.bh_a365
,t2.bh_a366
,t2.bh_a367
,t2.bh_a368
,t2.bh_a369
,t2.bh_a370
,t2.bh_a371
,t2.bh_a372
,t2.bh_a373
,t2.bh_a374
,t2.bh_a375
,t2.bh_a376
,t2.bh_a377
,t2.bh_a378
,t2.bh_a379
,t2.bh_a380
,t2.bh_a381
,t2.bh_a382
,t2.bh_a383
,t2.bh_a384
,t2.bh_a385
,t2.bh_a386
,t2.bh_a387
,t2.bh_a388
,t2.bh_a389
,t2.bh_a390
,t2.bh_a391
,t2.bh_a392
,t2.bh_a393
,t2.bh_qu001
,t2.bh_qu002
,t2.bh_qu003
,t2.bh_qu004
,t2.bh_qu005
,t2.bh_qu006
,t2.bh_qu007
,t2.bh_qu008
,t2.bh_qu009
,t2.bh_qu010
,t2.bh_qu011
,t2.bh_qu012
,t2.bh_qu013
,t2.bh_qu014
,t2.bh_qu015
,t2.bh_qu016
,t2.bh_qu017
,t2.bh_qu018
,t2.bh_qu019
,t2.bh_qu020
,t2.bh_qu021
,t2.bh_qu022
,t2.bh_qu023
,t2.bh_qu024
,t2.bh_x001
,t2.bh_x002
,t2.bh_x003
,t2.bh_x004
,t2.bh_x005
,t2.bh_x006
,t2.bh_x007
,t2.bh_x008
,t2.bh_x009
,t2.bh_x010
,t2.bh_x011
,t2.bh_x012
,t2.bh_x013
,t2.bh_x014
,t2.bh_x015
,t2.bh_x016
,t2.bh_x017
,t2.bh_x018
,t2.bh_x019
,t2.bh_x020
,t2.bh_x021
,t2.bh_x022
,t2.bh_x023
,t2.bh_x024
,t2.bh_x025
,t2.bh_x026
,t2.bh_x027
,t2.bh_x028
,t2.bh_x029
,t2.bh_x030
,t2.bh_x031
,t2.bh_x032
,t2.bh_x033
,t2.bh_x034
,t2.bh_x035
,t2.bh_x036
,t2.bh_x037
,t2.bh_x038
,t2.bh_x039
,t2.bh_x040
,t2.bh_x041
,t2.bh_x042
,t2.bh_x043
,t2.bh_x044
,t2.bh_x045
,t2.bh_x046
,t2.bh_x047
,t2.bh_x048
,t2.bh_x049
,t2.bh_x050
,t2.bh_x051
,t2.bh_x052
,t2.bh_x053
,t2.bh_x054
,t2.bh_x055
,t2.bh_x056
,t2.bh_x057
,t2.bh_x058
,t2.bh_x059
,t2.bh_x060
,t2.bh_x061
,t2.bh_x062
,t2.bh_x063
,t2.bh_x064
,t2.bh_x065
,t2.bh_x066
,t2.bh_x067
,t2.bh_x068
,t2.bh_x069
,t2.bh_x070
,t2.bh_x071
,t2.bh_x072
,t2.bh_x073
,t2.bh_x074
,t2.bh_x075
,t2.bh_x076
,t2.bh_x077
,t2.bh_x078
,t2.bh_x079
,t2.bh_x080
,t2.bh_x081
,t2.bh_x082
,t2.bh_x083
,t2.bh_x084
,t2.bh_x085
,t2.bh_x086
,t2.bh_x087
,t2.bh_x088
,t2.bh_x089
,t2.bh_x090
,t2.bh_x091
,t2.bh_x092
,t2.bh_x093
,t2.bh_x094
,t2.bh_x095
,t2.bh_x096
,t2.bh_x097
,t2.bh_x098
,t2.bh_x099
,t2.bh_x100
,t2.bh_x101
,t2.bh_x102
,t2.bh_x103
,t2.bh_x104
,t2.bh_x105
,t2.bh_x106
,t2.bh_x107
,t2.bh_x108
,t2.bh_x109
,t2.bh_x110
,t2.bh_x111
,t2.bh_x112
,t2.bh_x113
,t2.bh_x114
,t2.bh_x115
,t2.bh_x116
,t2.bh_x117
,t2.bh_x118
,t2.bh_x119
,t2.bh_x120
,t2.bh_x121
,t2.bh_x122
,t2.bh_x123
,t2.bh_x124
,t2.bh_x125
,t2.bh_x126
,t2.bh_x127
,t2.bh_x128
,t2.bh_x129
,t2.bh_x130
,t2.bh_x131
,t2.bh_x132
,t2.bh_x133
,t2.bh_x134
,t2.bh_x135
,t2.bh_x136
,t2.bh_x137
,t2.bh_x138
,t2.bh_x139
,t2.bh_x140
,t2.bh_x141
,t2.bh_x142
,t2.bh_x143
,t2.bh_x144
,t2.bh_q001_q0
,t2.bh_q001_q5
,t2.bh_q001_q10
,t2.bh_q001_q15
,t2.bh_q001_q20
,t2.bh_q001_q25
,t2.bh_q001_q30
,t2.bh_q001_q35
,t2.bh_q001_q40
,t2.bh_q001_q45
,t2.bh_q001_q50
,t2.bh_q001_q55
,t2.bh_q001_q60
,t2.bh_q001_q65
,t2.bh_q001_q70
,t2.bh_q001_q75
,t2.bh_q001_q80
,t2.bh_q001_q85
,t2.bh_q001_q90
,t2.bh_q001_q95
,t2.bh_q001_q100
,t2.bh_q002_q0
,t2.bh_q002_q5
,t2.bh_q002_q10
,t2.bh_q002_q15
,t2.bh_q002_q20
,t2.bh_q002_q25
,t2.bh_q002_q30
,t2.bh_q002_q35
,t2.bh_q002_q40
,t2.bh_q002_q45
,t2.bh_q002_q50
,t2.bh_q002_q55
,t2.bh_q002_q60
,t2.bh_q002_q65
,t2.bh_q002_q70
,t2.bh_q002_q75
,t2.bh_q002_q80
,t2.bh_q002_q85
,t2.bh_q002_q90
,t2.bh_q002_q95
,t2.bh_q002_q100
,t2.bh_q003_q0
,t2.bh_q003_q5
,t2.bh_q003_q10
,t2.bh_q003_q15
,t2.bh_q003_q20
,t2.bh_q003_q25
,t2.bh_q003_q30
,t2.bh_q003_q35
,t2.bh_q003_q40
,t2.bh_q003_q45
,t2.bh_q003_q50
,t2.bh_q003_q55
,t2.bh_q003_q60
,t2.bh_q003_q65
,t2.bh_q003_q70
,t2.bh_q003_q75
,t2.bh_q003_q80
,t2.bh_q003_q85
,t2.bh_q003_q90
,t2.bh_q003_q95
,t2.bh_q003_q100
,t2.bh_q004_q0
,t2.bh_q004_q5
,t2.bh_q004_q10
,t2.bh_q004_q15
,t2.bh_q004_q20
,t2.bh_q004_q25
,t2.bh_q004_q30
,t2.bh_q004_q35
,t2.bh_q004_q40
,t2.bh_q004_q45
,t2.bh_q004_q50
,t2.bh_q004_q55
,t2.bh_q004_q60
,t2.bh_q004_q65
,t2.bh_q004_q70
,t2.bh_q004_q75
,t2.bh_q004_q80
,t2.bh_q004_q85
,t2.bh_q004_q90
,t2.bh_q004_q95
,t2.bh_q004_q100
,t2.bh_q005_q0
,t2.bh_q005_q5
,t2.bh_q005_q10
,t2.bh_q005_q15
,t2.bh_q005_q20
,t2.bh_q005_q25
,t2.bh_q005_q30
,t2.bh_q005_q35
,t2.bh_q005_q40
,t2.bh_q005_q45
,t2.bh_q005_q50
,t2.bh_q005_q55
,t2.bh_q005_q60
,t2.bh_q005_q65
,t2.bh_q005_q70
,t2.bh_q005_q75
,t2.bh_q005_q80
,t2.bh_q005_q85
,t2.bh_q005_q90
,t2.bh_q005_q95
,t2.bh_q005_q100
,t2.bh_q006_q0
,t2.bh_q006_q5
,t2.bh_q006_q10
,t2.bh_q006_q15
,t2.bh_q006_q20
,t2.bh_q006_q25
,t2.bh_q006_q30
,t2.bh_q006_q35
,t2.bh_q006_q40
,t2.bh_q006_q45
,t2.bh_q006_q50
,t2.bh_q006_q55
,t2.bh_q006_q60
,t2.bh_q006_q65
,t2.bh_q006_q70
,t2.bh_q006_q75
,t2.bh_q006_q80
,t2.bh_q006_q85
,t2.bh_q006_q90
,t2.bh_q006_q95
,t2.bh_q006_q100
,t2.bh_q007_q0
,t2.bh_q007_q5
,t2.bh_q007_q10
,t2.bh_q007_q15
,t2.bh_q007_q20
,t2.bh_q007_q25
,t2.bh_q007_q30
,t2.bh_q007_q35
,t2.bh_q007_q40
,t2.bh_q007_q45
,t2.bh_q007_q50
,t2.bh_q007_q55
,t2.bh_q007_q60
,t2.bh_q007_q65
,t2.bh_q007_q70
,t2.bh_q007_q75
,t2.bh_q007_q80
,t2.bh_q007_q85
,t2.bh_q007_q90
,t2.bh_q007_q95
,t2.bh_q007_q100
,t2.bh_q008_q0
,t2.bh_q008_q5
,t2.bh_q008_q10
,t2.bh_q008_q15
,t2.bh_q008_q20
,t2.bh_q008_q25
,t2.bh_q008_q30
,t2.bh_q008_q35
,t2.bh_q008_q40
,t2.bh_q008_q45
,t2.bh_q008_q50
,t2.bh_q008_q55
,t2.bh_q008_q60
,t2.bh_q008_q65
,t2.bh_q008_q70
,t2.bh_q008_q75
,t2.bh_q008_q80
,t2.bh_q008_q85
,t2.bh_q008_q90
,t2.bh_q008_q95
,t2.bh_q008_q100
,t2.bh_q009_q0
,t2.bh_q009_q5
,t2.bh_q009_q10
,t2.bh_q009_q15
,t2.bh_q009_q20
,t2.bh_q009_q25
,t2.bh_q009_q30
,t2.bh_q009_q35
,t2.bh_q009_q40
,t2.bh_q009_q45
,t2.bh_q009_q50
,t2.bh_q009_q55
,t2.bh_q009_q60
,t2.bh_q009_q65
,t2.bh_q009_q70
,t2.bh_q009_q75
,t2.bh_q009_q80
,t2.bh_q009_q85
,t2.bh_q009_q90
,t2.bh_q009_q95
,t2.bh_q009_q100
,t2.bh_q010_q0
,t2.bh_q010_q5
,t2.bh_q010_q10
,t2.bh_q010_q15
,t2.bh_q010_q20
,t2.bh_q010_q25
,t2.bh_q010_q30
,t2.bh_q010_q35
,t2.bh_q010_q40
,t2.bh_q010_q45
,t2.bh_q010_q50
,t2.bh_q010_q55
,t2.bh_q010_q60
,t2.bh_q010_q65
,t2.bh_q010_q70
,t2.bh_q010_q75
,t2.bh_q010_q80
,t2.bh_q010_q85
,t2.bh_q010_q90
,t2.bh_q010_q95
,t2.bh_q010_q100
,t2.bh_q011_q0
,t2.bh_q011_q5
,t2.bh_q011_q10
,t2.bh_q011_q15
,t2.bh_q011_q20
,t2.bh_q011_q25
,t2.bh_q011_q30
,t2.bh_q011_q35
,t2.bh_q011_q40
,t2.bh_q011_q45
,t2.bh_q011_q50
,t2.bh_q011_q55
,t2.bh_q011_q60
,t2.bh_q011_q65
,t2.bh_q011_q70
,t2.bh_q011_q75
,t2.bh_q011_q80
,t2.bh_q011_q85
,t2.bh_q011_q90
,t2.bh_q011_q95
,t2.bh_q011_q100
,t2.bh_q012_q0
,t2.bh_q012_q5
,t2.bh_q012_q10
,t2.bh_q012_q15
,t2.bh_q012_q20
,t2.bh_q012_q25
,t2.bh_q012_q30
,t2.bh_q012_q35
,t2.bh_q012_q40
,t2.bh_q012_q45
,t2.bh_q012_q50
,t2.bh_q012_q55
,t2.bh_q012_q60
,t2.bh_q012_q65
,t2.bh_q012_q70
,t2.bh_q012_q75
,t2.bh_q012_q80
,t2.bh_q012_q85
,t2.bh_q012_q90
,t2.bh_q012_q95
,t2.bh_q012_q100
,t2.bh_q013_q0
,t2.bh_q013_q5
,t2.bh_q013_q10
,t2.bh_q013_q15
,t2.bh_q013_q20
,t2.bh_q013_q25
,t2.bh_q013_q30
,t2.bh_q013_q35
,t2.bh_q013_q40
,t2.bh_q013_q45
,t2.bh_q013_q50
,t2.bh_q013_q55
,t2.bh_q013_q60
,t2.bh_q013_q65
,t2.bh_q013_q70
,t2.bh_q013_q75
,t2.bh_q013_q80
,t2.bh_q013_q85
,t2.bh_q013_q90
,t2.bh_q013_q95
,t2.bh_q013_q100
,t2.bh_q014_q0
,t2.bh_q014_q5
,t2.bh_q014_q10
,t2.bh_q014_q15
,t2.bh_q014_q20
,t2.bh_q014_q25
,t2.bh_q014_q30
,t2.bh_q014_q35
,t2.bh_q014_q40
,t2.bh_q014_q45
,t2.bh_q014_q50
,t2.bh_q014_q55
,t2.bh_q014_q60
,t2.bh_q014_q65
,t2.bh_q014_q70
,t2.bh_q014_q75
,t2.bh_q014_q80
,t2.bh_q014_q85
,t2.bh_q014_q90
,t2.bh_q014_q95
,t2.bh_q014_q100
,t2.bh_q015_q0
,t2.bh_q015_q5
,t2.bh_q015_q10
,t2.bh_q015_q15
,t2.bh_q015_q20
,t2.bh_q015_q25
,t2.bh_q015_q30
,t2.bh_q015_q35
,t2.bh_q015_q40
,t2.bh_q015_q45
,t2.bh_q015_q50
,t2.bh_q015_q55
,t2.bh_q015_q60
,t2.bh_q015_q65
,t2.bh_q015_q70
,t2.bh_q015_q75
,t2.bh_q015_q80
,t2.bh_q015_q85
,t2.bh_q015_q90
,t2.bh_q015_q95
,t2.bh_q015_q100
,t2.bh_q016_q0
,t2.bh_q016_q5
,t2.bh_q016_q10
,t2.bh_q016_q15
,t2.bh_q016_q20
,t2.bh_q016_q25
,t2.bh_q016_q30
,t2.bh_q016_q35
,t2.bh_q016_q40
,t2.bh_q016_q45
,t2.bh_q016_q50
,t2.bh_q016_q55
,t2.bh_q016_q60
,t2.bh_q016_q65
,t2.bh_q016_q70
,t2.bh_q016_q75
,t2.bh_q016_q80
,t2.bh_q016_q85
,t2.bh_q016_q90
,t2.bh_q016_q95
,t2.bh_q016_q100
,t2.bh_q017_q0
,t2.bh_q017_q5
,t2.bh_q017_q10
,t2.bh_q017_q15
,t2.bh_q017_q20
,t2.bh_q017_q25
,t2.bh_q017_q30
,t2.bh_q017_q35
,t2.bh_q017_q40
,t2.bh_q017_q45
,t2.bh_q017_q50
,t2.bh_q017_q55
,t2.bh_q017_q60
,t2.bh_q017_q65
,t2.bh_q017_q70
,t2.bh_q017_q75
,t2.bh_q017_q80
,t2.bh_q017_q85
,t2.bh_q017_q90
,t2.bh_q017_q95
,t2.bh_q017_q100
,t2.bh_q018_q0
,t2.bh_q018_q5
,t2.bh_q018_q10
,t2.bh_q018_q15
,t2.bh_q018_q20
,t2.bh_q018_q25
,t2.bh_q018_q30
,t2.bh_q018_q35
,t2.bh_q018_q40
,t2.bh_q018_q45
,t2.bh_q018_q50
,t2.bh_q018_q55
,t2.bh_q018_q60
,t2.bh_q018_q65
,t2.bh_q018_q70
,t2.bh_q018_q75
,t2.bh_q018_q80
,t2.bh_q018_q85
,t2.bh_q018_q90
,t2.bh_q018_q95
,t2.bh_q018_q100
,t2.bh_q019_q0
,t2.bh_q019_q5
,t2.bh_q019_q10
,t2.bh_q019_q15
,t2.bh_q019_q20
,t2.bh_q019_q25
,t2.bh_q019_q30
,t2.bh_q019_q35
,t2.bh_q019_q40
,t2.bh_q019_q45
,t2.bh_q019_q50
,t2.bh_q019_q55
,t2.bh_q019_q60
,t2.bh_q019_q65
,t2.bh_q019_q70
,t2.bh_q019_q75
,t2.bh_q019_q80
,t2.bh_q019_q85
,t2.bh_q019_q90
,t2.bh_q019_q95
,t2.bh_q019_q100

--续侦变量
,t3.value_012
,t3.value_013
,t3.value_014
,t3.value_015
,t3.value_016
,t3.value_017
,t3.value_018
,t3.value_019
,t3.value_020
,t3.value_021
,t3.value_022
,t3.value_023
,t3.value_024
,t3.value_025
,t3.value_026
,t3.value_027
,t3.value_028
,t3.value_029
,t3.value_030
,t3.value_031
,t3.value_032
,t3.value_033
,t3.value_034
,t3.value_035
,t3.value_036
,t3.value_037
,t3.value_038
,t3.value_039
,t3.value_040
,t3.value_041
,t3.value_042
,t3.value_043
,t3.value_044
,t3.value_045
,t3.value_046
,t3.value_047
,t3.value_048
,t3.value_049
,t3.value_050
,t3.value_051
,t3.value_052
,t3.value_053
,t3.value_054
,t3.value_055
,t3.value_056
,t3.value_057
,t3.value_058
,t3.value_059
,t3.value_060
,t3.value_061
,t3.value_062
,t3.value_063
,t3.value_064
,t3.value_065
,t3.value_066
,t3.value_067
,t3.value_068
,t3.value_069
,t3.value_070
,t3.value_071
,t3.value_072
,t3.value_073
,t3.value_074
,t3.value_075
,t3.value_076
,t3.value_077
,t3.value_078
,t3.value_079
,t3.value_080
,t3.value_081
,t3.value_082
,t3.value_083
,t3.value_084
,t3.value_085
,t3.value_086
,t3.value_087
,t3.value_088
,t3.value_089
,t3.value_090
,t3.value_091
,t3.value_092
,t3.value_093
,t3.value_094
,t3.value_095
,t3.value_096
,t3.value_097
,t3.value_098
,t3.value_099
,t3.value_100
,t3.value_101
,t3.value_102
,t3.value_103
,t3.value_104
,t3.value_105
,t3.value_106
,t3.value_107
,t3.value_108
,t3.value_109
,t3.value_110
,t3.value_111
,t3.value_112
,t3.value_113
,t3.value_114
,t3.value_115
,t3.value_116
,t3.value_117
,t3.value_118
,t3.value_119
,t3.value_120
,t3.value_121
,t3.value_122
,t3.value_123
,t3.value_124
,t3.value_125
,t3.value_126
,t3.value_127
,t3.value_128
,t3.value_129
,t3.value_130
,t3.value_131
,t3.value_132
,t3.value_133
,t3.value_134
,t3.value_135
,t3.value_136
,t3.value_137
,t3.value_138
,t3.value_139
,t3.value_140
,t3.value_141
,t3.value_142
,t3.value_143
,t3.value_144
,t3.value_145
,t3.value_146
,t3.value_147
,t3.value_148
,t3.value_149
,t3.value_150
,t3.value_151
,t3.value_152
,t3.value_153
,t3.value_154
,t3.value_155
,t3.value_156
,t3.value_157
,t3.value_158
,t3.value_159
,t3.value_160
,t3.value_161
,t3.value_162
,t3.value_163
,t3.value_164
,t3.value_165
,t3.value_166
,t3.value_167
,t3.value_168
,t3.value_169
,t3.value_170
,t3.value_171
,t3.value_172
,t3.value_173
,t3.value_174
,t3.value_175
,t3.value_176
,t3.value_177
,t3.value_178
,t3.value_179
,t3.value_180
,t3.value_181
,t3.value_182
,t3.value_183
,t3.value_184
,t3.value_185
,t3.value_186
,t3.value_187
,t3.value_188
,t3.value_189
,t3.value_190
,t3.value_191
,t3.value_192
,t3.value_193
,t3.value_194
,t3.value_195
,t3.value_196
,t3.value_197
,t3.value_198
,t3.value_199
,t3.value_200
,t3.value_201
,t3.value_202
,t3.value_203
,t3.value_204
,t3.value_205
,t3.value_206
,t3.value_207
,t3.value_208
,t3.value_209
,t3.value_210
,t3.value_211
,t3.value_212
,t3.value_213
,t3.value_214
,t3.value_215
,t3.value_216
,t3.value_217
,t3.value_218
,t3.value_219
,t3.value_220
,t3.value_221
,t3.value_222
,t3.value_223
,t3.value_224
,t3.value_225
,t3.value_226
,t3.value_227
,t3.value_228
,t3.value_229
,t3.value_230
,t3.value_231
,t3.value_232
,t3.value_233
,t3.value_234
,t3.value_235
,t3.value_236
,t3.value_237
,t3.value_238
,t3.value_239
,t3.value_240
,t3.value_241
,t3.value_242
,t3.value_243
,t3.value_244
,t3.value_245
,t3.value_246
,t3.value_247
,t3.value_248
,t3.value_249
,t3.value_250
,t3.value_251
,t3.value_252
,t3.value_253
,t3.value_254
,t3.value_255
,t3.value_256
,t3.value_257
,t3.value_258
,t3.value_259
,t3.value_260
,t3.value_261
,t3.value_262
,t3.value_263
,t3.value_264
,t3.value_265
,t3.value_266
,t3.value_267
,t3.value_268
,t3.value_269
,t3.value_270
,t3.value_271
,t3.value_272
,t3.value_273
,t3.value_274
,t3.value_275
,t3.value_276
,t3.value_277
,t3.value_278
,t3.value_279
,t3.value_280
,t3.value_281
,t3.value_282
,t3.value_283
,t3.value_284
,t3.value_285
,t3.value_286
,t3.value_287
,t3.value_288
,t3.value_289
,t3.value_290
,t3.value_291
,t3.value_292
,t3.value_293
,t3.value_294
,t3.value_295
,t3.value_296
,t3.value_297
,t3.value_298
,t3.value_299
,t3.value_300
,t3.value_301
,t3.value_302
,t3.value_303
,t3.value_304
,t3.value_305
,t3.value_306
,t3.value_307
,t3.value_308
,t3.value_309
,t3.value_310
,t3.value_311
,t3.value_312
,t3.value_313
,t3.value_314
,t3.value_315
,t3.value_316
,t3.value_317
,t3.value_318
,t3.value_319
,t3.value_320
,t3.value_321
,t3.value_322
,t3.value_323
,t3.value_324
,t3.value_325
,t3.value_326
,t3.value_327
,t3.value_328
,t3.value_329
,t3.value_330
,t3.value_331
,t3.value_332
,t3.value_333
,t3.value_334
,t3.value_335
,t3.value_336
,t3.value_337
,t3.value_338
,t3.value_339
,t3.value_340
,t3.value_341
,t3.value_342
,t3.value_343
,t3.value_344
,t3.value_345
,t3.value_346
,t3.value_347
,t3.value_348
,t3.value_349
,t3.value_350
,t3.value_351
,t3.value_352
,t3.value_353
,t3.value_354
,t3.value_355
,t3.value_356
,t3.value_357
,t3.value_358
,t3.value_359
,t3.value_360
,t3.value_361
,t3.value_362
,t3.value_363
,t3.value_364
,t3.value_365
,t3.value_366
,t3.value_367
,t3.value_368
,t3.value_369
,t3.value_370
,t3.value_371
,t3.value_372
,t3.value_373
,t3.value_374
,t3.value_375
,t3.value_376
,t3.value_377
,t3.value_378
,t3.value_379
,t3.value_380
,t3.value_381
,t3.value_382
,t3.value_383
,t3.value_384
,t3.value_385
,t3.value_386
,t3.value_387
,t3.value_388
,t3.value_389
,t3.value_390
,t3.value_391
,t3.value_392
,t3.value_393
,t3.value_394
,t3.value_395
,t3.value_396
,t3.value_397
,t3.value_398
,t3.value_399
,t3.value_400
,t3.value_401
,t3.value_402
,t3.value_403
,t3.value_404
,t3.value_405
,t3.value_406
,t3.value_407
,t3.value_408
,t3.value_409
,t3.value_410
,t3.value_411
,t3.value_412
,t3.value_413
,t3.value_414
,t3.value_415
,t3.value_416
,t3.value_417
,t3.value_418
,t3.value_419
,t3.value_420
,t3.value_421
,t3.value_422
,t3.value_423
,t3.value_424
,t3.value_425
,t3.value_426
,t3.value_427

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 1) 
      and lending_time='{run_day}'
      and channel_id = '1'
    ) as t 
--离线风险模型子分
left join 
    (
    select t.* 
    from znzz_fintech_ads.dm_f_lxl_test_behave_model_merge_fpd30_score as t 
    where dt=date_sub('{run_day}',1)
    ) as t1 on t.id_no_des=t1.id_no_des

--洞侦变量
left join
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk 
    from znzz_fintech_dwd.dwd_beforeloan_data_source_bh_fqz_djv3_id as t
    where dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t2 on t.id_no_des=t2.id_no_des and t2.rk=1

--续侦变量
left join 
    (
    select t.*, row_number() over(partition by id_no_des order by dt desc) as rk
    from znzz_fintech_dwd.dwd_beforeloan_third_combine_sub_id as t  
    where ds='jzhl_thirds_platform_intf_bh_nfacq932_20240529'
      and dt <= date_sub('{run_day}', 0 )
      and dt >= date_sub('{run_day}', 29)
    ) as t3 on t.id_no_des=t3.id_no_des and t3.rk=1
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[9]:


data_time = pd.DataFrame({'run_day':list(df_sample_dict.keys())})
data_time['run_day'].value_counts()


# In[10]:


df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[11]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['id_no_des'].nunique())


# In[12]:


print(df_sample_['lending_time'].min(), df_sample_['lending_time'].max())


# In[13]:


df_sample_.dropna(how='all', axis=1, inplace=True)
print(df_sample_.shape)


# In[14]:


df_target_dict = {}


# In[15]:


# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-11-06', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,t.id_no_des
,t.channel_id
,t.lending_time
,substr(t.lending_time, 1, 7) as lending_month
,t.mob
,t.maxdpd
,t.fpd
,t.fpd10
,t.fpd30
,t.mob4dpd30
,t.diff_days
from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
where dt=date_sub(current_date(), 1) 
  and lending_time='{run_day}'
  and channel_id = '1'
;
'''
    print(f'=========================={run_day}=============================')
    df_target_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[16]:


df_target_ = pd.concat(df_target_dict.values(), ignore_index=True)
df_target_.info(show_counts=True)
df_target_.head()


# In[17]:


print(df_target_.shape[0], df_target_['order_no'].nunique())


# In[18]:


selected_cols = ['order_no']+[col for col in df_sample_.columns if col not in df_target_.columns]
print(selected_cols[:5])


# In[19]:


df_sample_=pd.merge(df_target_, df_sample_[selected_cols], how='inner', on=['order_no'])
df_sample_.info(show_counts=True)
# df_sample_.head()


# In[20]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[21]:


df_sample = df_sample_.query("diff_days<=30").reset_index(drop=True)
df_sample.info(show_counts=True)
# df_sample.head()


# In[22]:


df_sample.columns[:12]


# In[23]:


varsname = [col for col in df_sample.columns.to_list()[12:]]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[24]:


print(result_path)


# In[32]:


to_del = []


# In[38]:


df_sample[col].value_counts()


# In[39]:


print(col)
to_del.append(col)
print(to_del)
df_sample.drop([col],axis=1,inplace=True)


# In[40]:


for i, col in enumerate(varsname[773:]):
    if df_sample[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')


# In[41]:


df_sample.dropna(how='all', axis=1, inplace=True)
print(df_sample.shape)


# In[42]:


df_sample['bh_q019_q70'].head()


# In[43]:


pd.set_option('display.max_row',None)
df_sample.groupby(['lending_time','fpd30'])['order_no'].count().unstack()


# In[44]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-10-01' & lending_time<='2024-11-06'").index, 'data_set']='3_oot'


# In[45]:


target = 'fpd30'


# In[47]:


# df_sample[[target]+varsname].info(show_counts=True)
# df_sample[[target]+varsname].head()


# In[50]:


df_sample.to_csv(result_path + '桔子商城fpd30授信实时模型_建模数据集_250108.csv',index=False)
print(result_path + '桔子商城fpd30授信实时模型_建模数据集_250108.csv')


# # 1. 样本概况

# In[51]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[52]:


print(df_sample[target].value_counts())


# In[53]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[54]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[55]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[56]:


task_name


# In[57]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[59]:


# 2.1 变量分布
varsname = df_sample.columns.to_list()[12:-1]
df_explor = toad.detect(df_sample[varsname])


# In[60]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[62]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.1缺失值处理

# In[63]:


df_sample = df_sample.replace(-1, np.nan)
gc.collect()


# ## 2.2 数据探索

# In[64]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[65]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[66]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[67]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[68]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[69]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[70]:


df_iv.loc[to_drop_iv,:].head()


# In[71]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[105]:


df_sample[varsname_v1+[target]].info()


# In[106]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]].drop('standard_score',axis=1),
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[107]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[108]:


df_iv.loc[to_drop2,:].head()


# In[109]:


tmp = df_iv.loc[to_drop2,:].sort_values(by='iv', ascending=False)
tmp.head(20)


# In[110]:


# to_drop2 = []
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# In[111]:


df_sample[varsname_v2].info()


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[112]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[113]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[89]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[92]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[93]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[94]:


new_bins_dict


# In[95]:


combiner.load(new_bins_dict)


# In[96]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[97]:


to_drop_mode


# In[114]:


varsname_v2[-1]


# In[115]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), varsname_v2,                                    'lending_month', combiner, return_frame = False)
print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[116]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[117]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[118]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head())

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head())


# In[119]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[120]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# In[ ]:





# ### 删除不稳定特征

# In[123]:


len(list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index))


# In[124]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[130]:


df_iv_by_set.loc[drop_by_iv_set,:].head()


# In[131]:


df_psi_by_set.loc[drop_by_psi_set,:].head()


# In[134]:


df_miss_set.info()


# In[138]:


to_drop3 = [col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1]
# len([col for col in to_drop3 if df_miss_set.loc[col, '1_train']<=0.1])
print("剔除的变量有: ", len(to_drop3))


# In[139]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# In[140]:


# print(varsname_v3)


# ## 4.2 Y标签相关性删除

# In[141]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[142]:


df_sample_woe.head()


# In[143]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
    :param threshold: 相关系数的阈值，默认为0.85
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.corr(method=method)
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[144]:


gc.collect()


# In[145]:


df_iv_by_set.info()
df_iv_by_set.head()


# In[146]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe[varsname_v3],
                                                     df_iv_by_set['3_oot'],
                                                     method='kendall',
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[152]:


df_high_corr.info()
df_high_corr.head()


# In[154]:


print(to_drop4)


# In[155]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[157]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[168]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'lending_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='kendalltau'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[169]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[170]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[171]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[ ]:





# ## 4.3 逐步回归筛选

# In[78]:


# # 将woe转化后的数据做逐步回归
# train_woe = df_sample_woe.query("data_set=='1_train'")[varsname_v3+[target]]
# final_data, to_drop6 = toad.selection.stepwise(train_woe, target=target, estimator='ols', direction = 'both', \
#                                      criterion = 'aic', exclude = None, return_drop=True)

# print(final_data.shape) # 逐步回归从31个变量中选出了10个


# In[172]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# In[173]:


gc.collect()


# # 5.模型训练

# ## 5.1 模型训练

# In[209]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[388]:


# 2 定义超参空间
# hp.quniform("参数名称",下界,上界,步长)-适用于离散均匀分布的浮点点数
# hp.uniform("参数名称",下界, 下界)-适用于连续随机分布的浮点数
# hp.randint("参数名称",上界)-适用于[0,上界)的整数,区间为左闭右开
# hp.choice("参数名称",["字符串1","字符串2",...])-适用于字符串类型,最优参数由索引表示
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# 可以根据需要，注释掉偏后的一些不太重要的超参

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",21,200,1),
          "max_depth":2,
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",30,200,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.8,1.0),
#           "feature_fraction":1.0,
#           "bagging_fraction":1.0,
#           "min_gain_to_split":10,
          "min_gain_to_split":10,
#     "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 10.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 100, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 50
          }
spaces


# In[182]:


# 3，执行超参搜索
# 有了目标函数和参数空间,接下来要进行优化,需要了解以下参数:
# fmin:自定义使用的代理模型(参数algo),hyperopt支持如下搜索算法：
#       随机搜索(hyperopt.rand.suggest)
#       模拟退火(hyperopt.anneal.suggest)
#       TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）
# partial:修改算法涉及到的具体参数,包括模型具体使用了多少少个初始观测值(参数n_start_jobs),
#         以及在计算采集函数值时究竟考虑多少个样本(参数n_EI_candidates)
# trials:记录整个迭代过程,从hyperopt库中导入的方法Trials(),优化完成之后,
#        可以从保存好的trials中查看损失、参数等各种中间信息
# early_stop_fn:提前停止参数,从hyperopt库导入的方法no_progresss_loss(),可以输入具体的数字n,
#               表示当损失连续n次没有下降时,让算法提前停止
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds, init_model=None):

        """定义目标函数"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1,
                'num_threads': -1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                init_model=init_model,
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set,                                 num_boost_round=num_boost_round, init_model=init_model)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(30)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[385]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[386]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[179]:


len(varsname_v5)


# In[ ]:


selected_cols


# In[387]:


# 训练数据集
X_train = df_sample.query("data_set!='3_oot'")[selected_cols]
y_train = df_sample.query("data_set!='3_oot'")[target]
print(X_train.shape)


# In[ ]:


# 4，获取最优参数，调参过程
# 确定一个较高的学习率
# 对决策树基本参数调参
# 正则化参数调参
# 降低学习率
best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=10)


# In[390]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[391]:


print("最优参数best_params: ", best_params)


# In[392]:


### 添加无需调参的通用参数
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
bst_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
bst_params['learning_rate'] = spaces['learning_rate']
## 正则参数，防止过拟合
bst_params['bagging_fraction'] = best_params['bagging_fraction']    
bst_params['feature_fraction'] = best_params['feature_fraction'] 
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = spaces['lambda_l2']
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# 调参后的参数需要变成整数型
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = spaces['max_depth']
# 调参后的其他参
bst_params['min_gain_to_split'] = spaces['min_gain_to_split']


# In[393]:


print("最优参数bst_params: ", bst_params)


# In[394]:


# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[selected_cols],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[395]:


# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[190]:


gc.collect()


# In[396]:


# 最初评估模型效果
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[397]:


# 最初评估模型效果
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month = pd.concat([tmp, df_ks_auc_month], axis=1)
print(df_ks_auc_month)


df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set = pd.concat([tmp, df_ks_auc_set], axis=1)
print(df_ks_auc_set)


# In[192]:


# 最初评估模型效果
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month = pd.concat([tmp, df_ks_auc_month], axis=1)
print(df_ks_auc_month)


df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set = pd.concat([tmp, df_ks_auc_set], axis=1)
print(df_ks_auc_set)


# In[200]:


usecols = df_sample_.columns.to_list()[:12] + varsname_v5
print(len(usecols))


# In[201]:


print(usecols[:12], usecols[-10:])


# In[225]:


# df_sample_[usecols].to_csv(result_path + '桔子商城fdp30授信模型_原始数据集.csv', index=False)
print(result_path + '桔子商城fdp30授信模型_原始数据集.csv')


# In[205]:


df_sample_30 = df_sample_.query("diff_days>30")[usecols].reset_index(drop=True)
for col in varsname_v5:
    print(f"========={col}=========")
    df_sample_30[col] = pd.to_numeric(df_sample_30[col])


# In[206]:


# 最初评估模型效果
df_sample_30['y_prob'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[207]:


# 最初评估模型效果-30+客群
df_ks_auc_month_30 = model_ks_auc(df_sample_30, target, 'y_prob', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_30 = pd.concat([tmp, df_ks_auc_month_30], axis=1)
print(df_ks_auc_month_30)


# df_ks_auc_set_30 = model_ks_auc(df_sample_30, target, 'y_prob', 'data_set')
# tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
# df_ks_auc_set_30 = pd.concat([tmp, df_ks_auc_set_30], axis=1)
# print(df_ks_auc_set_30)


# In[211]:


model_ks_auc(df_sample, target, 'standard_score', 'lending_month')


# In[212]:


model_ks_auc(df_sample_30, target, 'standard_score', 'lending_month')


# In[196]:


# 模型变量重要性
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month = feature_importance(lgb_model) 
df_importance_month = pd.merge(df_importance_month, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month = df_importance_month.reset_index()
df_importance_month = df_importance_month.rename(columns={'index':'varsname'})
df_importance_month.head()


# In[197]:



# 效果评估后模型变量重要性
df_importance_set = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set = pd.merge(df_importance_set, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set['iv的变化幅度'] = df_importance_set['3_oot_iv']/df_importance_set['1_train_iv'] - 1
df_importance_set.drop(columns=['1_train_psi'], inplace=True)
df_importance_set = df_importance_set.reset_index()
df_importance_set = df_importance_set.rename(columns={'index':'varsname'})
df_importance_set.head()


# In[198]:


result_path


# In[199]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_{timestamp}.pkl')
print(result_path + f'{task_name}_{timestamp}.bin')


# In[213]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_模型结果分析_{task_name}_{timestamp}.xlsx') as writer:
    df_importance_month.to_excel(writer, sheet_name='df_importance_month')
    df_importance_set.to_excel(writer, sheet_name='df_importance_set')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
    df_ks_auc_month_30.to_excel(writer, sheet_name='df_ks_auc_month_30')
#     df_ks_auc_set_30.to_excel(writer, sheet_name='df_ks_auc_set_30')
print("数据存储完成！{timestamp}")
print(result_path + f'4_模型结果分析_{task_name}_{timestamp}.xlsx')


# ## 5.2 模型优化

# ### 5.2.3 模型融合

# In[489]:


### 优化调参1
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 65
opt_params['max_depth'] = 1
# 调参后的其他参
opt_params['min_gain_to_split'] = 10

# opt_params = {
#  'boosting': 'gbdt',
#  'objective': 'binary',
#  'metric': 'auc',
#  'bagging_freq': 1,
#  'scale_pos_weight': 1,
#  'seed': 1,
#  'num_threads': -1,
#  'learning_rate': 0.05,
#  'bagging_fraction': 0.8628008772208227,
#  'feature_fraction': 0.6177619614753441,
#  'lambda_l1': 0,
#  'lambda_l2': 300,
#  'early_stopping_rounds': 600,
#  'num_leaves': 75,
#  'min_data_in_leaf': 95,
#  'max_depth': 2,
#  'min_gain_to_split': 10}


# In[490]:


print("最优参数opt_params: ", opt_params)


# In[491]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[492]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[493]:


# 确定数据集参数后，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[['standard_score','y_prob_v3']],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[494]:


# # 确定数据集参数后，训练模型
# X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[selected_cols],
#                                                     df_sample.query("data_set!='3_oot'")[target],
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=df_sample.query("data_set!='3_oot'")[target])
# print(X_train.shape, X_test.shape)

# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(df_sample['data_set'].value_counts())


# In[495]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[496]:


# 优化后评估模型效果
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
lgb_model.params


# In[498]:


# 优化后评估模型效果
df_ks_auc_month_v2 = model_ks_auc(df_sample, target, 'y_prob_v1', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month_v2 = pd.concat([tmp, df_ks_auc_month_v2], axis=1)
print(df_ks_auc_month_v2)


df_ks_auc_set_v2 = model_ks_auc(df_sample, target, 'y_prob_v1', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
print(df_ks_auc_set_v2)


# In[499]:


model_ks_auc(df_sample, 'fpd10', 'y_prob_v1', 'lending_month')


# In[501]:


# 优化后评估模型效果
df_sample_30['y_prob_v1'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[502]:


# 优化后评估模型效果-30+客群
df_ks_auc_month_30_v2 = model_ks_auc(df_sample_30, target, 'y_prob_v1', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_30_v2 = pd.concat([tmp, df_ks_auc_month_30_v2], axis=1)
print(df_ks_auc_month_30_v2)


# df_ks_auc_set_30_v2 = model_ks_auc(df_sample.query("diff_days>30"), target, 'y_prob_v1', 'data_set')
# tmp = get_target_summary(df_sample.query("diff_days>30"), target, 'data_set').set_index('bins')
# df_ks_auc_set_30_v2 = pd.concat([tmp, df_ks_auc_set_30_v2], axis=1)
# print(df_ks_auc_set_30_v2)


# In[503]:


# 模型变量重要性
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_v2 = feature_importance(lgb_model) 
df_importance_v2 = pd.merge(df_importance_v2, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_v2 = df_importance_v2.reset_index()
df_importance_v2 = df_importance_v2.rename(columns={'index':'varsname'})
df_importance_v2


# In[604]:





# In[504]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_opt_{timestamp}.pkl')
print(result_path + f'{task_name}_opt_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_opt_{timestamp}.xlsx') as writer:
    df_importance_v2.to_excel(writer, sheet_name='df_importance_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')
    df_ks_auc_month_30_v2.to_excel(writer, sheet_name='df_ks_auc_month_30_v2')
#     df_ks_auc_set_30_v2.to_excel(writer, sheet_name='df_ks_auc_set_30_v2')      
print("数据存储完成！{timestamp}")
print(result_path + f'5_模型优化_{task_name}_模型融合_{timestamp}.xlsx')


# ### 5.2.2 特征优化

# In[506]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[507]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set!='3_oot'").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[508]:


to_drop6 = ['standard_score']
# + list(df_importance_month_v3[df_importance_month_v3['gain']>0].index
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[509]:


# 使用的数据，训练模型
X_train, X_test, y_train, y_test = train_test_split(df_sample.query("data_set!='3_oot'")[varsname_v6],
                                                    df_sample.query("data_set!='3_oot'")[target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample.query("data_set!='3_oot'")[target])
print(X_train.shape, X_test.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
df_sample['data_set'].value_counts()


# In[510]:


opt_params = {
 'boosting': 'gbdt',
 'objective': 'binary',
 'metric': 'auc',
 'bagging_freq': 1,
 'scale_pos_weight': 1,
 'seed': 1,
 'num_threads': -1,
 'learning_rate': 0.1,
 'bagging_fraction': 0.8628008772208227,
 'feature_fraction': 0.6177619614753441,
 'lambda_l1': 0,
 'lambda_l2': 300,
 'early_stopping_rounds': 600,
 'num_leaves': 75,
 'min_data_in_leaf': 95,
 'max_depth': 1,
 'min_gain_to_split': 10}


# In[511]:


# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[512]:


# 最初评估模型效果
df_sample['y_prob_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[513]:


# 最初评估模型效果
df_ks_auc_month_v3 = model_ks_auc(df_sample, target, 'y_prob_v3', 'lending_month')
tmp = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_ks_auc_month_v3 = pd.concat([tmp, df_ks_auc_month_v3], axis=1)
print(df_ks_auc_month_v3)


df_ks_auc_set_v3 = model_ks_auc(df_sample, target, 'y_prob_v3', 'data_set')
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v3 = pd.concat([tmp, df_ks_auc_set_v3], axis=1)
print(df_ks_auc_set_v3)


# In[514]:


model_ks_auc(df_sample, 'fpd10', 'y_prob_v3', 'lending_month')


# In[517]:


# lgb_model.feature_name()


# In[428]:


df_sample_30['y_prob_v3'] = lgb_model.predict(df_sample_30[X_train.columns], num_iteration=lgb_model.best_iteration)


# In[429]:



# 最初评估模型效果-30+客群
df_ks_auc_month_v3_30 = model_ks_auc(df_sample_30, target, 'y_prob_v3', 'lending_month')
tmp = get_target_summary(df_sample_30, target, 'lending_month').set_index('bins')
df_ks_auc_month_v3_30 = pd.concat([tmp, df_ks_auc_month_v3_30], axis=1)
print(df_ks_auc_month_v3_30)

# df_ks_auc_set_v3_30 = model_ks_auc(df_sample.query("diff_days>30"), target, 'y_prob_v3', 'data_set')
# tmp = get_target_summary(df_sample.query("diff_days>30"), target, 'data_set').set_index('bins')
# df_ks_auc_set_v3_30 = pd.concat([tmp, df_ks_auc_set_v3_30], axis=1)
# print(df_ks_auc_set_v3_30)


# In[515]:


# 模型变量重要性
# df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_by_month, how='inner', left_index=True,right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = df_importance_month_v3.rename(columns={'index':'varsname'})
df_importance_month_v3


# In[431]:


# 效果评估后模型变量重要性
df_importance_set_v3 = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_set, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_psi_iv, how='inner', left_index=True,right_index=True)
df_importance_set_v3['iv的变化幅度'] = df_importance_set_v3['3_oot_iv']/df_importance_set_v3['1_train_iv'] - 1
df_importance_set_v3.drop(columns=['1_train_psi'], inplace=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = df_importance_set_v3.rename(columns={'index':'varsname'})
df_importance_set_v3


# In[ ]:


selected_cols = 


# In[432]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')


# In[433]:



# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
# save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
# print(f"模型保存完成！：{timestamp}")
# print(result_path + f'{task_name}_v3_{timestamp}.pkl')
# print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')
    df_ks_auc_month_v3_30.to_excel(writer, sheet_name='df_ks_auc_month_v3_30')
#     df_ks_auc_set_v3_30.to_excel(writer, sheet_name='df_ks_auc_set_v3_30')    
print("数据存储完成！{timestamp}")
print(result_path + f'5_模型优化_{task_name}_v3_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# ### 5.3.1数据处理

# In[518]:


# usecols= ['order_no','channel_id', 'lending_time','lending_month', 'mob',\
#           'maxdpd', 'fpd', 'fpd10', 'fpd30', 'mob4dpd30', 'diff_days'] + varsname_v5
df_model_vars = pd.read_csv('./result/洞侦续侦模型fpd30/桔子商城fpd30授信实时模型_建模数据集_250110.csv')
df_model_vars.info(show_counts=True)
df_model_vars.head()


# In[520]:


df_model_vars[varsname_v6] = df_model_vars[varsname_v6].replace(-1, np.nan)
gc.collect()


# In[521]:


# df_model_vars[varsname_v5].describe().T


# In[708]:


# 衍生Y标签
print(df_model_vars['fpd'].min(), df_model_vars['fpd'].max())
print(df_model_vars['fpd30'].min(), df_model_vars['fpd30'].max())


# In[522]:


# 衍生Y标签
# df_model_vars['fpd10'] = df_model_vars['fpd'].apply(lambda x: 1 if x>10 else 0)
# df_model_vars['fpd20'] = df_model_vars['fpd'].apply(lambda x: 1 if x>20 else 0)


# In[525]:


df_model_vars.groupby(["lending_time",'fpd10'])['order_no'].count().unstack()


# In[526]:


# df_model_vars.loc[df_model_vars.query("lending_time>='2024-10-16'").index, 'fpd30'] = -1
# df_model_vars.loc[df_model_vars.query("lending_time>='2024-10-26'").index, 'fpd20'] = -1


# In[527]:


# df_model_vars.drop(index=df_model_vars.query("lending_time>='2024-11-06'").index, inplace=True)


# In[528]:


# df_model_vars = df_model_vars.reset_index(drop=True)


# In[530]:


# # 添加客群标签
# def diff_days_(x):
#     if x<=30:
#         days = 'T30-'
#     elif x>30:
#         days = 'T30+'
#     else:
#         days = np.nan
#     return days

# df_model_vars['客群'] = df_model_vars['diff_days'].apply(diff_days_)


# In[531]:


df_model_vars['diff_days'].max()


# In[532]:


# df_model_vars.info(show_counts=True)


# In[534]:


# 第一次模型打分 
lgb_model= load_model_from_pkl('./result/桔子商城授信模型fpd30/桔子商城授信模型fpd30_v3_20250110145432.pkl')
df_model_vars['y_prob_v3'] = lgb_model.predict(df_model_vars[varsname_v6],
                                               num_iteration=lgb_model.best_iteration)


# In[720]:


result_path


# In[537]:


# 最终模型打分
lgb_model= load_model_from_pkl('./result/桔子商城授信模型fpd30/桔子商城授信模型fpd30_20250110152634.pkl')
df_model_vars['y_prob_v1'] = lgb_model.predict(df_model_vars[['standard_score','y_prob_v3']],
                                               num_iteration=lgb_model.best_iteration)


# In[1]:


lgb_model.feathure_name()


# In[571]:


# 其他提现模型数据 
sql="""
select t1.order_no,channel_id, fpd10,fpd30,prob
from
(
    select order_no,channel_id, fpd10,fpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target as t 
    where dt=date_sub(current_date(), 1) 
      and lending_time>='2024-11-01'
      and lending_time<='2024-11-30'
) as t1 
inner join 
(select order_no,prob
from znzz_fintech_ads.fkmodel_ascore_fico_fpd10_v1_score as t 
where dt>='2024-11-01'
)as t2 on t1.order_no=t2.order_no
;

"""
df_tx_bj = get_data(sql)
df_tx_bj.info(show_counts=True)
df_tx_bj.head() 


# In[576]:


df_tx_bj['channel_type'] = df_tx_bj['channel_id'].astype(int).apply(channel_type)
df_tx_bj['channel_type'].value_counts()


# In[577]:


df_tx_bj['channel_rate'] = df_tx_bj['channel_id'].astype(int).apply(channel_rate)
df_tx_bj['channel_rate'].value_counts()


# In[587]:


fpr, tpr, _ = roc_curve(df_tx_bj.query("channel_type=='桔子商城'")['fpd30'], df_tx_bj.query("channel_type=='桔子商城'")['prob'], pos_label=1)
from sklearn.metrics import auc
auc_value = auc(fpr, tpr)
ks_value = max(abs(tpr - fpr))
print(auc_value, ks_value)


# In[540]:


# selected_cols = df_tx_bj.columns.to_list()[3:]
# df_tx_bj[selected_cols].describe().T


# In[541]:


# df_tx_bj_copy = df_tx_bj.copy()


# In[542]:


# result_path


# In[543]:


# df_tx_bj.to_csv(result_path + '全渠道其他提现模型分数_241218.csv')
# print(result_path + '全渠道其他提现模型分数_241218.csv')


# In[544]:


# # 好分数转为坏分数
# for i, col in enumerate(selected_cols):
#     print(f'第{i}个变量：{col}')
#     df_tx_bj[col] = 1 - df_tx_bj[col]


# In[545]:


# print(df_model_vars.shape, df_tx_bj.shape)


# In[546]:


# df_evalue = pd.merge(df_model_vars, df_tx_bj, how='left',on=['order_no'])
# print(df_evalue.shape, df_evalue['order_no'].nunique())


# In[547]:


# df_evalue.info(show_counts=True)


# In[548]:


# df_evalue.drop(columns=['apply_date','channel_id_y'], inplace=True)
# df_evalue.rename(columns={'channel_id_x':'channel_id'},inplace=True)


# ### 5.3.2 效果对比

# In[549]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col, percentile=0.95):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    badrate = df[label_col].mean()
    
    if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
    elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
        pct_n = df[score_col].quantile(percentile)
        pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
    else:
        print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
    if badrate>0 and pct_n_badrate>0:
        lift_n = pct_n_badrate/badrate
    else:
        lift_n = np.nan
    return pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}',
                                                    'top5lift':f'top5lift_{score_}'})
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[559]:


colsname = ['bad_score','y_prob_v3','y_prob_v1']

print(colsname)
target_list = ['fpd10', 'fpd20', 'fpd30']
labels_models_dict = {target: colsname for target in target_list}
print(labels_models_dict)


# In[551]:


df_model_vars['channel_id'].head()


# In[732]:


# groupkeys1 = ['lending_month']
# df_ksauc_all_v1 = cal_ks_auc(df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(loc=(len(groupkeys1)), column='渠道', value='全渠道', allow_duplicates=False)
# df_ksauc_all_v1.insert(loc=(len(groupkeys1)+1), column='客群', value='全体', allow_duplicates=False)
# df_ksauc_all_v1.head()


# In[553]:


def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247):
        if x == 227:
            channel='227'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# In[554]:


df_evalue = df_model_vars.copy()


# In[555]:


df_evalue['channel_type'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_type'].value_counts()


# In[556]:


df_evalue['channel_rate'] = df_evalue['channel_id'].apply(channel_rate)
df_evalue['channel_rate'].value_counts()


# In[735]:


# df_evalue['lending_month_new'] = df_evalue['lending_month']
# df_evalue.loc[df_evalue.query("lending_time>='2024-09-01' & lending_time<='2024-09-20'").index, 'lending_month_new']='2024-09_1train'
# df_evalue.loc[df_evalue.query("lending_time>='2024-09-21' & lending_time<='2024-09-30'").index, 'lending_month_new']='2024-09_3oot'


# In[560]:



groupkeys2 = ['lending_month', 'channel_type']
df_ksauc_all_v2 = cal_ks_auc(df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.insert(loc=(len(groupkeys2)), column='客群', value='全体', allow_duplicates=False)
df_ksauc_all_v2.head()


# In[749]:



# groupkeys3 = ['lending_month_new', '客群']
# df_ksauc_all_v3 = cal_ks_auc(df_evalue, groupkeys3, labels_models_dict)
# df_ksauc_all_v3.insert(loc=(len(groupkeys3)-1), column='渠道', value='全渠道', allow_duplicates=False)
# df_ksauc_all_v3.head()


# In[558]:



groupkeys4 = ['lending_month', 'channel_rate']
df_ksauc_all_v4 = cal_ks_auc(df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.head()


# In[588]:


df_ksauc_all_v2.query("channel_type=='桔子商城'")


# In[564]:


df_ksauc_all_v4.query("channel_rate=='227'")


# In[589]:


# target_list = ['fpd10', 'fpd20', 'fpd30']
# labels_models_dict_2 = {target: ['y_prob'] for target in target_list}
# print(labels_models_dict_2)


# In[590]:


# groupkeys5 = ['lending_month_new', '渠道', '客群', 'category']
# df_ksauc_all_v5 = cal_ks_auc(df_evalue, groupkeys5, labels_models_dict_2)
# df_ksauc_all_v5.head()


# In[591]:



# groupkeys6 = ['lending_month_new', '渠道', 'category']
# df_ksauc_all_v6 = cal_ks_auc(df_evalue, groupkeys6, labels_models_dict_2)
# df_ksauc_all_v6.insert(loc=(len(groupkeys6)), column='客群', value='全体', allow_duplicates=False)
# df_ksauc_all_v6.head()


# In[592]:


df_ksauc_all_1 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_2 = pd.concat([df_ksauc_all_v5,df_ksauc_all_v6], axis=0)


# In[593]:


# result_path


# In[594]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_效果对比分析_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
#     df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型对比分析_{task_name}_{timestamp}.xlsx')


# # 6. 评分分布

# In[374]:


result_path


# In[375]:


df_sample.to_csv(result_path + r'全渠道实时提现行为模型fpd30样本.csv',index=False)


# In[376]:


score = 'y_prob_v3'


# In[377]:


df_sample['lending_month'].value_counts()


# In[378]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[379]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-08'"), 
                                                [score], 'lending_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[381]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: 手机号md5授信实时m4d30融合模型_2501_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")


# In[2]:


pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# 设置数据存储
task_name = '手机号md5授信实时融合模型v2'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./手机号md5授信实时融合模型v2'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 0. 数据读取

# In[4]:


print(result_path)


# In[5]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


# In[7]:


df1 = pd.read_csv('前筛实时模型250919_ys.csv')
df1.info(show_counts=True)
df1.head()


# In[9]:


df2 = pd.read_csv('前筛实时模型250906.csv')
df2.info(show_counts=True)
df2.head()


# In[11]:


df_sample_ = pd.merge(df2, df1[['order_no','m1b0070','m1b0077']], how='left',on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[12]:


varsname = ['id5_off_m3d30_2507', 'id5_off_m4d30_2509v2', 'md5_off_m3d30_2507', 'md5_off_m4d30_2509v2', 'm1b0070', 'm1b0071', 'm1b0074', 'm1b0075', 'm1b0077', 'umeng_sdk_score', 'tianchuang_score', 'fico_model', 'haina_model']
print(len(varsname))
print(varsname)


# In[13]:



for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[14]:


print(df_sample_.shape[0], df_sample_['order_no'].nunique())


# In[15]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[16]:


df_sample = df_sample_.copy()
print(df_sample.shape)
df_sample = df_sample.dropna(subset=varsname, how='all').reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[18]:


df_sample.loc[df_sample.query("apply_date>='2025-01-01' & apply_date<='2025-02-14'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2025-02-15' & apply_date<='2025-02-28'").index, 'data_set']='2_test'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-03-09'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-10' & apply_date<='2025-04-15'").index, 'data_set']='3_oot2'


# In[19]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. 样本概况

# In[20]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[21]:


print(df_sample[target].value_counts())


# In[22]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[23]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[24]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[25]:


# # 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor


# ## 2.1缺失值处理

# In[ ]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[ ]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[26]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[27]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[28]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set)


# In[34]:



def calculate_iv_by_group(df, 
                          group_col, 
                          target, 
                          variables,
                          is_return_unique=False,
                          method='quantile', 
                          n_bins=10):
    """
    按分组字段计算每个变量在各组中的 IV 和 unique 值
    输出格式：所有 IV 列在前，所有 UNIQUE 列在后
    
    Parameters:
    -----------
    df : pd.DataFrame
        原始数据
    group_col : str
        分组列名（如 'sample_type', 'year_month' 等）
    target : str
        目标变量列名
    variables : list
        要分析的变量名列表
    method : str
        toad.quality 的分箱方法
    n_bins : int
        分箱数量
    
    Returns:
    --------
    pd.DataFrame
        索引: variable
        列:  {group1}_iv, {group2}_iv, ..., {group1}_unique, {group2}_unique, ...
    """
    # 检查列是否存在
    required_cols = [group_col, target] + variables
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame.")
    
    # 存储每组的结果
    iv_data = {}   # 存储所有 iv 列
    unique_data = {}  # 存储所有 unique 列
    
    # 按 group_col 分组
    for group_name, group_df in df.groupby(group_col):
        data = group_df[variables + [target]].copy()
        
        if data.empty:
            print(f"Warning: No data in group '{group_name}'")
            continue
        
        try:
            # 使用 toad 计算质量指标
            quality = toad.quality(
                data,
                target=target,
                iv_only=True,
                method=method,
                n_bins=n_bins
            )
            
            # 只保留 iv 和 unique
            cols_to_keep = ['iv', 'unique']
            existing_cols = [c for c in cols_to_keep if c in quality.columns]
            quality = quality[existing_cols]
            
            # 分开存储
            if 'iv' in quality.columns:
                iv_data[f"{group_name}_iv"] = quality['iv']
            
            if 'unique' in quality.columns:
                unique_data[f"{group_name}_unique"] = quality['unique']
                
        except Exception as e:
            print(f"Error processing group '{group_name}': {str(e)}")
            continue
    
    # 检查是否有数据
    if not iv_data and not unique_data:
        raise ValueError("No valid group data processed.")
    
    # 转换为 DataFrame
    df_iv = pd.DataFrame(iv_data)
    df_unique = pd.DataFrame(unique_data)
    
    # 合并：IV 在前，Unique 在后
    if is_return_unique:
        result = pd.concat([df_iv, df_unique], axis=1)
    else:
        result = df_iv.copy()
    
    # 填充缺失值
    result = result.fillna(value=pd.NA)
    
    # 设置索引名
    result.index.name = 'variable'
    
    return result


# In[ ]:


# 2.3 快速查看特征重要性
# df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
#                      method='quantile', n_bins=10)
# df_iv.index.name = 'variable'
# print(df_iv.head())


# In[35]:



df_iv = calculate_iv_by_group(
    df=df_sample,
    group_col='data_set',
    target=target,
    variables=varsname
)
df_iv


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[37]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[38]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[39]:


target


# In[40]:


varsname_v2 = varsname[:]


# In[41]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='quantile', n_bins=10, empty_separate=True) 


# In[42]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[43]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[44]:


new_bins_dict


# In[45]:


combiner.load(new_bins_dict)


# In[46]:


combiner.export()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[48]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[49]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[50]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[51]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[52]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[53]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[54]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[ ]:


from itertools import combinations

# 定义一个函数来进行递归特征消除
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # 使用当前最佳特征集训练模型
        # ✅ 构建当前特征子集的数据
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # 如果是 numpy array，用位置索引
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # 更新最佳特征集
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# booster = lgb.Booster(model_file=result_path+'友盟联合建模_v6_20250717140214.bin')  # 自动识别 .txt/.bin/.json


# ## 5.1 数据预处理

# In[ ]:


df_sample[target] = pd.to_numeric(df_sample[target])


# In[56]:


df_sample[target].value_counts()


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[modeltrian_target].value_counts()


# In[59]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[60]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[61]:


df_sample['channel_types'].value_counts()


# In[62]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[ ]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.07
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.7    
opt_params['feature_fraction'] = 0.7
opt_params['lambda_l1'] = 5
opt_params['lambda_l2'] = 7
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 8
opt_params['min_data_in_leaf'] = 800
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 80


# In[230]:


opt_params={'objective': 'binary',
  'boosting': 'gbdt',
  'metric': 'auc',
  'min_gain_to_split': 80,
  'early_stopping_rounds': 30,
  'scale_pos_weight': 1,
  'seed': 1,
  'verbose': -1,
  'bagging_fraction': 0.7411882093860374,
  'feature_fraction': 0.7515414702756096,
  'lambda_l1': 1.899961726818411,
  'lambda_l2': 201.84972131273844,
  'learning_rate': 0.09365668958835766,
  'max_depth': 5,
  'min_data_in_leaf': 150,
  'num_leaves': 15}


# In[241]:


print("最优参数opt_params: ", opt_params)


# In[265]:


opt_params={'objective': 'binary',
  'boosting': 'gbdt',
  'metric': 'auc',
  'min_gain_to_split': 80,
  'early_stopping_rounds': 30,
  'scale_pos_weight': 1,
  'seed': 1,
  'verbose': -1,
  'bagging_fraction': 0.6548218675559451,
  'feature_fraction': 0.5669296516597987,
  'lambda_l1': 1.6525147188526745,
  'lambda_l2': 138.00050549759732,
  'learning_rate': 0.08292453162668129,
  'max_depth': 6,
  'min_data_in_leaf': 700,
  'num_leaves': 23}

print("最优参数opt_params: ", opt_params)


# In[266]:


varsname


# In[267]:


varsname_base=['id5_off_m3d30_2507',
 'id5_off_m4d30_2509v2',
 'md5_off_m3d30_2507',
 'md5_off_m4d30_2509v2',
 'm1b0070',
 'm1b0071',
 'm1b0074',
 'm1b0075',
 'm1b0077',
 'umeng_sdk_score',
 'haina_model']


# In[268]:


print(len(varsname_base))
print(varsname_base)


# In[269]:


def model_train(data, selected_vars, target, opt_params):
    X_train = data[data['data_set']=='1_train'][selected_vars]
    y_train = data[data['data_set']=='1_train'][target]
    X_test = data[data['data_set']=='2_test'][selected_vars]
    y_test = data[data['data_set']=='2_test'][target]      
    
    best_features = rfe_with_lgb(X_train, y_train, X_test, y_test, opt_params)
    
    dtrain = lgb.Dataset(X_train[best_features], label=y_train)
    dtest = lgb.Dataset(X_test[best_features], label=y_test, reference=dtrain)
    lgb_model = lgb.train(opt_params, dtrain, valid_sets=dtest, num_boost_round=10000,verbose_eval=50)
    
    return lgb_model

from itertools import combinations

# 定义一个函数来进行递归特征消除
def rfe_with_lgb(X_train, y_train, X_test, y_test, params):
    feature_names = list(range(X_train.shape[1])) if isinstance(X_train, np.ndarray) else list(X_train.columns)
    best_features = feature_names[:]
    best_feature_count = len(feature_names)
    
    while len(best_features) > 0:
        # 使用当前最佳特征集训练模型
        # ✅ 构建当前特征子集的数据
        if isinstance(X_train, pd.DataFrame):
            train_set = lgb.Dataset(X_train[best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[best_features], label=y_test, reference=train_set)
        else:
            # 如果是 numpy array，用位置索引
            train_set = lgb.Dataset(X_train[:, best_features], label=y_train)
            valid_set = lgb.Dataset(X_test[:,best_features], label=y_test, reference=train_set)            

        lgb_model = lgb.train(params, train_set, valid_sets=valid_set, num_boost_round=10000)
        df_importance = feature_importance(lgb_model)
        df_importance = df_importance.reset_index()
        
        # 更新最佳特征集
        if all(df_importance['gain']>0):
            break
        
        best_features = df_importance[df_importance['gain']>0]['feature'].to_list()
        gc.collect()
    
    return best_features


# In[ ]:


# # 确定数据集参数后，训练模型
# X_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][varsname_base]
# y_train_ = df_sample[~df_sample['data_set'].isin(['3_oot1','3_oot2'])][modeltrian_target]
# print(df_sample.groupby(['data_set'])['order_no'].count())
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'
# print(X_train.shape)
# print(df_sample.groupby(['data_set'])['order_no'].count())


# In[270]:


# 6，训练/保存/评估模型
# 优化训练模型
lgb_model = model_train(df_sample, varsname_base, modeltrian_target, opt_params)


# In[ ]:





# In[271]:


# 优化后评估模型效果
df_sample['y_pred_v2'] = lgb_model.predict(df_sample[lgb_model.feature_name()], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v2'].head()


# In[272]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'data_set')
df_ks_auc_set_v1


# In[273]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'apply_month')
df_ks_auc_month_v1


# In[ ]:





# In[274]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[275]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[277]:


df_sample["customer_tags"].value_counts()


# In[279]:


# 按 flag 分组计算
df_ks_auc_set_all = (
    df_sample[(df_sample['umeng_sdk_score'].notna())]
    .groupby(['customer_tags'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v2', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all


# In[280]:


df_sample[df_sample['umeng_sdk_score'].notna()]['apply_date'].max()


# In[249]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[ ]:





# In[281]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')  
    df_ks_auc_set_all.to_excel(writer, sheet_name='分客群') 
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.2 参数优化

# In[ ]:


def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=3, max_evals=50):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数""" 
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': 0,
                'lambda_l2': 300,
                'min_gain_to_split':10,
                'early_stopping_rounds': 30,
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[ ]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[ ]:


# 确定数据集参数后，训练模型
X_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]


# In[ ]:


# 2 定义搜索空间
spaces = {
          # general parameters
          "learning_rate":hp.uniform('learning_rate', 0.05, 0.1),
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",15,31,1),
          'max_depth': hp.quniform('max_depth', 4, 6, 1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",50,150,10),
          "feature_fraction":hp.uniform("feature_fraction",0.7,0.9),
          "bagging_fraction":hp.uniform("bagging_fraction",0.7,0.9)
          }


# In[ ]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=None, y_test=None, max_evals=30)
print("best_params: ", best_params)


# In[ ]:


bst_params = {
        #general parameters
        'objective': 'binary',
        'boosting': 'gbdt',
        'metric': 'auc',
        'learning_rate': best_params['learning_rate'], # 学习率,超参
        #tuning parameters
        'num_leaves': int(best_params['num_leaves']), # 叶子节点数, 超参
        'min_data_in_leaf': int(best_params['min_data_in_leaf']), # 一个叶子上数据的最小数量, 超参
        'max_depth': int(best_params['max_depth']), # 树的深度
        'bagging_freq': 1,
        'bagging_fraction': best_params['bagging_fraction'], # 数据抽样, 超参
        'feature_fraction': best_params['feature_fraction'], # 特征抽样, 超参
        'lambda_l1': 0, # l1 正则化
        'lambda_l2': 300, # l2 正则化
        'min_gain_to_split':10, # 切分最小增益
        'early_stopping_rounds': 30,
        'scale_pos_weight': 1,
        'seed': 1
        }
print("最优参数bst_params: ", bst_params)


# In[ ]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")
plt.show()


# In[ ]:


opt_params = bst_params
print("最优参数opt_params: ", opt_params)


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# df_sample.to_parquet(result_path + 'df_sample.parquet')


# In[ ]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[ ]:


df_sample['data_set'].value_counts()


# In[ ]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[ ]:


# 优化后评估模型效果
df_sample['y_pred_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v3'].head()


# In[ ]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'apply_month')
df_ks_auc_month_v2


# In[ ]:





# In[ ]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v3', 'data_set')
df_ks_auc_set_v2


# In[ ]:


# 按 flag 分组计算
df_ks_auc_set_all_v2 = (
    df_sample
    .groupby(['fico数据是否缺失','flag'])
    .apply(
        lambda g: calculate_ks_auc(g, modeltrian_target, target, 'y_pred_v3', 'data_set')
    )
    .reset_index()
)
df_ks_auc_set_all_v2


# In[ ]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2


# In[ ]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[ ]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')   
    df_ks_auc_set_all_v2.to_excel(writer, sheet_name='分客群')  
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[282]:


df_sample.info(show_counts=True)


# In[83]:


vars_combiner_dict


# In[85]:


vars_combiner_list


# In[96]:


without_fico= load_model_from_pkl('pre_selection_20250919_without_fico.pkl')
without_fico = lgb.Booster(model_str = without_fico._handle)
print(without_fico.feature_name())
df_sample['tianchuang_id5@_main'] = without_fico.predict(df_sample[without_fico.feature_name()], num_iteration=without_fico.best_iteration)
df_sample['tianchuang_id5@_main'].head()


# In[100]:


df_sample['tianchuang_id5@_main'] = 1 - df_sample['tianchuang_id5@_main']


# In[99]:


fico_= load_model_from_pkl('pre_selection_20250919.pkl')
fico_ = lgb.Booster(model_str = fico_._handle)
print(fico_.feature_name())
df_sample['fico_model@tianchuang_score_id@_main'] = fico_.predict(df_sample[fico_.feature_name()], num_iteration=fico_.best_iteration)
df_sample['fico_model@tianchuang_score_id@_main'].head()


# In[101]:


df_sample['fico_model@tianchuang_score_id@_main'] = 1 - df_sample['fico_model@tianchuang_score_id@_main']


# ### 5.3.1数据处理

# In[ ]:


usecols = ['order_no', 'id_no_des', 'apply_date']
print(len(usecols))
# print(usecols)


# In[283]:


df_evalue = df_sample.copy()
df_evalue.info(show_counts=True)
df_evalue.head()


# In[184]:


list(vars_combiner_dict.values())


# In[185]:


# 获取所有值列表
lists = list(vars_combiner_dict.values())

# 将第一个列表转换为集合，作为初始交集
common_elements = set(lists[0])  # 使用第一个列表

# 与其余每个列表求交集
for lst in lists[1:]:
    common_elements &= set(lst)  # 等同于 common_elements = common_elements.intersection(set(lst))

print("所有列表共有的元素:", common_elements)


# In[186]:


varsname_base


# In[187]:


print(len(vars_combiner_list))
print(vars_combiner_list)


# In[198]:


list1 = [item.replace('@_main', '').split('@') for item in vars_combiner_list]
print(len(list1), list1)


# In[200]:


list2 = vars_combiner_list
print(len(list2), list2)

list3 = [['id5_off_m3d30_2507','id5_off_m4d30_2509v2','md5_off_m3d30_2507','md5_off_m4d30_2509v2']] * 15
print(len(list3), list3)


# In[202]:


# 生成三元组：(list1[i], list3[i], list2[i])
triplets = [(list2[i], list3[i], list1[i]) for i in range(len(list2))]

# 打印结果
for triplet in triplets:
    print(triplet)
    score_1, score_2, score_3 = triplet
    print(score_1, score_2, score_3)


# In[ ]:


df_evalue['target_mob4dpd30'].value_counts() 


# In[ ]:


df_evalue['fico_score']=df_evalue['fico_model']


# In[ ]:


# df_evalue['target_mob4dpd30_1'] = 1 -df_evalue['target_mob4dpd30']
# df_evalue['target_mob4dpd30_1'].value_counts() 


# In[284]:


filepath = '/home/liaoxilin/联合建模/友盟sdk&百行多头/'
umeng_model= load_model_from_pkl(filepath + 'result_友盟联合分融合模型/友盟联合分融合模型_v1_20250806155455.pkl')
print(umeng_model.feature_name())
df_evalue['id5_off_cpd30_2508'] = umeng_model.predict(df_evalue[umeng_model.feature_name()], num_iteration=umeng_model.best_iteration)
df_evalue['id5_off_cpd30_2508'].head()


# In[ ]:


umeng_fico_model= load_model_from_pkl(filepath + 'result_友盟Fico/友盟Fico离线融合_v2_20250812163020.pkl')
print(umeng_fico_model.feature_name())
df_evalue['id5_off_umeng_fico_m4d30_2508'] = umeng_fico_model.predict(df_evalue[umeng_fico_model.feature_name()], num_iteration=umeng_fico_model.best_iteration)
df_evalue['id5_off_umeng_fico_m4d30_2508'].head()


# In[ ]:


fico_model_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型/fico联合分融合模型_v2_20250902142052.pkl')
print(fico_model_v2.feature_name())
df_evalue['id5_off_fico_cpd30_2508'] = fico_model_v2.predict(df_evalue[fico_model_v2.feature_name()], num_iteration=fico_model_v2.best_iteration)
df_evalue['id5_off_fico_cpd30_2508'].head()


# In[ ]:


fico_modelv2_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v2/fico联合分融合模型v2_v1_20250912151712.pkl')
print(fico_modelv2_v1.feature_name())
df_evalue['id5_off_fico_v2_cpd30_2508'] = fico_modelv2_v1.predict(df_evalue[fico_modelv2_v1.feature_name()], num_iteration=fico_modelv2_v1.best_iteration)
df_evalue['id5_off_fico_v2_cpd30_2508'].head()


# In[ ]:


# ./result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl


# In[ ]:


fico_modelv3_v1= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v1_20250915110118.pkl')
print(fico_modelv3_v1.feature_name())
df_evalue['id5_off_fico_v3_1_cpd30_2508'] = fico_modelv3_v1.predict(df_evalue[fico_modelv3_v1.feature_name()], num_iteration=fico_modelv3_v1.best_iteration)
df_evalue['id5_off_fico_v3_1_cpd30_2508'].head()


# In[ ]:


fico_modelv3_v2= load_model_from_pkl(filepath + 'result_fico联合分融合模型v3/fico联合分融合模型v3_v2_20250915112542.pkl')
print(fico_modelv3_v2.feature_name())
df_evalue['id5_off_fico_v3_2_cpd30_2508'] = fico_modelv3_v2.predict(df_evalue[fico_modelv3_v2.feature_name()], num_iteration=fico_modelv3_v2.best_iteration)
df_evalue['id5_off_fico_v3_2_cpd30_2508'].head()


# In[ ]:


# 方法2：使用向量化操作（更高效）
a_has_data = df_evalue['fico_model'].notna()  # 等价于 ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 
c_has_data = df_evalue['tianchuang_score' umeng_sdk_score].notna()

df_evalue['友盟fico是否缺失'] = np.where(
    a_has_data & b_has_data & c_has_data,  # 都有数据（都不是NaN）
    '1_都不缺失',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # 都无数据（都是NaN）
        '2_都有缺失',
        None  # 其他情况（一个有数据一个无数据）
    )
)


# In[ ]:


df_evalue['友盟fico是否缺失'].value_counts(dropna=False)


# ### 5.3.2 效果对比

# In[285]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    data = pd.Series({'KS': ks_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
#         df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
#         df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)      
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[286]:


df_evalue = df_evalue.dropna(subset=['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'],how='all')
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)
df_evalue.head()


# In[287]:


map_dict = {'3_oot2':'20250310-20250415','3_oot1':'20250301-20250309','2_test':'20250215-20250228','1_train':'20250101-20250214'}


# In[288]:


def rename_models(original_list):
    # 定义替换映射（按长度降序排列，避免短名称干扰长名称，比如 umeng 被 fico_model 包含）
    replacements = {
        'umeng_sdk_score': '友盟',
        'fico_model': 'fico',
        'haina_model': '海纳',
        'tianchuang_score': '天创'
    }
    
    result = []
    for name in original_list:
        new_name = name
        for old, new in replacements.items():
            new_name = new_name.replace(old, new)
        result.append(new_name)
    
    return result


# In[209]:



with pd.ExcelWriter('授信场景m4d30_友盟_海纳_fico_天创联合建模增益评估250922_v1.xlsx') as writer:

    for i, triplet in enumerate(triplets):
        score_1, score_2, score_3 = triplet
        score_list = [score_1] + score_2 + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
        print(len(score_list),score_list)
        
        target_list = ['target_mob4dpd30_1'] 
        labels_models_dict = {target: score_list for target in target_list}
        print(labels_models_dict)

        print(df_evalue.shape[0])
        tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
        tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_3].notna().any(axis=1),:]
        print(tmp_df_evalue.shape[0])
        # 整体客群
        groupkeys2 = ['channel_types', 'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

        df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        # 分客群
        groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
        df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
        df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

        groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
        df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
        df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

        groupkeys1 = [ 'customer_tags', 'data_set']
        df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
        df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

        df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
        
        # 合并数据
        df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
        df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)
        score_2_new = [f"KS_{col}" for col in score_2]
        df_auc_ks_all['KS最大值'] = df_auc_ks_all[score_2_new].max(axis=1)
        ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
        df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
        df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)
        original_list = [score_1]
        new_original_list = rename_models(original_list)
        score_2_new = new_original_list[0]
        df_auc_ks_all.to_excel(writer, sheet_name=f'{score_2_new}')
        
        gc.collect()


# In[210]:



with pd.ExcelWriter(result_path + '授信场景m4d30_固定参数_增益评估250922_v1.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[211]:



with pd.ExcelWriter(result_path + '授信场景m4d30_固定参数_增益评估250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'fico_model', 'tianchuang_score', 'haina_model'] + vars_combiner_list + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[212]:



with pd.ExcelWriter(result_path + '授信场景m4d30_无fico_固定参数_增益评估250922_v1.xlsx') as writer:
    score_list = ['umeng_sdk_score', 'tianchuang_score', 'haina_model'] + vars_combiner_list + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[213]:



with pd.ExcelWriter(result_path + '授信场景m4d30_无fico_固定参数_增益评估250922_v2.xlsx') as writer:

    score_list = ['umeng_sdk_score', 'tianchuang_score', 'haina_model'] + vars_combiner_list + varsname_base + ['id5_off_cpd30_2508','tianchuang_id5@_main','fico_model@tianchuang_score_id@_main']
    print(len(score_list),score_list)

    target_list = ['target_mob4dpd30_1'] 
    labels_models_dict = {target: score_list for target in target_list}
    print(labels_models_dict)

    print(df_evalue.shape[0])
    tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

    print(tmp_df_evalue.shape[0])
    # 整体客群
    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    # 分客群
    groupkeys2 = ['channel_types', 'customer_tags',  'data_set']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'customer_tags',  'data_set']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = [ 'customer_tags', 'data_set']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_4 = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)

    # 合并数据
    df_ksauc_all_1.insert(1, 'customer_tags', value='全部客群', allow_duplicates=False)
    df_auc_ks_all =  pd.concat([df_ksauc_all_1, df_ksauc_all_4],axis=0, ignore_index=True)

    ks_columns = [col for col in df_auc_ks_all.columns if 'KS' in col]
    df_auc_ks_all[ks_columns] = df_auc_ks_all[ks_columns].applymap(float_format)
    df_auc_ks_all.insert(1,'time_windowns',value=df_auc_ks_all['data_set'].map(map_dict),allow_duplicates=False)

    df_auc_ks_all.to_excel(writer, index=False)

    gc.collect()


# In[ ]:


# 方法2：使用向量化操作（更高效）
a_has_data = df_evalue['haina_model'].notna()  # 等价于 ~df['col_a'].isna()
b_has_data = df_evalue['umeng_sdk_score'].notna() 

df_evalue['友盟海纳是否缺失'] = np.where(
    a_has_data & b_has_data & c_has_data,  # 都有数据（都不是NaN）
    '1_都不缺失',
    np.where(
        ~a_has_data & ~b_has_data & ~c_has_data,  # 都无数据（都是NaN）
        '2_都有缺失',
        None  # 其他情况（一个有数据一个无数据）
    )
)


# In[290]:


score_list = ['y_pred_v1','y_pred_v2','id5_off_cpd30_2508','umeng_sdk_score', 'haina_model']
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)

df_ksauc_all1.insert(0, 'time_windowns', value=df_ksauc_all1['data_set'].map(map_dict), allow_duplicates=False)
df_ksauc_all1


# In[292]:


score_list = ['y_pred_v1','y_pred_v2','id5_off_cpd30_2508','umeng_sdk_score', 'haina_model']
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]


groupkeys2 = ['customer_tags','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['customer_tags','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['customer_tags','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(1, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all3.insert(1, 'time_windowns', value=df_ksauc_all3['data_set'].map(map_dict), allow_duplicates=False)

df_ksauc_all3


# In[293]:



# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='整体')
    df_ksauc_all2.to_excel(writer, sheet_name='整体_有无数据')
    df_ksauc_all3.to_excel(writer, sheet_name='分客群')
    df_ksauc_all4.to_excel(writer, sheet_name='分客群_有无数据')    
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[ ]:


score_list = ['id5_off_fico_cpd30_2509','fico_model','id5_off_m3d30_2507']
print(len(score_list))
print(score_list)

target_list = ['target_cpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['flag','fico数据是否缺失','channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['flag','fico数据是否缺失','channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['flag','fico数据是否缺失','data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(2, 'channel', value='全渠道', allow_duplicates=False)

tmp = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
tmp.insert(1, 'time_windowns', value=tmp['data_set'].map(map_dict), allow_duplicates=False)
tmp


# In[ ]:


tmp.query("fico数据是否缺失=='1_不缺失' & flag=='1_新客' & channel=='金科渠道'").reset_index(drop=True)


# # 6. 评分分布

# In[ ]:





# In[294]:


df_sample['data_set'].value_counts()


# In[304]:


score = 'y_pred_v2'


# In[305]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins2'] = c.transform(df_sample[score], labels=True)


# In[306]:


df_sample['score_bins2'].head()


# In[307]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[308]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[309]:


df_psi_matrix_set = df_psi_matrix.loc['1_train',:]
print(df_psi_matrix_set)


# In[ ]:


df_psi_matrix_month = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix_month)


# In[310]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins2'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[311]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[303]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[312]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[314]:


df_sample.to_csv(result_path + '手机号md5授信实时融合模型v2_report.csv',index=False)
print(result_path + '手机号md5授信实时融合模型v2_report.csv')


# # 7.模型部署和回溯

# In[ ]:


df_sample.columns


# In[ ]:


df_back = df_sample[['order_no', 'id_no_des', 'user_id', 'channel_id','apply_date', 'y_pred_v2']]
df_back.rename(columns={'y_pred_v2':'score'},inplace=True)
df_back['third_data_source']= 'umeng_sdk' 
df_back = df_back[['order_no','id_no_des','user_id','channel_id','apply_date','third_data_source','score']]
df_back.to_csv('umeng_sdk_score.csv',index=False)


# In[ ]:


df_back.info(show_counts=True)


# In[ ]:


df_back.to_csv('umeng_sdk_score.csv',index=False,sep='|',header=None)


# In[ ]:


feature_importance(lgb_model)


# In[ ]:



from hl_data_mc_upload_v2_0 import DataUploadMc

upload = DataUploadMc(username='liaoxilin',
                      password='j02vYCxx',
                      env='prd')


upload.upload_data_to_table(    
        ## 字段名称
        fields='{"id_no_des":"string","user_id":"bigint","order_no":"string","channel_id":"bigint","apply_date":"string","score":"double"}',
        ## 本地文件，注意：只写文件名即可，参数是 list 类型
        csv_filename_list=['天创模型分数v2.csv'],
        ## 本地文件路径，注意：需要本地的绝对路径
        input_path='/data/home/liaoxilin/联合建模/友盟sdk&百行多头/',                    
        ## 上传的数据库
        database='znzz_fintech_ads',        
        ## 上传的表名
        table_name='lxl_model_',
        # 分区字段
        partition='ds=lxl_tianchuang,dt=2025-07-30',
        # 自定义分隔符
        delimiter='|'
       ) 




#==============================================================================
# File: 批量处理离线变量.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
from jinja2 import Template
import os


# # 配置参数

# In[2]:


# 配置参数
BATCH_SIZE = 1500
OUTPUT_DIR = "./generated_sql"
MAIN_SQL_FILE = "main_table.sql"  # 你可以把主表 SQL 写入文件中备用

os.makedirs(OUTPUT_DIR, exist_ok=True)


# # 1. 核心代码

# In[260]:


# 加载变量清单
variables_df = pd.read_excel("variables.xlsx")
variables_df = variables_df[variables_df['keep'] == 1]
variables_df = variables_df.sort_values(by='table_name')  # 均匀分布变量

# 分批处理
batches = [variables_df[i:i+BATCH_SIZE] for i in range(0, len(variables_df), BATCH_SIZE)]


# In[791]:


# Jinja2 模板：特征变量部分
feature_template = Template("""
left join 
(
select 
    t.id_no_des,
    {% for var in variables -%}
        t.{{ var }}{% if not loop.last %}, {% endif %}
    {%- endfor %},
    ROW_NUMBER() OVER (PARTITION BY id_no_des ORDER BY dt DESC) AS rk 
from {{ full_table_name }} as t 
where dt <= date_sub('$[last_day(yyyy-MM-dd)]', 1) 
  and dt >= date_sub('$[last_day(yyyy-MM-dd)]', 100)
) as {{ alias_name }} on t.id_no_des = {{ alias_name }}.id_no_des and {{ alias_name }}.rk = 1
""")


# In[792]:




# 主表模板占位符
MAIN_TEMPLATE = """
WITH main_base AS (
    -- 主表 SQL 放在这里
),
features AS (
    -- 所有 left join 的特征表放在这里
)
SELECT *
FROM main_base
LEFT JOIN features USING (id_no_des)
"""


# In[ ]:


# 读取主表 SQL（假设你已保存为文本文件）
with open(MAIN_SQL_FILE, 'r') as f:
    main_sql = f.read()


# In[793]:





for idx, batch in enumerate(batches):
    print(f"\nProcessing Batch {idx + 1} / {len(batches)}")
    
    # 按 table_name 分组，得到 { 'tableA': ['var1', 'var2'], ... }
    grouped = batch.groupby('table_name')['variable_name'].apply(list).to_dict()
    
    feature_joins = []
    alias_counter = 1
    
    for table_name, vars_list in grouped.items():
        alias_name = f"t{alias_counter}"
        alias_counter += 1
        
        # 构造每个特征表的 SELECT 字段列表
        sql_part = feature_template.render(
            full_table_name=table_name,
            variables=vars_list,
            alias_name=alias_name
        )
        
        feature_joins.append(sql_part)
    
    # 合并所有 left join 子句
    all_features_sql = "\n".join(feature_joins)
    
    # 将主表和特征 join 合并
    final_sql = f"""
    WITH main_base AS (
        {main_sql}
    ),
    features AS (
        SELECT * FROM main_base
        {all_features_sql}
    )
    SELECT * FROM features
    """
    # 替换日期变量（如果需要）
    final_sql = final_sql.replace("$[last_day(yyyy-MM-dd)]", "2025-06-30")
    
    # 保存到文件
    output_file = os.path.join(OUTPUT_DIR, f"batch_{idx + 1}.sql")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_sql)
    
    print(f"✅ 已生成 SQL 文件: {output_file}")


# In[794]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique(), 
      df_sample_['id_no_des'].nunique(), df_sample_['order_no_auth'].nunique())


# In[795]:


df_sample_.select_dtypes('object').columns


# In[797]:


drop_columns = ['fk01s1odc','fk02s1odc','fk03s1odc','fk06s1odc','fk12s1odc','fk01s0odc','fk02s0odc','fk03s0odc','fk06s0odc','fk12s0odc','fk01s1odcr','fk02s1odcr','fk03s1odcr','fk06s1odcr','fk12s1odcr','fk01s0odcr','fk02s0odcr','fk03s0odcr','fk06s0odcr','fk12s0odcr','fk01s1lms','fk02s1lms','fk03s1lms','fk06s1lms','fk12s1lms','fk01s1lmm','fk02s1lmm','fk03s1lmm','fk06s1lmm','fk12s1lmm','fk01s1lml','fk02s1lml','fk03s1lml','fk06s1lml','fk12s1lml','fk01s0lmm','fk02s0lmm','fk03s0lmm','fk06s0lmm','fk12s0lmm','fk01s1lmsr','fk02s1lmsr','fk03s1lmsr','fk06s1lmsr','fk12s1lmsr','fk01s1lmmr','fk02s1lmmr','fk03s1lmmr','fk06s1lmmr','fk12s1lmmr','fk01s0lmsr','fk02s0lmsr','fk03s0lmsr','fk06s0lmsr','fk12s0lmsr','fk01s0lmmr','fk02s0lmmr','fk03s0lmmr','fk06s0lmmr','fk12s0lmmr','fk01chlc','fk02chlc','fk03chlc','fk06chlc','fk12chlc','fk01s1chlc','fk02s1chlc','fk03s1chlc','fk06s1chlc','fk12s1chlc','fk01s0chlc','fk02s0chlc','fk03s0chlc','fk06s0chlc','fk12s0chlc','fk01chlcfk02chlcr','fk01chlcfk03chlcr','fk03chlcfk06chlcr','fk03chlcfk12chlcr','fk06chlcfk12chlcr','fk01s0chlcfk02s0chlcr','fk01s0chlcfk03s0chlcr','fk03s0chlcfk06s0chlcr','fk03s0chlcfk12s0chlcr','fk06s0chlcfk12s0chlcr','od01lt1c','od02lt1c','od03lt1c','od06lt1c','od12lt1c','od01lt2c','od02lt2c','od03lt2c','od06lt2c','od12lt2c','od01lt1cr','od02lt1cr','od03lt1cr','od06lt1cr','od12lt1cr','od01lt2cr','od02lt2cr','od03lt2cr','od06lt2cr','od12lt2cr','od01cod02cr','od01cod03cr','od03cod06cr','od03cod12cr','od06cod12cr','od01lmm','od02lmm','od03lmm','od06lmm','od12lmm','od01lt2lms','od02lt2lms','od03lt2lms','od06lt2lms','od12lt2lms','od01lt2lmm','od02lt2lmm','od03lt2lmm','od06lt2lmm','od12lt2lmm','od01lt1lmsr','od02lt1lmsr','od03lt1lmsr','od06lt1lmsr','od12lt1lmsr','od01lt1lmmr','od02lt1lmmr','od03lt1lmmr','od06lt1lmmr','od12lt1lmmr','od01lt2lmsr','od02lt2lmsr','od03lt2lmsr','od06lt2lmsr','od12lt2lmsr','od01lt2lmmr','od02lt2lmmr','od03lt2lmmr','od06lt2lmmr','od12lt2lmmr','od01chlc','od02chlc','od03chlc','od06chlc','od12chlc','ovd1stdn','p1ovdm15dc','p1ovdm30dc','p1ovds0c','pnovdm15dc','pnovdm30dc','pnovds0c','r01rpdovdfs','r02rpdovdfs','r03rpdovdfs','r06rpdovdfs','r12rpdovdfs','r01rpdovdfsrpdpalsr','r02rpdovdfsrpdpalsr','r03rpdovdfsrpdpalsr','r06rpdovdfsrpdpalsr','r12rpdovdfsrpdpalsr','r01rpdintsrpdpalsr','r02rpdintsrpdpalsr','r03rpdintsrpdpalsr','r06rpdintsrpdpalsr','r12rpdintsrpdpalsr','r01rpdpalsr','r02rpdpalsr','r03rpdpalsr','r06rpdpalsr','r12rpdpalsr','r01p1ovdodc','r02p1ovdodc','r03p1ovdodc','r06p1ovdodc','r12p1ovdodc','r01p1ovdodcr','r02p1ovdodcr','r03p1ovdodcr','r06p1ovdodcr','r12p1ovdodcr','r01p1epyodc','r02p1epyodc','r03p1epyodc','r06p1epyodc','r12p1epyodc','r01p1epyodcr','r02p1epyodcr','r03p1epyodcr','r06p1epyodcr','r12p1epyodcr','r01epys1c','r02epys1c','r03epys1c','r06epys1c','r12epys1c','r01epys1cr','r02epys1cr','r03epys1cr','r06epys1cr','r12epys1cr','r01p1ovdc','r02p1ovdc','r03p1ovdc','r06p1ovdc','r12p1ovdc','r01p1epyc','r02p1epyc','r03p1epyc','r06p1epyc','r12p1epyc','r01ovdds','r02ovdds','r03ovdds','r06ovdds','r12ovdds','r01ovddm','r02ovddm','r03ovddm','r06ovddm','r12ovddm','r01pdovddm','r02pdovddm','r03pdovddm','r06pdovddm','r12pdovddm']
varsname = [col for col in df_sample_.columns.to_list()[15:] if col not in drop_columns]

print(varsname[1:10],varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[798]:


for col in varsname:
    if df_sample_[col].dtype=='object':
        print(col)
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[799]:


df_sample_['order_no'].value_counts().head()


# In[802]:


df_sample_.drop_duplicates(inplace=True)


# In[803]:


df_sample_.shape


# In[804]:


df_sample_.to_csv(r'df_sample_.csv',index=False)


# In[270]:


df_sample = df_sample_[df_sample_.columns.to_list()[:15]+varsname]
df_sample.info()
df_sample.head()


# In[272]:


del df_sample_0907,df_sample_0907_dict
gc.collect()


# In[273]:


for col in varsname:
    if df_sample[col].dtype=='object':
        print(col)
        df_sample[col] = pd.to_numeric(df_sample[col])


# In[274]:


# df_sample = pd.read_csv(r'behave_model_20241112.csv')
# df_sample.info()
# df_sample.head()
print(len(drop_columns))


# In[275]:


print(df_sample['lending_time'].min(), df_sample['lending_time'].max())


# In[276]:


df_sample.select_dtypes('object').columns


# In[435]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-07-31'").index, 'lending_month']='2024-07(1_train)'
df_sample.loc[df_sample.query("lending_time>='2024-08-01' & lending_time<='2024-08-20'").index, 'lending_month']='2024-08(1_train)'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'lending_month']='2024-08(3_oot)'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'lending_month']='2024-09(3_oot)'


# In[330]:


df_sample.loc[df_sample.query("lending_time>='2024-07-21' & lending_time<='2024-07-31'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-08-01' & lending_time<='2024-08-20'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'data_set']='3_oot'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'data_set']='3_oot'


# In[ ]:





# In[279]:


target = 'fpd30'


# In[280]:


df_sample[[target]+varsname].info()
df_sample[[target]+varsname].head()


# In[281]:


df_sample.to_csv(r'behave_model_fpd30_data_241115.csv',index=False)


# # 1. 样本概况

# In[282]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
    total_summary = df[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).to_frame().T
    total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
    result = pd.concat([grouped, total_summary], ignore_index=True)
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[283]:


print(df_sample[target].value_counts())


# In[284]:


df_target_summary_month = get_target_summary(df_sample, target, 'lending_month')
print(df_target_summary_month)


# In[285]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[286]:


df_target_summary = pd.concat([df_target_summary_month, df_target_summary_set], axis=0, ignore_index=True)
df_target_summary


# In[287]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./模型开发/行为模型/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    df_target_summary.to_excel(writer, sheet_name='df_target_summary')
    
print(f"数据存储完成: {timestamp}")


# # 2.数据探索性分析

# In[288]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])


# In[289]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[290]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 提取月份信息
    # df[timestamp_col] = pd.to_datetime(df[timestamp_col])
    # df['month'] = df[timestamp_col].dt.to_period('M')

    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T
#     missing_rates['mean'] = missing_rates.mean(axis=1)
#     missing_rates['std'] = missing_rates.std(axis=1)
#     missing_rates['cv'] = missing_rates['std'] / missing_rates['mean']

    return missing_rates


# In[291]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'lending_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss.head())


# In[292]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[293]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[294]:


# timestamp = datetime.now().strftime('%Y%m%d')
# directory = f'D:/联合建模/百行支付/RES-明东华第二批20W/result_user_fpd30_{timestamp}'
# if not os.path.exists(directory):
#         os.makedirs(directory, exist_ok=True)
# result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss.to_excel(writer, sheet_name='df_miss')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[766]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
# df_miss_drop = df_miss_set.drop(columns=['mean','std','cv'])
to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<0.02].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[767]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")


# ## 3.2 基于相关性删除变量
# 

# In[768]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]], target=target, 
                                                empty=0.90, iv=0.02, corr=0.70, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[769]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)), to_drop2)


# In[770]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[300]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = combiner.transform(df[cols+[target, month_col]], labels=True)
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            # total = data.groupby(col)[target].count()
            # bad = data.groupby(col)[target].sum()
            # regroup = pd.concat([total, bad], axis=1)
            # regroup.columns = ['total', 'bad']
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
            regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()
    result['mean'] = result.mean(axis=1)
    result['std'] = result.std(axis=1)
    result['cv'] = result['std'] / result['mean']       
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars:
            data = df[df[group_col] == var]
            # total = data.groupby(col)[target].count()
            # bad = data.groupby(col)[target].sum()
            # regroup = pd.concat([total, bad], axis=1)
            # regroup.columns = ['total', 'bad']
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum()
            regroup['good_pct'] = regroup['good']/regroup['good'].sum()
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result

# 调用函数绘制堆叠柱状图
def plot_stacked_bar(df, var, month_col, bins, values, filename=None):
    # 假设df是一个DataFrame，包含您的数据
    # month_col 是月份列
    # bins 是分箱列
    # values 是要绘制的值列
    # 创建一个透视表
    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values, fill_value=0)

    # 初始化图形
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # 获取所有的分箱类别
    bins_list = pivot_df.columns.tolist()
    
    # 计算每个柱子的宽度
    bar_width = 0.8
    
    # 计算x轴上的位置
    x_pos = range(len(pivot_df.index))
    
    # 初始化底部
    bottom = [0] * len(x_pos)
    
    # 遍历每个分箱，并绘制柱状图
    for bin in bins_list:
        # 使用fillna(0)处理NaN值
        pivot_df[bin] = pivot_df[bin].fillna(0)
        
        ax.bar(x_pos,
               pivot_df[bin],
               width=bar_width,
               label=bin,
               bottom=bottom,
               align='center',
               alpha=0.8)
        
        # 更新底部的值
        bottom = [b + v for b, v in zip(bottom, pivot_df[bin])]
    
    # 设置图形属性
    ax.set_title(f'{values}——{var}')
    # ax.set_xlabel('Month')
    ax.set_ylabel(f'{values}')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(pivot_df.index)
    ax.legend()

    plt.grid(axis='y', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show() 
    
# 调用函数绘制时间序列图
def draw_time_series(df, var, month_col, bins, values, filename=None):

    pivot_df = df.pivot_table(index=month_col, columns=bins, values=values)
    
    # 绘制时间序列折线图
    plt.figure(figsize=(14, 7))
    for bin in pivot_df.columns:
        i = list(pivot_df.index)
        j = list(pivot_df[bin])
        plt.plot(i, j, label=bin, marker='o')

    plt.title(f'{values}——{var}')
    # plt.xlabel('Month')
    plt.ylabel(f'{values}')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    plt.show()


def draw_line_bar(df, col, bin, var1, var2, var3, filename=None):
    # 处理数据
    bins = df[bin]
    varsname = df[col].unique()[0]
    values1 = list(df[var1].fillna(0))
    values2 = list(df[var2].fillna(0))
    values3 = df[var3].unique()[0]

    # 创建图形和主轴
    fig, ax1 = plt.subplots()

    # 主纵轴 - 柱状图 (分箱占比)
    color = 'tab:blue'
    ax1.set_xlabel(f'{varsname}')
    ax1.set_ylabel(f'{var1}', color=color)
    bars = ax1.bar(bins, values1, color=color)
    # 在柱状图上添加数值标签
    for bar in bars:
        yval = bar.get_height()
        # va='bottom' to place label below the bar
        ax1.text(bar.get_x() + bar.get_width()/2.0, yval, f'{round(yval, 3)}', ha='center', va='top') 
    
    ax1.tick_params(axis='y', labelcolor=color, rotation=45)

    # 副纵轴 - 折线图 (坏占比)
    ax2 = ax1.twinx()  # 创建第二个纵坐标轴
    color = 'tab:red'
    ax2.set_ylabel(f'{var2}', color=color)  # 设置标签颜色
    _ = ax2.plot(values2, color=color, marker='o')  # 绘制折线图
    # 在折线图上添加数值标签
    for xtick, txt in zip(ax1.get_xticks(), values2):
        ax2.text(xtick, txt, f'{round(txt, 3)}', ha='center', va='top', rotation=45)
    ax2.tick_params(axis='y', labelcolor=color)

    # 获取x轴的刻度位置
    xtick_positions = range(len(bins))
    # 设置X轴标签自动调整
    ax1.set_xticks(xtick_positions)
    ax1.set_xticklabels(bins, rotation=45, ha='right')
    
    # 设置标题和网格
    ax1.set_title(f'{varsname}')
    ax1.grid(False)

    # 在左上角添加文本
    ax1.text(0.05, 0.95, f'IV value:{round(values3,3)}', transform=ax1.transAxes, verticalalignment='top')
    # 调整布局
    # fig.tight_layout()
    # 如果提供了文件名，则保存图表
    if filename:
        plt.savefig(filename, dpi=300)
    # 显示图表
    plt.show()
    


# In[301]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=6, min_samples = 0.05, empty_separate=True) 


# In[680]:


# 计算psi
# df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07(1_trian)'"), varsname_v2, \
#                                    'lending_month', combiner, return_frame = False)
# print(df_psi_by_month.head(10))

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print(df_psi_by_set.head(10))


# In[303]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_sample, varsname_v2, target, 'lending_month', combiner)
print(df_iv_by_month.head(10))

df_iv_by_set = cal_iv_by_month(df_sample, varsname_v2, target, 'data_set', combiner)
print(df_iv_by_set.head(10)) 


# In[304]:


df_iv_by_month.to_csv(r'df_iv_by_month_241116.csv')


# In[305]:


df_bins = combiner.transform(df_sample[varsname_v2+[target, 'lending_month','data_set']], labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[306]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'lending_month')[selected_cols] 
print(df_group_month.head() )

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print(df_group_set.head() )


# In[ ]:


# plot_stacked_bar(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total'"), 
#                  'br_fpd_score', 'groupvars', 'bins', 'total_pct', filename=None)


# In[ ]:


# draw_time_series(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total'"),
#                  'br_fpd_score', 'groupvars', 'bins', 'bad_rate', filename=None)


# In[ ]:


# draw_line_bar(df_group_month.query("varsname=='rec1_top_inst' & bins!='Total' & groupvars=='1_train'"), 
#               'varsname', 'bins', 'total_pct', 'bad_rate', 'iv', filename=None)


# In[307]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[308]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print(df_total_bad_set.head() )
print(pivot_df_iv_set.head() )


# ### 删除不稳定特征

# In[309]:


drop_by_psi = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index)
print("drop_by_psi: ", len(drop_by_psi))

df_iv_by_month.drop(columns=['mean', 'std', 'cv'], inplace=True)
drop_by_iv1 = list(df_iv_by_month[df_iv_by_month<0.01].dropna(how='all').index)
drop_by_iv2 = ['accuagel','age','ad06odpr','ad06dc_f']
drop_by_iv = drop_by_iv1 + drop_by_iv2
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[771]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## Y标签相关性删除

# In[311]:


# 计算相关性
exclude = [target, 'lending_month', 'data_set']

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins[varsname_v3+exclude], df_bins[target], exclude=exclude)
print(df_sample_woe.shape)


# In[312]:


df_sample_woe.head()


# In[313]:


def find_high_correlation_pairs(df, iv_series, method='kendall', threshold=0.85):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
    :param threshold: 相关系数的阈值，默认为0.85
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.corr(method=method)
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 > iv2:
            to_remove.add(var2)
        else:
            to_remove.add(var1)
    
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[314]:


# 调用函数
df_high_corr, to_drop4 = find_high_correlation_pairs(df_sample_woe, df_iv['iv'],                                                     method='kendall', threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[315]:


df_high_corr.head(5)


# In[772]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")


# In[317]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[318]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe, 'lending_month',
                                                                    varsname_v4, target,method='pointbiserialr')

# 查看前几行
print(df_corr_vars_target.head(10))
print(df_pvalue_vars_target.head(10))


# In[319]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[773]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[321]:


print(varsname_v5)


# In[322]:


# timestamp = datetime.now().strftime('%Y%m%d')
# directory = f'D:/联合建模/百行支付/RES-明东华第二批20W/result_user_fpd30_{timestamp}'
# if not os.path.exists(directory):
#         os.makedirs(directory, exist_ok=True)
# result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
print(f"数据存储完成时间：{timestamp}！")        


# In[107]:


# result_path = 'D:/模型开发/授信模型开发240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}stacked_bar_totalpct_{plot_var}_{timestamp}.png'

# plot_stacked_bar(df_group.query("varsname=='br_fpd_score' & bins!='Total'"), 
#                  plot_var, 'groupvars', 'bins', 'total_pct', filename=filename)


# In[108]:


# result_path = 'D:/模型开发/授信模型开发240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}line_char_badrate_{plot_var}_{timestamp}.png'

# draw_time_series( df_group.query("varsname=='br_fpd_score' & bins!='Total'"),
#                  plot_var, 'groupvars', 'bins', 'bad_rate', filename=filename)


# In[109]:


# result_path = 'D:/模型开发/授信模型开发240828/result/'
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# plot_var = 'br_fpd_score'
# filename = f'{result_path}line_chat_iv_{plot_var}_{timestamp}.png'

# draw_time_series( df_group.query("varsname=='br_fpd_score' & bins=='Total'"), 
#                  plot_var, 'groupvars', None, 'iv', filename=filename)


# In[323]:


gc.collect()


# # 5.模型训练

# ## 5.1 模型训练及对比

# In[ ]:


# 数据抽样
def resample_negative_samples(X_train, y_train, date_column, amplification_factor, total_samples=200000):
   
    # 计算每个月的正负样本比例
    df = pd.concat([X_train, y_train], axis=1)
    df['flag'] = y_train
    df[date_column] = pd.to_datetime(df[date_column])
    df['month'] = df[date_column].dt.to_period('M').astype(str) 
    df_month_ratios = df.pivot_table(index='month', values='flag', aggfunc='mean')
    month_ratios = df_month_ratios['flag'].to_dict()
    
    # 分离正负样本
    pos_samples = df[df['flag'] == 1]
    neg_samples = df[df['flag'] == 0]
    
    # 计算正样本总数
    pos_count = len(pos_samples)

    # 计算负样本应该有的总数
    neg_count = total_samples - pos_count
       
    # 计算每个时间窗口需要的目标负样本数量
    target_counts = {}
    for month, ratio in month_ratios.items():
        # 获取该月份的正样本数量
        pos_count = len(pos_samples[pos_samples['month'] == month])
        # 计算目标负样本数量
        target_neg_count = pos_count / (amplification_factor * ratio / (1 - ratio))
        target_counts[month] = int(target_neg_count)
    
    # 根据总负样本数量重新分配各月的负样本数量
    total_target_neg_count = sum(target_counts.values())
    adjusted_counts = {}
    for month, count in target_counts.items():
        adjusted_counts[month] = int((count / total_target_neg_count) * neg_count)
    
    # 初始化一个空DataFrame来存储重采样的结果
    resampled_neg_samples = pd.DataFrame()

    # 对每个时间窗口的负样本进行重采样
    for month, target_count in adjusted_counts.items():
        # 获取当前时间窗口的负样本
        current_neg_samples = neg_samples[neg_samples['month'] == month]
        # 如果当前时间窗口的负样本数量少于目标数量，则直接添加到结果中
        if len(current_neg_samples) <= target_count:
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples])
        else:
            # 否则，从当前时间窗口的负样本中随机抽取目标数量的样本
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples.sample(n=target_count)])

    # 合并正样本和重采样后的负样本
    X_resampled = pd.concat([pos_samples, resampled_neg_samples])[X_train.columns]
    y_resampled = y_train.loc[X_resampled.index]

    return (X_resampled, y_resampled)


# In[81]:


# X_train_ = df_sample.query("data_set=='1_train'").drop(columns=[target])
# y_train = df_sample.query("data_set=='1_train'")[target]
# X_test_ = df_sample.query("data_set=='2_test'").drop(columns=[target])
# y_test = df_sample.query("data_set=='2_test'")[target]
# print(X_train_.shape, X_test_.shape)


# In[78]:


# X_resampled, y_resampled = resample_negative_samples(X_train, y_train, 'apply_date', 20)
# print(X_train.shape, y_train.shape)
# print(X_resampled.shape, y_resampled.shape)
# print(y_resampled.value_counts())


# In[82]:


# X_resampled = X_resampled[cols]
# X_train = X_train_[varsname_v5]
# X_test = X_test_[varsname_v5]
# print(X_train.shape, X_test.shape)


# In[324]:


# 2 定义超参空间
# hp.quniform("参数名称",下界,上界,步长)-适用于离散均匀分布的浮点点数
# hp.uniform("参数名称",下界, 下界)-适用于连续随机分布的浮点数
# hp.randint("参数名称",上界)-适用于[0,上界)的整数,区间为左闭右开
# hp.choice("参数名称",["字符串1","字符串2",...])-适用于字符串类型,最优参数由索引表示
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# 可以根据需要，注释掉偏后的一些不太重要的超参

spaces = {
          # general parameters
#           "learning_rate":hp.loguniform("learning_rate",np.log(0.001), np.log(0.2)),
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,7,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
#           "feature_fraction":hp.uniform("feature_fraction",0.7,1.0),
#           "bagging_fraction":hp.uniform("bagging_fraction",0.7,1.0),
          "feature_fraction":0.9,
          "bagging_fraction":0.7,
#           "min_gain_to_split":10,
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.1, 1.0),
          "lambda_l1": 0,
#           "lambda_l1": hp.randint("lambda_l1", 1),
#           "lambda_l2": hp.uniform("lambda_l2", 300, 1000),
          "lambda_l2": 300,
#           "early_stopping_rounds": hp.quniform("early_stopping_rounds", 50, 60, 10)
          "early_stopping_rounds": 30
          }


# In[325]:


# 3，执行超参搜索
# 有了目标函数和参数空间,接下来要进行优化,需要了解以下参数:
# fmin:自定义使用的代理模型(参数algo),hyperopt支持如下搜索算法：
#       随机搜索(hyperopt.rand.suggest)
#       模拟退火(hyperopt.anneal.suggest)
#       TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）
# partial:修改算法涉及到的具体参数,包括模型具体使用了多少少个初始观测值(参数n_start_jobs),
#         以及在计算采集函数值时究竟考虑多少个样本(参数n_EI_candidates)
# trials:记录整个迭代过程,从hyperopt库中导入的方法Trials(),优化完成之后,
#        可以从保存好的trials中查看损失、参数等各种中间信息
# early_stop_fn:提前停止参数,从hyperopt库导入的方法no_progresss_loss(),可以输入具体的数字n,
#               表示当损失连续n次没有下降时,让算法提前停止
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=20):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test, num_iteration=clf_obj.best_iteration))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    return (best_params, trials)


# In[774]:


# 4，获取最优参数，调参过程
## 确定一个较高的学习率
## 对决策树基本参数调参
## 正则化参数调参
## 降低学习率



X_train, X_test, y_train, y_test = train_test_split(df_sample[df_sample['data_set']=='1_train'][varsname_v5],
                                                    df_sample[df_sample['data_set']=='1_train'][target],
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=df_sample[df_sample['data_set']=='1_train'][target])
df_sample.loc[X_train.index, 'data_type']='1_train'
df_sample.loc[X_test.index, 'data_type']='2_test'
df_sample.loc[df_sample.query("lending_time>='2024-08-21' & lending_time<='2024-08-31'").index, 'data_type']='3_oot'
df_sample.loc[df_sample.query("lending_time>='2024-09-01' & lending_time<='2024-09-12'").index, 'data_type']='3_oot'



best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=5)


# In[775]:


# 5，绘制搜索过程
losses = [x["result"]["loss"] for x in trials.trials]
minlosses = [np.min(losses[0:i+1]) for i in range(len(losses))] 
steps = range(len(losses))

fig,ax = plt.subplots(figsize=(6,3.7),dpi=144)
ax.scatter(x = steps, y = losses, alpha = 0.3)
ax.plot(steps,minlosses,color = "red",axes = ax)
plt.xlabel("step")
plt.ylabel("loss")


# In[776]:


print("最优参数best_params: ", best_params)


# In[709]:


### 添加无需调参的通用参数
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
# 调参时设置成不用调参的参数
bst_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
bst_params['bagging_fraction'] = 0.6    
bst_params['feature_fraction'] = 0.99
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = 1000
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# 调参后的参数需要变成整数型
bst_params['num_leaves'] = 59
bst_params['min_data_in_leaf'] = 115
bst_params['max_depth'] = 4
# 调参后的其他参
bst_params['min_gain_to_split'] = 0.6


# In[784]:


### 添加无需调参的通用参数
bst_params = {}
bst_params['boosting'] = 'gbdt'
bst_params['objective'] = 'binary'
bst_params['metric'] = 'auc'
bst_params['bagging_freq'] = 1
bst_params['scale_pos_weight'] = 1 
bst_params['seed'] = 1 
# 调参时设置成不用调参的参数
bst_params['learning_rate'] = spaces['learning_rate']
## 正则参数，防止过拟合
bst_params['bagging_fraction'] = spaces['bagging_fraction']    
bst_params['feature_fraction'] = spaces['feature_fraction']
bst_params['lambda_l1'] = spaces['lambda_l1']
bst_params['lambda_l2'] = 1000
bst_params['early_stopping_rounds'] = spaces['early_stopping_rounds']

# 调参后的参数需要变成整数型
bst_params['num_leaves'] = int(best_params['num_leaves'] )
bst_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'] )
bst_params['max_depth'] = int(best_params['max_depth'] )
# 调参后的其他参
bst_params['min_gain_to_split'] = best_params['min_gain_to_split']


# In[785]:


print("最优参数bst_params: ", bst_params)


# In[779]:


to_drop6 = ['au03odcau06odcr_f','r01o00pals','au01dchlm','au03chlcau06chlcr','ad06dc_p',            'au03mnchlm']+list(df_iv[df_iv['iv']<0.02].index)
varsname_v6 = [col for col in varsname_v5 if col not in to_drop6]
print(len(varsname_v6))


# In[786]:



X_train = df_sample[df_sample['data_type']=='1_train'][varsname_v5]
y_train = df_sample[df_sample['data_type']=='1_train'][target]
X_test = df_sample[df_sample['data_type']=='2_test'][varsname_v5]
y_test = df_sample[df_sample['data_type']=='2_test'][target] 
# 6，训练/保存/评估模型
# 训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test)
lgb_model = lgb.train(bst_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[789]:


# 保存模型
# joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118.pkl')
# joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118_v2.pkl')
joblib.dump(lgb_model,'./behave_model_fpd30_lgb_241118_v3.pkl')


# In[1]:


def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[2]:


lgb_model = load_model_from_pkl('./behave_model_fpd30_lgb_241118.pkl')


# In[724]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(tpr-fpr)
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc

def plt_ks_auc(df, target, y_pred, group_col, filename=None):
    fpr, tpr, _ = roc_curve(df[target], df[y_pred], pos_label=1)
    # 计算 AUC
    auc_ = roc_auc_score(df[target], df[y_pred])
    # KS 曲线的最大距离点
    ks_x = fpr[np.argmax(tpr - fpr)]
    ks_y = tpr[np.argmax(tpr - fpr)]
    
    fig = plt.figure(figsize=(10,10))
    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'{group_col} ROC curve (Area = {auc_:.2f})')
    # 绘制 KS 点
    plt.plot(ks_x, ks_y, 'ko', label=f'KS Point (KS = {ks_:.2f})')
    # 添加 KS 直线
    plt.plot([0, ks_x], [ks_y, ks_y], 'k--')
    plt.plot([ks_x, ks_x], [0, ks_y], 'k--')
    # 绘制对角线
    plt.plot([0,1],[0,1],color='gray', lw=1, linestyle='--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    # 设置图表标题和坐标轴标签
    plt.xlabel('False Positive Rate',fontsize=16)
    plt.ylabel('True Positive Rate',fontsize=16)
    plt.title(f'ROC curve for {col}',fontsize=25)
    # 显示图例
    plt.legend(loc='lower right',fontsize=20)
    # 保存图表
    if filename:
        plt.savefig(filename)
    # 显示图表
    plt.show()


# In[787]:


# 评估模型效果
df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
print(df_ks_auc_month)
df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
print(df_ks_auc_set)


# In[640]:


# # 评估模型效果
# df_sample['y_prob'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
# df_ks_auc_month = model_ks_auc(df_sample, target, 'y_prob', 'lending_month')
# print(df_ks_auc_month)
# df_ks_auc_set = model_ks_auc(df_sample, target, 'y_prob', 'data_set')
# print(df_ks_auc_set)


# In[623]:


# model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')


# In[788]:


model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')


# In[726]:


df_ks_auc_month_227 = model_ks_auc(df_sample[df_sample['channel_id'].isin(['227'])], target, 'y_prob', 'lending_month')
print(df_ks_auc_month_227)


# In[727]:


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (LGBMClassifier, LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        df1_dict = model.get_booster().get_score(importance_type='weight')
        importance_type_split = pd.DataFrame.from_dict(df1_dict, orient='index')
        importance_type_split.columns = ['split']
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        df2_dict = model.get_booster().get_score(importance_type='gain')
        importance_type_gain = pd.DataFrame.from_dict(df2_dict, orient='index')
        importance_type_gain.columns = ['gain']
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.concat([importance_type_gain, importance_type_split], axis=1)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# In[728]:



# 模型变量重要性
df_importance = feature_importance(lgb_model) 
df_psi_iv = pd.merge(df_psi_by_set, df_iv_by_month, how='inner', left_index=True,right_index=True, suffixes=('_psi', '_iv'))
df_importance = pd.merge(df_importance, df_psi_iv, how='inner', left_index=True,right_index=True)
# # df_importance['descrse_iv'] = df_importance['3_oot_iv']/df_importance['2_test_iv'] - 1
df_importance.drop(columns=['1_train'],inplace=True)
# df_importance = df_importance.reset_index()
# df_importance = df_importance.rename(columns={'index':'varsname'})
df_importance.head(10)


# In[643]:


df_importance.to_excel(r'df_importance_1118.xlsx')


# In[729]:


df_sample = df_sample.copy()


# In[736]:


df_sample_all = get_target_summary(df_sample, target, 'lending_month').set_index('bins')
df_sample_all = pd.concat([df_sample_all, model_ks_auc(df_sample, target, 'y_prob', 'lending_month')], axis=1)
df_sample_all


# In[737]:


df_tmp = df_sample.query("diff_days>30")
df_sample_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_30 = pd.concat([df_sample_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_30


# In[738]:


df_tmp = df_sample.query("channel_id=='227'")
df_sample_227 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_227 = pd.concat([df_sample_227, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_227


# In[739]:


df_tmp = df_sample.query("channel_id=='227' & diff_days>30")
df_sample_227_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_227_30 = pd.concat([df_sample_227_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_227_30


# In[740]:


df_tmp = df_sample.query("channel_id in ('209','213','226','229','231','233','234','235','236')")
df_sample_no_227 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_no_227 = pd.concat([df_sample_no_227, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_no_227


# In[741]:


df_tmp = df_sample.query("channel_id in ('209','213','226','229','231','233','234','235','236') & diff_days>30")
df_sample_no_227_30 = get_target_summary(df_tmp, target, 'lending_month').set_index('bins')
df_sample_no_227_30 = pd.concat([df_sample_no_227_30, model_ks_auc(df_tmp, target, 'y_prob', 'lending_month')], axis=1)
df_sample_no_227_30


# In[743]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./模型开发/行为模型/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'4_模型结果分析_{timestamp}.xlsx') as writer:
    df_sample.to_excel(writer, sheet_name='df_sample')
    df_sample_30.to_excel(writer, sheet_name='df_sample_30')
    df_sample_227.to_excel(writer, sheet_name='df_sample_227')
    df_sample_227_30.to_excel(writer, sheet_name='df_sample_227_30')
    df_sample_no_227.to_excel(writer, sheet_name='df_sample_no_227')
    df_sample_no_227_30.to_excel(writer, sheet_name='df_sample_no_227_30')
print("数据存储完成！")


# In[744]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./模型开发/行为模型/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'4_模型结果分析_{timestamp}.xlsx') as writer:
    df_importance.to_excel(writer, sheet_name='df_importance')
#     df_importance_v1.to_excel(writer, sheet_name='df_importance_v1')
    df_ks_auc_month.to_excel(writer, sheet_name='df_ks_auc_month')
#     df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set.to_excel(writer, sheet_name='df_ks_auc_set')
#     df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')
print("数据存储完成！")


# ## 5.2保存模型和分数

# In[746]:



# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)


def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

# PMML方式保存和读取模型
# sklearn接口的xgboost，可使用sklearn2pmml生成pmml文件
def save_model_as_pmml(model, save_file_path):
    """
    保存模型到路径save_file_path
    :param x: 训练数据特征
    :param y: 训练数据标签
    :param save_file_path: 保存的目标路径
    """
    
    # 模型结果保存
    sklearn2pmml(model, save_file_path, with_repr=True)


# PMML格式读取
def load_model_from_pmml(load_file_path):
    """
    从路径load_file_path加载模型
    :param load_file_path: pmml文件路径
    """
    model = Model.fromFile(load_file_path)
    return model

# xgb模型保存、加载.bin 和 .pkl 格式
def save_model_as_bin(model, save_file_path):
    #保存xgb模型为pkl\bin格式
    model.save_model(save_file_path)
    # pickle.dump(model, open(f"{file_prefix_name}.pkl", "wb"))

def load_model(model_path):
    #加载xgb模型，支持pkl\bin格式
    if os.path.splitext(model_path)[-1] == '.pkl':
        return pickle.load(open(model_path, 'rb'))
    else:
        model = xgb.XGBClassifier()
        model._Booster = xgb.Booster(model_file=model_path)
        return model
    
    
def save_lgb_model(lgb_model, file_prefix_name):
    #保存lgb模型为pkl\bin格式
    lgb_model.booster_.save_model(f"{file_prefix_name}.bin")
    pickle.dump(lgb_model, open(f"{file_prefix_name}.pkl", "wb"))

def load_lgb_model(model_path):
    #加载lgb模型，支持pkl\bin格式
    import lightgbm as lgb
    if os.path.splitext(model_path)[-1] == '.pkl':
        return pickle.load(open(model_path, 'rb'))
    else:
        model = lgb.Booster(model_file=model_path)
        return model


# In[764]:


timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./模型开发/行为模型/result_order_fpd30_{timestamp}'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
model_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

save_model_as_pkl(lgb_model, model_path + f'model_order_fpd30_{timestamp}.pkl')
save_model_as_bin(lgb_model, model_path + f'model_order_fpd30_{timestamp}.bin')

df_sample.to_csv(model_path + f'model_order_fpd30_score_{timestamp}.csv')
print(model_path + f'model_order_fpd30_{timestamp}.pkl')
print(model_path + f'model_order_fpd30_score_{timestamp}.csv')


# ## 5.3 评分分布

# In[748]:


score = 'y_prob'


# In[754]:


df_sample['lending_month'].value_counts()


# In[755]:


c = toad.transform.Combiner()
c.fit(df_sample.query("lending_month=='2024-07(1_train)'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[756]:


df_sample['score_bins'].head()


# In[757]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07(1_train)'"), 
                                                [score], 'lending_month', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("lending_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[758]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[760]:


df_psi_matrix = get_model_psi(df_sample, score, 'lending_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[ ]:


# df_psi_matrix_set = get_model_psi(df_sample, 'score', 'data_set', c)

# # 打印最终的 PSI 矩阵
# print(df_psi_matrix_set)


# In[ ]:


# score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
# score_group_by_dataset_1 = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum',
#                                                  'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
# # df_score_group_by_dataset = score_group_by_dataset.pivot_table(index='bins', columns='groupvars', values='total_pct')
# # print(df_score_group_by_dataset)


# In[ ]:


# score_group_by_dataset_1 = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum',
#                                                  'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
# score_group_by_dataset_1.head(2)


# In[763]:


result_path = f'{directory}/'
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'5_评分分布_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
#     score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"数据存储完成！:{timestamp}")




#==============================================================================
# File: 报告_授信全渠道人行衍生M6D30离线模型_2409_2412.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# # 一、模型简介

# 本模型为授信全渠道人行mob6dpd30离线模型，Y标签为mob6dpd30。模型算法使用lgb算法进。
# train样本：选取2024年9月1日至2024年11月17日，api渠道、金科渠道的授信成功且放款成功的用户。
# oot1样本：选取2024年11月18日至2024年11月30日，api渠道、金科渠道的授信成功且放款成功的用户。
# oot2样本：选取2024年12月1日至2024年12月17日，api渠道、金科渠道的授信成功且放款成功的用户。

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# # 二、 样本概况

# In[2]:


def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[3]:


# 最终模型打分 
lgb_model= load_model_from_pkl('./result/授信全渠道人行衍生mob6dpd30离线模型_v3_20250721135643.pkl')


# In[ ]:


lgb_model.params


# In[ ]:


varsname = lgb_model.feature_name()
print(len(varsname), varsname)


# In[ ]:


df_sample = pd.read_csv('./result/授信全渠道人行衍生M6D30离线模型_2409_2412.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[7]:


# 设置标签和分数列
target = 'target_mob6dpd30'
score = 'y_pred_v4'


# In[8]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[ ]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
total_summary = df_sample[target].agg(total=lambda x: len(x), 
        bad=lambda x: x.sum(), 
        good=lambda x: (x== 0).sum(), 
        bad_rate=lambda x: x.mean()).to_frame().T
total_summary['bins'] = 'Total'
# 将汇总行添加到分组结果中
df_target_summary_month = pd.concat([df_target_summary_month, total_summary], ignore_index=True)
df_target_summary_month


# In[10]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
df_target_summary_set.insert(0, 'time_windows', value=['20240901-20241117','20240901-20241117','20241118-20241130','20241201-20241217'])
df_target_summary_set


# # 三、变量重要性（入模372个，展示前40个）

# In[14]:


filepath = './result/4_模型训练_授信全渠道人行衍生mob6dpd30离线模型_v3_20250721135643.xlsx'
file = pd.read_excel(filepath, sheet_name=None) 


# In[ ]:


file.keys()


# In[ ]:


df_importance.info(show_counts=True)


# In[16]:


df_importance = file['df_importance_set_v3'].copy()
df_importance.drop(columns=['Unnamed: 0'],inplace=True)
# df_importance = pd.merge(df_vars_des, df_importance, how='right', on='feature')
df_importance.head(40)


# # 四、模型效果评估

# In[19]:


df_ks_auc_set_v1 = file['df_ks_auc_set_v3'].copy()
df_ks_auc_set_v1.rename(columns={'Unnamed: 0':'data_set'},inplace=True)
df_ks_auc_set_v1.insert(0,'time_windows', value=['20240901-20241117','20240901-20241117','20241118-20241130','20241201-20241217']*6)
df_ks_auc_set_v1


# In[ ]:


## 模型效果对比


# In[ ]:


filepath_c = './result_v4/6_模型效果对比_提现人行征信模型fpd30标签_20250527191319.xlsx'
file_c = pd.read_excel(filepath_c, sheet_name=None)


# In[ ]:


file_c.keys()


# In[ ]:


fpd30 = file_c['fpd30']
fpd30.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
fpd30.rename(columns={'KS_y_prob_base_v4':'KS_本次开发的模型','AUC_y_prob_base_v4':'AUC_本次开发的模型'},inplace=True)
fpd30


# In[ ]:


mob4dpd30 = file_c['mob4dpd30']
mob4dpd30.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
mob4dpd30.rename(columns={'KS_y_prob_base_v4':'KS_本次开发的模型','AUC_y_prob_base_v4':'AUC_本次开发的模型'},inplace=True)
mob4dpd30


# In[ ]:


### 在T30+客群模型效果对比


# In[ ]:


fpd30_plus = file_c['30_fpd30']
fpd30_plus.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
fpd30_plus.rename(columns={'KS_y_prob_base_v4':'KS_本次开发的模型','AUC_y_prob_base_v4':'AUC_本次开发的模型'},inplace=True)
fpd30_plus


# In[ ]:


mob4dpd30_plus = file_c['30_mob4dpd30']
mob4dpd30_plus.drop(columns=['Unnamed: 0','KS_y_prob_base_v1','KS_y_prob_base_v2','KS_y_prob_base_v5','AUC_y_prob_base_v1','AUC_y_prob_base_v2','AUC_y_prob_base_v5'],inplace=True)
mob4dpd30_plus.rename(columns={'KS_y_prob_base_v4':'KS_本次开发的模型','AUC_y_prob_base_v4':'AUC_本次开发的模型'},inplace=True)
mob4dpd30_plus


# # 五、评分分布

# In[25]:


filepath_score = './result/6_评分分布_授信全渠道人行衍生mob6dpd30离线模型_20250721160250.xlsx'
file_score = pd.read_excel(filepath_score, sheet_name=None)


# In[27]:


df_psi_matrix = file_score['df_psi_matrix'].copy()
df_psi_matrix.rename(columns={'Unnamed: 0':'psi'},inplace=True)
df_psi_matrix.set_index('psi',inplace=True)
df_psi_matrix.T


# In[28]:


score_group_by_dataset = file_score['score_group_by_dataset'].copy()
score_group_by_dataset.drop(columns=['Unnamed: 0'],inplace=True)
score_group_by_dataset = score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[29]:


score_group_by_dataset


# # 六、变量分布

# In[39]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # 使用更深的对色盲友好的颜色
 color_train = '#004494'  # 深蓝色
 color_oot = '#D66100'    # 深橙色

 # 绘制柱状图
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT2',
                 color=color_oot, alpha=0.6)

 # 柱状图数据标签，字体颜色设为黑色
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # 折线图，使用更深的颜色和标记
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT2')  # 使用方形标记
 ax2.set_ylabel('Bad Rate')

 # 折线图数据标签，字体颜色设为黑色
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # 添加IV值
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT2 IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # 调整图例位置
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # 调整图表布局，给顶部图例留出空间
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[ ]:


df_importance.head()


# In[47]:


importance_dict = df_importance.set_index('feature')['comment'].to_dict()


# In[42]:


df_group_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_授信全渠道人行衍生mob6dpd30离线模型_20250722094044.xlsx',sheet_name='df_group_set')


# In[ ]:


df_group_set.head()


# In[50]:



for i, col in enumerate(df_importance['feature'].to_list()):
    
    var_des = importance_dict[col]
    print(f"--------第{i+1}个变量：{col},{var_des}--------")
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot2'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot2'],3)
    # 调用函数
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot',iv_train, iv_oot)


# In[ ]:


# 输出模型报告
jupyter nbconvert --to html --no-input /data/home/liaoxilin/模型开发/03贷中模型/02授信全渠道人行衍生M6D30离线模型/报告_授信全渠道人行衍生M6D30离线模型_2409_2412.ipynb




#==============================================================================
# File: 报告_授信全渠道高成本fpd30融合模型_2411_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# # 一、模型简介

# 本模型为授信全渠道高成本fpd30融合模型，Y标签为fpd30，特征变量有百融子分、洞侦子分、续侦子分、征信子分、三方子分。模型算法使用lgb算法进。
# train样本：选取2024年11月1日至2025年2月28日，api渠道、金科渠道的授信成功且放款的借据。
# oot1样本：选取2024年10月1日至2024年10月31日，api渠道、金科渠道的授信成功且放款的借据。
# oot2样本：选取2025年3月1日至2025年3月31日，api渠道、金科渠道的授信成功且放款的借据。
# 

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# # 二、 样本概况

# In[2]:


def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model


# In[3]:


# 最终模型打分
lgb_model= load_model_from_pkl('./result/授信全渠道高成本fpd30融合模型_2411_2502_v6_20250612152358.pkl')


# In[49]:


varsname = lgb_model.feature_name()
print(len(varsname), varsname)


# In[ ]:


df_sample = pd.read_csv('./result/授信全渠道高成本fpd30融合模型_2411_2502_report.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[51]:


usecols = ['order_no','y_prob_base_v6'] + ['duxiaoman_6', 'hengpu_4', 'hengpu_5', 'rong360_4', 'tianchuang_7', 'pudao_20', 'pudao_68', 'pudao_54', 'baihang_28', 'ali_fraud_score3', 'ppcm_behav_score', 'umeng_score_v5', 'ali_fraud_score9', 'br_fpd', 'pd_fpd', 'br_mob4_2', 'm1a0028_g_p', 'm1a0038_g_p', 'xz_v1_mob4', 'm1a0033_g_p', 'm1a0043_g_p', 'm1a0040_g_p', 'a_bhdj_fpd10_v1', 'm1a0035_g_p', 'dz_v2_fpd', 'xz_v2_fpd', 'm1a0041_g_p', 'm1a0044_g_p', 'm1a0036_g_p']
tmp = df_sample[usecols].head(3)
tmp.to_csv('test.csv',index=False)


# In[6]:


# 设置标签和分数列
target = 'target_fpd30'
score = 'y_prob_base_v6'


# In[7]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[8]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
total_summary = df_sample[target].agg(total=lambda x: len(x), 
        bad=lambda x: x.sum(), 
        good=lambda x: (x== 0).sum(), 
        bad_rate=lambda x: x.mean()).to_frame().T
total_summary['bins'] = 'Total'
# 将汇总行添加到分组结果中
df_target_summary_month = pd.concat([df_target_summary_month, total_summary], ignore_index=True)
df_target_summary_month


# In[ ]:


# # 确定数据集参数后，训练模型

# X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname]
# y_train_ = df_sample.query("data_set not in ('3_oot')")[target]
# X_train, X_test, y_train, y_test = train_test_split(X_train_,
#                                                     y_train_,
#                                                     test_size=0.2, 
#                                                     random_state=22, 
#                                                     stratify=y_train_
#                                                    )
# df_sample.loc[X_train.index, 'data_set']='1_train'
# df_sample.loc[X_test.index, 'data_set']='2_test'


# In[9]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
df_target_summary_set.insert(0, 'time_windows', value=['20241101-20250228','20241101-20250228','20241001-20241031','20250301-20250331'])
df_target_summary_set


# # 三、入模变量重要性

# In[10]:


filepath = './result/4_模型训练_授信全渠道高成本fpd30融合模型_2411_2502_v6_20250612152358.xlsx'
file = pd.read_excel(filepath, sheet_name=None) 


# In[11]:


vars_des = {
 'dz_v2_fpd':'授信全渠道洞侦多头模型v2首期标签202505'
,'xz_v2_fpd':'授信全渠道续侦多头模型v2首期标签202505'
,'m1a0043_g_p':'提现全渠道人行fpd30模型v1_2411_2502_好概率'
,'m1a0041_g_p':'提现全渠道百融加衍生fpd30模型'
,'hengpu_4':'恒普-反欺诈分M3'
,'m1a0033_g_p':'授信全渠道百融加衍生fpd30模型2408_2411_好概率分'
,'pudao_34':'朴道-避雷针定制分V1'
,'ruizhi_6':'FICO联合建模定制分2'
,'pd_fpd':'授信全渠道朴道多头模型fpd30标签202504'
,'hengpu_5':'恒普-定制信用分Y'
,'baihang_13':'百行-灵犀产品-孚临-107'
,'pudao_54':'朴道-哈啰-hl-火眼分v7'
,'pboc_dpd20':'授信通用人行模型dpd20标签202410'
,'pudao_20':'朴道-腾讯天御反欺诈V7通用版'
,'aliyun_5':'朴道-阿里申请反欺诈V5'
,'m1a0028_g_p':'授信全渠道人行mob4dpd30模型V1_2408_2410_好概率'
,'baihang_28':'fico反欺诈洞见3.0'
,'pudao_82':'朴道-天创-玄辰信用分_AC1501'
,'m1a0035_g_p':'授信全渠道人行fpd7模型202408_2411'
,'m1a0038_g_p':'授信全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率分'
,'zhirongfen':'同盾智融分'
,'tianchuang_7':'天创-联合分A1502'
,'a_bhdj_fpd10_v1':'授信全渠道百行洞见模型v1fpd10标签'
,'duxiaoman_6':'度小满-欺诈因子V4'
,'m1a0044_g_p':'提现全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率'
,'rong360_4':'数据源_rong360'
,'ali_fraud_score3':'阿里申请反欺诈子分score3'
,'xz_v1_mob4':'授信全渠道续侦多头模型v1四期标签202505'
,'pudao_87':'朴道-字节-互联网行为评分25128'
,'m1a0020_g_p':'授信金科渠道洞侦加衍生fpd30模型202409_2411'
,'a_pboc_fpd0_v1':'授信全渠道人行模型v1入催标签202409'
,'br_fpd':'授信百融多头首期标签模型202404'
,'m1a0040_g_p':'授信全渠道朴道多头四期模型V1_2409_2411_好概率'
,'pudao_84':'朴道-友盟-建模分score_v3'
,'m1a0037_g_p':'授信全渠道人行加衍生fpd20_v1模型2410_2411'
,'bileizhenv1':'避雷针v1h1'
,'pudao_68':'朴道-银商银杏定制分'
,'br_mob4_2':'授信百融多头v2四期标签202407'
,'m1a0036_g_p':'授信全渠道人行加衍生fpd1_v2模型2409_2411'
,'umeng_score_v5':'友盟-小额分V5.0'
,'ppcm_behav_score':'海纳支付行为分'
,'ali_fraud_score9':'阿里云-申请反欺诈子分v2_子分9'
,'ali_fraud_score3':'阿里云-申请反欺诈子分v2_子分'
}


# In[ ]:


df_importance = file['df_importance_month_v6'].drop(columns=['Unnamed: 0'])
df_importance['desc'] = df_importance['feature'].map(vars_des)
df_importance.insert(1,'变量名称', df_importance['desc'])
df_importance.drop(columns=['desc'],inplace=True)
df_importance


# In[13]:


file2 = pd.read_excel('./result/3_变量分析_dis_iv_psi_授信全渠道高成本fpd30融合模型_2411_2502_20250605194218.xlsx',sheet_name=None)


# In[ ]:


file2.keys()


# In[15]:


file3 = pd.read_excel('./result/2_数据探索性分析_授信全渠道高成本fpd30融合模型_2411_2502_20250605172607.xlsx',sheet_name=None)


# In[ ]:


file3.keys()


# In[ ]:


# 模型变量重要性 
df_iv_by_set = file2['df_iv_by_set'].set_index('Unnamed: 0')
df_iv_by_set.drop(columns=['3_oot1'],inplace=True)
df_psi_by_set = file2['df_psi_by_set'].set_index('Unnamed: 0')
df_psi_by_set.drop(columns=['3_oot1'],inplace=True)
df_miss_set = file3['df_miss_set'].set_index('variable')
df_miss_set.drop(columns=['3_oot1'],inplace=True)
df_iv_psi_miss_set = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set], axis=1)
df_iv_psi_miss_set.columns = [f'{col}_iv' for col in df_iv_by_set.columns]+[f'{col}_psi' for col in df_psi_by_set.columns]+[f'{col}_na' for col in df_miss_set.columns]
df_iv_psi_miss_set = df_iv_psi_miss_set.reset_index()
df_iv_psi_miss_set.rename(columns={"index":"feature"},inplace=True)


# In[ ]:


df_iv_psi_miss_set.head()


# In[ ]:


# 模型变量重要性
# df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance, df_iv_psi_miss_set, how='left',on='feature')
# df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[17]:


# 模型变量重要性
df_iv_by_month = file2['df_iv_by_month'].set_index('Unnamed: 0')
df_iv_by_month.drop(columns=['2025-04'],inplace=True)
df_psi_by_month = file2['df_psi_by_month'].set_index('Unnamed: 0')
df_psi_by_month.drop(columns=['2025-04'],inplace=True)
df_miss_month = file3['df_miss_month'].set_index('variable')
df_miss_month.drop(columns=['2025-04'],inplace=True)
df_iv_psi_miss = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month], axis=1)
df_iv_psi_miss.columns = [f'{col}_iv' for col in df_iv_by_month.columns]+[f'{col}_psi' for col in df_psi_by_month.columns]+[f'{col}_na' for col in df_miss_month.columns]
df_iv_psi_miss = df_iv_psi_miss.reset_index()
df_iv_psi_miss.rename(columns={"index":"feature"},inplace=True)


# In[ ]:


df_iv_by_month.head()


# In[18]:


# 模型变量重要性
# df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance, df_iv_psi_miss, how='left',on='feature')
# df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[ ]:





# In[ ]:


# df_corr.loc[varsname,varsname]


# In[ ]:


# df_corr.loc[varsname,varsname].to_csv('df_corr.csv')


# # 四、模型效果评估

# In[19]:


df_ks_auc_month_v1 = file['df_ks_auc_month_v6'].copy()
df_ks_auc_month_v1.rename(columns={'Unnamed: 0':'apply_month'},inplace=True)
# df_ks_auc_month_v1.drop(index=df_ks_auc_month_v1.query("apply_month=='2025-04'").index,inplace=True)
df_ks_auc_month_v1.reset_index(drop=True,inplace=True)
df_ks_auc_month_v1


# In[ ]:


file.keys()


# In[20]:


df_ks_auc_set_v1 = file['df_ks_auc_set_v6'].copy()
df_ks_auc_set_v1.rename(columns={'Unnamed: 0':'data_set'},inplace=True)
# df_ks_auc_set_v1.drop(index=df_ks_auc_set_v1.query("data_set=='3_oot1'").index,inplace=True)
df_ks_auc_set_v1.insert(0,'time_windows',value=['20241101-20250228','20241101-20250228','20241001-20241031','20250301-20250331']*6)
df_ks_auc_set_v1.reset_index(drop=True, inplace=True)
df_ks_auc_set_v1


# In[ ]:


df_ks_auc_set_v12 = df_ks_auc_set_v1.copy()


# In[ ]:


df_ks_auc_set_v1.loc[ 3,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[ 5,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[ 7,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[11,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[11,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[17,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[15,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[23,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[19,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[29,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1.loc[23,['total','bad','good','bad_rate','KS','AUC']]=df_ks_auc_month_v1.loc[35,['total','bad','good','bad_rate','KS','AUC']]
df_ks_auc_set_v1


# ### 0. 模型与入模变量间的相关性

# In[23]:


df_sample['high_p_f30_2506'] = df_sample['y_prob_base_v6']


# In[24]:


df_sample[['high_p_f30_2506']+varsname].corr()


# ### 1.在全渠道与融合模型效果对比

# In[ ]:


file4path = './result/6_模型效果对比_授信全渠道高成本fpd30融合模型_2411_2502_20250612154707.xlsx'
file4 = pd.read_excel(file4path, sheet_name=None)
file4.keys()


# In[33]:


df_c1 = file4['fpd30_融合'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c1.drop(columns=drop_cols,inplace=True)
# df_c1.drop(index=df_c1.query("apply_month=='2025-04'").index,inplace=True)
df_c1 = df_c1.reset_index(drop=True)
df_c1.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506","KS_m1a0030_g_p":"KS_high_p_f30_2504","AUC_m1a0030_g_p":"AUC_high_p_f30_2504"},inplace=True)
df_c1


# In[34]:


df_c2 = file4['fpd30_三方'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c2.drop(columns=drop_cols,inplace=True)
# df_c2.drop(index=df_c2.query("apply_month=='2025-04'").index,inplace=True)
df_c2 = df_c2.reset_index(drop=True)
df_c2.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506"},inplace=True)
df_c2


# ### 2.在征信渠道与融合模型效果对比

# In[ ]:


#(227,213,231,233,240,245,241,246)


# In[ ]:


file5path = './result/6_模型效果对比_授信全渠道高成本fpd30融合模型_2411_2502_pboc_20250612154845.xlsx'
file5 = pd.read_excel(file5path, sheet_name=None)
file5.keys()


# In[36]:


df_c3 = file5['pboc_fpd30_融合'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c3.drop(columns=drop_cols,inplace=True)
# df_c3.drop(index=df_c3.query("apply_month=='2025-04'").index,inplace=True)
df_c3 = df_c3.reset_index(drop=True)
df_c3.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506","KS_m1a0030_g_p":"KS_high_p_f30_2504","AUC_m1a0030_g_p":"AUC_high_p_f30_2504"},inplace=True)
df_c3


# In[37]:


df_c4 = file5['pboc_pd30_三方'].copy()
drop_cols1 = [f'KS_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols2 = [f'AUC_{col}' for col in ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v7']]
drop_cols = drop_cols1 + drop_cols2
drop_cols.append('Unnamed: 0')
df_c4.drop(columns=drop_cols,inplace=True)
# df_c4.drop(index=df_c4.query("apply_month=='2025-04'").index,inplace=True)
df_c4 = df_c4.reset_index(drop=True)
df_c4.rename(columns={"KS_y_prob_base_v6":"KS_high_p_f30_2506","AUC_y_prob_base_v6":"AUC_high_p_f30_2506"},inplace=True)
df_c4


# # 五、评分分布

# In[38]:


filepath_score = './result/6_评分分布_授信全渠道高成本fpd30融合模型_2411_2502_20250612155850.xlsx'
file_score = pd.read_excel(filepath_score, sheet_name=None)


# In[39]:


df_psi_matrix = file_score['df_psi_matrix']
df_psi_matrix.rename(columns={'Unnamed: 0':'psi'},inplace=True)
# df_psi_matrix.drop(columns=['2025-04'],inplace=True)
df_psi_matrix.set_index('psi',inplace=True)
# df_psi_matrix.drop(index=['2025-04'],inplace=True)
df_psi_matrix


# In[42]:


score_group_by_dataset = file_score['score_group_by_dataset'].copy()
score_group_by_dataset.drop(columns=['Unnamed: 0'],inplace=True)
score_group_by_dataset = score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")
score_group_by_dataset = score_group_by_dataset.reset_index(drop=True)
score_group_by_dataset.drop(columns=['ks_bin'],inplace=True)
score_group_by_dataset


# # 六、变量分布

# In[43]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # 使用更深的对色盲友好的颜色
 color_train = '#004494'  # 深蓝色
 color_oot = '#D66100'    # 深橙色

 # 绘制柱状图
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # 柱状图数据标签，字体颜色设为黑色
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # 折线图，使用更深的颜色和标记
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # 使用方形标记
 ax2.set_ylabel('Bad Rate')

 # 折线图数据标签，字体颜色设为黑色
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # 添加IV值
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # 调整图例位置
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # 调整图表布局，给顶部图例留出空间
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[44]:


df_importance.head()


# In[45]:


importance_dict = df_importance.set_index('feature')['变量名称'].to_dict()


# In[ ]:


# df_group_set = pd.read_excel('./result/3_变量分析_dis_iv_psi_授信全渠道子分融合模型三期标签v2_20250331205148.xlsx',sheet_name='df_group_set')
df_group_set = file2['df_group_set'].copy()
df_group_set.head()


# In[ ]:


df_group_month = file2['df_group_month'].copy()
df_group_month.head()


# In[48]:



for col in importance_dict.keys():
    var_des = importance_dict[col]
    print(f"----------{col}:{var_des}----------")
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot2'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    var_des = importance_dict[col]
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot2'],3)
    # 调用函数
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot',iv_train, iv_oot)


# In[ ]:


# 输出模型报告
jupyter nbconvert --to html --no-input ./模型开发/01授信模型/05授信全渠道高成本fpd30融合模型_2411_2502/报告_授信全渠道高成本fpd30融合模型_2411_2502.ipynb




#==============================================================================
# File: 抽样函数.py
#==============================================================================

# 数据抽样
def resample_negative_samples(X_train, y_train, date_column, amplification_factor, total_samples=200000):
   
    # 计算每个月的正负样本比例
    df = pd.concat([X_train, y_train], axis=1)
    df['flag'] = y_train
    df[date_column] = pd.to_datetime(df[date_column])
    df['month'] = df[date_column].dt.to_period('M').astype(str) 
    df_month_ratios = df.pivot_table(index='month', values='flag', aggfunc='mean')
    month_ratios = df_month_ratios['flag'].to_dict()
    
    # 分离正负样本
    pos_samples = df[df['flag'] == 1]
    neg_samples = df[df['flag'] == 0]
    
    # 计算正样本总数
    pos_count = len(pos_samples)

    # 计算负样本应该有的总数
    neg_count = total_samples - pos_count
       
    # 计算每个时间窗口需要的目标负样本数量
    target_counts = {}
    for month, ratio in month_ratios.items():
        # 获取该月份的正样本数量
        pos_count = len(pos_samples[pos_samples['month'] == month])
        # 计算目标负样本数量
        target_neg_count = pos_count / (amplification_factor * ratio / (1 - ratio))
        target_counts[month] = int(target_neg_count)
    
    # 根据总负样本数量重新分配各月的负样本数量
    total_target_neg_count = sum(target_counts.values())
    adjusted_counts = {}
    for month, count in target_counts.items():
        adjusted_counts[month] = int((count / total_target_neg_count) * neg_count)
    
    # 初始化一个空DataFrame来存储重采样的结果
    resampled_neg_samples = pd.DataFrame()

    # 对每个时间窗口的负样本进行重采样
    for month, target_count in adjusted_counts.items():
        # 获取当前时间窗口的负样本
        current_neg_samples = neg_samples[neg_samples['month'] == month]
        # 如果当前时间窗口的负样本数量少于目标数量，则直接添加到结果中
        if len(current_neg_samples) <= target_count:
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples])
        else:
            # 否则，从当前时间窗口的负样本中随机抽取目标数量的样本
            resampled_neg_samples = pd.concat([resampled_neg_samples, current_neg_samples.sample(n=target_count)])

    # 合并正样本和重采样后的负样本
    X_resampled = pd.concat([pos_samples, resampled_neg_samples])[X_train.columns]
    y_resampled = y_train.loc[X_resampled.index]

    return (X_resampled, y_resampled)



#==============================================================================
# File: 授信_离线特征变量工程化处理.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import time 
from datetime import datetime
import re
from IPython.core.interactiveshell import InteractiveShell
import warnings
import gc
from jinja2 import Template
import os

warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = 'all'


# # 配置参数

# In[2]:



# 配置参数
BATCH_SIZE = 1500
OUTPUT_DIR = "./generated_sql"
MAIN_SQL_FILE = "main_table.sql"  # 你可以把主表 SQL 写入文件中备用

os.makedirs(OUTPUT_DIR, exist_ok=True)


# # 生成sql脚本

# In[3]:




# 示例变量清单格式：
# variable_name | table_name
variables_df = pd.read_excel("人行变量清单2506.xlsx",sheet_name='离线变量清单')
variables_df.rename(columns={"Table": "table_name","var":"variable_name"}, inplace=True)
variables_df.info(show_counts=True)
variables_df.head()


# In[4]:


# 分批处理
batches = [variables_df[i:i+BATCH_SIZE] for i in range(0, len(variables_df), BATCH_SIZE)]
print(len(batches))


# In[5]:


# Jinja2 模板：特征变量部分
feature_template = Template("""
left join 
(
select t.*,
   ROW_NUMBER() OVER (PARTITION BY id_no_des ORDER BY dt DESC) AS rk 
from {{ full_table_name }} as t 
where dt <= date_sub('$[last_day(yyyy-MM-dd)]', 1) 
  and dt >= date_sub('$[last_day(yyyy-MM-dd)]', 100)
) as {{ alias_name }} on t.id_no_des = {{ alias_name }}.id_no_des and {{ alias_name }}.rk = 1
""")


# In[6]:



# 主表模板占位符
MAIN_TEMPLATE = """
WITH main_base AS (
    -- 主表 SQL 放在这里
),
features AS (
    -- 所有 left join 的特征表放在这里
)
SELECT *
FROM main_base
LEFT JOIN features USING (id_no_des)
"""


# In[7]:


# 读取主表 SQL（假设你已保存为文本文件）
with open(MAIN_SQL_FILE, 'r') as f:
    main_sql = f.read()


# In[8]:


print(main_sql)


# In[9]:


for idx, batch in enumerate(batches):
    print(f"\nProcessing Batch {idx + 1} / {len(batches)}")
    
    # 按 table_name 分组，得到 { 'tableA': ['var1', 'var2'], ... }
    grouped = batch.groupby('table_name')['variable_name'].apply(list).to_dict()
    
    feature_joins = []
    alias_counter = 1
    
    # 新增：记录每个表对应的别名和变量列表
    batch_aliases = {}

    for table_name, vars_list in grouped.items():
        alias_name = f"t{alias_counter}"
        alias_counter += 1
        
        # 记录当前表的别名和字段
        batch_aliases[table_name] = (alias_name, vars_list)
        
        # 填充模板，构造每个特征表的 SELECT 字段列表
        sql_part = feature_template.render(
            full_table_name=table_name,
            variables=vars_list,
            alias_name=alias_name
        )
        feature_joins.append(sql_part)
    
    # 合并所有 left join 子句
    all_features_sql = "\n".join(feature_joins)

    # 构造 SELECT 列表：t.*, t1.var1, t1.var2, t2.var101, ...
    feature_columns = [
        f"{alias}.{var}"
        for table_name, (alias, vars_list) in batch_aliases.items()
        for var in vars_list
    ]
    features_select_clause = ",\n        ".join(feature_columns)

    # 将主表和特征 join 合并
    final_sql = f"""
    WITH main_base AS (
        {main_sql}
    ),
    features_with_vars AS (
        SELECT 
            t.*,
            {features_select_clause}
        FROM main_base AS t
        {all_features_sql}
    )
    SELECT * FROM features_with_vars
    """
    
    # 保存到文件
    output_file = os.path.join(OUTPUT_DIR, f"batch_{idx + 1}.sql")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_sql)
        
    print(f"✅ 已生成 SQL 文件: {output_file}")


# # 处理数据

# In[10]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
import glob
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[11]:



# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance

def model_trian(df,target,varsname_base):
    ### 模型参数
    opt_params = {}
    opt_params['boosting'] = 'gbdt'
    opt_params['objective'] = 'binary'
    opt_params['metric'] = 'auc'
    opt_params['bagging_freq'] = 1
    opt_params['scale_pos_weight'] = 1 
    opt_params['seed'] = 1 
    opt_params['num_threads'] = -1 
    # 调参时设置成不用调参的参数
    opt_params['learning_rate'] = 0.1
    ## 正则参数，防止过拟合
    opt_params['bagging_fraction'] = 0.8628008772208227     
    opt_params['feature_fraction'] = 0.6177619614753441
    opt_params['lambda_l1'] = 0
    opt_params['lambda_l2'] = 300
    opt_params['early_stopping_rounds'] = 50

    # 调参后的参数需要变成整数型
    opt_params['num_leaves'] = 21
    opt_params['min_data_in_leaf'] = 103
    opt_params['max_depth'] = 2
    # 调参后的其他参
    opt_params['min_gain_to_split'] = 10
    
    df_sample = df[df[target].notna()]
    df_sample[target] = df_sample[target].astype('int')
    df_sample = df_sample[df_sample[target]>=0].reset_index(drop=True)
    df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-10-31'").index, 'data_set']='1_train'
    df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot1'
    df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-12-14'").index, 'data_set']='3_oot2'

    # 确定数据集参数后，训练模型
    for i, col in enumerate(varsname_base):
        if df_sample[col].dtype=='object':
            df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')
    
    X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
    y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[target]
    X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                        y_train_,
                                                        test_size=0.2, 
                                                        random_state=22, 
                                                        stratify=y_train_
                                                       )
    df_sample.loc[X_train.index, 'data_set']='1_train'
    df_sample.loc[X_test.index, 'data_set']='2_test'

    train_set = lgb.Dataset(X_train, label=y_train)
    valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
    lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)
    df_importance = feature_importance(lgb_model) 
    df_importance = df_importance.reset_index()
    return df_importance


def load_sql_from_file(filepath):
    """读取 .sql 文件内容"""
    with open(filepath, 'r', encoding='utf-8') as f:
        sql_content = f.read()
    return sql_content



def run_all_sql_files(target, non_feature_cols, input_dir='./generated_sql/', output_file='all_features.csv'):
    """
    读取所有 SQL 文件，依次执行并合并结果
    """
    sql_files = sorted(glob.glob(os.path.join(input_dir, "*.sql")))
    all_data = []
    for idx, file in enumerate(sql_files):
        print(f"\n🚀 正在处理第 {idx+1} / {len(sql_files)} 个 SQL 文件: {file}")

        try:
            sql = load_sql_from_file(file)
            df_sample_dict = {}
            
            # 计算今天的时间
            from datetime import datetime, timedelta, date
            this_day =datetime.strptime('2024-12-14', '%Y-%m-%d')
            end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

            while this_day >= end_day:
                run_day = this_day.strftime('%Y-%m-%d')
                # 替换日期变量（如果需要）
                final_sql = sql.replace("$[last_day(yyyy-MM-dd)]", f"{run_day}")
                print(f'=========================={run_day}=============================')
                df_sample_dict[run_day] = get_data(final_sql)
                this_day = this_day - timedelta(days=1)
            df = pd.concat(df_sample_dict.values(), ignore_index=True)
            
            if not df.empty:
                # 可选：删除重复字段（如 id_no_des 已存在于主表）
                cols_to_drop = [col for col in df.columns if df.columns.tolist().count(col) > 1]
                df = df.loc[:, ~df.columns.duplicated()]
                print(f"✅ 成功加载数据，行数: {len(df)}, 列数: {len(df.columns)}")
                varsname_base = [col for col in df.columns if col not in non_feature_cols]
                df_importance = model_trian(df, target, varsname_base)
                all_data.append(df_importance)
            else:
                print(f"⚠️ 警告：{file} 返回空数据")

        except Exception as e:
            print(f"❌ 执行失败: {file}")
            print("错误详情:", str(e))
            continue

    if all_data:
        print("\n📊 正在合并所有批次数据...")
        final_df = pd.concat(all_data, axis=0)

        # 保存最终结果
#         final_df.to_parquet(output_file)
        final_df.to_csv(output_file)
        print(f"💾 数据已保存至: {output_file}")
        return final_df
    else:
        print("没有成功加载任何数据。")
        return None


# In[ ]:


non_feature_cols = ['order_no','user_id','id_no_des','channel_id','apply_date',
                    'target_fpd30','target_cpd30','target_mob4dpd30','target_mob6dpd30']
target = 'target_mob6dpd30'
final_data = run_all_sql_files(target, non_feature_cols, input_dir="./generated_sql", output_file="final_features.csv")
print("最终数据集列数:", len(final_data.columns))


# In[13]:


final_data


# In[14]:


final_data.to_csv('20000个离线人行变量特征重要性排序.csv')


# In[15]:


final_data_v1 = final_data[['gain']]
final_data_v1.info(show_counts=True)
final_data_v1.head()


# In[21]:


final_data_v2 = final_data_v1[final_data_v1>0]
final_data_v2.info(show_counts=True)


# In[22]:


final_data_v2 = final_data_v2.dropna(how='all')
final_data_v2 =final_data_v2.reset_index()
final_data_v2.info(show_counts=True)


# In[23]:


final_data_v2.to_csv('13500个离线人行变量特征重要性排序大于0.csv')




#==============================================================================
# File: 授信全渠到行为数据模型四期标签.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


# df_sample_dict = {}


# In[5]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[6]:


print(len(table_name_list))


# In[8]:


df_target = pd.read_csv(result_path + '00_model_target.csv')


# In[9]:


df_feature1 = pd.read_csv(result_path + '01_behave_features.csv')
df_feature1 = df_feature1.set_index('order_no')

df_feature2 = pd.read_csv(result_path + '02_behave_features.csv')
df_feature2 = df_feature2.set_index('order_no')

df_feature3 = pd.read_csv(result_path + '03_behave_features.csv')
df_feature3 = df_feature3.set_index('order_no')

df_feature4 = pd.read_csv(result_path + '04_behave_features.csv')
df_feature4 = df_feature4.set_index('order_no')

df_feature5 = pd.read_csv(result_path + '05_behave_features.csv')
df_feature5 = df_feature5.set_index('order_no')

df_feature6 = pd.read_csv(result_path + '06_behave_features.csv')
df_feature6 = df_feature6.set_index('order_no')


# In[15]:


df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5,df_feature6],axis=1)
df_feature = df_feature.reset_index()
df_feature.shape


# In[16]:


df_target = df_target.reset_index()
df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[24]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[27]:


varsname = df_sample.columns.to_list()[22:]


print("初始特征变量个数：",len(varsname))

print(varsname[:5], varsname[-5:])


# In[28]:


pd.set_option('display.max_columns',None)


# In[30]:


df_sample.head(2)


# In[33]:


df_sample.to_csv(result_path + '授信全渠到行为数据模型四期标签.csv',index=False)
print(result_path + '授信全渠到行为数据模型四期标签.csv')


# In[36]:


df_sample[varsname].info()


# # 1. 样本概况

# In[150]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # 返回结果
    return result


# In[48]:


target = 'mob4dpd30'


# In[49]:


print(df_sample[target].value_counts())


# In[50]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[51]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[52]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[53]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[54]:


# for col in varsname:
#     if df_sample[col].min()<0:
#         print(f"--{col}--")
#         df_sample.loc[df_sample[col]<0, col] = np.nan
# gc.collect()


# In[55]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[250]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[251]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[57]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[58]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[59]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[60]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[61]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
#         df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[286]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_month[(df_miss_month>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.0001].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[287]:


df_iv.loc[to_drop_iv,:].head()


# In[290]:


df_vars_list = pd.read_excel(r'行为特征变量清单.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[419]:


varsname_all = [col for col in varsname if col not in  [col for col in varsname if f'{col}'.startswith('uti')]]


# In[296]:


to_drop1 = [col for col in varsname if f'{col}'.startswith('uti')] + to_drop1


# In[297]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1)


# ## 3.2 基于相关性删除变量
# 

# In[298]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.0001, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[299]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[300]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")
print(varsname_v2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[301]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[302]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[303]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[304]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[305]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======第{i+1}个变量：{col}, 非空箱子个数：{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[306]:


new_bins_dict


# In[307]:


combiner.update(new_bins_dict)


# In[308]:


combiner.export()


# In[309]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[310]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[311]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[313]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)


# In[ ]:





# In[314]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[315]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[316]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[317]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # 使用更深的对色盲友好的颜色
 color_train = '#004494'  # 深蓝色
 color_oot = '#D66100'    # 深橙色

 # 绘制柱状图
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # 柱状图数据标签，字体颜色设为黑色
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # 折线图，使用更深的颜色和标记
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # 使用方形标记
 ax2.set_ylabel('Bad Rate')

 # 折线图数据标签，字体颜色设为黑色
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # 添加IV值
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # 调整图例位置
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # 调整图表布局，给顶部图例留出空间
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[318]:


df_vars_list = pd.read_excel(r'行为特征变量清单.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[319]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[320]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # 调用函数
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### 删除不稳定特征

# In[322]:


# drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
# drop_by_psi = drop_by_psi_month + drop_by_psi_set
drop_by_psi = drop_by_psi_set
print("drop_by_psi: ", len(drop_by_psi))

# df_iv_by_set.drop(columns=['mean','std','cv'], inplace=True)
# drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.0001].dropna(how='all').index)
# drop_by_iv = drop_by_iv_month + drop_by_iv_set
drop_by_iv = drop_by_iv_set
print("drop_by_iv: ", len(drop_by_iv))

to_drop3 = list(set(drop_by_psi + drop_by_iv))
print("剔除的变量有: ", len(to_drop3))


# In[323]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[324]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[325]:


to_drop3 = []


# In[326]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[327]:


target


# In[328]:


df_bins.shape
df_bins.head()


# In[329]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[330]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[331]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[337]:


df_sample_woe[varsname_v3].head()


# In[338]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[340]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[341]:


c.head()


# In[346]:


table_drop_ = ['znzz_fintech_ads.dim_pub_user_fd_tal_vars','znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n']
table_drop_ = list(df_vars_list.query("表名 in @table_drop_")['name'])


# In[348]:


len(table_drop_)


# In[349]:


table_drop = [col for col in df_corr_matrix if col in  table_drop]
table_drop


# In[350]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[351]:


df_high_corr


# In[352]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[353]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[355]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[356]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[357]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[358]:


to_drop5 


# In[ ]:





# In[359]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[360]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[361]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 数据预处理

# In[362]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']
df_sample['mob4dpd30'].value_counts()


# In[363]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[364]:


df_sample['data_set'].value_counts()


# In[365]:


# # 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[366]:


df_sample['channel_types'].value_counts()


# In[367]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[403]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.60


# In[386]:


print("最优参数opt_params: ", opt_params)


# In[370]:


print(len(varsname_v5))
print(varsname_v5)


# In[371]:


# varsname_base = varsname_v5[:]


# In[399]:


xx = ['au00dn_f','au00dn_p','au12odc_f','r03odsa','rnns1rpddnl','ap03odcap12odcr_f','ap00dn','ap00dn_f','rnnt1tubal','r03pall','r00ol01sedn','rnns0nplam','ad06odc','au06chlcau12chlcr','rnntalubcam','ap03lml','ap06odcap12odcr_f','rnnt1tucaubarm','rnnt1tufacfara','rnnt1tuam','au01odc_f','rnnt1tt1c','rnns1selamdim','r01palm','r02rnrm','ap12chlodcap12chlodcup_f','r24o00pals','rnns0rpddnl','rnns0c','r06palslwwlammr','ap03lmlap12lmlr','ap06lmsap12lmsr_p','r24palsl24lammr','rnns1c','r02ol03cr06ol03cr','rnns1rwlnpls','r02prdtpsdfl','l03laml','r01palsrwwsplmr','r06o00palsr12o00palsr','ap06odcap06odcr_p','au12odpr','rnnt1tucal','ap06chlcap06chlcr','ap12lms_f','ap12mnc','rnnt1tt0c','r03o00palsr06o00palsr','r24odss','r01s1prnodsm','rnntalucaubcarl','r01s1prnodsdnm','ap06lmmap12lmmr','ap03chlodcap12chlodcup_p','r01nplsr02nplsr','r03palsl03lammr','ap06lmlap12lmlr','r02o00palsr03o00palsr','ap06chlodcap12chlodcup_f','r01pr6odsl','r03pr6c','ap03dchlmodc_p','r12s1prnodsdnm','ap03chlodcap12chlodcup_f','au06odcau12odcr_p','rnnt1tufacfarl','ap02mnoddcm','r03prdtpsdf0c','ap03dchlmodc_f']
xxx = [col for col in xx if col not in df_sample.columns]
print(len(xxx))


# In[420]:


df_sample['data_set'].value_counts()


# In[421]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_all]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[422]:


df_sample['data_set'].value_counts()


# In[392]:


# 2 定义超参空间
# hp.quniform("参数名称",下界,上界,步长)-适用于离散均匀分布的浮点点数
# hp.uniform("参数名称",下界, 下界)-适用于连续随机分布的浮点数
# hp.randint("参数名称",上界)-适用于[0,上界)的整数,区间为左闭右开
# hp.choice("参数名称",["字符串1","字符串2",...])-适用于字符串类型,最优参数由索引表示
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# 可以根据需要，注释掉偏后的一些不太重要的超参

spaces = {
          # general parameters
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,5,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.6,1.0),
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.0,1.0),
          "lambda_l1": 0,
          "lambda_l2": 300,
          "early_stopping_rounds": 50
          }


# In[396]:


# 3，执行超参搜索
# 有了目标函数和参数空间,接下来要进行优化,需要了解以下参数:
# fmin:自定义使用的代理模型(参数algo),hyperopt支持如下搜索算法：
#       随机搜索(hyperopt.rand.suggest)
#       模拟退火(hyperopt.anneal.suggest)
#       TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）
# partial:修改算法涉及到的具体参数,包括模型具体使用了多少少个初始观测值(参数n_start_jobs),
#         以及在计算采集函数值时究竟考虑多少个样本(参数n_EI_candidates)
# trials:记录整个迭代过程,从hyperopt库中导入的方法Trials(),优化完成之后,
#        可以从保存好的trials中查看损失、参数等各种中间信息
# early_stop_fn:提前停止参数,从hyperopt库导入的方法no_progresss_loss(),可以输入具体的数字n,
#               表示当损失连续n次没有下降时,让算法提前停止
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=100):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    best_params['boosting'] = 'gbdt'
    best_params['objective'] = 'binary'
    best_params['metric'] = 'auc'
    best_params['num_leaves'] = int(best_params['num_leaves'])
    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])
    best_params['max_depth'] = int(best_params['max_depth'])
    best_params['bagging_freq'] = 1 
    best_params['early_stopping_rounds'] = 50
    best_params['scale_pos_weight'] = 1 
    best_params['seed'] = 1 
    print("最优参数", best_params)
    
    return (best_params, trials)


# In[415]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=10)


# In[416]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(best_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[423]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[417]:


# 优化后评估模型效果
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[382]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[401]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[418]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[410]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[424]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[402]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[391]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[285]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1


# In[196]:


# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2.2 base模型_剔除特征全为空值的样本

# In[191]:


df_sample_notna = df_sample.loc[~df_sample[varsname_base].isna().all(axis=1),:]
df_sample_notna.shape


# In[192]:


# 查看训练数据集
df_sample_notna['data_set'].value_counts()


# In[193]:


# 训练数据集
X_train = df_sample_notna.query("data_set in ('1_train')")[varsname_base]
y_train = df_sample_notna.query("data_set in ('1_train')")[modeltrian_target]

X_test = df_sample_notna.query("data_set in ('2_test')")[varsname_base]
y_test = df_sample_notna.query("data_set in ('2_test')")[modeltrian_target]


# In[194]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[195]:


# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[197]:


# 最初评估模型效果
df_sample_notna['y_prob_v2'] = lgb_model.predict(df_sample_notna[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_notna['y_prob_v2'].head()


# In[ ]:





# In[198]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample_notna, modeltrian_target, target, 'y_prob_v2', 'apply_month')
df_ks_auc_month_v2


# In[200]:


df_ks_auc_set_v2 = model_ks_auc(df_sample_notna, modeltrian_target, 'y_prob_v2', 'data_set')
df_ks_auc_set_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_notna, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
df_ks_auc_set_v2


# In[223]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model)
df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[224]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = pd.merge(df_vars_list.drop(columns='type'), df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[225]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# In[204]:


# 优化后评估模型效果
df_sample['y_prob_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v2'].head()


# In[206]:


df_ks_auc_month_v2_all = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v2', 'apply_month')
df_ks_auc_month_v2_all


# In[207]:


df_ks_auc_set_v2_all = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v2', 'data_set')
df_ks_auc_set_v2_all['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2_all = pd.concat([tmp, df_ks_auc_set_v2_all], axis=1)
df_ks_auc_set_v2_all


# In[256]:


with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_all_{timestamp}.xlsx') as writer:
    df_importance_month_v2_all.to_excel(writer, sheet_name='df_importance_month_v2_all')
    df_importance_set_v2_all.to_excel(writer, sheet_name='df_importance_set_v2_all')
    df_ks_auc_month_v2_all.to_excel(writer, sheet_name='df_ks_auc_month_v2_all')
    df_ks_auc_set_v2_all.to_excel(writer, sheet_name='df_ks_auc_set_v2_all')      
print(result_path + f'4_模型训练_{task_name}_v2_all_{timestamp}.xlsx')


# ### 5.2.3 base模型_调整训练样本

# In[227]:


df_sample_09 = df_sample.query("apply_date>='2024-09-01'")
df_sample_09.shape


# In[228]:


# 查看训练数据集
df_sample_09['data_set'].value_counts()


# In[230]:


toad.detect(df_sample_09[varsname_base])


# In[231]:


# 查看训练数据集
df_sample_09.loc[df_sample_09.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample_09['data_set'].value_counts()


# In[232]:


# 确定参数后，确定训练集和测试集

X_train_ = df_sample_09.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample_09.query("data_set not in ('3_oot')")[modeltrian_target]


X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample_09.loc[X_train.index, 'data_set']='1_train'
df_sample_09.loc[X_test.index, 'data_set']='2_test'


# In[234]:


print(df_sample_09['data_set'].value_counts())


# In[235]:



### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[236]:



# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[240]:


# 优化后评估模型效果
df_sample_09['y_prob_v3'] = lgb_model.predict(df_sample_09[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_09['y_prob_v3'].head()


# In[241]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample_09, modeltrian_target, target, 'y_prob_v3', 'apply_month')
df_ks_auc_month_v3


# In[242]:


df_ks_auc_set_v3 = model_ks_auc(df_sample_09, modeltrian_target, 'y_prob_v3', 'data_set')
df_ks_auc_set_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_09, target, 'data_set').set_index('bins')
df_ks_auc_set_v3 = pd.concat([tmp, df_ks_auc_set_v3], axis=1)
df_ks_auc_set_v3


# In[243]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = pd.merge(df_vars_list.drop(columns='type'), df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[244]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = pd.merge(df_vars_list.drop(columns='type'), df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[245]:


# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# ### 5.2.3 加入三方缓存数据

# In[504]:


df_sample_30 = df_sample.query("channel_id!=1").reset_index(drop=True)
df_sample_30.shape


# In[281]:


print(len(varsname_v5))
print(varsname_v5)


# In[505]:


varsname_three = ['duxiaoman_6', 'hengpu_4', 'aliyun_5', 'pudao_34', 'feicuifen', 'pudao_20',
                  'pudao_68', 'ruizhi_6', 'pudao_21']
                   
varsname_base_v3 = varsname_base_v2 + varsname_three
print(len(varsname_base_v3), varsname_base_v3)


# In[506]:


varsname_base_v3.remove('feicuifen')
varsname_base_v3.remove('pudao_20')
varsname_base_v3.remove('score_fpd10_v2')


# In[489]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[507]:


# 查看训练数据集
df_sample_30['data_set'].value_counts()


# In[52]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[508]:


# 查看训练数据集
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[509]:


# 查看训练数据集
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[53]:


# 训练数据集
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]

# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[510]:



# 训练数据集
X_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample_30.loc[X_train.index, 'data_set']='1_train'
df_sample_30.loc[X_test.index, 'data_set']='2_test'
print(df_sample_30['data_set'].value_counts())


# In[511]:



### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[512]:



# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[56]:



# 最初评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head() 


# In[289]:


# 最初评估模型效果
df_sample['y_prob_base2_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v3'].head()   


# In[513]:


# 最初评估模型效果
df_sample['y_prob_base2_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v3'].head()   


# In[57]:



# 最初评估模型效果 
df_ks_auc_month_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v3 = pd.concat([tmp, df_ks_auc_month_base_v3], axis=1)
print(df_ks_auc_month_base_v3)


df_ks_auc_set_base_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_base_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v3 = pd.concat([tmp, df_ks_auc_set_base_v3], axis=1)
print(df_ks_auc_set_base_v3)


# In[520]:


df_ks_auc_month_base2_v3 = calculate_ks_auc(df_sample, modeltrian_target,target,'y_prob_base2_v3','apply_month')


# In[515]:


modeltrian_target


# In[368]:


# 最初评估模型效果 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2', 'apply_month')
df_tmp_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[584]:


# 最初评估模型效果 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_tmp_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[502]:


# 最初评估模型效果 
df_tmp_base2_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_tmp_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


# In[ ]:


# 最初评估模型效果 
df_tmp_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_tmp_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_tmp_base2_v3 = pd.concat([tmp, df_tmp_base2_v3], axis=1)
print(df_tmp_base2_v3)


df_ks_auc_set_base2_v3 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_tmp_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v3 = pd.concat([tmp, df_ks_auc_set_base2_v3], axis=1)
print(df_ks_auc_set_base2_v3)


# In[585]:



# 最初评估模型效果 
df_ks_auc_month_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
df_ks_auc_month_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_30, target, 'apply_month').set_index('bins')
df_ks_auc_month_base2_v3 = pd.concat([tmp, df_ks_auc_month_base2_v3], axis=1)
print(df_ks_auc_month_base2_v3)


df_ks_auc_set_base2_v3 = model_ks_auc(df_sample_30, modeltrian_target, 'y_prob_base2_v3', 'data_set')
df_ks_auc_set_base2_v3['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_30, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v3 = pd.concat([tmp, df_ks_auc_set_base2_v3], axis=1)
print(df_ks_auc_set_base2_v3)


# In[58]:



# 最初评估模型效果 
df_ks_auc_month_base_v3_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v3', 'apply_month')
    tmp1['渠道'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v3_type = pd.concat([df_ks_auc_month_base_v3_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base_v3_type)


# 最初评估模型效果 
df_ks_auc_month_base_v3_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v3', 'apply_month')
    tmp1['渠道'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v3_rate = pd.concat([df_ks_auc_month_base_v3_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base_v3_rate)


# In[291]:




# 最初评估模型效果 
df_ks_auc_month_base2_v3_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
    tmp1['渠道'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v3_type = pd.concat([df_ks_auc_month_base2_v3_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base2_v3_type)


# 最初评估模型效果 
df_ks_auc_month_base2_v3_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v3', 'apply_month')
    tmp1['渠道'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v3_rate = pd.concat([df_ks_auc_month_base2_v3_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base2_v3_rate)


# In[59]:



# 合并
df_ks_auc_month_base_v3 = pd.concat([df_ks_auc_month_base_v3, df_ks_auc_month_base_v3_type, df_ks_auc_month_base_v3_rate])
df_ks_auc_month_base_v3.head()


# In[292]:


# 合并
df_ks_auc_month_base2_v3 = pd.concat([df_ks_auc_month_base2_v3, df_ks_auc_month_base2_v3_type, df_ks_auc_month_base2_v3_rate])
df_ks_auc_month_base2_v3.head()


# In[61]:



# 模型变量重要性
df_importance_base_v3 = feature_importance(lgb_model) 
df_importance_base_v3 = pd.merge(df_importance_base_v3, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base_v3 = df_importance_base_v3.reset_index()
df_importance_base_v3 = df_importance_base_v3.rename(columns={'index':'varsname'})
df_importance_base_v3


# In[521]:


# 模型变量重要性
df_importance_base2_v3 = feature_importance(lgb_model) 
df_importance_base2_v3 = pd.merge(df_importance_base2_v3, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base2_v3 = df_importance_base2_v3.reset_index()
df_importance_base2_v3 = df_importance_base2_v3.rename(columns={'index':'varsname'})
df_importance_base2_v3


# In[62]:


# 模型相关性
df_corr_base_v3 = df_corr_matrix.loc[varsname_base_v3, varsname_base_v3]
df_iv_base_v3 = df_iv_by_set.loc[varsname_base_v3,:]

df_corr_base_v3


# In[522]:


# 模型相关性
df_corr_base2_v3 = df_corr_matrix.loc[varsname_base_v3, varsname_base_v3]
df_iv_base2_v3 = df_iv_by_set.loc[varsname_base_v3,:]


# In[63]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base_v3_{timestamp}.xlsx') as writer:
    df_importance_base_v3.to_excel(writer, sheet_name='df_importance_base_v3')
    df_ks_auc_month_base_v3.to_excel(writer, sheet_name='df_ks_auc_month_base_v3')
    df_ks_auc_set_base_v3.to_excel(writer, sheet_name='df_ks_auc_set_base_v3')
    df_corr_base_v3.to_excel(writer, sheet_name='df_corr_base_v3')
    df_iv_base_v3.to_excel(writer, sheet_name='df_iv_base_v3')  

print(result_path + f'5_模型优化_{task_name}_base_v3_{timestamp}.xlsx')


# In[295]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base2_v3_{timestamp}.xlsx') as writer:
    df_importance_base2_v3.to_excel(writer, sheet_name='df_importance_base2_v3')
    df_ks_auc_month_base2_v3.to_excel(writer, sheet_name='df_ks_auc_month_base2_v3')
    df_ks_auc_set_base2_v3.to_excel(writer, sheet_name='df_ks_auc_set_base2_v3')
    df_corr_base2_v3.to_excel(writer, sheet_name='df_corr_base2_v3')
    df_iv_base2_v3.to_excel(writer, sheet_name='df_iv_base2_v3')  

print(result_path + f'5_模型优化_{task_name}_base2_v3_{timestamp}.xlsx')


# In[523]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base2_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base2_v3_{timestamp}.xlsx') as writer:
    df_importance_base2_v3.to_excel(writer, sheet_name='df_importance_base2_v3')
    df_ks_auc_month_base2_v3.to_excel(writer, sheet_name='df_ks_auc_month_base2_v3')
    df_ks_auc_set_base2_v3.to_excel(writer, sheet_name='df_ks_auc_set_base2_v3')
    df_corr_base2_v3.to_excel(writer, sheet_name='df_corr_base2_v3')
    df_iv_base2_v3.to_excel(writer, sheet_name='df_iv_base2_v3')  

print(result_path + f'5_模型优化_{task_name}_base2_v3_{timestamp}.xlsx')


# ### 5.2.4 加入融合模型子分

# In[296]:


print(len(varsname_v5))
print(varsname_v5)


# In[535]:


varsname_merge_score = ['all_a_app_free_fpd30_202502_s',
                   'hlv_d_holo_jk_certno_fpd1_score',
                   'hlv_d_holo_jk_certno_varcode_standard_bd0004',
                       'hlv_d_holo_jk_certno_score_fpd30_v1']
                   
varsname_base_v4 = varsname_base_v3 + varsname_merge_score
print(len(varsname_base_v4),varsname_base_v4)


# In[ ]:


score_fpd6_v1	0.000000	0	0.011333	0.018843	0.026104	0.02051	0.02822
24	score_fpd10_v1	0.000000	0	0.022598	0.020146	0.021562	0.0212	0.022973
25	hlv_d_holo_certno_variablecode_dpd30_6m_bd0001...	0.000000	0	0.12608	0.154373	0.154025	0.083876	0.079164
26	hlv_d_holo_certno_variablecode_dpd30_4m_bd0002...	0.000000	0	0.12311	0.167428	0.145912	0.098665	0.11505
27	hlv_d_holo_jk_certno_score_fpd30_v1	0.00


# In[536]:


varsname_base_v4.remove('score_fpd6_v1')
varsname_base_v4.remove('score_fpd10_v1')
varsname_base_v4.remove('hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard')
varsname_base_v4.remove('hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard')
varsname_base_v4.remove('hlv_d_holo_jk_certno_score_fpd30_v1')


# In[549]:



varsname_base_v4.remove('hlv_d_holo_certno_variablecode_standard_bd003')


# In[67]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[525]:


# 查看训练数据集
df_sample_30['data_set'].value_counts()


# In[68]:




# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[550]:


# 查看训练数据集
df_sample_30.loc[df_sample_30.query("data_set not in ('3_oot1', '3_oot2')").index, 'data_set']='1_train'
df_sample_30['data_set'].value_counts()


# In[69]:



# 训练数据集
X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(df_sample['data_set'].value_counts())


# In[551]:



# 训练数据集
X_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample_30.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
print(X_train_.shape)

# 确定参数后，确定训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape, X_test.shape)

df_sample_30.loc[X_train.index, 'data_set']='1_train'
df_sample_30.loc[X_test.index, 'data_set']='2_test'
print(df_sample_30['data_set'].value_counts())


# In[552]:



### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[553]:



# 6，训练/保存/评估模型
# 最初训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000, init_model=None)


# In[72]:


# 最初评估模型效果
df_sample['y_prob_base_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v4'].head()   


# In[554]:


# 最初评估模型效果
df_sample['y_prob_base2_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base2_v4'].head()   


# In[73]:



# 最初评估模型效果 
df_ks_auc_month_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v4', 'apply_month')
df_ks_auc_month_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base_v4 = pd.concat([tmp, df_ks_auc_month_base_v4], axis=1)
print(df_ks_auc_month_base_v4)


df_ks_auc_set_base_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v4', 'data_set')
df_ks_auc_set_base_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base_v4 = pd.concat([tmp, df_ks_auc_set_base_v4], axis=1)
print(df_ks_auc_set_base_v4)


# In[304]:



# 最初评估模型效果 
df_ks_auc_month_base2_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
df_ks_auc_month_base2_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'apply_month').set_index('bins')
df_ks_auc_month_base2_v4 = pd.concat([tmp, df_ks_auc_month_base2_v4], axis=1)
print(df_ks_auc_month_base2_v4)


df_ks_auc_set_base2_v4 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base2_v4', 'data_set')
df_ks_auc_set_base2_v4['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_base2_v4 = pd.concat([tmp, df_ks_auc_set_base2_v4], axis=1)
print(df_ks_auc_set_base2_v4)


# In[74]:



# 最初评估模型效果 
df_ks_auc_month_base_v4_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v4', 'apply_month')
    tmp1['渠道'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v4_type = pd.concat([df_ks_auc_month_base_v4_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base_v4_type)


# 最初评估模型效果 
df_ks_auc_month_base_v4_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base_v4', 'apply_month')
    tmp1['渠道'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base_v4_rate = pd.concat([df_ks_auc_month_base_v4_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base_v4_rate)


# In[305]:



# 最初评估模型效果 
df_ks_auc_month_base2_v4_type = pd.DataFrame()
for type_, tmp_df in df_sample.groupby('channel_types'):
    print(f'--------{type_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
    tmp1['渠道'] = type_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v4_type = pd.concat([df_ks_auc_month_base2_v4_type,pd.concat([tmp2, tmp1], axis=1)],axis=0)
    
print(df_ks_auc_month_base2_v4_type)


# 最初评估模型效果 
df_ks_auc_month_base2_v4_rate = pd.DataFrame()
for rate_, tmp_df in df_sample.groupby('channel_rates'):
    print(f'--------{rate_}----------')
    tmp1 = model_ks_auc(tmp_df, modeltrian_target, 'y_prob_base2_v4', 'apply_month')
    tmp1['渠道'] = rate_
    tmp2 = get_target_summary(tmp_df, target, 'apply_month').set_index('bins')
    df_ks_auc_month_base2_v4_rate = pd.concat([df_ks_auc_month_base2_v4_rate,pd.concat([tmp2, tmp1], axis=1)],axis=0)

print(df_ks_auc_month_base2_v4_rate)


# In[75]:


# 合并
df_ks_auc_month_base_v4 = pd.concat([df_ks_auc_month_base_v4, df_ks_auc_month_base_v4_type, df_ks_auc_month_base_v4_rate])
df_ks_auc_month_base_v4.head()


# In[306]:


# 合并
df_ks_auc_month_base2_v4 = pd.concat([df_ks_auc_month_base2_v4, df_ks_auc_month_base2_v4_type, df_ks_auc_month_base2_v4_rate])
df_ks_auc_month_base2_v4.head()


# In[555]:


df_ks_auc_month_base2_v4 = calculate_ks_auc(df_sample, modeltrian_target,target,'y_prob_base2_v4','apply_month')
df_ks_auc_month_base2_v4


# In[547]:


df_ks_auc_month_base2_v3


# In[544]:


# 模型变量重要性
df_importance_base_v4 = feature_importance(lgb_model) 
df_importance_base_v4 = pd.merge(df_importance_base_v4, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base_v4 = df_importance_base_v4.reset_index()
df_importance_base_v4 = df_importance_base_v4.rename(columns={'index':'varsname'})
df_importance_base_v4


# In[545]:


# 模型变量重要性
df_importance_base2_v4 = feature_importance(lgb_model) 
df_importance_base2_v4 = pd.merge(df_importance_base2_v4, df_iv_by_month, how='left', left_index=True,right_index=True)
df_importance_base2_v4 = df_importance_base2_v4.reset_index()
df_importance_base2_v4 = df_importance_base2_v4.rename(columns={'index':'varsname'})
df_importance_base2_v4


# In[546]:


# 模型相关性
df_corr_base_v4 = df_corr_matrix.loc[varsname_base_v4, varsname_base_v4]
df_iv_base_v4 = df_iv_by_set.loc[varsname_base_v4,:]


# In[308]:


# 模型相关性
df_corr_base2_v4 = df_corr_matrix.loc[varsname_base_v4, varsname_base_v4]
df_iv_base2_v4 = df_iv_by_set.loc[varsname_base_v4,:]


# In[78]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base_v4_{timestamp}.xlsx') as writer:
    df_importance_base_v4.to_excel(writer, sheet_name='df_importance_base_v4')
    df_ks_auc_month_base_v4.to_excel(writer, sheet_name='df_ks_auc_month_base_v4')
    df_ks_auc_set_base_v4.to_excel(writer, sheet_name='df_ks_auc_set_base_v4')
    df_corr_base_v4.to_excel(writer, sheet_name='df_corr_base_v4')
    df_iv_base_v4.to_excel(writer, sheet_name='df_iv_base_v4')  

print(result_path + f'5_模型优化_{task_name}_base_v4_{timestamp}.xlsx')


# In[309]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base2_v4_{timestamp}.xlsx') as writer:
    df_importance_base2_v4.to_excel(writer, sheet_name='df_importance_base2_v4')
    df_ks_auc_month_base2_v4.to_excel(writer, sheet_name='df_ks_auc_month_base2_v4')
    df_ks_auc_set_base2_v4.to_excel(writer, sheet_name='df_ks_auc_set_base2_v4')
    df_corr_base2_v4.to_excel(writer, sheet_name='df_corr_base2_v4')
    df_iv_base2_v4.to_excel(writer, sheet_name='df_iv_base2_v4')  

print(result_path + f'5_模型优化_{task_name}_base2_v4_{timestamp}.xlsx')


# In[548]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_base2_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_base2_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_base2_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'5_模型优化_{task_name}_base2_v4_{timestamp}.xlsx') as writer:
    df_importance_base2_v4.to_excel(writer, sheet_name='df_importance_base2_v4')
    df_ks_auc_month_base2_v4.to_excel(writer, sheet_name='df_ks_auc_month_base2_v4')
    df_ks_auc_set_base2_v4.to_excel(writer, sheet_name='df_ks_auc_set_base2_v4')
    df_corr_base2_v4.to_excel(writer, sheet_name='df_corr_base2_v4')
    df_iv_base2_v4.to_excel(writer, sheet_name='df_iv_base2_v4')  

print(result_path + f'5_模型优化_{task_name}_base2_v4_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[310]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[311]:


# base模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base1_20250304142048.pkl')
print(lgb_model.feature_name()==varsname_base)
df_sample['y_prob_base'] = lgb_model.predict(df_sample[varsname_base],num_iteration=lgb_model.best_iteration)


# In[557]:


# base2模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_20250304144553.pkl')
print(lgb_model.feature_name()==varsname_base)
df_sample['y_prob_base2'] = lgb_model.predict(df_sample[lgb_model.feature_name()],num_iteration=lgb_model.best_iteration)


# In[313]:


# base_v1模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base_v1_20250304160800.pkl')
print(lgb_model.feature_name()==varsname_base_v1)
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[varsname_base_v1],num_iteration=lgb_model.best_iteration)


# In[314]:


df_sample.info(show_counts=True)


# In[92]:


df_sample.loc[df_sample[varsname_base_v4].isna().all(axis=1),:]


# In[95]:


df_sample.loc[df_sample[varsname_base].isna().all(axis=1),:]['apply_month'].value_counts()


# In[97]:


df_sample.loc[df_sample[varsname_base].isna().all(axis=1),:]['y_prob_base'].value_counts()


# In[99]:


df_sample_c = pd.read_csv('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411/补充_建模数据集_250305.csv')
df_sample_c.info()


# In[558]:


df_evalue = pd.merge(df_sample, df_sample_c,how='left',on='order_no')
df_evalue.info(show_counts=True)


# ### 5.3.2 效果对比

# In[572]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[562]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['t_br_fpd','t_br_mob4','t_br2_fpd','t_br2_mob4','t_beha3_fpd','t_beha3_mob4','t_gen_fpd',
               't_gen_mob4','t_gen3_fpd','t_gen3_mob4','t_mix_low_fpd','t_mix_low_mob4','t_mix_nopboc_fpd',
               't_mix_nopboc_mob4','hlv_d_holo_certno_variablecode_standard_bd003',
               'hlv_d_holo_jk_certno_fpd1_score','hlv_d_holo_jk_certno_varcode_standard_bd0004',
               'all_a_app_free_fpd30_202502_s']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[563]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p',
               'all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p',
               'ypy_bhxz_a_fpd30_v1_prob_good','xz_fpd']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[564]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['m1a0022_g_p','m1a0023_g_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)


df_ksauc_all_3 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[565]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd',
               'all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[566]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['score_fpd0_v1','score_fpd6_v1','score_fpd10_v1','score_fpd10_v2','score_fpd30_v1',
               't_mix_pboc2_dpd20','t_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[567]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[323]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[324]:


df_sample.to_csv(result_path + r'提现全渠道无成本子分融合模型fpd30标签_2409_2411.csv',index=False)


# ## 5.4 特征变量贡献度

# ### 5.4.1 特征缺失时，KS值变化

# In[325]:


varsname_base_v3


# In[574]:


# base_v4模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****数据量：{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='桔子商城'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # 找到当前行的最小值
        min_value = row.min()
        # 找出所有等于最小值的列名
        min_cols = row[row == min_value].index.tolist()
        # 构建字典：列名为键，最小值为值
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# 输出结果
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_特征变量贡献度_特征缺失时.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_特征变量贡献度_特征缺失时.xlsx')


# ## 5.5 在授信场景的评估

# In[223]:


df_sample_auth_dict={}


# In[232]:



# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------离线模型子分-----------------
--贷中离线子分融合模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--授信全渠道行为特征模型fpd1标签_标准分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--贷中截面风险dpd30_6m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--贷中提现风险dpd30_4m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--贷中行为模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- 人行离线子分
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------三方缓存数据-----------------    
--近100天缓存三方评分数据
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------无成本或者低成本的实时数据-----------------   
--北京团队子分
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--百融子分
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--百融子分v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--百融子分v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--百融子分v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--授信百行洞见fpd30标签202502_好概率
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--续侦子分
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--授信全渠道无成本数据融合模型fp30标签_分数
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2模型打分 
# lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1模型打分 
lgb_model1= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx')


# # 6. 评分分布

# In[65]:


df_sample['apply_month'].value_counts()


# In[66]:


score = 'y_prob_base2'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample_30.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-08'"), 
                                                [score], 'apply_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: 授信全渠到行为数据模型四期标签v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[3]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签v2'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result_v2'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[4]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[5]:


df_sample_dict = {}


# In[6]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[7]:


print(len(table_name_list))


# In[8]:


df_target = pd.read_csv('./result/00_model_target.csv')
df_target.info(show_counts=True)


# In[9]:


df_target['mob4dpd30'].value_counts()


# In[10]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv')
df_feature1.info()


# In[13]:


target = 'mob4dpd30'


# In[20]:


df_target['data_set'].value_counts()


# In[23]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv')
df1 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature1, how='left', on='order_no')
print(df1.shape)

df1_selected, df1_dropped = toad.selection.select(df1,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.85, 
                                                    return_drop=True, exclude=['order_no'])
print(df1_selected.shape)
df1_selected.drop(columns=['mob4dpd30'],inplace=True)
df1_drop_cols = []
for k, v in df1_dropped.items():
    print(k, ":", len(v))
    df1_drop_cols.extend(list(v))
df1_drop_cols = list(set(df1_drop_cols))
print(len(df1_drop_cols))


# In[17]:


df1_drop_cols = []
for k, v in df1_dropped.items():
    print(k, ":", len(v))
    df1_drop_cols.extend(list(v))
df1_drop_cols = list(set(df1_drop_cols))
print(len(df1_drop_cols))


# In[26]:


df_feature2 = pd.read_csv('./result/02_behave_features_v2.csv')
df2 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature2, how='left', on='order_no')
print(df2.shape)

df2_selected, df2_dropped = toad.selection.select(df2,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df2_selected.shape)
df2_selected.drop(columns=['mob4dpd30'],inplace=True)
df2_drop_cols = []
for k, v in df2_dropped.items():
    print(k, ":", len(v))
    df2_drop_cols.extend(list(v))
df2_drop_cols = list(set(df2_drop_cols))
print(len(df2_drop_cols))


# In[27]:


df_feature3 = pd.read_csv('./result/03_behave_features_v2.csv')
df3 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature3, how='left', on='order_no')
print(df3.shape)

df3_selected, df3_dropped = toad.selection.select(df3,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df3_selected.shape)
df3_selected.drop(columns=['mob4dpd30'],inplace=True)
df3_drop_cols = []
for k, v in df3_dropped.items():
    print(k, ":", len(v))
    df3_drop_cols.extend(list(v))
df3_drop_cols = list(set(df3_drop_cols))
print(len(df3_drop_cols))


# In[28]:


df_feature4 = pd.read_csv('./result/04_behave_features_v2.csv')
df4 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature4, how='left', on='order_no')
print(df4.shape)

df4_selected, df4_dropped = toad.selection.select(df4,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df4_selected.shape)
df4_selected.drop(columns=['mob4dpd30'],inplace=True)
df4_drop_cols = []
for k, v in df4_dropped.items():
    print(k, ":", len(v))
    df4_drop_cols.extend(list(v))
df4_drop_cols = list(set(df4_drop_cols))
print(len(df4_drop_cols))


# In[29]:


del df_feature1,df_feature2,df_feature3,df_feature4
gc.collect()


# In[32]:


usecols = list(set(df1_selected.columns) | set(df2_selected.columns) | set(df3_selected.columns) | set(df4_selected.columns))
print(usecols)


# In[33]:


usecols.remove('order_no')
print(len(usecols))


# In[34]:


df_feature5 = pd.read_csv('./result/05_behave_features_v3.csv')
df5 = pd.merge(df_target.query("data_set=='1_train'")[['order_no','mob4dpd30']], df_feature5, how='left', on='order_no')
print(df5.shape)

df5_selected, df5_dropped = toad.selection.select(df5,
                                                    target=target, 
                                                    empty=0.90, iv=0.01, corr=0.95, 
                                                    return_drop=True, exclude=['order_no'])
print(df5_selected.shape)
df5_selected.drop(columns=['mob4dpd30'],inplace=True)
df5_drop_cols = []
for k, v in df5_dropped.items():
    print(k, ":", len(v))
    df5_drop_cols.extend(list(v))
df5_drop_cols = list(set(df5_drop_cols))
print(len(df5_drop_cols))


# In[46]:


usecols = usecols+df5_selected.columns.to_list()
usecols.remove('order_no')
print(len(usecols))
print(usecols)


# In[49]:


df_feature1 = pd.read_csv('./result/01_behave_features_v2.csv',usecols=df1_selected.columns.to_list())
df_feature1 = df_feature1.set_index('order_no')
print(df_feature1.shape)

df_feature2 = pd.read_csv('./result/02_behave_features_v2.csv',usecols=df2_selected.columns.to_list())
df_feature2 = df_feature2.set_index('order_no')
print(df_feature2.shape)

df_feature3 = pd.read_csv('./result/03_behave_features_v2.csv',usecols=df3_selected.columns.to_list())
df_feature3 = df_feature3.set_index('order_no')
print(df_feature3.shape)

df_feature4 = pd.read_csv('./result/04_behave_features_v2.csv',usecols=df4_selected.columns.to_list())
df_feature4 = df_feature4.set_index('order_no')
print(df_feature4.shape)

df_feature5 = pd.read_csv('./result/05_behave_features_v3.csv',usecols=df5_selected.columns.to_list())
df_feature5 = df_feature5.set_index('order_no')
print(df_feature5.shape)


# In[50]:


df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5],axis=1)
df_feature = df_feature.reset_index()
df_feature.shape


# In[51]:


df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
df_sample.info(show_counts=True)
df_sample.head()


# In[52]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[53]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[54]:


df_sample.loc[df_sample[usecols].isna().all(axis=1), :].shape


# In[55]:


df_sample_copy = df_sample.copy()


# In[56]:


df_sample.drop(index=df_sample[df_sample[usecols].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# In[59]:


df_sample[target].value_counts(normalize=True)


# In[60]:


df_sample[target].value_counts()


# In[61]:


varsname = df_sample.columns.to_list()[22:]

print("初始特征变量个数：",len(varsname))

print(varsname[:5], varsname[-5:])


# In[63]:


df_sample.info(show_counts=True)


# In[64]:


df_sample.to_csv(result_path + '授信全渠到行为数据模型四期标签_250325_notna.csv',index=False)
print(result_path + '授信全渠到行为数据模型四期标签_250325_notna.csv')


# # 1. 样本概况

# In[65]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # 返回结果
    return result


# In[66]:


target = 'mob4dpd30'


# In[67]:


print(df_sample[target].value_counts())


# In[68]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[69]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[70]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[73]:


df_vars_list = pd.read_excel(r'行为特征变量清单.xlsx')
df_vars_list.info()
df_vars_list.head(2)


# In[74]:


[col for col in varsname if col in df_vars_list.query("表名=='znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n'")['name']]


# In[75]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# In[76]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[77]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[78]:


df_explor.head()


# In[79]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[80]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[81]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[82]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[83]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[84]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[88]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[df_miss_set['1_train']>=0.90].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[90]:


df_iv.loc[to_drop_iv,:]


# In[91]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1)


# ## 3.2 基于相关性删除变量
# 

# In[92]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[93]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[94]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")
print(varsname_v2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[95]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col, combiner):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[96]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[97]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[98]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[99]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======第{i+1}个变量：{col}, 非空箱子个数：{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[100]:


new_bins_dict


# In[101]:


combiner.update(new_bins_dict)


# In[102]:


combiner.export()


# In[103]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[104]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[105]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[106]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month', combiner)

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set', combiner)


# In[ ]:





# In[107]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[108]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[109]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[110]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # 使用更深的对色盲友好的颜色
 color_train = '#004494'  # 深蓝色
 color_oot = '#D66100'    # 深橙色

 # 绘制柱状图
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # 柱状图数据标签，字体颜色设为黑色
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # 折线图，使用更深的颜色和标记
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # 使用方形标记
 ax2.set_ylabel('Bad Rate')

 # 折线图数据标签，字体颜色设为黑色
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # 添加IV值
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # 调整图例位置
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # 调整图表布局，给顶部图例留出空间
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[111]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[112]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # 调用函数
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### 删除不稳定特征

# In[115]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))

drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[118]:


df_psi_by_month.loc[drop_by_psi_month,:]


# In[120]:


df_iv_by_month.loc[drop_by_iv_month,:]


# In[123]:


to_drop3 = list(df_iv_by_month[df_iv_by_month["2024-11"]<0.01].index)


# In[124]:


to_drop3.remove('acccagel')
to_drop3.append('r06spll')
to_drop3.append('r02odsmr06odsmr')


# In[126]:


[col for col in varsname_v2 if col in list(df_vars_list.query("表名=='znzz_fintech_ads.dim_pub_user_fd_t1f_vars'")['name'])]


# In[127]:


print("剔除的变量有: ", len(to_drop3))


# In[128]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[129]:


target


# In[130]:


df_bins.shape
df_bins.head()


# In[131]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[132]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[133]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[134]:


df_sample_woe.head()


# In[135]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[136]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[137]:


df_corr_matrix.head()


# In[138]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[139]:


df_high_corr


# In[140]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[141]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[142]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[143]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[144]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[145]:


to_drop5


# In[146]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[147]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[148]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 数据预处理

# In[149]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']


# In[150]:


df_sample['mob4dpd30'].value_counts()


# In[151]:


df_sample['mob4dpd30_1'].value_counts()


# In[152]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[153]:


df_sample['data_set'].value_counts()


# In[154]:


# # 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[155]:


df_sample['channel_types'].value_counts()


# In[156]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[157]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.60


# In[158]:


print("最优参数opt_params: ", opt_params)


# In[159]:


print(len(varsname_v5))
print(varsname_v5)


# In[160]:


varsname_base = varsname_v5[:]


# In[161]:


df_sample['data_set'].value_counts()


# In[162]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[163]:


df_sample['data_set'].value_counts()


# In[164]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[165]:


# 优化后评估模型效果
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[166]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[167]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[168]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1


# In[169]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1


# In[170]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[171]:


df_sample_copy.shape


# In[172]:


# 优化后评估模型效果
df_sample_copy['y_prob_all'] = lgb_model.predict(df_sample_copy[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_copy['y_prob_all'].head()


# In[175]:


df_sample_copy['mob4dpd30_1'] = 1 - df_sample_copy['mob4dpd30']


# In[176]:


df_ks_auc_month_all = calculate_ks_auc(df_sample_copy, modeltrian_target, target, 'y_prob_all', 'apply_month')
df_ks_auc_month_all


# In[177]:


df_ks_auc_set_all = model_ks_auc(df_sample_copy, modeltrian_target, 'y_prob_all', 'data_set')
df_ks_auc_set_all['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_copy, target, 'data_set').set_index('bins')
df_ks_auc_set_all = pd.concat([tmp, df_ks_auc_set_all], axis=1)
df_ks_auc_set_all


# In[178]:


# 效果评估后保存模型
# timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_all_{timestamp}.xlsx') as writer:
    df_ks_auc_month_all.to_excel(writer, sheet_name='df_ks_auc_month_all')
    df_ks_auc_set_all.to_excel(writer, sheet_name='df_ks_auc_set_all')      
print(result_path + f'4_模型训练_{task_name}_all_{timestamp}.xlsx')
print(f"模型保存完成！：{timestamp}")


# In[ ]:





# ## 5.3 模型效果对比

# In[179]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[182]:


data_dict = {}


# In[186]:



# 计算今天的时间
today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2024-09-16', '%Y-%m-%d')
end_day = datetime.strptime('2024-07-21', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
 t.order_no
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003

from 
    (
    select * 
    from znzz_fintech_ads.dm_f_lxl_test_auth_Y_target_2502 as t 
    where dt = '2025-03-24'
      and apply_date= '{run_day}'
    ) as t 
------------------离线模型子分-----------------
--贷中截面风险dpd30_6m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--贷中提现风险dpd30_4m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--贷中行为模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

'''
    print(f'=========================={run_day}=============================')
    data_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[187]:


df_compare = pd.concat(data_dict.values(), ignore_index=True)
df_compare.to_csv(result_path + 'df_compare_data.csv')


# In[188]:


df_compare.info(show_counts=True)


# In[ ]:


df_compare.drop(columns=['hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard'],inplace=True)


# In[215]:


data_dict.keys()


# In[196]:


df_evalue = pd.merge(df_sample_copy, df_compare, how='left', on='order_no')
df_evalue.info(show_counts=True)


# In[197]:


df_evalue['fpd30_1'] = 1 - df_evalue['fpd30']
print(df_evalue['fpd30'].value_counts(),df_evalue['fpd30_1'].value_counts())


# ### 5.3.2 效果对比

# In[206]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[ ]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30', 'mob4dpd30']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

for label_ in target_list:
    label_1 = f'{label_}_'
    for score_ in score_list:
        df1 = calculate_ks_auc(tmp_df_evalue, label_1, label_, score_, 'apply_month')


# In[207]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list))
df_evalue[score_list].info(show_counts=True)


# In[208]:


# df_evalue['hlv_d_holo_certno_variablecode_standard_bd003'] = pd.to_numeric(df_evalue['hlv_d_holo_certno_variablecode_standard_bd003'])


# In[214]:


tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
print(tmp_df_evalue.shape)
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)


# In[209]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys1 = ['apply_month']
df_ksauc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_v1.insert(loc=0, column='channel', value='全渠道')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_1 = pd.concat([df_ksauc_v1, df_ksauc_v2,df_ksauc_v4], axis=0)
df_ksauc_1


# In[210]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard',
              'hlv_d_holo_certno_variablecode_standard_bd003']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[211]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_1.to_excel(writer, sheet_name='df_ksauc_notna')
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[213]:


df_sample_copy.to_csv(result_path + r'授信全渠到行为数据模型四期标签v2.csv',index=False)
print(result_path + r'授信全渠到行为数据模型四期标签v2.csv')


# ## 5.4 特征变量贡献度

# ### 5.4.1 特征缺失时，KS值变化

# In[325]:


varsname_base_v3


# In[574]:


# base_v4模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****数据量：{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='桔子商城'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # 找到当前行的最小值
        min_value = row.min()
        # 找出所有等于最小值的列名
        min_cols = row[row == min_value].index.tolist()
        # 构建字典：列名为键，最小值为值
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# 输出结果
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_特征变量贡献度_特征缺失时.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_特征变量贡献度_特征缺失时.xlsx')


# ## 5.5 在授信场景的评估

# In[223]:


df_sample_auth_dict={}


# In[232]:



# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------离线模型子分-----------------
--贷中离线子分融合模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--授信全渠道行为特征模型fpd1标签_标准分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--贷中截面风险dpd30_6m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--贷中提现风险dpd30_4m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--贷中行为模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- 人行离线子分
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------三方缓存数据-----------------    
--近100天缓存三方评分数据
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------无成本或者低成本的实时数据-----------------   
--北京团队子分
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--百融子分
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--百融子分v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--百融子分v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--百融子分v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--授信百行洞见fpd30标签202502_好概率
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--续侦子分
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--授信全渠道无成本数据融合模型fp30标签_分数
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2模型打分 
# lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1模型打分 
lgb_model1= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx')


# # 6. 评分分布

# In[216]:


df_sample['apply_month'].value_counts()


# In[218]:


score = 'y_prob_v1'
flag = 'mob4dpd30_1'


# In[219]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, flag]], y=flag, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[220]:


df_sample['score_bins'].head()


# In[221]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[222]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[223]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[224]:


score_group_by_dataset.head()


# In[225]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: 授信全渠到行为数据模型四期标签v3.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime, timedelta, date
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道行为数据模型四期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


# df_sample_dict = {}


# In[5]:



table_name_list = [
# old
'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn06'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_Mn12'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part4'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part6'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_6m'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_1n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_2n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_his'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_3n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_5n'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_R0'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_var_R1'
,'znzz_fintech_ads.dim_pub_user_fd_tal_vars'
,'znzz_fintech_ads.dim_pub_user_fd_id_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1t_vars'
,'znzz_fintech_ads.dim_pub_user_fd_t1f_vars'
,'znzz_fintech_ads.dwd_order_custom_ticket_cs_fd_vars'
,'znzz_fintech_ads.dwd_afterloan_repay_base_fd_vars'
,'znzz_fintech_ads.dwd_beforeloan_order_examine_fd_var_part7'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Y1'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part2'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part3'

,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_part5'
,'znzz_fintech_ads.dwd_beforeloan_auth_examine_fd_var_Mn03'
,'znzz_fintech_ads.dim_pub_user_fd_addr_vars'
,'znzz_fintech_ads.dwd_auth_examine_fd_gps_vars'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn01'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn02'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn03'
,'znzz_fintech_ads.dwd_beforeloan_admission_rules_id_var_Mn06'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_od'
,'znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_fk'

# new
,'znzz_fintech_ads.dwd_auth_examine_fd_gpsic_vars'
,'znzz_fintech_ads.dim_pub_user_fd_adim_vars'
,'znzz_fintech_ads.llji_id_var_order_his_fd'
,'znzz_fintech_ads.llji_id_var_settle_his_fd'
,'znzz_fintech_ads.llji_id_var_overdue_his_fd'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_vars_add'
,'znzz_fintech_ads.llji_user_var_credit_uti_plus'
,'znzz_fintech_ads.dwd_afterloan_repay_follow_record_fd_vars'
,'znzz_fintech_ads.dim_channel_capital_jk_vars'
,'znzz_fintech_ads.dwd_cap_repay_plan_fd_chan_id_vars'    
]


# In[6]:


print(len(table_name_list))


# In[7]:


# df_target = pd.read_csv(result_path + '00_model_target.csv')
# df_target.info()


# In[8]:


# df_feature1 = pd.read_csv(result_path + '01_behave_features.csv')
# df_feature1 = df_feature1.set_index('order_no')

# df_feature2 = pd.read_csv(result_path + '02_behave_features.csv')
# df_feature2 = df_feature2.set_index('order_no')

# df_feature3 = pd.read_csv(result_path + '03_behave_features.csv')
# df_feature3 = df_feature3.set_index('order_no')

# df_feature4 = pd.read_csv(result_path + '04_behave_features.csv')
# df_feature4 = df_feature4.set_index('order_no')

# df_feature5 = pd.read_csv(result_path + '05_behave_features.csv')
# df_feature5 = df_feature5.set_index('order_no')

# df_feature6 = pd.read_csv(result_path + '06_behave_features.csv')
# df_feature6 = df_feature6.set_index('order_no')


# In[9]:


# df_feature = pd.concat([df_feature1,df_feature2,df_feature3,df_feature4,df_feature5,df_feature6],axis=1)
# df_feature = df_feature.reset_index()
# df_feature.shape


# In[10]:


# df_target = df_target.reset_index()
# df_sample = pd.merge(df_target, df_feature, how='left', on='order_no')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[11]:


df_sample = pd.read_csv(result_path + '授信全渠到行为数据模型四期标签.csv')
df_sample.info(show_counts=True)
df_sample.head()


# In[12]:


print(df_sample.shape[0], df_sample['order_no'].nunique(), df_sample['user_id'].nunique())


# In[13]:


print(df_sample['apply_date'].min(), df_sample['apply_date'].max())


# In[14]:


varsname = df_sample.columns.to_list()[22:]
varsname = list(filter(lambda col: not col.startswith('uti'), varsname))

print("初始特征变量个数：",len(varsname))

print(varsname[:5], varsname[-5:])


# In[15]:


pd.set_option('display.max_columns',None)


# In[16]:


# df_sample.to_csv(result_path + '授信全渠到行为数据模型四期标签.csv',index=False)
# print(result_path + '授信全渠到行为数据模型四期标签.csv')


# In[17]:


df_sample[varsname].info()


# In[18]:


df_sample.loc[df_sample[varsname].isna().all(axis=1), :].shape


# In[19]:


df_sample_copy = df_sample.copy()


# In[20]:


df_sample.drop(index=df_sample[df_sample[varsname].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# # 1. 样本概况

# In[21]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 计算整个 DataFrame 的聚合统计量
#     total_summary = df[target].agg(total=lambda x: len(x), 
#             bad=lambda x: x.sum(), 
#             good=lambda x: (x== 0).sum(), 
#             bad_rate=lambda x: x.mean()).to_frame().T
#     total_summary[groupby_col] = 'Total'
    
    # 将汇总行添加到分组结果中
#     result = pd.concat([grouped, total_summary], ignore_index=True)
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    
    # 返回结果
    return result


# In[22]:


target = 'mob4dpd30'


# In[23]:


print(df_sample[target].value_counts())


# In[24]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[25]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[26]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[27]:


df_vars_list = pd.read_excel('行为特征变量清单.xlsx')
df_vars_list.drop(columns=['type'],inplace=True)


# In[28]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# In[29]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[30]:


df_explor = pd.merge(df_vars_list.set_index('name'), df_explor, how='right',left_index=True,right_index=True)


# In[31]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_数据探索性分析_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[32]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[33]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[34]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[35]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索统计分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[37]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[df_miss_set['1_train']>=0.90].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor[df_explor.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor[df_explor.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor[df_explor.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor[df_explor.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[38]:


df_iv.loc[to_drop_iv,:].head()


# In[39]:


# varsname_all = [col for col in varsname if col not in  [col for col in varsname if f'{col}'.startswith('uti')]]


# In[40]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1)


# ## 3.2 基于相关性删除变量
# 

# In[42]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[43]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[44]:


df_vars_list[df_vars_list['name'].isin(to_drop2)]


# In[45]:


to_drop2=[]


# In[46]:


varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")
print(varsname_v2)


# In[47]:


df_sample[df_sample[varsname_v2].isna().all(axis=1)].shape


# In[48]:


df_sample.drop(index=df_sample[df_sample[varsname_v2].isna().all(axis=1)].index,inplace=True)
df_sample = df_sample.reset_index(drop=True)
df_sample.shape


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[49]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[50]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[51]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[52]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[53]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    print(f"======第{i+1}个变量：{col}, 非空箱子个数：{len(not_empty)}=========")
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[54]:


new_bins_dict


# In[55]:


combiner.update(new_bins_dict)


# In[56]:


combiner.export()


# In[57]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[58]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)

df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)


# In[59]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[60]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')


# In[ ]:





# In[61]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   


# In[62]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print(df_total_bad.head())
print(pivot_df_iv.head())


# In[63]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))


# In[64]:



def plot_combined_chart(df,varsname,var_des,bins_col,totalpct_train,                     totalpct_oot,badrate_train, badrate_oot,iv_train, iv_oot,                         filename="../SourceHanSansSC-Bold.otf"):
 import matplotlib
 # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
 zhfont1 = matplotlib.font_manager.FontProperties(fname=filename) 
 fig, ax1 = plt.subplots(figsize=(14, 7))

 bar_width = 0.35
 index = np.arange(len(df))

 # 使用更深的对色盲友好的颜色
 color_train = '#004494'  # 深蓝色
 color_oot = '#D66100'    # 深橙色

 # 绘制柱状图
 bars1 = ax1.bar(index, df[totalpct_train], bar_width, label=f'Total Pct Train',
                 color=color_train, alpha=0.6)
 bars2 = ax1.bar(index + bar_width, df[totalpct_oot], bar_width, label=f'Total Pct OOT',
                 color=color_oot, alpha=0.6)

 # 柱状图数据标签，字体颜色设为黑色
 for bar in bars1:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for bar in bars2:
     height = bar.get_height()
     ax1.text(bar.get_x() + bar.get_width()/2., height,
              f'{height:.2%}', ha='center', va='bottom', fontsize=9, color='black')



 ax1.set_ylabel('Percentage')
 ax1.set_title(f'Distribution and Bad Rate of {varsname}  {var_des}',fontproperties=zhfont1)
 ax1.set_xticks(index + bar_width / 2)
 ax1.set_xticklabels(df[bins_col], rotation=45, ha='right')

 ax2 = ax1.twinx()
 
 # 折线图，使用更深的颜色和标记
 data_train = df[badrate_train].to_numpy()
 line1, = ax2.plot(index + bar_width / 2, data_train, color=color_train, marker='o',
                   linestyle='-', label=f'Bad Rate Train')
 
 data_oot = df[badrate_oot].to_numpy()
 line2, = ax2.plot(index + bar_width / 2, data_oot, color=color_oot, marker='s',
                   linestyle='--', label=f'Bad Rate OOT')  # 使用方形标记
 ax2.set_ylabel('Bad Rate')

 # 折线图数据标签，字体颜色设为黑色
 for x, y in zip(index + bar_width / 2, df[badrate_train]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 for x, y in zip(index + bar_width / 2, df[badrate_oot]):
     ax2.text(x, y, f'{y:.2%}', ha='center', va='bottom', fontsize=9, color='black')
 # 添加IV值
 ax1.text(0.05, 0.90, f'Train IV: {iv_train}\nOOT IV: {iv_oot}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top', 
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.4))
 # 调整图例位置
 lines, labels = ax1.get_legend_handles_labels()
 lines2, labels2 = ax2.get_legend_handles_labels()
 ax2.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=2, frameon=False)
 plt.tight_layout(rect=[0, 0, 1, 0.95])  # 调整图表布局，给顶部图例留出空间
#     plt.savefig(f'{varsname}.png',dpi=300, bbox_inches='tight', pad_inches=0.1)
 plt.show()


# In[65]:


# df_vars_list = pd.read_excel(r'行为特征变量清单.xlsx')
# df_vars_list.drop(columns=['type'],inplace=True)
# df_vars_list.info()
# df_vars_list.head(2)


# In[66]:


name_comment_dict = df_vars_list.set_index('name')['comment'].to_dict()


# In[67]:



for col in varsname_v2:
    df_train_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='1_train'")
    df_train_tmp = df_train_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_oot_tmp = df_group_set.query("varsname==@col & bins!='Total' & groupvars=='3_oot'")
    df_oot_tmp = df_oot_tmp[['varsname','bins','total_pct','bad_rate']]
    
    df_pct_bad = pd.merge(df_train_tmp,df_oot_tmp,how='inner',on=['varsname','bins'],suffixes=('_train','_oot'))
    df_pct_bad = df_pct_bad[['varsname','bins','total_pct_train','total_pct_oot','bad_rate_train','bad_rate_oot']]
    try:
        var_des = name_comment_dict[col]
    except Exception as e:
        print(f"Error converting value for feature {col}: {e}")
        var_des = col
    
    df_tmp = df_group_set.query("varsname==@col & bins=='Total'")
    df_tmp = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    iv_train = round(df_tmp.loc[0,'1_train'],3)
    iv_oot = round(df_tmp.loc[0,'3_oot'],3)
    # 调用函数
    plot_combined_chart(df_pct_bad,col,var_des,'bins','total_pct_train','total_pct_oot',
                        'bad_rate_train','bad_rate_oot',iv_train, iv_oot,
                       filename="SourceHanSansSC-Bold.otf")


# ### 删除不稳定特征

# In[68]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))

drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[69]:


df_psi_by_month.loc[drop_by_psi_month,:]


# In[70]:


df_iv_by_month.loc[drop_by_iv_month,:]


# In[71]:


to_drop3 = list(df_iv_by_month[df_iv_by_month["2024-11"]<0.01].index)


# In[72]:


to_drop3.remove('acccagel')


# In[75]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[76]:


target


# In[77]:


df_bins.shape
df_bins.head()


# In[78]:



def calculate_woe(df, col, target):
    """
    计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
    :param df: DataFrame 包含分箱和目标变量
    :param binned_col: 分箱变量名
    :param target_col: 目标变量名
    :return: WOE值的字典
    """
    regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
    regroup['good'] = regroup['total'] - regroup['bad']
    regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
    regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
    regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

    return regroup['woe'].to_dict()   


# In[79]:


# for var in varsname_v3[20:]:
#     print(f"------{var}-------")
#     woe_dict = calculate_woe(df_bins, var, target)
#     df_sample_30_woe[var] = df_sample_30_woe[var].map(woe_dict)

# df_sample_30_woe.head()


# In[80]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
df_sample_woe.shape


# In[81]:


df_sample_woe[varsname_v3].head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[118]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[119]:


df_corr_matrix.head()


# In[120]:


table_drop_ = ['znzz_fintech_ads.dim_pub_user_fd_t1f_vars','znzz_fintech_ads.dwd_order_life_cycle_info_fd_var_rp_1n']
table_drop_ = list(df_vars_list.query("表名 in @table_drop_")['name'])


# In[121]:


len(table_drop_)


# In[122]:


table_drop = [col for col in df_corr_matrix.index if col in  table_drop_]
table_drop


# In[123]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.85)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[124]:


df_high_corr


# In[125]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[126]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[127]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[128]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[129]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[130]:


to_drop5 


# In[131]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[132]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[133]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


# ## 5.1 数据预处理

# In[134]:


df_sample['mob4dpd30_1'] = 1 - df_sample['mob4dpd30']
df_sample['mob4dpd30'].value_counts()


# In[135]:


modeltrian_target = 'mob4dpd30_1'
target = 'mob4dpd30'


# In[136]:


df_sample['data_set'].value_counts()


# In[137]:


# # 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[138]:


df_sample['channel_types'].value_counts()


# In[139]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[140]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.60


# In[141]:


print("最优参数opt_params: ", opt_params)


# In[142]:


print(len(varsname_v5))
print(varsname_v5)


# In[143]:


varsname_base = varsname_v5[:]


# In[144]:


df_sample['data_set'].value_counts()


# In[145]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[146]:


df_sample['data_set'].value_counts()


# In[392]:


# 2 定义超参空间
# hp.quniform("参数名称",下界,上界,步长)-适用于离散均匀分布的浮点点数
# hp.uniform("参数名称",下界, 下界)-适用于连续随机分布的浮点数
# hp.randint("参数名称",上界)-适用于[0,上界)的整数,区间为左闭右开
# hp.choice("参数名称",["字符串1","字符串2",...])-适用于字符串类型,最优参数由索引表示
# hp.loguniform: continuous log uniform (floats spaced evenly on a log scale)
# choice : categorical variables
# quniform : discrete uniform (integers spaced evenly)
# uniform: continuous uniform (floats spaced evenly)
# loguniform: continuous log uniform (floats spaced evenly on a log scale)
# 可以根据需要，注释掉偏后的一些不太重要的超参

spaces = {
          # general parameters
          "learning_rate":0.1,
          # tuning parameters
          "num_leaves":hp.quniform("num_leaves",20,150,1),
          "max_depth":hp.quniform("max_depth",2,5,1),
          "min_data_in_leaf":hp.quniform("min_data_in_leaf",20,150,1),
          "feature_fraction":hp.uniform("feature_fraction",0.6,1.0),
          "bagging_fraction":hp.uniform("bagging_fraction",0.6,1.0),
          "min_gain_to_split":hp.uniform("min_gain_to_split",0.0,1.0),
          "lambda_l1": 0,
          "lambda_l2": 300,
          "early_stopping_rounds": 50
          }


# In[396]:


# 3，执行超参搜索
# 有了目标函数和参数空间,接下来要进行优化,需要了解以下参数:
# fmin:自定义使用的代理模型(参数algo),hyperopt支持如下搜索算法：
#       随机搜索(hyperopt.rand.suggest)
#       模拟退火(hyperopt.anneal.suggest)
#       TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）
# partial:修改算法涉及到的具体参数,包括模型具体使用了多少少个初始观测值(参数n_start_jobs),
#         以及在计算采集函数值时究竟考虑多少个样本(参数n_EI_candidates)
# trials:记录整个迭代过程,从hyperopt库中导入的方法Trials(),优化完成之后,
#        可以从保存好的trials中查看损失、参数等各种中间信息
# early_stop_fn:提前停止参数,从hyperopt库导入的方法no_progresss_loss(),可以输入具体的数字n,
#               表示当损失连续n次没有下降时,让算法提前停止
def param_hyperopt(param_spaces, X_train, y_train, X_test=None, y_test=None, 
                   num_boost_round=10000, nfolds=5, max_evals=100):
    """
    贝叶斯调参, 确定其他参数
    """
    
    # 1 定义目标函数
    def lgb_hyperopt_object(params, num_boost_round=num_boost_round, n_folds=nfolds):

        """定义目标函数"""
        param = {
                # general parameters
                'objective': 'binary',
                'boosting': 'gbdt',
                'metric': 'auc',
                'learning_rate': params['learning_rate'],
                # tuning parameters
                'num_leaves': int(params['num_leaves']),
                'min_data_in_leaf': int(params['min_data_in_leaf']),
                'max_depth': int(params['max_depth']),
                'bagging_freq': 1,
                'bagging_fraction': params['bagging_fraction'],
                'feature_fraction': params['feature_fraction'],
                'lambda_l1': params['lambda_l1'],
                'lambda_l2': params['lambda_l2'],
                'min_gain_to_split':params['min_gain_to_split'],
                'early_stopping_rounds': int(params['early_stopping_rounds']),
                'scale_pos_weight': 1,
                'seed': 1
                }
        train_set = lgb.Dataset(X_train, label=y_train)
        if X_test is None:
            cv_results = lgb.cv(param, 
                                train_set, 
                                num_boost_round=num_boost_round,
                                nfold = n_folds, 
                                stratified=True, 
                                shuffle=True, 
                                metrics='auc',
                                seed=1
                                )
            best_score = max(cv_results['valid auc-mean'])
            loss = 1 - best_score
            
        else:
            valid_set = lgb.Dataset(X_test, label=y_test)
            clf_obj = lgb.train(param, train_set, valid_sets=valid_set, num_boost_round=num_boost_round)
            loss = 1 - roc_auc_score(y_test, clf_obj.predict(X_test))
        
        return loss
    
    #保存迭代过程
    trials = Trials()
    #设置提前停止
    early_stop_fn = no_progress_loss(50)
    #定义代理模型
    #algo = partial(tpe.suggest, n_startup_jobs=20, r_EI_candidates=50)
    
    best_params = fmin(lgb_hyperopt_object #目标函数
                      ,space=param_spaces  #参数空间
                      ,algo = tpe.suggest  #代理模型
                      ,max_evals=max_evals #允许的迭代次数
                      ,verbose=True
                      ,trials = trials
                      ,early_stop_fn = early_stop_fn
                       )
    
    best_params['boosting'] = 'gbdt'
    best_params['objective'] = 'binary'
    best_params['metric'] = 'auc'
    best_params['num_leaves'] = int(best_params['num_leaves'])
    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])
    best_params['max_depth'] = int(best_params['max_depth'])
    best_params['bagging_freq'] = 1 
    best_params['early_stopping_rounds'] = 50
    best_params['scale_pos_weight'] = 1 
    best_params['seed'] = 1 
    print("最优参数", best_params)
    
    return (best_params, trials)


# In[415]:


best_params, trials = param_hyperopt(spaces, X_train, y_train, X_test=X_test, y_test=y_test, max_evals=10)


# In[116]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(best_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[147]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[148]:


# 优化后评估模型效果
df_sample['y_prob_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_v1'].head()


# In[149]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_v1', 'apply_month')
df_ks_auc_month_v1


# In[150]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[151]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[152]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[153]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[154]:


# 优化后评估模型效果
df_sample_copy['y_prob_all'] = lgb_model.predict(df_sample_copy[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample_copy['y_prob_all'].head()


# In[156]:


df_sample_copy['mob4dpd30_1'] = 1 - df_sample_copy['mob4dpd30']
print(df_sample_copy['mob4dpd30'].value_counts(), df_sample_copy['mob4dpd30_1'].value_counts())


# In[157]:


df_ks_auc_month_all = calculate_ks_auc(df_sample_copy, modeltrian_target, target, 'y_prob_all', 'apply_month')
df_ks_auc_month_all


# In[158]:


df_ks_auc_set_all = model_ks_auc(df_sample_copy, modeltrian_target, 'y_prob_all', 'data_set')
df_ks_auc_set_all['渠道'] = '全渠道'
tmp = get_target_summary(df_sample_copy, target, 'data_set').set_index('bins')
df_ks_auc_set_all = pd.concat([tmp, df_ks_auc_set_all], axis=1)
df_ks_auc_set_all


# In[159]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_all_{timestamp}.xlsx') as writer:
    df_ks_auc_month_all.to_excel(writer, sheet_name='df_ks_auc_month_all')
    df_ks_auc_set_all.to_excel(writer, sheet_name='df_ks_auc_set_all')      
print(result_path + f'4_模型训练_{task_name}_all_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[160]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[162]:


df_compare = pd.read_csv('./result_v2/df_compare_data.csv')
df_compare.info(show_counts=True)


# In[163]:


df_compare.drop(columns=['Unnamed: 0','hlv_d_holo_certno_variablecode_dpd30_6m_bd0001_standard'],inplace=True)


# In[164]:


df_evalue = pd.merge(df_sample_copy, df_compare, how='left', on='order_no')
df_evalue.info(show_counts=True)


# In[165]:


df_evalue['fpd30_1'] = 1 - df_evalue['fpd30']


# In[166]:


print(df_evalue['fpd30'].value_counts(),df_evalue['fpd30_1'].value_counts())


# In[167]:


print(df_evalue['mob4dpd30'].value_counts(),df_evalue['mob4dpd30_1'].value_counts())


# ### 5.3.2 效果对比

# In[168]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[169]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list))
df_evalue[score_list].info(show_counts=True)


# In[170]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[varsname_base].notna().any(axis=1),:]
print(tmp_df_evalue.shape)
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)

groupkeys1 = ['apply_month']
df_ksauc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_v1.insert(loc=0, column='channel', value='全渠道')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_1 = pd.concat([df_ksauc_v1, df_ksauc_v2,df_ksauc_v4], axis=0)
df_ksauc_1


# In[171]:


model_score = ['y_prob_all']
vars_score = ['hlv_d_holo_certno_variablecode_standard_bd003',
              'hlv_d_holo_certno_variablecode_dpd30_4m_bd0002_standard']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1', 'mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)


tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]
print(tmp_df_evalue.shape)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1


# In[172]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_1.to_excel(writer, sheet_name='df_ksauc_notna')
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[173]:


df_sample_copy.to_csv(result_path + r'授信全渠到行为数据模型四期标签v3.csv',index=False)


# ## 5.4 特征变量贡献度

# ### 5.4.1 特征缺失时，KS值变化

# In[325]:


varsname_base_v3


# In[574]:


# base_v4模型打分 
lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250313205254.pkl')
print(lgb_model.feature_name()==varsname_base_v3)


# In[568]:


tmp_df_evalue = df_sample.query("data_set=='3_oot2'").reset_index(drop=True)
tmp_df_evalue.shape


# In[569]:


tmp_df_evalue.info(show_counts=True)


# In[575]:


df_ksauc_all_null_base2 = pd.DataFrame()
for col in varsname_base_v3:
    print(f"-----------{col}-----------")
    data = tmp_df_evalue[tmp_df_evalue[col].notna()]
    print(f'****数据量：{data.shape[0]}****')
    data[col] = np.nan
    data[f'null_{col}'] = lgb_model.predict(data[varsname_base_v3], num_iteration=lgb_model.best_iteration) 
    
    score_list = ['y_prob_base2_v3',f'null_{col}']
    target_list = ['fpd30_1']
    labels_models_dict = {target: score_list for target in target_list}

    groupkeys1 = ['data_set']
    df_ksauc_all_v1 = cal_ks_auc(data.query("channel_types!='桔子商城'"), groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

    groupkeys2 = ['channel_types', 'data_set']
    df_ksauc_all_v2 = cal_ks_auc(data, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'data_set']
    df_ksauc_all_v4 = cal_ks_auc(data, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    ks_auc_tmp = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    ks_auc_tmp.rename(columns={"KS_y_prob_base2_v3":"KS_notna",f'KS_null_{col}':'KS_na',
                              "AUC_y_prob_base2_v3":"AUC_notna",f'AUC_null_{col}':'AUC_na',
                               'target_type':'target'},inplace=True)
    ks_auc_tmp.insert(loc=0, column='varsname', value=col)
    ks_auc_tmp['data_set']='2024-12'
    ks_auc_tmp['target']='FPD30'
    ks_auc_tmp['missrate']=1-data.shape[0]/tmp_df_evalue.shape[0]
    df_ksauc_all_null_base2 = pd.concat([df_ksauc_all_null_base2, ks_auc_tmp], axis=0)
    
    gc.collect()
    
    


# In[336]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2.xlsx')


# In[576]:


df_ksauc_all_null_base2.to_excel(result_path + '7_特征变量贡献度_特征缺失时_base2_v3.xlsx')


# In[166]:


model_score = ['y_prob_base_v4']
score_list = model_score + null_score_list
print(len(score_list), score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(df_evalue_null, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(loc=0, column='channel', value='全渠道')

# groupkeys2 = ['channel_types', 'data_set']
# df_ksauc_all_v2 = cal_ks_auc(df_evalue_null, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'data_set']
# df_ksauc_all_v4 = cal_ks_auc(df_evalue_null, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_null = pd.concat([df_ksauc_all_v1, df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_null.head()

gc.collect()


# In[169]:


df_ksauc_all_null.info()
df_ksauc_all_null.head()


# In[182]:


df_ksauc_all_null_sub = df_ksauc_all_null[['channel']]
for col in [x for x in df_ksauc_all_null.columns[8:] if "KS" in x]:
    print(f"-------{col}--------")
    df_ksauc_all_null_sub[col] = pd.to_numeric(df_ksauc_all_null[col]) - pd.to_numeric(df_ksauc_all_null['KS_y_prob_base_v4'])

df_ksauc_all_null_sub.set_index('channel', inplace=True)


# In[183]:


df_ksauc_all_null_sub.min(axis=1)


# In[186]:


def get_min_columns_as_keys(df):
    result_dicts = []
    for idx, row in df.iterrows():
        # 找到当前行的最小值
        min_value = row.min()
        # 找出所有等于最小值的列名
        min_cols = row[row == min_value].index.tolist()
        # 构建字典：列名为键，最小值为值
        row_dict = {col: min_value for col in min_cols}
        result_dicts.append(row_dict)
    return result_dicts

result_ = get_min_columns_as_keys(df_ksauc_all_null_sub)

# 输出结果
for i, r in enumerate(result_):
    print(f"Row {i}: {r}")


# In[189]:


result_df = pd.DataFrame.from_records(result_)
result_df


# In[184]:


with pd.ExcelWriter(result_path + '7_特征变量贡献度_特征缺失时.xlsx') as writer:
    df_ksauc_all_null.to_excel(writer, sheet_name='df_ksauc_all_null')
    df_ksauc_all_null_sub.to_excel(writer, sheet_name='df_ksauc_all_null_sub')

print(result_path + '7_特征变量贡献度_特征缺失时.xlsx')


# ## 5.5 在授信场景的评估

# In[223]:


df_sample_auth_dict={}


# In[232]:



# 计算今天的时间
from datetime import datetime, timedelta, date

today = datetime.now().strftime('%Y-%m-%d')
print(today)

this_day =datetime.strptime('2025-01-01', '%Y-%m-%d')
end_day = datetime.strptime('2024-08-01', '%Y-%m-%d')

while this_day >= end_day:
    run_day = this_day.strftime('%Y-%m-%d')
    sql = f'''
select 
t.order_no,
t.user_id,
t.id_no_des,
t.channel_id,
t.apply_date,
t.lending_time,
t.order_no_t,
t.apply_date_auth,
t.diff_days,
t.fpd,
t.spd,
t.tpd,
t.fpd0,
t.fpd1,
t.fpd3,
t.fpd7,
t.fpd10,
t.fpd15,
t.fpd20,
t.fpd30,
t.mob4dpd30
,all_a_app_free_fpd30_202502_s
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_fpd1_score
,HLV_D_HOLO_jk_certNo_score_fpd30_v1
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
,ypy_bhxz_a_fpd30_v1_prob_good
,score_fpd0_v1	
,score_fpd6_v1	
,score_fpd10_v1	
,score_fpd10_v2	
,score_fpd30_v1
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pudao_21
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,t_br_fpd
,t_br_mob4
,t_br2_fpd
,t_br2_mob4
,t_beha3_fpd
,t_beha3_mob4
,dz_fpd
,xz_fpd
from 
    (
    select 
    t.order_no_auth as order_no,
    t.user_id,
    t.id_no_des,
    t.channel_id,
    t.apply_date,
    t.lending_time,
    t.order_no as order_no_t,
    t.apply_date_auth,
    t.diff_days,
    t.fpd,
    t.spd,
    t.tpd,
    t.fpd0,
    t.fpd1,
    t.fpd3,
    t.fpd7,
    t.fpd10,
    t.fpd15,
    t.fpd20,
    t.fpd30,
    t.mob4dpd30
    from znzz_fintech_ads.dm_f_lxl_test_order_Y_target_2502 as t 
    where dt=date_sub(current_date(), 1) 
      and apply_date_auth='{run_day}'
      and diff_days<=30
    ) as t 
------------------离线模型子分-----------------
--贷中离线子分融合模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_varCode_standard_BD0004
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_varCode_standard_BD0004'
      and variable_value is not null 
    ) as t2 on t.order_no=t2.order_no
--fpd30离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd30_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd30_v1'
      and variable_value is not null 
    ) as t3 on t.order_no=t3.order_no
--fpd7离线子分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_score_fpd7_v1
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1'
      and variable_value is not null 
    ) as t4 on t.order_no=t4.order_no
--授信全渠道行为特征模型fpd1标签_标准分
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_jk_certNo_fpd1_score
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_jk_certNo_fpd1_score'
      and variable_value is not null 
    ) as t6 on t.order_no=t6.order_no
--贷中截面风险dpd30_6m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard'
      and variable_value is not null 
    ) as t7 on t.order_no=t7.order_no
--贷中提现风险dpd30_4m模型
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard'
      and variable_value is not null 
    ) as t8 on t.order_no=t8.order_no
--贷中行为模型fpd30标签_分数
left join 
    (
    select order_no, variable_value as HLV_D_HOLO_certNo_variableCode_standard_BD003
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003'
      and variable_value is not null 
    ) as t9 on t.order_no=t9.order_no 

-- 人行离线子分
left join 
    (
    select 
     id_no_des
    ,score_fpd0_v1	
    ,score_fpd6_v1	
    ,score_fpd10_v1	
    ,score_fpd10_v2	
    ,score_fpd30_v1
    from znzz_fintech_ads.llji_yhx_ascore_model_all_score_flow_fd
    where dt = date_sub('{run_day}', 1)
    ) as t13 on t.id_no_des=t13.id_no_des

------------------三方缓存数据-----------------    
--近100天缓存三方评分数据
left join 
    (
    select 
     id_no_des
    ,duxiaoman_6
    ,hengpu_4
    ,aliyun_5
    ,baihang_28
    ,pudao_34
    ,feicuifen
    ,wanxiangfen
    ,pudao_20
    ,pudao_68
    ,ruizhi_6
    ,hengpu_5
    ,pudao_21
    ,bh_alic002_1
    ,bh_alic002_2
    ,bh_alic002_3
    ,bh_alic002_4
    from znzz_fintech_ads.lxl_r100_three_score_data as t 
    where dt=date_sub('{run_day}', 1)
    ) as t11 on t.id_no_des=t11.id_no_des

------------------无成本或者低成本的实时数据-----------------   
--北京团队子分
left join 
    (
    select
    order_no,
    t_br_fpd,
    t_br_mob4,
    t_br2_fpd,
    t_br2_mob4,
    t_beha3_fpd,
    t_beha3_mob4,
    dz_fpd,
    xz_fpd
    from znzz_fintech_ads.dm_f_cnn_test_tx_allchan_mxf_model as t 
    where apply_date='{run_day}'
    ) as t12 on t.order_no=t12.order_no
  
--百融子分
left join 
    (
    select order_no, variable_value as all_a_br_derived_fpd30_202408_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_fpd30_202408_g_p'
      and variable_value is not null 
    ) as t14 on t.order_no=t14.order_no 
--百融子分v1
left join 
    (
    select order_no, variable_value as all_a_br_derived_v1_mob4dpd30_202502_st_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p'
      and variable_value is not null 
    ) as t15 on t.order_no=t15.order_no     
--百融子分v2
left join 
    (
    select order_no, variable_value as all_a_br_derived_v2_fpd30_202411_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p'
      and variable_value is not null 
    ) as t16 on t.order_no=t16.order_no     
--百融子分v3
left join 
    (
    select order_no, variable_value as all_a_br_derived_v3_fpd30_202412_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p'
      and variable_value is not null 
    ) as t17 on t.order_no=t17.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v1_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t18 on t.order_no=t18.order_no  
--洞侦子分
left join 
    (
    select order_no, variable_value as all_a_dz_derived_v2_fpd30_202502_g_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p'
      and variable_value is not null 
    ) as t19 on t.order_no=t19.order_no  
--授信百行洞见fpd30标签202502_好概率
left join 
    (
    select order_no, variable_value as all_a_bhdj_fpd10_v1_p
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_bhdj_fpd10_v1_p'
      and variable_value is not null 
    ) as t5 on t.order_no=t5.order_no    
--续侦子分
left join 
    (
    select order_no, variable_value as ypy_bhxz_a_fpd30_v1_prob_good
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good'
      and variable_value is not null 
    ) as t20 on t.order_no=t20.order_no  
--授信全渠道无成本数据融合模型fp30标签_分数
left join 
    (
    select order_no, variable_value as all_a_app_free_fpd30_202502_s
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time='{run_day}'
      and variable_code = 'all_a_app_free_fpd30_202502_s'
      and variable_value is not null 
    ) as t1 on t.order_no=t1.order_no    
;
'''
    print(f'=========================={run_day}=============================')
    df_sample_auth_dict[run_day] = get_data(sql)
    this_day = this_day - timedelta(days=1)


# In[348]:


df_sample_auth = pd.concat(df_sample_auth_dict.values(), ignore_index=True)
df_sample_auth.info(show_counts=True)
df_sample_auth.head()


# In[349]:


for i, col in enumerate(varsname):
    if df_sample_auth[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_auth[col] = pd.to_numeric(df_sample_auth[col], errors='raise')


# In[343]:


df_br = pd.read_csv(r'br_mob4.csv')
df_br.info(show_counts=True)


# In[350]:


df_sample_auth = pd.merge(df_sample_auth.drop(columns=['all_a_br_derived_v1_mob4dpd30_202502_st_p']), df_br, how='left', on='order_no')
df_sample_auth.info(show_counts=True)


# In[369]:


df_sample_auth.to_csv('df_sample_auth.csv',index=False)


# In[577]:


# base2模型打分 
# lgb_model= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base2_v3_20250311142715.pkl')
# print(lgb_model.feature_name()==varsname_base_v3)
df_sample_auth['y_prob_base2_v3'] = lgb_model.predict(df_sample_auth[varsname_base_v3],num_iteration=lgb_model.best_iteration)


# In[362]:


# base1模型打分 
lgb_model1= load_model_from_pkl('./result/06_提现全渠道无成本子分融合模型fpd30标签_2410_2411_base_v4_20250304203258.pkl')
print(lgb_model1.feature_name()==varsname_base_v4)
df_sample_auth['y_prob_base_v4'] = lgb_model1.predict(df_sample_auth[varsname_base_v4],num_iteration=lgb_model1.best_iteration)


# In[351]:


df_sample_auth.drop_duplicates(subset=['order_no','order_no_t'],inplace=True)
df_sample_auth.shape


# In[578]:


df_sample_auth['fpd30'].value_counts()


# In[354]:


df_sample_auth['diff_days'].value_counts()


# In[355]:


df_sample_auth = df_sample_auth.query("fpd30>=0").reset_index(drop=True)


# In[356]:


df_sample_auth['fpd30_1'] = 1 - df_sample_auth['fpd30']


# In[358]:


df_sample_auth['apply_month'] = df_sample_auth['apply_date_auth'].str[0:7]


# In[579]:



def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    df_ks_auc = model_ks_auc(df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
#         print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
#         print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# In[360]:


df_sample_auth['channel_types'] = df_sample_auth['channel_id'].apply(channel_type)
df_sample_auth['channel_rates'] = df_sample_auth['channel_id'].apply(channel_rate)


# In[580]:


df_auth_ksauc = calculate_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base2_v3', 'apply_month')
df_auth_ksauc


# In[363]:


df_auth_ksauc_v4 = cal_ks_auc(df_sample_auth, 'fpd30_1', 'fpd30', 'y_prob_base_v4', 'apply_month')
df_auth_ksauc_v4


# In[581]:


with pd.ExcelWriter(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx') as writer:
    df_auth_ksauc.to_excel(writer, sheet_name='df_auth_ksauc_base2_v3')
    df_auth_ksauc_v4.to_excel(writer, sheet_name='df_auth_ksauc_base_v4')

print(result_path + f'8_授信场景评估_{task_name}_{timestamp}.xlsx')


# # 6. 评分分布

# In[174]:


df_sample['apply_month'].value_counts()


# In[177]:


score = 'y_prob_v1'
flag = 'mob4dpd30_1'


# In[178]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, flag]], y=flag, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)
df_sample['score_bins'].head()


# In[179]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[180]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[181]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]
score_group_by_dataset.head()


# In[182]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[183]:



df_sample_copy['score_bins'] = c.transform(df_sample_copy['y_prob_all'], labels=True)
df_sample_copy['score_bins'].head()


# In[184]:


df_sample_copy.to_csv(result_path + r'授信全渠到行为数据模型四期标签v3.csv',index=False)




#==============================================================================
# File: 授信全渠道人行衍生M6D30离线模型_2409_2412.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道人行衍生mob6dpd30离线模型'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[4]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[5]:


sql = """
select * from znzz_fintech_ads.lxl_tmp_pboc_model_sample
"""
df_sample_ = get_data(sql)


# In[6]:


df_sample_.info(show_counts=True)
df_sample_.head()


# In[7]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[8]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[9]:


df_sample_['order_no'].value_counts()


# In[10]:


df_sample_.drop_duplicates(subset=None,inplace=True)


# In[12]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[13]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[15]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col],errors='coerce')


# In[17]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[ ]:





# In[18]:


df_sample = df_sample_.query("target_mob6dpd30>=0").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[19]:


df_sample.groupby(['apply_date','target_mob6dpd30'])['order_no'].count().unstack()


# In[20]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-17'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-11-18' & apply_date<='2024-11-30'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2024-12-17'").index, 'data_set']='3_oot2'


# In[ ]:





# In[21]:


target = 'target_mob6dpd30'


# In[ ]:





# # 1. 样本概况

# In[22]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[23]:


print(df_sample[target].value_counts())


# In[24]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[25]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[26]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# In[27]:


df_sample.dropna(how='all',axis=1,inplace=True)
df_sample.shape


# # 2.数据探索性分析

# In[28]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[ ]:





# In[29]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[30]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[31]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[32]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[33]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[34]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[35]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[36]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[ ]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:


print(len(to_drop_iv))
to_drop_iv


# In[ ]:


print(len(to_drop_missing))
to_drop_missing


# In[ ]:


df_iv.loc[to_drop_iv,:]


# In[ ]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[c,:]


# In[ ]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[37]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[38]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[40]:


varsname_v2 = varsname[:]


# In[41]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[42]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[44]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}, {len(existing_bins_dict[col])}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[45]:


new_bins_dict


# In[46]:


combiner.load(new_bins_dict)


# In[ ]:


combiner.export()


# In[47]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[48]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[49]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[ ]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[300]:


varsname_v2 = varsname_base_v2[:]


# In[314]:


df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")

# df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
# print("-------")


# In[316]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[315]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[ ]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[ ]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[ ]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[ ]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[ ]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[ ]:


target


# In[ ]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[ ]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[ ]:


df_sample_woe.head()


# In[ ]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[ ]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[ ]:


df_corr_matrix.head()


# In[ ]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[ ]:


df_high_corr


# In[ ]:


print(to_drop4)


# In[ ]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[ ]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[ ]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[ ]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[ ]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[ ]:


print(to_drop5)


# In[ ]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[317]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
#         df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
#         df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
#         df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[56]:


target


# In[57]:


df_sample[target].value_counts()


# In[58]:


modeltrian_target = 'target_mob6dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[134]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[135]:


print("最优参数opt_params: ", opt_params)


# In[136]:


varsname_base = varsname[:]
print(len(varsname_base))
# print(varsname_base)


# In[137]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[138]:


df_sample['data_set'].value_counts()


# In[139]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[140]:


# 优化后评估模型效果
df_sample['y_pred_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v1'].head()


# In[141]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'data_set')
df_ks_auc_set_v1


# In[142]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v1', 'apply_month')
df_ks_auc_month_v1


# In[143]:


df_vars_desc = pd.read_excel('人行变量清单2506.xlsx',sheet_name='离线变量清单')
df_vars_desc.info(show_counts=True)


# In[144]:


df_vars_desc.rename(columns={'var':'feature'},inplace=True)
df_vars_desc['feature']=df_vars_desc['feature'].str.lower()

df_vars_desc = df_vars_desc[['feature','comment']]


# In[145]:


df_iv_psi_miss = pd.concat([df_iv_by_month, df_psi_by_month, df_explor.loc[:,'missing']], axis=1)
df_iv_psi_miss.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + ['total_na']
df_iv_psi_miss.head()


# In[146]:


# 模型变量重要性
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
df_importance_month_v1 = pd.merge(df_vars_desc, df_importance_month_v1, how='right',on='feature')
df_importance_month_v1


# In[148]:


# df_iv_by_set.columns = [f'{col}_iv' for col in df_iv_by_set.columns]
# df_psi_by_set.columns = [f'{col}_psi' for col in df_psi_by_set.columns]
# df_miss = df_explor.loc[:,'missing']

# df_set_iv_psi_miss = pd.concat([df_iv_by_set, df_psi_by_set, df_miss], axis=1)
df_set_iv_psi_miss.columns=['1_train_iv','3_oot1_iv','3_oot2_iv','1_train_psi','3_oot1_psi','3_oot2_psi','missing']
df_set_iv_psi_miss.head()


# In[149]:


# 模型变量重要性
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
df_importance_set_v1 = pd.merge(df_vars_desc, df_importance_set_v1, how='right',on='feature')
df_importance_set_v1


# In[150]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# In[88]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 特征变量优化1

# In[151]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[152]:


print("最优参数opt_params: ", opt_params)


# In[92]:


# df_corr_matric_person = df_sample[varsname_base].corr()
# df_corr_matric_person.to_csv('df_corr.csv')


# In[ ]:


df_corr_matric_person = df_sample[varsname_base].corr()

# 调用函数
df_corr_drop, to_drop_vars = find_high_correlation_pairs(df_corr_matric_person,
                                                     df_iv_by_month['2025-03'],
                                                     threshold=0.75)

# 查看结果
print("删除的变量有：", len(to_drop_vars))


# In[ ]:


print(to_drop_vars)


# In[153]:


varsname_base_v2 = df_importance_month_v1[df_importance_month_v1['gain']>0]['feature'].to_list()
print(len(varsname_base_v2))
# print(varsname_base_v2)


# In[154]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[155]:


df_sample['data_set'].value_counts()


# In[156]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[157]:


# 优化后评估模型效果
df_sample['y_pred_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v2'].head()


# In[158]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'apply_month')
df_ks_auc_month_v2


# In[159]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v2', 'data_set')
df_ks_auc_set_v2


# In[160]:


# 模型变量重要性
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
df_importance_month_v2 = pd.merge(df_vars_desc, df_importance_month_v2, how='right',on='feature')
df_importance_month_v2


# In[161]:


# 模型变量重要性
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
df_importance_set_v2 = pd.merge(df_vars_desc, df_importance_set_v2, how='right',on='feature')
df_importance_set_v2


# In[162]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 特征变量优化2

# In[273]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[191]:


# varsname_base_v3 = df_importance_month_v2[df_importance_month_v2['gain']>0]['feature'].to_list()
# print(len(varsname_base_v3))


# In[259]:


varsname_base_v3 = df_importance_set_v3[df_importance_set_v3['gain']>15]['feature'].to_list()
print(len(varsname_base_v3))


# In[274]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )

print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[275]:


df_sample['data_set'].value_counts()


# In[276]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[277]:


# 优化后评估模型效果
df_sample['y_pred_v4'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v4'].head()


# In[278]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v4', 'apply_month')
df_ks_auc_month_v3


# In[279]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v4', 'data_set')
df_ks_auc_set_v3


# In[280]:


# 模型变量重要性
df_importance_month_v3 = feature_importance(lgb_model) 
df_importance_month_v3 = pd.merge(df_importance_month_v3, df_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
df_importance_month_v3 = pd.merge(df_vars_desc, df_importance_month_v3, how='right',on='feature')
df_importance_month_v3


# In[281]:


# 模型变量重要性
df_importance_set_v3 = feature_importance(lgb_model) 
df_importance_set_v3 = pd.merge(df_importance_set_v3, df_set_iv_psi_miss, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
df_importance_set_v3 = pd.merge(df_vars_desc, df_importance_set_v3, how='right',on='feature')
df_importance_set_v3


# In[282]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[205]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# ### 5.3 特征变量优化3

# In[303]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 69
opt_params['max_depth'] = 3
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[304]:


df_vars_desc = pd.read_excel('人行变量清单2506.xlsx',sheet_name='离线变量清单')
df_vars_desc.rename(columns={'var':'feature'},inplace=True)
df_vars_desc['feature']=df_vars_desc['feature'].str.lower()
df_vars_desc.info(show_counts=True)

# df_vars_desc = df_vars_desc[['feature','comment']]


# In[305]:


to_drop_cols4 = df_vars_desc[df_vars_desc['Source']=='人行实时调用变量']['feature'].to_list()
print(len(to_drop_cols4))


# In[307]:


varsname_base_v4 = [col for col in varsname if col not in to_drop_cols4]
print(len(varsname_base_v4))
print(varsname_base_v4) 


# In[308]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )

print(X_train.shape)
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'

df_sample['data_set'].value_counts()


# In[309]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[310]:


# 优化后评估模型效果
df_sample['y_pred_v5'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_pred_v5'].head()


# In[311]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v5', 'apply_month')
df_ks_auc_month_v4


# In[312]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_pred_v5', 'data_set')
df_ks_auc_set_v4


# In[313]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v4 = pd.merge(df_importance_month_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v4 = pd.merge(df_vars_list, df_importance_month_v4, how='right',left_on='name',right_on='feature')
df_importance_month_v4


# In[ ]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v4 = pd.merge(df_importance_set_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v4 = pd.merge(df_vars_list, df_importance_set_v4, how='right',left_on='name',right_on='feature')
df_importance_set_v4


# In[ ]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[ ]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v5_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v5_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v5')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v5')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v5')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v5')      
print(result_path + f'4_模型训练_{task_name}_v5_{timestamp}.xlsx')


# In[ ]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx')


# In[ ]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[283]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[284]:


target


# In[285]:


usecols = ['order_no','channel_id','apply_date','apply_month', 'data_set', 'target_mob6dpd30', 'target_mob6dpd30_1','channel_types', 'channel_rates', 'y_pred_v1', 'y_pred_v2', 'y_pred_v3','y_pred_v4']
print(len(usecols))


# In[286]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 效果对比

# In[287]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)       
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[288]:


score_list = ['y_pred_v1','y_pred_v2','y_pred_v3','y_pred_v4']
print(len(score_list))
print(score_list)

target_list = ['target_mob6dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all1


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2


# In[318]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_1_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='allchannel')
#     df_ksauc_all2.to_excel(writer, sheet_name='pboc')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_1_{timestamp}.xlsx')


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[ ]:



# df_evalue_fpd30 = df_evalue.query("target_mob4dpd30>=0")
# df_evalue_fpd30.info(show_counts=True)


# In[ ]:


df_evalue_fpd30['target_mob4dpd30_1'] = 1 - df_evalue_fpd30['target_mob4dpd30']
df_evalue_fpd30['target_mob4dpd30_1'].value_counts()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_3.head()


# In[ ]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_all_2.to_excel(writer, sheet_name='fpd30_融合')
    df_ksauc_all_3.to_excel(writer, sheet_name='fpd30_三方')
    df_ksauc_all_4.to_excel(writer, sheet_name='fpd30_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='mob4_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='mob4_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# ##### 调用征信的渠道

# In[ ]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_month_pboc


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[ ]:


df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc_fpd30.info(show_counts=True)


# In[ ]:


model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_2.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_3.head()


# In[ ]:





# In[ ]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_fpd30_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_pd30_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_fpd30_离线')
#     df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_mob4_融合')
#     df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_mob4_三方')
#     df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx')


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# # 6. 评分分布

# In[ ]:





# In[289]:


df_sample['apply_month'].value_counts()


# In[326]:


score = 'y_pred_v4'


# In[327]:


c = toad.transform.Combiner()
c.fit(df_sample[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[328]:


df_sample['score_bins'].head()


# In[329]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[330]:


df_psi_matrix = get_model_psi(df_sample, score, 'data_set', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[331]:


df_psi_matrix = df_psi_matrix.loc['1_train',:]
df_psi_matrix


# In[332]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[333]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[298]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[299]:


df_evalue.to_csv(result_path + '授信全渠道人行衍生M6D30离线模型_2409_2412.csv',index=False)
print(result_path + '授信全渠道人行衍生M6D30离线模型_2409_2412.csv')


# In[ ]:







#==============================================================================
# File: 授信全渠道高成本fpd30融合模型_2411_2502.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道高成本fpd30融合模型_2411_2502'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[ ]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[ ]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--融合模型
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- 深圳团队子分
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- 北京团队模型子分
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- 三方数据子分
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--行为模型子分
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    -- 行为模型子分
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- 子分实时模型
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- 授信融合模型
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[3]:


df_sample_1 = pd.read_csv(result_path + '授信全渠道高成本融合模型250604.csv')
df_merge_off= pd.read_csv(result_path + 'df_off_merge2.csv')
df_sample_ = pd.merge(df_sample_1, df_merge_off, how='left', on='order_no')
df_sample_.info(show_counts=True)
df_sample_.head()


# In[4]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[5]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[ ]:


# df_sample_.to_csv(result_path + '授信全渠道高成本融合模型250604.csv',index=False)
# print(result_path + '授信全渠道高成本融合模型250604.csv')


# In[10]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
df_sample_.drop(columns=drop_cols, inplace=True)
df_sample_.shape


# In[11]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:10], varsname[-10:])
print("初始特征变量个数：",len(varsname))


# In[12]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[13]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[ ]:





# In[15]:


df_sample = df_sample_.query("target_fpd30>=0 & channel_id > 1 & apply_date>='2024-10-01'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[17]:


df_sample.groupby(['apply_date','target_fpd30'])['order_no'].count().unstack()


# In[18]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2025-02-29'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-10-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2025-03-01' & apply_date<='2025-04-02'").index, 'data_set']='3_oot2'


# In[19]:


df_sample.to_csv(result_path + 'model_授信全渠道高成本fd30融合模型_2411_2502.csv',index=False)
print(result_path + 'model_授信全渠道高成本fd30融合模型_2411_2502.csv')


# In[22]:


target = 'target_fpd30'


# In[ ]:





# # 1. 样本概况

# In[23]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[24]:


print(df_sample[target].value_counts())


# In[25]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[26]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[27]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# In[29]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[31]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# In[35]:


df_m1a0044 = pd.read_csv(result_path + 'df_m1a0044.csv')
df_m1a0044.info(show_counts=True)


# In[36]:


df_sample = pd.merge(df_sample, df_m1a0044, how='left',on='order_no')
df_sample.shape


# # 2.数据探索性分析

# In[37]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[38]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[39]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[40]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[41]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[42]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[43]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[44]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[45]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[46]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[48]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[49]:


print(len(to_drop_iv))
to_drop_iv


# In[50]:


print(len(to_drop_missing))
to_drop_missing


# In[51]:


df_iv.loc[to_drop_iv,:]


# In[52]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[53]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[54]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[55]:


df_iv.loc[c,:]


# In[56]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")
print(to_drop2)


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[57]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[58]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[59]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[60]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[61]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[62]:


new_bins_dict


# In[63]:


combiner.load(new_bins_dict)


# In[64]:


combiner.export()


# In[65]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[66]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[67]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[68]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[69]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[70]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[71]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[89]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[90]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[91]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[92]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[93]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[77]:


target


# In[78]:


df_bins.shape
df_bins.head()


# In[79]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[80]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[81]:


df_sample_woe.head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[83]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[84]:


df_corr_matrix.head()


# In[85]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[86]:


df_high_corr


# In[87]:


print(to_drop4)


# In[94]:


# varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[95]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[96]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[97]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[98]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[99]:


print(to_drop5)


# In[100]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[101]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[208]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270,226, 227, 231, 234, 245, 246, 247, 251,263,265,267):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 237, 240, 241, 244, 248,249,252,254,256,258,270):
            channel='24利率'
        elif x in (226, 231, 234, 245, 246, 247, 251,263,265,267):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[209]:


df_sample = pd.read_csv(result_path + '授信全渠道高成本fpd30融合模型_2411_2502_report.csv')
df_sample = df_sample.query("apply_date<'2025-04-01'").reset_index(drop=True)


# In[210]:


target


# In[211]:


modeltrian_target = 'target_fpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[212]:


df_sample[target].value_counts()


# In[213]:


df_sample[modeltrian_target].value_counts()


# In[214]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[215]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[216]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[217]:


df_sample['channel_types'].value_counts()


# In[218]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[219]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[220]:


print("最优参数opt_params: ", opt_params)


# In[221]:


print(len(varsname_v5))
print(varsname_v5)


# In[222]:


to_drop_all1 = ['feicuifen','pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = [col for col in varsname_v5 if 'm1b' in col]
to_drop_all3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
to_drop_all4 = ['pboc_dpd20']


to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[223]:


varsname_base = [col for col in varsname_v5 if col not in to_drop_all]
print(len(varsname_base))
print(varsname_base)


# In[224]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[225]:


df_sample['data_set'].value_counts()


# In[226]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[227]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[228]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[229]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[230]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[231]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[232]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 特征变量优化1

# In[245]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[246]:


print("最优参数opt_params: ", opt_params)


# In[247]:


df_corr_matric_person = df_sample[varsname_base].corr()
df_corr_matric_person.to_csv('df_corr.csv')


# In[248]:


df_corr_matric_person = df_sample[varsname_base].corr()

# 调用函数
df_corr_drop, to_drop_vars = find_high_correlation_pairs(df_corr_matric_person,
                                                     df_iv_by_month['2025-03'],
                                                     threshold=0.75)

# 查看结果
print("删除的变量有：", len(to_drop_vars))


# In[249]:


print(to_drop_vars)


# In[250]:


df_corr_drop


# In[281]:


to_drop_cols1 = ['dz_v1_mob4', 'xz_fpd', 'm1a0037_g_p', 'a_pboc_fpd10_v2', 'a_pboc_fpd0_v1', 'm1a0026_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0031_g_p', 'm1a0021_g_p', 'dz_fpd', 'hengpu_7', 'm1a0020_g_p', 'tengxun_1', 'br_fpd_2', 'm1a0016_g_p']
to_drop_cols2 = ['br_v3_fpd', 'dz_fpd', 'br_fpd_2', 'a_pboc_fpd10_v2', 'm1a0011_g_p', 'm1a0034_g_p', 'm1a0009_g_p', 'a_pboc_fpd10_v1', 'm1a0027_g_p', 'a_pboc_fpd30_v1', 'm1a0021_g_p', 'xz_fpd', 'm1a0016_g_p', 'tengxun_1', 'm1a0022_g_p', 'a_pboc_fpd6_v1', 'hengpu_7', 'br_v3_mob4', 'm1a0023_g_p', 'm1a0026_g_p', 'br_mob4']

to_drol_cols = to_drop_cols1 + to_drop_cols2
print(to_drol_cols)
print(len(set(to_drol_cols)))


# In[282]:


varsname_base_v2 = [col for col in varsname_base if col not in to_drol_cols]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[ ]:


varsname_base_v2 = ['dz_v2_fpd', 'm1a0033_g_p', 'm1a0043_g_p', 'xz_v2_fpd', 'hengpu_4', 'm1a0041_g_p', 'pudao_34', 'ruizhi_6', 'm1a0038_g_p', 'pd_fpd', 'hengpu_5', 'baihang_13', 'pudao_54', 'pboc_dpd20', 'pudao_20', 'm1a0028_g_p', 'aliyun_5', 'pudao_82', 'baihang_28', 'a_bhdj_fpd10_v1', 'zhirongfen', 'duxiaoman_6', 'tianchuang_7', 'm1a0040_g_p', 'rong360_4', 'm1a0035_g_p', 'xz_v1_mob4', 'ali_fraud_score3', 'm1a0020_g_p', 'pudao_87', 'm1a0037_g_p', 'm1a0044_g_p', 'bileizhenv1', 'a_pboc_fpd0_v1', 'pudao_68', 'pudao_84', 'br_fpd']
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[283]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[284]:


df_sample['data_set'].value_counts()


# In[285]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[286]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[287]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[288]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[289]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[290]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[291]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 特征变量优化2

# In[355]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[295]:


to_drop_cols3 = ['m1a0044_g_p','m1a0041_g_p']


# In[296]:



varsname_base_v3 = [col for col in varsname_base_v2 if col not in to_drop_cols3]
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[297]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[298]:


df_sample['data_set'].value_counts()


# In[299]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[300]:


# 优化后评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[301]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[302]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[303]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[304]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[305]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# ### 5.3 特征变量优化3

# In[402]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[403]:


df_importance_set_v2


# In[416]:


# to_drop_cols4 = ['ali_fraud_score9','ali_fraud_score3','ppcm_behav_score','umeng_score_v5','tianchuang_7','duxiaoman_6']
# to_drop_cols4 = list(df_importance_set_v2[df_importance_set_v2['gain']<400]['feature'])
to_drop_cols4 = ['ali_fraud_score3','ali_fraud_score9']
print(len(to_drop_cols4))
print(to_drop_cols4)


# In[ ]:


# to_drop_cols4.append('m1a0041_g_p')


# In[417]:


varsname_base_v4 = [col for col in varsname_base_v2 if col not in to_drop_cols4]
print(len(varsname_base_v4))
print(varsname_base_v4) 


# In[418]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v4]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'

df_sample['data_set'].value_counts()


# In[419]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[420]:


# 优化后评估模型效果
df_sample['y_prob_base_v7'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v7'].head()


# In[409]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'apply_month')
df_ks_auc_month_v4


# In[421]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'apply_month')
df_ks_auc_month_v4


# In[412]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v6', 'data_set')
df_ks_auc_set_v4


# In[422]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v7', 'data_set')
df_ks_auc_set_v4


# In[423]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v4 = pd.merge(df_importance_month_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v4 = pd.merge(df_vars_list, df_importance_month_v4, how='right',left_on='name',right_on='feature')
df_importance_month_v4


# In[424]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v4 = pd.merge(df_importance_set_v4, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v4 = pd.merge(df_vars_list, df_importance_set_v4, how='right',left_on='name',right_on='feature')
df_importance_set_v4


# In[332]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[343]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v5_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v5_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v5_{timestamp}.pkl')
print(result_path + f'{task_name}_v5_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v5_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v5')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v5')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v5')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v5')      
print(result_path + f'4_模型训练_{task_name}_v5_{timestamp}.xlsx')


# In[415]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx')


# In[425]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[426]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[427]:


print(df_sample.columns[-17:].to_list())


# In[429]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'channel_types', 'channel_rates', 'y_prob_base_v1', 'y_prob_base_v2', 'y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7','t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506']
print(len(usecols))
print(usecols)


# In[430]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 效果对比

# In[431]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[432]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc.info(show_counts=True)


# In[433]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all1


# In[434]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all2


# In[435]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_1_{timestamp}.xlsx') as writer:
    df_ksauc_all1.to_excel(writer, sheet_name='allchannel')
    df_ksauc_all2.to_excel(writer, sheet_name='pboc')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_1_{timestamp}.xlsx')


# In[437]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[438]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[440]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[ ]:



# df_evalue_fpd30 = df_evalue.query("target_mob4dpd30>=0")
# df_evalue_fpd30.info(show_counts=True)


# In[ ]:


df_evalue_fpd30['target_mob4dpd30_1'] = 1 - df_evalue_fpd30['target_mob4dpd30']
df_evalue_fpd30['target_mob4dpd30_1'].value_counts()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_2.head()


# In[ ]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_fpd30_3.head()


# In[441]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='fpd30')
    df_ksauc_all_2.to_excel(writer, sheet_name='fpd30_融合')
    df_ksauc_all_3.to_excel(writer, sheet_name='fpd30_三方')
    df_ksauc_all_4.to_excel(writer, sheet_name='fpd30_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='mob4_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='mob4_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# ##### 调用征信的渠道

# In[442]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[ ]:


# df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
# df_ks_auc_month_pboc


# In[443]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[444]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new','third3_low_fpd', 'third3_high_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[445]:


model_score =  ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5','y_prob_base_v6','y_prob_base_v7']

vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[ ]:


df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc_fpd30.info(show_counts=True)


# In[ ]:


model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504',
              'gen7_fpd','gen7_mob4','dz_v2_fpd','xz_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_2.head()


# In[ ]:



model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']
vars_score = ['m1a0029_g_p', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_fpd30_3.head()


# In[ ]:





# In[446]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_fpd30_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_pd30_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_fpd30_离线')
#     df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_mob4_融合')
#     df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_mob4_三方')
#     df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx')


# In[ ]:


score_list = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3']

target_list = ['target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# # 6. 评分分布

# In[ ]:





# In[447]:


df_sample['apply_month'].value_counts()


# In[448]:


score = 'y_prob_base_v6'


# In[449]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[450]:


df_sample['score_bins'].head()


# In[451]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[452]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[453]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[454]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'")


# In[455]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + '授信全渠道高成本fpd30融合模型_2411_2502_report.csv',index=False)
print(result_path + '授信全渠道高成本fpd30融合模型_2411_2502_report.csv')


# In[ ]:







#==============================================================================
# File: 授信全渠道高成本mob4dpd30融合模型_2409_2411.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道高成本mob4dpd30融合模型_2409_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--融合模型
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- 深圳团队子分
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- 北京团队模型子分
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- 三方数据子分
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--行为模型子分
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    -- 行为模型子分
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- 子分实时模型
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- 授信融合模型
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


df_sample_['order_no'].value_counts().head(3)


# In[9]:


df_sample_.query("order_no=='auth_122980696020240825115424'")


# In[13]:


df_sample_.drop_duplicates(subset=['order_no'],inplace=True)


# In[14]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[15]:


df_sample_.to_csv(result_path + '授信全渠道高成本融合模型250604.csv',index=False)
print(result_path + '授信全渠道高成本融合模型250604.csv')


# In[16]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:5], varsname[-5:])
print("初始特征变量个数：",len(varsname))


# In[17]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[18]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[23]:


df_sample = df_sample_.query("target_mob4dpd30>=0 & channel_id > 1 & apply_date<='2025-01-05'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[24]:


df_sample.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[25]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-01-05'").index, 'data_set']='3_oot2'


# In[26]:


df_sample.to_csv(result_path + 'model_授信全渠道高成本mob4dpd30融合模型_2409_2411.csv',index=False)
print(result_path + 'model_授信全渠道高成本mob4dpd30融合模型_2409_2411.csv')


# In[27]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. 样本概况

# In[28]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[29]:


print(df_sample[target].value_counts())


# In[30]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[31]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[32]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# In[246]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# # 2.数据探索性分析

# In[247]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[248]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[249]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[250]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[251]:


df_explor_v1['missing'].value_counts()


# In[252]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[253]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[254]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[255]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[256]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[257]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# In[44]:


sql = """
    select 
     order_no
    ,max(case when variable_code = 'm1a0044' then good_score else null end) as M1A0044_g_p 
    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
"""
df_m1a0044 = get_data(sql)


# In[45]:


df_m1a0044.info()


# In[46]:


df_sample.drop(columns=['m1a0044_g_p'],inplace=True)


# In[47]:


df_sample = pd.merge(df_sample, df_m1a0044, how='inner',on='order_no')


# In[48]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[54]:


varsname_v1 = [col for col in varsname if col in df_sample.columns ]
len(varsname_v1)


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[259]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:





# In[261]:


print(len(to_drop_iv))
to_drop_iv


# In[262]:


print(len(to_drop_missing))
to_drop_missing


# In[263]:


df_iv.loc[to_drop_iv,:]


# In[266]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


# train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
#                                                 target=target, 
#                                                 empty=0.90, iv=0.01, corr=0.85, 
#                                                 return_drop=True, exclude=None)
# train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:]


# In[270]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[271]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[274]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[275]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[276]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[277]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[278]:


new_bins_dict


# In[279]:


combiner.load(new_bins_dict)


# In[280]:


combiner.export()


# In[281]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[282]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[283]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[284]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[285]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[286]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[287]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[288]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[289]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[290]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[291]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[294]:


print(len(to_drop3))
print(to_drop3)


# In[295]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[296]:


target


# In[297]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[298]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[299]:


df_sample_woe.head()


# In[300]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[301]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[302]:


df_corr_matrix.head()


# In[303]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[304]:


df_high_corr


# In[306]:


print(to_drop4)


# In[307]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[308]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[309]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[310]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[311]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[312]:


print(to_drop5)


# In[313]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[314]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[56]:


target


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[target].value_counts()


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# # 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# In[ ]:





# ### 5.2.1 base模型

# In[64]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[65]:


print("最优参数opt_params: ", opt_params)


# In[67]:


varsname_v5 = varsname_v1[:]
print(len(varsname_v5))
print(varsname_v5)


# In[82]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
print(len(drop_cols))


# In[83]:


varsname_base = [col for col in varsname_v5 if col not in drop_cols]


# In[84]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[85]:


df_sample['data_set'].value_counts()


# In[86]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[87]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[88]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[89]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[90]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[91]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[92]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 特征变量优化1

# In[93]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[94]:


print("最优参数opt_params: ", opt_params)


# In[97]:


drop_cols_v2 = [col for col in varsname_base if 'm1b' in col]


# In[98]:


drop_cols_v2


# In[99]:


varsname_base_v2 = [col for col in varsname_base if col not in drop_cols_v2]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[100]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[101]:


df_sample['data_set'].value_counts()


# In[102]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[103]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[104]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[106]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[107]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[108]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[109]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 特征变量优化2

# In[120]:


# sql = '''
# select 
#  t.order_no
# ,t_off_m4d30_2504
# ,t_off_f30_2504
# ,t_off_f30_2506
# ,t_off_m3d30_2506
# ,off_m4d30_2504
# ,off_f30_2504

# from 
#     (
#     select distinct t2.order_no
#     from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
#     inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
#     on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
#     where t1.dt = date_sub(current_date(), 1) 
#       and t2.dt = date_sub(current_date(), 1) 
#       and t2.auth_status = 6
#       and t2.apply_date >= '2024-08-01'
#       and t2.apply_date <= '2025-01-05'
#     ) as t 

# -- 深圳团队的子分
# left join 
#     (
#     select 
#      order_no
#     ,max(case when variable_code = 't_off_m4d30_2504' then standard_score else null end) as t_off_m4d30_2504 
#     ,max(case when variable_code = 't_off_f30_2504' then standard_score else null end) as t_off_f30_2504 
#     ,max(case when variable_code = 't_off_f30_2506' then standard_score else null end) as t_off_f30_2506 
#     ,max(case when variable_code = 't_off_m3d30_2506' then standard_score else null end) as t_off_m3d30_2506 
#     ,max(case when variable_code = 'off_m4d30_2504' then standard_score else null end) as off_m4d30_2504 
#     ,max(case when variable_code = 'off_f30_2504' then standard_score else null end) as off_f30_2504 
#     from znzz_fintech_ads.apply_model01_scores_off 
#     where apply_time >= '2024-08-01'
#       and apply_time <= '2025-01-05'
#     group by order_no
#     ) as t2 on t.order_no=t2.order_no 
# ;
# '''

# df_off_merge = get_data(sql)


# In[122]:


df_off = pd.read_csv(result_path + 'df_off_merge.csv')
df_off.info(show_counts=True)


# In[124]:


df_off.drop(columns=['Unnamed: 0'],inplace=True)


# In[126]:


df_m4d30 = pd.read_csv(result_path + 't_off_m4d30_2504.csv')
df_m4d30.drop(columns=['Unnamed: 0'],inplace=True)
df_m4d30.info(show_counts=True)


# In[127]:


df_off_merge = pd.merge(df_m4d30, df_off,how='inner',on='order_no')
df_off_merge.info(show_counts=True)


# In[164]:


drop_cols3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']


# In[165]:


df_off_merge2.info(show_counts=True)


# In[166]:


df_sample = pd.merge(df_sample.drop(columns=drop_cols3), df_off_merge2,how='left',on='order_no')
df_sample.info(show_counts=True)


# In[167]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[168]:


print(df_off_merge.columns.to_list())


# In[169]:



varsname_base_v3 = varsname_base + ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[170]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[171]:


df_sample['data_set'].value_counts()


# In[172]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[173]:


# 优化后评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[174]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[175]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[176]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[177]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[178]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# In[212]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[213]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[214]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[151]:


# df_off_merge2 = pd.read_csv(result_path + 'df_off_merge2.csv')
# df_off_merge2.info(show_counts=True)


# In[152]:


# df_sample_ = pd.merge(df_sample_, df_off_merge2, how='left', on='order_no')


# In[418]:


df_evalue_all = df_sample_.query("target_fpd30>=0 & channel_id>1 & apply_date<='2025-03-31'")
df_evalue_all['target_fpd30_1'] = 1 - df_evalue_all['target_fpd30']


# In[419]:



lgb_model_v1 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v1_20250604190522.pkl')
varsnamev1 = lgb_model_v1.feature_name()
df_evalue_all['y_prob_base_v1'] = lgb_model_v1.predict(df_evalue_all[varsnamev1], num_iteration=lgb_model_v1.best_iteration)


# In[420]:


lgb_model_v2 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v2_20250604191712.pkl')
varsnamev2 = lgb_model_v2.feature_name()
df_evalue_all['y_prob_base_v2'] = lgb_model_v2.predict(df_evalue_all[varsnamev2], num_iteration=lgb_model_v2.best_iteration)


# In[421]:


lgb_model_v3 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v3_20250604210408.pkl')
varsnamev3 = lgb_model_v3.feature_name()
df_evalue_all['y_prob_base_v3'] = lgb_model_v3.predict(df_evalue_all[varsnamev3], num_iteration=lgb_model_v3.best_iteration)


# In[423]:


df_evalue_all['apply_month'] = df_evalue_all['apply_date'].str[0:7]


# In[422]:


df_evalue_all['channel_types'] = df_evalue_all['channel_id'].apply(channel_type)
df_evalue_all['channel_rates'] = df_evalue_all['channel_id'].apply(channel_rate)


# In[ ]:


# dfks_v3 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v3', 'apply_month')
# dfks_v3


# In[ ]:


dfks_v1 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v1', 'apply_month')
dfks_v1


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# ### 5.4 特征变量优化3

# In[736]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[737]:


varsname_base_v4 = ['dz_v1_mob4', 'xz_v1_mob4', 'hengpu_4', 'm1a0038_g_p', 'm1a0028_g_p', 'm1a0033_g_p', 'pudao_34', 'm1a0027_g_p', 'duxiaoman_6', 'pudao_87', 'hengpu_5', 'baihang_28', 'pudao_68', 'm1a0031_g_p', 'aliyun_5', 'pboc_dpd20', 'pudao_54', 'ruizhi_6', 'pudao_20', 'm1a0040_g_p', 'pudao_82', 'm1a0043_g_p', 'pudao_84', 'baihang_13', 'tianchuang_7', 'm1a0035_g_p', 'br_v3_mob4', 'a_pboc_fpd10_v2', 'ali_fraud_score3', 'a_bhdj_fpd10_v1', 'rong360_4', 'br_fpd_2', 'br_mob4_2', 'bileizhenv1', 'm1a0021_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0011_g_p', 'pudao_91', 'm1a0044_g_p', 'baihang_31', 'm1a0026_g_p', 'a_pboc_fpd30_v1', 'xz_v2_fpd', 'umeng_score_v5', 'dz_v2_fpd', 'ali_fraud_score9', 'zhirongfen', 'm1a0023_g_p', 'br_fpd']
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[738]:


to_drop_all1 = ['pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = ['tianchuang_7','ali_fraud_score3','umeng_score_v5','dz_v2_fpd','m1a0044_g_p','a_pboc_fpd30_v1','ali_fraud_score9','m1a0023_g_p','br_fpd']
to_drop_all3 = ['m1a0027_g_p','dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[739]:


varsname_base_v5 = [col for col in varsname_base_v4 if col not in to_drop_all]
print(len(varsname_base_v5))
print(varsname_base_v5)


# In[740]:


varsname_base_v6 = varsname_base_v5[:] + ['dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
varsname_base_v6.remove('xz_v1_mob4')
print(len(varsname_base_v6))
print(varsname_base_v6)


# In[741]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v6]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[742]:



# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[743]:


# 优化后评估模型效果
df_sample['y_prob_base_v13'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v13'].head()


# In[ ]:


# df_sample = df_sample.query("apply_date>='2024-09-01'")
# df_sample = df_sample.reset_index(drop=True)


# In[744]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'apply_month')
df_ks_auc_month_v4


# In[745]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'data_set')
df_ks_auc_set_v4


# In[746]:


vars_des = {
 'dz_v1_mob4':'授信全渠道洞侦多头模型v1四期标签202505'
,'xz_v1_mob4':'授信全渠道续侦多头模型v1四期标签202505'
,'hengpu_4':'恒普-反欺诈分M3'
,'m1a0038_g_p':'授信全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率分'
,'m1a0028_g_p':'授信全渠道人行mob4dpd30模型V1_2408_2410_好概率'
,'m1a0033_g_p':'授信全渠道百融加衍生fpd30模型2408_2411_好概率分'
,'pudao_34':'朴道-避雷针定制分V1'
,'m1a0027_g_p':'提现全渠道百融加衍生mob4dpd30模型2407_2409_好概率分'
,'duxiaoman_6':'度小满-欺诈因子V4'
,'pudao_87':'朴道-字节-互联网行为评分25128'
,'hengpu_5':'恒普-定制信用分Y'
,'baihang_28':'fico反欺诈洞见3.0'
,'aliyun_5':'朴道-阿里申请反欺诈V5'
,'pudao_68':'朴道-银商银杏定制分'
,'m1a0031_g_p':'提现全渠道人行mob5dpd30模型v1_2406_2409_好概率'
,'m1a0026_g_p':'授信全渠道续侦加衍生mob4dpd30模型2407_2409_好概率分'
,'pudao_54':'朴道-哈啰-hl-火眼分v7'
,'pudao_20':'朴道-腾讯天御反欺诈V7通用版'
,'pboc_dpd20':'授信通用人行模型dpd20标签202410'
,'m1a0043_g_p':'提现全渠道人行fpd30模型v1_2411_2502_好概率'
,'m1a0040_g_p':'授信全渠道朴道多头四期模型V1_2409_2411_好概率'
,'ruizhi_6':'FICO联合建模定制分2'
,'pudao_82':'朴道-天创-玄辰信用分_AC1501'
,'br_v3_mob4':'授信全渠道百融多头模型v3四期标签202502'
,'pudao_84':'朴道-友盟-建模分score_v3'
,'a_pboc_fpd10_v2':'授信全渠道人行模型v2fpd10标签202410'
,'rong360_4':'数据源_rong360'
,'a_bhdj_fpd10_v1':'授信全渠道百行洞见模型v1fpd10标签'
,'tianchuang_7':'天创-联合分A1502'
,'br_fpd_2':'授信百融多头模型v2首期标签202407'
,'baihang_13':'百行-灵犀产品-孚临-107'
,'m1a0011_g_p':'授信百融多头衍生v3首期标签模型202412'
,'a_pboc_fpd6_v1':'授信全渠道人行模型v1fpd6标签202409'
,'m1a0035_g_p':'授信全渠道人行fpd7模型202408_2411'
,'br_mob4_2':'授信百融多头v2四期标签202407'
,'ali_fraud_score3':'阿里申请反欺诈子分score3'
,'m1a0021_g_p':'授信全渠道洞侦加衍生fpd30模型202409_2411'
,'baihang_31':'fico普惠小版分'
,'pudao_91':'朴道-度小满-小满分桔子数科定制v2'
,'br_v3_fpd':'授信全渠道百融多头模型v3首期标签202502'
,'bileizhenv1':'避雷针v1h1'
,'xz_v2_fpd':'授信全渠道续侦多头模型v2首期标签202505'
,'dz_v2_fpd':'授信全渠道洞侦多头模型v2首期标签202505'
,'m1a0044_g_p':'提现全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率'
,'umeng_score_v5':'友盟-小额分V5.0'
,'a_pboc_fpd30_v1':'授信全渠道人行模型v1fpd30标签'
,'m1a0023_g_p':'提现金科渠道续侦加衍生fpd30模型202407_2411_好概率分'
,'zhirongfen':'同盾智融分'
,'ali_fraud_score9':'阿里申请反欺诈子分score9'
,'br_fpd':'授信百融多头首期标签模型202404'
}


# In[747]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v4['变量名称'] = df_importance_month_v4['feature'].map(vars_des)
df_importance_month_v4


# In[748]:




# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v4['变量名称'] = df_importance_set_v4['feature'].map(vars_des)
df_importance_set_v4


# In[401]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[517]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx')


# In[563]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx')


# In[573]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v8_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v8_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v8_{timestamp}.pkl')
print(result_path + f'{task_name}_v8_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v8_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v8')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v8')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v8')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v8')      
print(result_path + f'4_模型训练_{task_name}_v8_{timestamp}.xlsx')


# In[604]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v11_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v11_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v11_{timestamp}.pkl')
print(result_path + f'{task_name}_v11_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v11_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v11')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v11')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v11')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v11')      
print(result_path + f'4_模型训练_{task_name}_v11_{timestamp}.xlsx')


# In[707]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v12_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v12_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v12_{timestamp}.pkl')
print(result_path + f'{task_name}_v12_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v12')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v12')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v12')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v12')      
print(result_path + f'4_模型训练_{task_name}_v12_{timestamp}.xlsx')


# In[749]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v13_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v13_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v13_{timestamp}.pkl')
print(result_path + f'{task_name}_v13_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v13')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v13')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v13')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v13')      
print(result_path + f'4_模型训练_{task_name}_v13_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[750]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[709]:


# lgb_model_v5 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v4_20250605181337.pkl')
# varsnamev5 = lgb_model_v5.feature_name()
# df_sample['y_prob_base_v5'] = lgb_model_v5.predict(df_sample[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[751]:


print(df_sample.columns[-17:].to_list())


# In[752]:


df_sample['high_p_m4d30_2506_v12'] = df_sample['y_prob_base_v12']
df_sample['high_p_m4d30_2506_v13'] = df_sample['y_prob_base_v13']


# In[754]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'target_mob4dpd30_1', 'channel_types', 'channel_rates', 'high_p_m4d30_2506_v12','high_p_m4d30_2506_v13','t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
print(len(usecols))
print(usecols)


# In[755]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 效果对比

# In[756]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[778]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5


# In[757]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# In[758]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_m3d30_2504', 'low_m3d30_2504','free_v1_fpd', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[759]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new', 'high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[760]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[761]:


# # lgb_model_v5 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v4_20250605181337.pkl')
# # varsnamev5 = lgb_model_v5.feature_name()
# df_evalue_all['y_prob_base_v5'] = lgb_model_v5.predict(df_evalue_all[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[762]:


# varsnamev4 = lgb_model.feature_name()
# df_evalue_all['y_prob_base_v4'] = lgb_model.predict(df_evalue_all[varsnamev4], num_iteration=lgb_model.best_iteration)


# In[763]:


# usecols_fpd30 = [col for col in usecols if col not in ['data_set', 'target_mob4dpd30_1']]
# df_evalue_fpd30 = df_evalue_all[usecols_fpd30]
# df_evalue_fpd30.info(show_counts=True)


# In[764]:


# df_evalue_fpd30['target_fpd30_1'].value_counts()


# In[765]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30.head()


# In[766]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_2.head()


# In[767]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_3.head()


# In[768]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_融合')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_三方')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_v13_{timestamp}.xlsx')


# In[482]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_融合')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_三方')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# ##### 调用征信的渠道

# In[769]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[770]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p',  'low_m3d30_2504','free_m3d30_2504', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[771]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new','high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[772]:



model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[773]:


# df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc_fpd30.info(show_counts=True)


# In[774]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30.head()


# In[775]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_2.head()


# In[776]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_3.head()


# In[442]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_离线')
    df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_fpd30_融合')
    df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_pd30_三方')
    df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx')


# In[777]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_v13_{timestamp}.xlsx')


# # 6. 评分分布

# In[ ]:





# In[779]:


df_sample['apply_month'].value_counts()


# In[780]:


score = 'y_prob_base_v12'


# In[781]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[782]:


df_sample['score_bins'].head()


# In[783]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[784]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[785]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[787]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'").drop(columns=['ks_bin'])


# In[788]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_v12_{timestamp}.xlsx')


# In[496]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report.csv',index=False)
print(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report.csv')


# In[789]:


df_sample.to_csv(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report_0610.csv',index=False)
print(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report_0610.csv')




#==============================================================================
# File: 授信全渠道高成本mob4dpd30融合模型_2411_2501.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道高成本mob4dpd30融合模型_2409_2411'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
--融合模型
,M1A0029_g_p
,M1A0030_g_p
,M1A0032_g_p
,high_p_f30_2504_g_p
,high_p_m3d30_2504_g_p
,mix_pboc_dpd20
,third3_low_fpd
,third3_high_fpd
,free_v1_fpd
,low_v2_fpd
,free_m3d30_2504
,low_m3d30_2504
,high_v1_fpd10
,low_np_f30_2505_new
,high_np_f30_2505_new
,sf_mob4_1_v2
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen6_fpd
,gen6_mob4
,gen7_fpd
,gen7_mob4
-- 深圳团队子分
,a_bhdj_fpd10_v1
,a_pboc_fpd0_v1
,a_pboc_fpd6_v1
,a_pboc_fpd10_v1
,a_pboc_fpd10_v2
,a_pboc_fpd30_v1
,M1A0009_g_p
,M1A0011_g_p
,M1A0016_g_p
,M1A0020_g_p
,M1A0021_g_p
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1A0031_g_p
,M1A0033_g_p
,M1A0034_g_p
,M1A0035_g_p
,M1A0036_g_p
,M1A0037_g_p
,M1A0038_g_p
,M1A0040_g_p
,M1A0041_g_p
,M1A0043_g_p
,M1A0044_g_p
-- 北京团队模型子分
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,dz_fpd
,xz_fpd
,pd_fpd
,pboc_dpd20
,dz_v2_fpd
,dz_v1_mob4
,xz_v2_fpd
,xz_v1_mob4

-- 三方数据子分
,aliyun_5
,bileizhenv1
,duxiaoman_6
,hengpu_4
,hengpu_5
,pudao_20
,pudao_34
,rong360_4
,tengxun_1
,tianchuang_7
,wanxiangfen
,feicuifen
,zhirongfen
,pudao_35
,baihang_28
,hengpu_7
,pudao_68
,pudao_91
,ruizhi_6
,ali_fraud_score3
,ali_fraud_score9
,umeng_score_v5
,tengxun_cash_score
,ppcm_behav_score
,bh_lx_115
,dianhuabang_score
,jd_proba_payment
,jd_probs_mix
,duxiaoman_credit_score
,duxiaoman_cash_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,haluo_cto_score
,bh_alic002_1
,bh_alic002_2
,bh_alic002_3
,bh_alic002_4
,pudao_54
,baihang_13
,pudao_93
,pudao_87
,tianchuang36
,tianchuang24
,baihang_5
,baihang_24
,pd_dhb_mobile_risk_score_1
,pd_dhb_mobile_risk_score_2
,pd_dhb_mobile_scale_score
,pudao_78
,pudao_83
,pudao_82
,pd_jd_fraud_v2
,pudao_84
,pudao_85
,aliyun_2
,baihang_23
,baihang_25
,baihang_26
,baihang_31
,baihang_8
,bairong_14
,bairong_15
,bairong_8
,bh_lx_101
,fulin_2
,hangliezhi_1
,pd_jd_pangu5_score1
,pudao_15
,pudao_21
,pudao_32
,pudao_43
,pudao_77
,pudao_81
,pudao_86
,bh_umeng_score_m3
,bh_umeng_score_v1
,pd_hl_jzscore_v2
,pd_jdxyd
,pd_kf_score
,pd_kx_score
,pd_ty_280
,pd_unif_numberrisk_level_new
,qx_model_c
,qx_model_f
,ruizhi_4
,shangtang_1
,tianchuang_8
,zhixin_1
--行为模型子分
,M1B0001_g_s
,M1B0002_g_s
,M1B0004_g_s
,M1B0011_g_s
,M1B0012_g_s
,M1B0013_g_s
,M1B0025_g_s
,M1B0029_g_s
,M1B0030_g_s
,M1B0031_g_s

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    ,ROW_NUMBER() OVER (PARTITION BY t2.order_no ORDER BY t2.create_time DESC) AS rk
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-04-02'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-04-02'
      and dt>=''
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    -- 行为模型子分
    ,max(case when variable_code = 'M1B0001' then standard_score else null end) as M1B0001_g_s 
    ,max(case when variable_code = 'M1B0002' then standard_score else null end) as M1B0002_g_s 
    ,max(case when variable_code = 'M1B0004' then standard_score else null end) as M1B0004_g_s 
    ,max(case when variable_code = 'M1B0011' then standard_score else null end) as M1B0011_g_s 
    ,max(case when variable_code = 'M1B0012' then standard_score else null end) as M1B0012_g_s 
    ,max(case when variable_code = 'M1B0013' then standard_score else null end) as M1B0013_g_s 
    ,max(case when variable_code = 'M1B0025' then standard_score else null end) as M1B0025_g_s 
    ,max(case when variable_code = 'M1B0029' then standard_score else null end) as M1B0029_g_s 
    ,max(case when variable_code = 'M1B0030' then standard_score else null end) as M1B0030_g_s 
    ,max(case when variable_code = 'M1B0031' then standard_score else null end) as M1B0031_g_s
    -- 子分实时模型
    ,max(case when variable_code = 'a_bhdj_fpd10_v1' then good_score else null end) as a_bhdj_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd0_v1' then good_score else null end) as a_pboc_fpd0_v1
    ,max(case when variable_code = 'a_pboc_fpd6_v1' then good_score else null end) as a_pboc_fpd6_v1  
    ,max(case when variable_code = 'a_pboc_fpd10_v1' then good_score else null end) as a_pboc_fpd10_v1 
    ,max(case when variable_code = 'a_pboc_fpd10_v2' then good_score else null end) as a_pboc_fpd10_v2 
    ,max(case when variable_code = 'a_pboc_fpd30_v1' then good_score else null end) as a_pboc_fpd30_v1 
    ,max(case when variable_code = 'M1A0009' then good_score else null end) as M1A0009_g_p 
    ,max(case when variable_code = 'M1A0011' then good_score else null end) as M1A0011_g_p
    ,max(case when variable_code = 'M1A0016' then good_score else null end) as M1A0016_g_p 
    ,max(case when variable_code = 'M1A0020' then good_score else null end) as M1A0020_g_p 
    ,max(case when variable_code = 'M1A0021' then good_score else null end) as M1A0021_g_p 
    ,max(case when variable_code = 'M1A0022' then good_score else null end) as M1A0022_g_p
    ,max(case when variable_code = 'M1A0023' then good_score else null end) as M1A0023_g_p
    ,max(case when variable_code = 'M1A0026' then good_score else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027' then good_score else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028' then good_score else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1A0031' then good_score else null end) as M1A0031_g_p
    ,max(case when variable_code = 'M1A0033' then good_score else null end) as M1A0033_g_p 
    ,max(case when variable_code = 'M1A0034' then good_score else null end) as M1A0034_g_p 
    ,max(case when variable_code = 'M1A0035' then good_score else null end) as M1A0035_g_p
    ,max(case when variable_code = 'M1A0036' then good_score else null end) as M1A0036_g_p
    ,max(case when variable_code = 'M1A0037' then good_score else null end) as M1A0037_g_p
    ,max(case when variable_code = 'M1A0038' then good_score else null end) as M1A0038_g_p 
    ,max(case when variable_code = 'M1A0040' then good_score else null end) as M1A0040_g_p 
    ,max(case when variable_code = 'M1A0041' then good_score else null end) as M1A0041_g_p 
    ,max(case when variable_code = 'M1A0043' then good_score else null end) as M1A0043_g_p 
    ,max(case when variable_code = 'M1A0044' then good_score else null end) as M1A0044_g_p 

    -- 授信融合模型
    ,max(case when variable_code = 'M1A0029' then good_score else null end) as M1A0029_g_p 
    ,max(case when variable_code = 'M1A0030' then good_score else null end) as M1A0030_g_p 
    ,max(case when variable_code = 'M1A0032' then good_score else null end) as M1A0032_g_p 
    ,max(case when variable_code = 'high_p_f30_2504' then good_score else null end) as high_p_f30_2504_g_p 
    ,max(case when variable_code = 'high_p_m3d30_2504' then good_score else null end) as high_p_m3d30_2504_g_p 

    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-08-01'
      and dt <= '2025-04-02'
    ) as t3 on t.order_no=t3.order_no
;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


df_sample_['order_no'].value_counts().head(3)


# In[9]:


df_sample_.query("order_no=='auth_122980696020240825115424'")


# In[13]:


df_sample_.drop_duplicates(subset=['order_no'],inplace=True)


# In[14]:


print(df_sample_.shape, df_sample_['order_no'].nunique())


# In[15]:


df_sample_.to_csv(result_path + '授信全渠道高成本融合模型250604.csv',index=False)
print(result_path + '授信全渠道高成本融合模型250604.csv')


# In[16]:


varsname = df_sample_.columns.to_list()[33:]

print(varsname[:5], varsname[-5:])
print("初始特征变量个数：",len(varsname))


# In[17]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[18]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[23]:


df_sample = df_sample_.query("target_mob4dpd30>=0 & channel_id > 1 & apply_date<='2025-01-05'").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[24]:


df_sample.groupby(['apply_date','target_mob4dpd30'])['order_no'].count().unstack()


# In[25]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-09-01' & apply_date<='2024-11-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-08-31'").index, 'data_set']='3_oot1'
df_sample.loc[df_sample.query("apply_date>='2024-12-01' & apply_date<='2025-01-05'").index, 'data_set']='3_oot2'


# In[26]:


df_sample.to_csv(result_path + 'model_授信全渠道高成本mob4dpd30融合模型_2409_2411.csv',index=False)
print(result_path + 'model_授信全渠道高成本mob4dpd30融合模型_2409_2411.csv')


# In[27]:


target = 'target_mob4dpd30'


# In[ ]:





# # 1. 样本概况

# In[28]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[29]:


print(df_sample[target].value_counts())


# In[30]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[31]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[32]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# In[246]:


varsname = ['a_bhdj_fpd10_v1', 'a_pboc_fpd0_v1', 'a_pboc_fpd6_v1', 'a_pboc_fpd10_v1', 'a_pboc_fpd10_v2', 'a_pboc_fpd30_v1', 'm1a0009_g_p', 'm1a0011_g_p', 'm1a0016_g_p', 'm1a0020_g_p', 'm1a0021_g_p', 'm1a0022_g_p', 'm1a0023_g_p', 'm1a0026_g_p', 'm1a0027_g_p', 'm1a0028_g_p', 'm1a0031_g_p', 'm1a0033_g_p', 'm1a0034_g_p', 'm1a0035_g_p', 'm1a0036_g_p', 'm1a0037_g_p', 'm1a0038_g_p', 'm1a0040_g_p', 'm1a0041_g_p', 'm1a0043_g_p', 'm1a0044_g_p', 'br_fpd', 'br_mob4', 'br_fpd_2', 'br_mob4_2', 'br_v3_fpd', 'br_v3_mob4', 'dz_fpd', 'xz_fpd', 'pd_fpd', 'pboc_dpd20', 'dz_v2_fpd', 'dz_v1_mob4', 'xz_v2_fpd', 'xz_v1_mob4', 'aliyun_5', 'bileizhenv1', 'duxiaoman_6', 'hengpu_4', 'hengpu_5', 'pudao_20', 'pudao_34', 'rong360_4', 'tengxun_1', 'tianchuang_7', 'feicuifen', 'zhirongfen', 'baihang_28', 'hengpu_7', 'pudao_68', 'pudao_91', 'ruizhi_6', 'ali_fraud_score3', 'ali_fraud_score9', 'umeng_score_v5', 'ppcm_behav_score', 'pudao_54', 'baihang_13', 'pudao_87', 'pudao_82', 'pudao_84', 'baihang_31', 'm1b0001_g_s', 'm1b0002_g_s', 'm1b0004_g_s', 'm1b0011_g_s', 'm1b0012_g_s', 'm1b0013_g_s', 'm1b0025_g_s', 'm1b0029_g_s', 'm1b0030_g_s', 'm1b0031_g_s', 't_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
len(varsname)


# # 2.数据探索性分析

# In[247]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[248]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[249]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1.head()


# In[250]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[251]:


df_explor_v1['missing'].value_counts()


# In[252]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[253]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[254]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[255]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[256]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[257]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# In[44]:


sql = """
    select 
     order_no
    ,max(case when variable_code = 'm1a0044' then good_score else null end) as M1A0044_g_p 
    from znzz_fintech_ads.apply_model01_scores_off 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-04-02'
    group by order_no
"""
df_m1a0044 = get_data(sql)


# In[45]:


df_m1a0044.info()


# In[46]:


df_sample.drop(columns=['m1a0044_g_p'],inplace=True)


# In[47]:


df_sample = pd.merge(df_sample, df_m1a0044, how='inner',on='order_no')


# In[48]:


df_sample.dropna(how='all',axis=1,inplace=True)


# In[54]:


varsname_v1 = [col for col in varsname if col in df_sample.columns ]
len(varsname_v1)


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[259]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.95).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.95].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==0].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.95].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:





# In[261]:


print(len(to_drop_iv))
to_drop_iv


# In[262]:


print(len(to_drop_missing))
to_drop_missing


# In[263]:


df_iv.loc[to_drop_iv,:]


# In[266]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]
varsname_v1 = varsname[:]
print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[ ]:


# train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
#                                                 target=target, 
#                                                 empty=0.90, iv=0.01, corr=0.85, 
#                                                 return_drop=True, exclude=None)
# train_selected.shape


# In[ ]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[ ]:


df_iv.loc[to_drop2,:]


# In[270]:



# varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]
varsname_v2 = varsname_v1[:]
print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[271]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[274]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return cutoffpoints


# In[275]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[276]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[277]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty


# In[278]:


new_bins_dict


# In[279]:


combiner.load(new_bins_dict)


# In[280]:


combiner.export()


# In[281]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[282]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[283]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[284]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[285]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[286]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[287]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[288]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[289]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[290]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[291]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[294]:


print(len(to_drop3))
print(to_drop3)


# In[295]:


varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
varsname_v3 = varsname_v2[:]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[296]:


target


# In[297]:


df_bins.shape
df_bins.head()


# In[ ]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[298]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[299]:


df_sample_woe.head()


# In[300]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[301]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[302]:


df_corr_matrix.head()


# In[303]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot2'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[304]:


df_high_corr


# In[306]:


print(to_drop4)


# In[307]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
varsname_v4 = varsname_v3[:]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[308]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[309]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[310]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[311]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[312]:


print(to_drop5)


# In[313]:


# varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
varsname_v5 = varsname_v4[:]
print(f"保留的变量{len(varsname_v5)}个")


# In[314]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[55]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[56]:


target


# In[57]:


modeltrian_target = 'target_mob4dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[58]:


df_sample[target].value_counts()


# In[59]:


df_sample[modeltrian_target].value_counts()


# In[60]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[ ]:


# # 查看训练数据集
# df_sample.loc[df_sample.query("data_set not in ('3_oot1','3_oot2')").index, 'data_set']='1_train'
# df_sample['data_set'].value_counts()


# In[61]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[62]:


df_sample['channel_types'].value_counts()


# In[63]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# In[ ]:





# ### 5.2.1 base模型

# In[64]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[65]:


print("最优参数opt_params: ", opt_params)


# In[67]:


varsname_v5 = varsname_v1[:]
print(len(varsname_v5))
print(varsname_v5)


# In[82]:


drop_cols = [
 'pudao_21'
,'bh_lx_115'
,'fulin_2'
,'pd_dhb_mobile_scale_score'
,'bh_alic002_3'
,'pd_dhb_mobile_risk_score_1'
,'bairong_15'
,'pudao_81'
,'baihang_26'
,'bh_alic002_1'
,'bairong_8'
,'bh_alic002_4'
,'bairong_14'
,'baihang_24'
,'baihang_23'
,'bh_lx_101'
,'tianchuang36'
,'pudao_78'
,'pd_jdxyd'
,'pudao_83'
,'pudao_86'
,'shangtang_1'
,'hangliezhi_1'
,'pudao_15'
,'pd_jd_fraud_v2'
,'pudao_77'	
,'pudao_32'
,'ruizhi_4'
,'pudao_93'
,'pudao_43'
,'aliyun_2'
,'baihang_25'
,'tengxun_cash_score'
,'tianchuang_8'
,'bh_alic002_2'
,'wanxiangfen'
,'pd_dhb_mobile_risk_score_2'
,'tianchuang24'
,'baihang_5'
,'pudao_35'
,'baihang_8']
print(len(drop_cols))


# In[83]:


varsname_base = [col for col in varsname_v5 if col not in drop_cols]


# In[84]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[85]:


df_sample['data_set'].value_counts()


# In[86]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[87]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[88]:


df_ks_auc_set_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1


# In[89]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[90]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
# df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[91]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
# df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[92]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 特征变量优化1

# In[93]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[94]:


print("最优参数opt_params: ", opt_params)


# In[97]:


drop_cols_v2 = [col for col in varsname_base if 'm1b' in col]


# In[98]:


drop_cols_v2


# In[99]:


varsname_base_v2 = [col for col in varsname_base if col not in drop_cols_v2]
print(len(varsname_base_v2))
print(varsname_base_v2)


# In[100]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v2]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[101]:


df_sample['data_set'].value_counts()


# In[102]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[103]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[104]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[106]:


df_ks_auc_set_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2


# In[107]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
# df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[108]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
# df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[109]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ### 5.3 特征变量优化2

# In[120]:


# sql = '''
# select 
#  t.order_no
# ,t_off_m4d30_2504
# ,t_off_f30_2504
# ,t_off_f30_2506
# ,t_off_m3d30_2506
# ,off_m4d30_2504
# ,off_f30_2504

# from 
#     (
#     select distinct t2.order_no
#     from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
#     inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
#     on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.id_no_des=t2.id_no_des and t1.dt=t2.dt
#     where t1.dt = date_sub(current_date(), 1) 
#       and t2.dt = date_sub(current_date(), 1) 
#       and t2.auth_status = 6
#       and t2.apply_date >= '2024-08-01'
#       and t2.apply_date <= '2025-01-05'
#     ) as t 

# -- 深圳团队的子分
# left join 
#     (
#     select 
#      order_no
#     ,max(case when variable_code = 't_off_m4d30_2504' then standard_score else null end) as t_off_m4d30_2504 
#     ,max(case when variable_code = 't_off_f30_2504' then standard_score else null end) as t_off_f30_2504 
#     ,max(case when variable_code = 't_off_f30_2506' then standard_score else null end) as t_off_f30_2506 
#     ,max(case when variable_code = 't_off_m3d30_2506' then standard_score else null end) as t_off_m3d30_2506 
#     ,max(case when variable_code = 'off_m4d30_2504' then standard_score else null end) as off_m4d30_2504 
#     ,max(case when variable_code = 'off_f30_2504' then standard_score else null end) as off_f30_2504 
#     from znzz_fintech_ads.apply_model01_scores_off 
#     where apply_time >= '2024-08-01'
#       and apply_time <= '2025-01-05'
#     group by order_no
#     ) as t2 on t.order_no=t2.order_no 
# ;
# '''

# df_off_merge = get_data(sql)


# In[122]:


df_off = pd.read_csv(result_path + 'df_off_merge.csv')
df_off.info(show_counts=True)


# In[124]:


df_off.drop(columns=['Unnamed: 0'],inplace=True)


# In[126]:


df_m4d30 = pd.read_csv(result_path + 't_off_m4d30_2504.csv')
df_m4d30.drop(columns=['Unnamed: 0'],inplace=True)
df_m4d30.info(show_counts=True)


# In[127]:


df_off_merge = pd.merge(df_m4d30, df_off,how='inner',on='order_no')
df_off_merge.info(show_counts=True)


# In[164]:


drop_cols3 = ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']


# In[165]:


df_off_merge2.info(show_counts=True)


# In[166]:


df_sample = pd.merge(df_sample.drop(columns=drop_cols3), df_off_merge2,how='left',on='order_no')
df_sample.info(show_counts=True)


# In[167]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[168]:


print(df_off_merge.columns.to_list())


# In[169]:



varsname_base_v3 = varsname_base + ['t_off_m4d30_2504', 't_off_f30_2504', 't_off_f30_2506', 't_off_m3d30_2506', 'off_m4d30_2504', 'off_f30_2504']
print(len(varsname_base_v3))
print(varsname_base_v3)


# In[170]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v3]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[171]:


df_sample['data_set'].value_counts()


# In[172]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[173]:


# 优化后评估模型效果
df_sample['y_prob_base_v3'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v3'].head()


# In[174]:


df_ks_auc_month_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'apply_month')
df_ks_auc_month_v3


# In[175]:


df_ks_auc_set_v3 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v3', 'data_set')
df_ks_auc_set_v3


# In[176]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v3 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v3 = df_importance_month_v3.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v3


# In[177]:


# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v3 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v3 = df_importance_set_v3.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v3


# In[178]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v3_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v3_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v3_{timestamp}.pkl')
print(result_path + f'{task_name}_v3_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx') as writer:
    df_importance_month_v3.to_excel(writer, sheet_name='df_importance_month_v3')
    df_importance_set_v3.to_excel(writer, sheet_name='df_importance_set_v3')
    df_ks_auc_month_v3.to_excel(writer, sheet_name='df_ks_auc_month_v3')
    df_ks_auc_set_v3.to_excel(writer, sheet_name='df_ks_auc_set_v3')      
print(result_path + f'4_模型训练_{task_name}_v3_{timestamp}.xlsx')


# In[212]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[213]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[214]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_sample[modeltrian_target], df_sample['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[151]:


# df_off_merge2 = pd.read_csv(result_path + 'df_off_merge2.csv')
# df_off_merge2.info(show_counts=True)


# In[152]:


# df_sample_ = pd.merge(df_sample_, df_off_merge2, how='left', on='order_no')


# In[418]:


df_evalue_all = df_sample_.query("target_fpd30>=0 & channel_id>1 & apply_date<='2025-03-31'")
df_evalue_all['target_fpd30_1'] = 1 - df_evalue_all['target_fpd30']


# In[419]:



lgb_model_v1 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v1_20250604190522.pkl')
varsnamev1 = lgb_model_v1.feature_name()
df_evalue_all['y_prob_base_v1'] = lgb_model_v1.predict(df_evalue_all[varsnamev1], num_iteration=lgb_model_v1.best_iteration)


# In[420]:


lgb_model_v2 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v2_20250604191712.pkl')
varsnamev2 = lgb_model_v2.feature_name()
df_evalue_all['y_prob_base_v2'] = lgb_model_v2.predict(df_evalue_all[varsnamev2], num_iteration=lgb_model_v2.best_iteration)


# In[421]:


lgb_model_v3 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v3_20250604210408.pkl')
varsnamev3 = lgb_model_v3.feature_name()
df_evalue_all['y_prob_base_v3'] = lgb_model_v3.predict(df_evalue_all[varsnamev3], num_iteration=lgb_model_v3.best_iteration)


# In[423]:


df_evalue_all['apply_month'] = df_evalue_all['apply_date'].str[0:7]


# In[422]:


df_evalue_all['channel_types'] = df_evalue_all['channel_id'].apply(channel_type)
df_evalue_all['channel_rates'] = df_evalue_all['channel_id'].apply(channel_rate)


# In[ ]:


# dfks_v3 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v3', 'apply_month')
# dfks_v3


# In[ ]:


dfks_v1 = calculate_ks_auc(df_tmp, 'target_fpd30_1', 'target_fpd30', 'y_prob_base_v1', 'apply_month')
dfks_v1


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v1'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v2'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# In[ ]:


from sklearn.metrics import auc
fpr, tpr, _ = roc_curve(df_tmp['target_fpd30_1'], df_tmp['y_prob_base_v3'], pos_label=1)
auc_value = auc(fpr, tpr)
ks_value = max(tpr - fpr)
print(auc_value,ks_value)


# ### 5.4 特征变量优化3

# In[736]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[737]:


varsname_base_v4 = ['dz_v1_mob4', 'xz_v1_mob4', 'hengpu_4', 'm1a0038_g_p', 'm1a0028_g_p', 'm1a0033_g_p', 'pudao_34', 'm1a0027_g_p', 'duxiaoman_6', 'pudao_87', 'hengpu_5', 'baihang_28', 'pudao_68', 'm1a0031_g_p', 'aliyun_5', 'pboc_dpd20', 'pudao_54', 'ruizhi_6', 'pudao_20', 'm1a0040_g_p', 'pudao_82', 'm1a0043_g_p', 'pudao_84', 'baihang_13', 'tianchuang_7', 'm1a0035_g_p', 'br_v3_mob4', 'a_pboc_fpd10_v2', 'ali_fraud_score3', 'a_bhdj_fpd10_v1', 'rong360_4', 'br_fpd_2', 'br_mob4_2', 'bileizhenv1', 'm1a0021_g_p', 'br_v3_fpd', 'a_pboc_fpd6_v1', 'm1a0011_g_p', 'pudao_91', 'm1a0044_g_p', 'baihang_31', 'm1a0026_g_p', 'a_pboc_fpd30_v1', 'xz_v2_fpd', 'umeng_score_v5', 'dz_v2_fpd', 'ali_fraud_score9', 'zhirongfen', 'm1a0023_g_p', 'br_fpd']
print(len(varsname_base_v4))
print(varsname_base_v4)


# In[738]:


to_drop_all1 = ['pudao_34','pudao_87','pboc_dpd20','ruizhi_6','pudao_82','pudao_84','baihang_13','bileizhenv1','pudao_91','baihang_31','zhirongfen']
to_drop_all2 = ['tianchuang_7','ali_fraud_score3','umeng_score_v5','dz_v2_fpd','m1a0044_g_p','a_pboc_fpd30_v1','ali_fraud_score9','m1a0023_g_p','br_fpd']
to_drop_all3 = ['m1a0027_g_p','dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
to_drop_all = to_drop_all1 + to_drop_all2 + to_drop_all3
len(to_drop_all)


# In[739]:


varsname_base_v5 = [col for col in varsname_base_v4 if col not in to_drop_all]
print(len(varsname_base_v5))
print(varsname_base_v5)


# In[740]:


varsname_base_v6 = varsname_base_v5[:] + ['dz_v1_mob4','br_v3_fpd','m1a0026_g_p','pudao_20','a_pboc_fpd10_v2','br_fpd_2','xz_v2_fpd','m1a0021_g_p']
varsname_base_v6.remove('xz_v1_mob4')
print(len(varsname_base_v6))
print(varsname_base_v6)


# In[741]:



# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[varsname_base_v6]
y_train_ = df_sample.query("data_set not in ('3_oot1','3_oot2')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'
print(X_train.shape)
df_sample['data_set'].value_counts()


# In[742]:



# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[743]:


# 优化后评估模型效果
df_sample['y_prob_base_v13'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v13'].head()


# In[ ]:


# df_sample = df_sample.query("apply_date>='2024-09-01'")
# df_sample = df_sample.reset_index(drop=True)


# In[744]:


df_ks_auc_month_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'apply_month')
df_ks_auc_month_v4


# In[745]:


df_ks_auc_set_v4 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v13', 'data_set')
df_ks_auc_set_v4


# In[746]:


vars_des = {
 'dz_v1_mob4':'授信全渠道洞侦多头模型v1四期标签202505'
,'xz_v1_mob4':'授信全渠道续侦多头模型v1四期标签202505'
,'hengpu_4':'恒普-反欺诈分M3'
,'m1a0038_g_p':'授信全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率分'
,'m1a0028_g_p':'授信全渠道人行mob4dpd30模型V1_2408_2410_好概率'
,'m1a0033_g_p':'授信全渠道百融加衍生fpd30模型2408_2411_好概率分'
,'pudao_34':'朴道-避雷针定制分V1'
,'m1a0027_g_p':'提现全渠道百融加衍生mob4dpd30模型2407_2409_好概率分'
,'duxiaoman_6':'度小满-欺诈因子V4'
,'pudao_87':'朴道-字节-互联网行为评分25128'
,'hengpu_5':'恒普-定制信用分Y'
,'baihang_28':'fico反欺诈洞见3.0'
,'aliyun_5':'朴道-阿里申请反欺诈V5'
,'pudao_68':'朴道-银商银杏定制分'
,'m1a0031_g_p':'提现全渠道人行mob5dpd30模型v1_2406_2409_好概率'
,'m1a0026_g_p':'授信全渠道续侦加衍生mob4dpd30模型2407_2409_好概率分'
,'pudao_54':'朴道-哈啰-hl-火眼分v7'
,'pudao_20':'朴道-腾讯天御反欺诈V7通用版'
,'pboc_dpd20':'授信通用人行模型dpd20标签202410'
,'m1a0043_g_p':'提现全渠道人行fpd30模型v1_2411_2502_好概率'
,'m1a0040_g_p':'授信全渠道朴道多头四期模型V1_2409_2411_好概率'
,'ruizhi_6':'FICO联合建模定制分2'
,'pudao_82':'朴道-天创-玄辰信用分_AC1501'
,'br_v3_mob4':'授信全渠道百融多头模型v3四期标签202502'
,'pudao_84':'朴道-友盟-建模分score_v3'
,'a_pboc_fpd10_v2':'授信全渠道人行模型v2fpd10标签202410'
,'rong360_4':'数据源_rong360'
,'a_bhdj_fpd10_v1':'授信全渠道百行洞见模型v1fpd10标签'
,'tianchuang_7':'天创-联合分A1502'
,'br_fpd_2':'授信百融多头模型v2首期标签202407'
,'baihang_13':'百行-灵犀产品-孚临-107'
,'m1a0011_g_p':'授信百融多头衍生v3首期标签模型202412'
,'a_pboc_fpd6_v1':'授信全渠道人行模型v1fpd6标签202409'
,'m1a0035_g_p':'授信全渠道人行fpd7模型202408_2411'
,'br_mob4_2':'授信百融多头v2四期标签202407'
,'ali_fraud_score3':'阿里申请反欺诈子分score3'
,'m1a0021_g_p':'授信全渠道洞侦加衍生fpd30模型202409_2411'
,'baihang_31':'fico普惠小版分'
,'pudao_91':'朴道-度小满-小满分桔子数科定制v2'
,'br_v3_fpd':'授信全渠道百融多头模型v3首期标签202502'
,'bileizhenv1':'避雷针v1h1'
,'xz_v2_fpd':'授信全渠道续侦多头模型v2首期标签202505'
,'dz_v2_fpd':'授信全渠道洞侦多头模型v2首期标签202505'
,'m1a0044_g_p':'提现全渠道洞侦加衍生mob4dpd30模型2409_2411_好概率'
,'umeng_score_v5':'友盟-小额分V5.0'
,'a_pboc_fpd30_v1':'授信全渠道人行模型v1fpd30标签'
,'m1a0023_g_p':'提现金科渠道续侦加衍生fpd30模型202407_2411_好概率分'
,'zhirongfen':'同盾智融分'
,'ali_fraud_score9':'阿里申请反欺诈子分score9'
,'br_fpd':'授信百融多头首期标签模型202404'
}


# In[747]:



# 模型变量重要性
# tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v4 = feature_importance(lgb_model) 
# df_importance_month_v3 = pd.merge(df_importance_month_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v4 = df_importance_month_v4.reset_index()
# df_importance_month_v3 = pd.merge(df_vars_list, df_importance_month_v3, how='right',left_on='name',right_on='feature')
df_importance_month_v4['变量名称'] = df_importance_month_v4['feature'].map(vars_des)
df_importance_month_v4


# In[748]:




# 模型变量重要性
# tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
# tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v4 = feature_importance(lgb_model) 
# df_importance_set_v3 = pd.merge(df_importance_set_v3, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v4 = df_importance_set_v4.reset_index()
# df_importance_set_v3 = pd.merge(df_vars_list, df_importance_set_v3, how='right',left_on='name',right_on='feature')
df_importance_set_v4['变量名称'] = df_importance_set_v4['feature'].map(vars_des)
df_importance_set_v4


# In[401]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v4_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v4_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v4_{timestamp}.pkl')
print(result_path + f'{task_name}_v4_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v4')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v4')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v4')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v4')      
print(result_path + f'4_模型训练_{task_name}_v4_{timestamp}.xlsx')


# In[517]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v6_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v6_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v6_{timestamp}.pkl')
print(result_path + f'{task_name}_v6_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v6')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v6')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v6')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v6')      
print(result_path + f'4_模型训练_{task_name}_v6_{timestamp}.xlsx')


# In[563]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v7_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v7_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v7_{timestamp}.pkl')
print(result_path + f'{task_name}_v7_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v7')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v7')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v7')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v7')      
print(result_path + f'4_模型训练_{task_name}_v7_{timestamp}.xlsx')


# In[573]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v8_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v8_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v8_{timestamp}.pkl')
print(result_path + f'{task_name}_v8_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v8_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v8')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v8')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v8')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v8')      
print(result_path + f'4_模型训练_{task_name}_v8_{timestamp}.xlsx')


# In[604]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v11_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v11_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v11_{timestamp}.pkl')
print(result_path + f'{task_name}_v11_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v11_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v11')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v11')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v11')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v11')      
print(result_path + f'4_模型训练_{task_name}_v11_{timestamp}.xlsx')


# In[707]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v12_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v12_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v12_{timestamp}.pkl')
print(result_path + f'{task_name}_v12_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v12')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v12')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v12')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v12')      
print(result_path + f'4_模型训练_{task_name}_v12_{timestamp}.xlsx')


# In[749]:



# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v13_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v13_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v13_{timestamp}.pkl')
print(result_path + f'{task_name}_v13_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_importance_month_v4.to_excel(writer, sheet_name='df_importance_month_v13')
    df_importance_set_v4.to_excel(writer, sheet_name='df_importance_set_v13')
    df_ks_auc_month_v4.to_excel(writer, sheet_name='df_ks_auc_month_v13')
    df_ks_auc_set_v4.to_excel(writer, sheet_name='df_ks_auc_set_v13')      
print(result_path + f'4_模型训练_{task_name}_v13_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[750]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[709]:


# lgb_model_v5 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v4_20250605181337.pkl')
# varsnamev5 = lgb_model_v5.feature_name()
# df_sample['y_prob_base_v5'] = lgb_model_v5.predict(df_sample[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[751]:


print(df_sample.columns[-17:].to_list())


# In[752]:


df_sample['high_p_m4d30_2506_v12'] = df_sample['y_prob_base_v12']
df_sample['high_p_m4d30_2506_v13'] = df_sample['y_prob_base_v13']


# In[754]:


usecols = df_sample.columns.to_list()[0:33] + ['apply_month', 'data_set', 'target_fpd30_1', 'target_mob4dpd30_1', 'channel_types', 'channel_rates', 'high_p_m4d30_2506_v12','high_p_m4d30_2506_v13','t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
print(len(usecols))
print(usecols)


# In[755]:


df_evalue = df_sample[usecols]
df_evalue.info(show_counts=True)
df_evalue.shape


# ### 5.3.2 效果对比

# In[756]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


def concat_ks_auc(tmp_df_evalue, labels_models_dict):
    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    groupkeys1 = ['apply_month']
    df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
    df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

    df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
    
    return df_ksauc_all_2


# In[778]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'data_set']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'data_set']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['data_set']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5


# In[757]:


score_list = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_1 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_1.head()


# In[758]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_m3d30_2504', 'low_m3d30_2504','free_v1_fpd', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[759]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new', 'high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[760]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[761]:


# # lgb_model_v5 = load_model_from_pkl('./result/授信全渠道高成本mob4dpd30融合模型_2409_2411_v4_20250605181337.pkl')
# # varsnamev5 = lgb_model_v5.feature_name()
# df_evalue_all['y_prob_base_v5'] = lgb_model_v5.predict(df_evalue_all[varsnamev5], num_iteration=lgb_model_v5.best_iteration)


# In[762]:


# varsnamev4 = lgb_model.feature_name()
# df_evalue_all['y_prob_base_v4'] = lgb_model.predict(df_evalue_all[varsnamev4], num_iteration=lgb_model.best_iteration)


# In[763]:


# usecols_fpd30 = [col for col in usecols if col not in ['data_set', 'target_mob4dpd30_1']]
# df_evalue_fpd30 = df_evalue_all[usecols_fpd30]
# df_evalue_fpd30.info(show_counts=True)


# In[764]:


# df_evalue_fpd30['target_fpd30_1'].value_counts()


# In[765]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30.head()


# In[766]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_2.head()


# In[767]:


# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_fpd30.loc[df_evalue_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_fpd30_3.head()


# In[768]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_融合')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_三方')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_v13_{timestamp}.xlsx')


# In[482]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
#     df_ksauc_all_1.to_excel(writer, sheet_name='mob4')
    df_ksauc_all_2.to_excel(writer, sheet_name='mob4_融合')
    df_ksauc_all_4.to_excel(writer, sheet_name='mob4_三方')
    df_ksauc_all_3.to_excel(writer, sheet_name='mob4_离线')
#     df_ksauc_all_fpd30.to_excel(writer, sheet_name='fpd30_融合')
#     df_ksauc_all_fpd30_2.to_excel(writer, sheet_name='fpd30_三方')
#     df_ksauc_all_fpd30_3.to_excel(writer, sheet_name='fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# ##### 调用征信的渠道

# In[769]:


df_evalue_pboc = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
df_evalue_pboc.info(show_counts=True)


# In[770]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['m1a0029_g_p', 'm1a0030_g_p',  'low_m3d30_2504','free_m3d30_2504', 'low_v2_fpd']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_2.head()


# In[771]:


model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['low_np_f30_2505_new', 'high_np_f30_2505_new','high_v1_fpd10']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_3.head()


# In[772]:



model_score = ['high_p_m4d30_2506_v12','high_p_m4d30_2506_v13']
vars_score = ['t_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
score_list = model_score + vars_score
print(len(score_list))
print(score_list)

target_list = ['target_mob4dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue_pboc.loc[df_evalue_pboc[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_pboc_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_pboc_4.head()


# In[773]:


# df_evalue_pboc_fpd30 = df_evalue_fpd30.query("channel_id in (227,213,231,233,240,245,241,246)")
# df_evalue_pboc_fpd30.info(show_counts=True)


# In[774]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['m1a0029_g_p', 'm1a0030_g_p', 'free_v1_fpd', 'low_v2_fpd', 'free_m3d30_2504', 'low_m3d30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30.head()


# In[775]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['third3_low_fpd', 'third3_high_fpd', 'high_v1_fpd10', 'low_np_f30_2505_new', 'high_np_f30_2505_new']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_2.head()


# In[776]:



# model_score = ['y_prob_base_v1','y_prob_base_v2','y_prob_base_v3','y_prob_base_v4','y_prob_base_v5']
# vars_score = ['xz_v1_mob4', 'dz_v1_mob4', 't_off_m4d30_2504', 't_off_f30_2504','off_m4d30_2504','off_f30_2504']
# score_list = model_score + vars_score
# print(len(score_list))
# print(score_list)

# target_list = ['target_fpd30_1']
# labels_models_dict = {target: score_list for target in target_list}

# tmp_df_evalue = df_evalue_pboc_fpd30.loc[df_evalue_pboc_fpd30[score_list].notna().all(axis=1),:]

# groupkeys2 = ['channel_types', 'apply_month']
# df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
# df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

# groupkeys4 = ['channel_rates', 'apply_month']
# df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
# df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

# groupkeys1 = ['apply_month']
# df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
# df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

# df_ksauc_all_pboc_fpd30_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
# df_ksauc_all_pboc_fpd30_3.head()


# In[442]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_离线')
    df_ksauc_all_pboc_fpd30.to_excel(writer, sheet_name='pboc_fpd30_融合')
    df_ksauc_all_pboc_fpd30_2.to_excel(writer, sheet_name='pboc_pd30_三方')
    df_ksauc_all_pboc_fpd30_3.to_excel(writer, sheet_name='pboc_fpd30_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx')


# In[777]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_v13_{timestamp}.xlsx') as writer:
    df_ksauc_all_pboc_2.to_excel(writer, sheet_name='pboc_mob4_融合')
    df_ksauc_all_pboc_3.to_excel(writer, sheet_name='pboc_mob4_三方')
    df_ksauc_all_pboc_4.to_excel(writer, sheet_name='pboc_mob4_离线')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_v13_{timestamp}.xlsx')


# # 6. 评分分布

# In[ ]:





# In[779]:


df_sample['apply_month'].value_counts()


# In[780]:


score = 'y_prob_base_v12'


# In[781]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[782]:


df_sample['score_bins'].head()


# In[783]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[784]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[785]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[787]:


score_group_by_dataset.query("groupvars=='3_oot2' & bins!='Total'").drop(columns=['ks_bin'])


# In[788]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_v12_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_v12_{timestamp}.xlsx')


# In[496]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[456]:


df_sample.to_csv(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report.csv',index=False)
print(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report.csv')


# In[789]:


df_sample.to_csv(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report_0610.csv',index=False)
print(result_path + '授信全渠道高成本mob4dpd30融合模型_2409_2411_report_0610.csv')




#==============================================================================
# File: 授信全渠道高成本子分融合模型三期标签-Copy1.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道子分融合模型三期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- 深圳团队子分
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,all_a_third_pdv3_fpd30_v_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1B0011_st_score
,M1B0012_st_score
,M1B0013_st_score
,M1B0014_st_score
,M1B0015_st_score
,ypy_bhxz_a_fpd30_v1_prob_good
,ypy_pboc_a_fpd7_v1_prob_good

-- 三方数据子分
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pd_unif_numberrisk_level_new
,ali_fraud_score3
,ali_fraud_score9
,tengxun_cash_score
,ppcm_behav_score
,umeng_score_v5
,dianhuabang_score
,duxiaoman_credit_score
,duxiaoman_cash_score
,haluo_cto_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,pudao_54
,tianchuang36
,tianchuang24
,baihang_13
,hengpu_7
,pudao_35
,pudao_78
,pudao_82
,pudao_84
,pudao_85
,rong360_4
,tengxun_1
,zhirongfen
,bh_lx_115
,baihang_31
,pudao_91
,bileizhenv1
,tianchuang_7
,hangliezhi_1
-- 北京团队模型子分
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,pboc_dpd20
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,dz_fpd
,xz_fpd
,free_v1_fpd

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-07-01'
      and t2.apply_date <= '2024-11-30'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-07-01'
      and apply_date <= '2024-11-30'
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'all_a_bhdj_fpd10_v1_p' then variable_value else null end) as all_a_bhdj_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_br_derived_fpd30_202408_g_p' then variable_value else null end) as all_a_br_derived_fpd30_202408_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p' then variable_value else null end) as all_a_br_derived_v1_mob4dpd30_202502_st_p 
    ,max(case when variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p' then variable_value else null end) as all_a_br_derived_v2_fpd30_202411_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p' then variable_value else null end) as all_a_br_derived_v3_fpd30_202412_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v1_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v2_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
    ,max(case when variable_code = 'all_a_third_pdv3_fpd30_v_p' then variable_value else null end) as all_a_third_pdv3_fpd30_v_p 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_standard_BD003 
    ,max(case when variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1' then variable_value else null end) as HLV_D_HOLO_jk_certNo_score_fpd7_v1 
    ,max(case when variable_code = 'M1A0022_g_p' then variable_value else null end) as M1A0022_g_p 
    ,max(case when variable_code = 'M1A0023_g_p' then variable_value else null end) as M1A0023_g_p 
    ,max(case when variable_code = 'M1A0026_g_p' then variable_value else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027_g_p' then variable_value else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028_g_p' then variable_value else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1B0011_st_score' then variable_value else null end) as M1B0011_st_score 
    ,max(case when variable_code = 'M1B0012_st_score' then variable_value else null end) as M1B0012_st_score 
    ,max(case when variable_code = 'M1B0013_st_score' then variable_value else null end) as M1B0013_st_score 
    ,max(case when variable_code = 'M1B0014_st_score' then variable_value else null end) as M1B0014_st_score 
    ,max(case when variable_code = 'M1B0015_st_score' then variable_value else null end) as M1B0015_st_score 
    ,max(case when variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good' then variable_value else null end) as ypy_bhxz_a_fpd30_v1_prob_good 
    ,max(case when variable_code = 'ypy_pboc_a_fpd7_v1_prob_good' then variable_value else null end) as ypy_pboc_a_fpd7_v1_prob_good 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-07-01'
      and apply_time <= '2024-11-30'
      and variable_value is not null 
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-07-01'
      and dt <= '2024-11-30'
    ) as t3 on t.order_no=t3.order_no

;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:5], varsname[-5:])
print("初始特征变量个数：",len(varsname))


# In[9]:


print(result_path)


# In[10]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[11]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob3dpd30'])['order_no'].count().unstack()


# In[12]:


df_sample_.to_csv(result_path + '授信全渠道高成本子分融合模型三期标签.csv',index=False)
print(result_path + '授信全渠道高成本子分融合模型三期标签.csv')


# In[13]:


df_sample = df_sample_.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[14]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]

df_sample.loc[df_sample.query("apply_date>='2024-07-01' & apply_date<='2024-09-30'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-10-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot'


# In[15]:


df_sample.to_csv(result_path + 'model_授信全渠道高成本子分融合模型三期标签.csv',index=False)
print(result_path + 'model_授信全渠道高成本子分融合模型三期标签.csv')


# In[16]:


target = 'target_mob3dpd30'


# In[ ]:





# # 1. 样本概况

# In[24]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[25]:


print(df_sample[target].value_counts())


# In[26]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[27]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[28]:


task_name


# In[29]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[30]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[31]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[ ]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1


# In[33]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[34]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[35]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[36]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[37]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[38]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[39]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[40]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:





# In[41]:


to_drop_iv


# In[42]:


to_drop_missing


# In[43]:


df_iv.loc[to_drop_iv,:]


# In[44]:


varsname_v1 = ['all_a_bhdj_fpd10_v1_p','all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p','all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p','all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','all_a_third_pdv3_fpd30_v_p','M1A0022_g_p','M1A0023_g_p','M1A0026_g_p','M1A0027_g_p','M1A0028_g_p','ypy_bhxz_a_fpd30_v1_prob_good','ypy_pboc_a_fpd7_v1_prob_good','duxiaoman_6','hengpu_4','aliyun_5','feicuifen','hengpu_5','rong360_4','tengxun_1','zhirongfen','bileizhenv1','tianchuang_7','hangliezhi_1','br_fpd','br_mob4','br_fpd_2','br_mob4_2','br_v3_fpd','br_v3_mob4','pboc_dpd20','dz_fpd','xz_fpd']


# In[47]:


varsname_v1 = [f'{col}'.lower() for col in varsname_v1]


# In[48]:


# varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[49]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[50]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[51]:


df_iv.loc[to_drop2,:]


# In[53]:


to_drop2 = ['all_a_third_pdv3_fpd30_v_p']
varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[54]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[55]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[56]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[57]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[59]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[60]:


new_bins_dict


# In[61]:


combiner.update(new_bins_dict)


# In[62]:


combiner.export()


# In[63]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[64]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[67]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[68]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[69]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[70]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[71]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[72]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[73]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[74]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[75]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[76]:


to_drop3 = []
varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[77]:


target


# In[78]:


df_bins.shape
df_bins.head()


# In[79]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[80]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[81]:


df_sample_woe.head()


# In[82]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[83]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[84]:


df_corr_matrix.head()


# In[85]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[86]:


df_high_corr


# In[87]:


print(to_drop4)


# In[88]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[89]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[91]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[92]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[93]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[94]:


to_drop5


# In[95]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[96]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[97]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[98]:


# df_sample = pd.read_csv(r'提现全渠道无成本子分融合模型fpd30标签_2409_2411.csv')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[99]:


target


# In[100]:


modeltrian_target = 'target_mob3dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[101]:


df_sample[target].value_counts()


# In[102]:


df_sample[modeltrian_target].value_counts()


# In[103]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[104]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[105]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[106]:


df_sample['channel_types'].value_counts()


# In[107]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[108]:


### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.1
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.8628008772208227     
opt_params['feature_fraction'] = 0.6177619614753441
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 21
opt_params['min_data_in_leaf'] = 103
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 10


# In[109]:


print("最优参数opt_params: ", opt_params)


# In[110]:


print(len(varsname_v5))
print(varsname_v5)


# In[111]:


varsname_base = varsname_v5[:]


# In[112]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[113]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[114]:


# 优化后评估模型效果
df_sample['y_prob_base_all'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_all'].head()


# In[115]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_all', 'apply_month')
df_ks_auc_month_v1


# In[117]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_all', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[119]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[120]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[122]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[123]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# ### 5.3.2 效果对比

# In[124]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[129]:


df_ksauc_all = pd.DataFrame()
for col in varsname_v1:
    model_score = ['y_prob_base_all']
    vars_score = [col]

    score_list = model_score + vars_score
    target_list = ['target_mob3dpd30_1']
    labels_models_dict = {target: score_list for target in target_list}
    
    tmp_df_evalue = df_sample.loc[df_sample[score_list].notna().all(axis=1),:]

    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    df_ksauc_all = pd.concat([df_ksauc_all, df_ksauc_all_1], axis=0)


# In[97]:


df_ksauc_all_1.to_excel(r'base模型加入不同子分时融合模型的效果对比_cpd30.xlsx')


# In[95]:


df_sample_new['fpd30_1'].value_counts()


# In[96]:


df_sample_new['fpd30_2'].value_counts()


# In[563]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p',
               'all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p',
               'ypy_bhxz_a_fpd30_v1_prob_good','xz_fpd']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[564]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['m1a0022_g_p','m1a0023_g_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)


df_ksauc_all_3 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[565]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd',
               'all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[566]:


model_score = ['y_prob_base','y_prob_base2','y_prob_base_v1','y_prob_base2_v1','y_prob_base_v2','y_prob_base2_v2',
               'y_prob_base_v3','y_prob_base2_v3','y_prob_base_v4','y_prob_base2_v4']
vars_score = ['score_fpd0_v1','score_fpd6_v1','score_fpd10_v1','score_fpd10_v2','score_fpd30_v1',
               't_mix_pboc2_dpd20','t_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list),score_list)

target_list = ['fpd30_1']
labels_models_dict = {target: score_list for target in target_list}
print(labels_models_dict)

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()

del tmp_df_evalue,score_list,labels_models_dict,model_score,vars_score
gc.collect()


# In[567]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[323]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_1.to_excel(writer, sheet_name='df_ksauc_all_1')
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# In[324]:


df_sample.to_csv(result_path + r'提现全渠道无成本子分融合模型fpd30标签_2409_2411.csv',index=False)


# # 6. 评分分布

# In[65]:


df_sample['apply_month'].value_counts()


# In[66]:


score = 'y_prob_base2'


# In[ ]:


c = toad.transform.Combiner()
c.fit(df_sample_30.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[ ]:


df_sample['score_bins'].head()


# In[380]:


score_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-08'"), 
                                                [score], 'apply_month_new', c, return_frame = False)
print(score_psi_by_month)

# score_psi_by_dataset = cal_psi_by_month(df_sample, df_sample.query("apply_month=='2024-07'"), 
#                                                 [score], 'data_set', c, return_frame = False)
# print(score_psi_by_dataset)


# In[ ]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[382]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[383]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[384]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    score_psi_by_month.to_excel(writer, sheet_name='score_psi_by_month')
#     score_psi_by_dataset.to_excel(writer, sheet_name='score_psi_by_dataset')
#     df_score_group_by_month.to_excel(writer, sheet_name='df_score_group_by_month')
#     score_group_by_month.to_excel(writer, sheet_name='score_group_by_month')
#     df_score_group_by_dataset.to_excel(writer, sheet_name='df_score_group_by_dataset')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
#     score_group_by_dataset_1.to_excel(writer, sheet_name='score_group_by_dataset_1')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')




#==============================================================================
# File: 授信全渠道高成本子分融合模型三期标签v2.py
#==============================================================================

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import toad
import lightgbm as lgb
# import shap
import hyperopt 
from hyperopt import fmin, hp, Trials, tpe, rand, anneal, STATUS_OK, partial, space_eval
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score
import joblib
import pickle
import time
from datetime import datetime
import os 
import gc
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
pd.set_option('display.width',None)
pd.set_option('display.precision', 6)


# In[2]:


# 设置数据存储
task_name = '授信全渠道子分融合模型三期标签'
timestamp = datetime.now().strftime('%Y%m%d')
directory = f'./result'
if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
result_path = f'{directory}/'


# # 函数定义

# In[3]:


# 获取数据
def get_data(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    import multiprocessing
    
    # 获取cpu核的数量
    n_process = multiprocessing.cpu_count()
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')

    print('开始跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    instance = conn.execute_sql(sql)
    # 输出执行结果
    with instance.open_reader() as reader:
        print('-------数据开始转换为DataFrame--------')
        data = reader.to_pandas(n_process=n_process) # 多核处理，避免单核处理

    end = time.time()
    print('结束跑数：' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   

    return data

# 插入数据
def execute_sql(sql):
    from odps import ODPS
    import time
    from datetime import datetime
    # 输入账号密码
    conn= ODPS(username='liaoxilin', password='j02vYCxx')
    
    print('开始跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    start = time.time()
    # 执行脚本
    conn.execute_sql(sql)
    end = time.time()
    print('结束跑数' + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("运行事件：{}秒".format(end-start))   


# # 0. 数据读取

# In[4]:


sql = f'''
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- 深圳团队子分
,all_a_bhdj_fpd10_v1_p
,all_a_br_derived_fpd30_202408_g_p
,all_a_br_derived_v1_mob4dpd30_202502_st_p
,all_a_br_derived_v2_fpd30_202411_g_p
,all_a_br_derived_v3_fpd30_202412_g_p
,all_a_dz_derived_v1_fpd30_202502_g_p
,all_a_dz_derived_v2_fpd30_202502_g_p
,all_a_rh_fpd0_v1_p
,all_a_rh_fpd10_v1_p
,all_a_rh_fpd10_v2_p
,all_a_rh_fpd30_v1_p
,all_a_rh_fpd6_v1_p
,all_a_third_pdv3_fpd30_v_p
,HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard
,HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard
,HLV_D_HOLO_certNo_variableCode_standard_BD003
,HLV_D_HOLO_jk_certNo_score_fpd7_v1
,M1A0022_g_p
,M1A0023_g_p
,M1A0026_g_p
,M1A0027_g_p
,M1A0028_g_p
,M1B0011_st_score
,M1B0012_st_score
,M1B0013_st_score
,M1B0014_st_score
,M1B0015_st_score
,ypy_bhxz_a_fpd30_v1_prob_good
,ypy_pboc_a_fpd7_v1_prob_good

-- 三方数据子分
,duxiaoman_6
,hengpu_4
,aliyun_5
,baihang_28
,pudao_34
,feicuifen
,wanxiangfen
,pudao_20
,pudao_68
,ruizhi_6
,hengpu_5
,pd_unif_numberrisk_level_new
,ali_fraud_score3
,ali_fraud_score9
,tengxun_cash_score
,ppcm_behav_score
,umeng_score_v5
,dianhuabang_score
,duxiaoman_credit_score
,duxiaoman_cash_score
,haluo_cto_score
,hengpu_dz_62_score
,hengpu_m4_v3_score
,pudao_54
,tianchuang36
,tianchuang24
,baihang_13
,hengpu_7
,pudao_35
,pudao_78
,pudao_82
,pudao_84
,pudao_85
,rong360_4
,tengxun_1
,zhirongfen
,bh_lx_115
,baihang_31
,pudao_91
,bileizhenv1
,tianchuang_7
,hangliezhi_1
-- 北京团队模型子分
,br_fpd
,br_mob4
,br_fpd_2
,br_mob4_2
,br_v3_fpd
,br_v3_mob4
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,pboc_dpd20
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,dz_fpd
,xz_fpd
,free_v1_fpd

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-07-01'
      and t2.apply_date <= '2024-11-30'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-07-01'
      and apply_date <= '2024-11-30'
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'all_a_bhdj_fpd10_v1_p' then variable_value else null end) as all_a_bhdj_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_br_derived_fpd30_202408_g_p' then variable_value else null end) as all_a_br_derived_fpd30_202408_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v1_mob4dpd30_202502_st_p' then variable_value else null end) as all_a_br_derived_v1_mob4dpd30_202502_st_p 
    ,max(case when variable_code = 'all_a_br_derived_v2_fpd30_202411_g_p' then variable_value else null end) as all_a_br_derived_v2_fpd30_202411_g_p 
    ,max(case when variable_code = 'all_a_br_derived_v3_fpd30_202412_g_p' then variable_value else null end) as all_a_br_derived_v3_fpd30_202412_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v1_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v1_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_dz_derived_v2_fpd30_202502_g_p' then variable_value else null end) as all_a_dz_derived_v2_fpd30_202502_g_p 
    ,max(case when variable_code = 'all_a_rh_fpd0_v1_p' then variable_value else null end) as all_a_rh_fpd0_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v1_p' then variable_value else null end) as all_a_rh_fpd10_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd10_v2_p' then variable_value else null end) as all_a_rh_fpd10_v2_p 
    ,max(case when variable_code = 'all_a_rh_fpd30_v1_p' then variable_value else null end) as all_a_rh_fpd30_v1_p 
    ,max(case when variable_code = 'all_a_rh_fpd6_v1_p' then variable_value else null end) as all_a_rh_fpd6_v1_p 
    ,max(case when variable_code = 'all_a_third_pdv3_fpd30_v_p' then variable_value else null end) as all_a_third_pdv3_fpd30_v_p 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_4m_BD0002_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_dpd30_6m_BD0001_standard 
    ,max(case when variable_code = 'HLV_D_HOLO_certNo_variableCode_standard_BD003' then variable_value else null end) as HLV_D_HOLO_certNo_variableCode_standard_BD003 
    ,max(case when variable_code = 'HLV_D_HOLO_jk_certNo_score_fpd7_v1' then variable_value else null end) as HLV_D_HOLO_jk_certNo_score_fpd7_v1 
    ,max(case when variable_code = 'M1A0022_g_p' then variable_value else null end) as M1A0022_g_p 
    ,max(case when variable_code = 'M1A0023_g_p' then variable_value else null end) as M1A0023_g_p 
    ,max(case when variable_code = 'M1A0026_g_p' then variable_value else null end) as M1A0026_g_p 
    ,max(case when variable_code = 'M1A0027_g_p' then variable_value else null end) as M1A0027_g_p 
    ,max(case when variable_code = 'M1A0028_g_p' then variable_value else null end) as M1A0028_g_p 
    ,max(case when variable_code = 'M1B0011_st_score' then variable_value else null end) as M1B0011_st_score 
    ,max(case when variable_code = 'M1B0012_st_score' then variable_value else null end) as M1B0012_st_score 
    ,max(case when variable_code = 'M1B0013_st_score' then variable_value else null end) as M1B0013_st_score 
    ,max(case when variable_code = 'M1B0014_st_score' then variable_value else null end) as M1B0014_st_score 
    ,max(case when variable_code = 'M1B0015_st_score' then variable_value else null end) as M1B0015_st_score 
    ,max(case when variable_code = 'ypy_bhxz_a_fpd30_v1_prob_good' then variable_value else null end) as ypy_bhxz_a_fpd30_v1_prob_good 
    ,max(case when variable_code = 'ypy_pboc_a_fpd7_v1_prob_good' then variable_value else null end) as ypy_pboc_a_fpd7_v1_prob_good 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-07-01'
      and apply_time <= '2024-11-30'
      and variable_value is not null 
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
 
------------------三方数据-----------------   
left join 
    (
    select t.*
    from znzz_fintech_ads.lxl_a_r30_three_score_data as t 
    where dt >= '2024-07-01'
      and dt <= '2024-11-30'
    ) as t3 on t.order_no=t3.order_no

;
'''

df_sample_ = get_data(sql)


# In[5]:


# df_sample_ = pd.concat(df_sample_dict.values(), ignore_index=True)
df_sample_.info(show_counts=True)
df_sample_.head()


# In[6]:


print(df_sample_.shape, df_sample_['order_no'].nunique(), df_sample_['user_id'].nunique())


# In[7]:


print(df_sample_['apply_date'].min(), df_sample_['apply_date'].max())


# In[8]:


varsname = df_sample_.columns.to_list()[9:]

print(varsname[:5], varsname[-5:])
print("初始特征变量个数：",len(varsname))


# In[9]:


print(result_path)


# In[10]:


for i, col in enumerate(varsname):
    if df_sample_[col].dtype=='object':
        print(f"======第{i}个变量：{col}========")
        df_sample_[col] = pd.to_numeric(df_sample_[col])


# In[11]:


pd.set_option('display.max_row',None)
df_sample_.groupby(['apply_date','target_mob3dpd30'])['order_no'].count().unstack()


# In[12]:


df_sample_.to_csv(result_path + '授信全渠道高成本子分融合模型三期标签.csv',index=False)
print(result_path + '授信全渠道高成本子分融合模型三期标签.csv')


# In[13]:


df_sample = df_sample_.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[25]:


df_sample = pd.read_csv(result_path + 'model_授信全渠道高成本子分融合模型三期标签.csv')


# In[26]:


varsname = ['all_a_bhdj_fpd10_v1_p','all_a_br_derived_fpd30_202408_g_p','all_a_br_derived_v1_mob4dpd30_202502_st_p','all_a_br_derived_v2_fpd30_202411_g_p','all_a_br_derived_v3_fpd30_202412_g_p','all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','all_a_rh_fpd0_v1_p','all_a_rh_fpd10_v1_p','all_a_rh_fpd10_v2_p','all_a_rh_fpd30_v1_p','all_a_rh_fpd6_v1_p','all_a_third_pdv3_fpd30_v_p','M1A0022_g_p','M1A0023_g_p','M1A0026_g_p','M1A0027_g_p','M1A0028_g_p','ypy_bhxz_a_fpd30_v1_prob_good','ypy_pboc_a_fpd7_v1_prob_good','duxiaoman_6','hengpu_4','aliyun_5','feicuifen','hengpu_5','rong360_4','tengxun_1','zhirongfen','bileizhenv1','tianchuang_7','hangliezhi_1','br_fpd','br_mob4','br_fpd_2','br_mob4_2','br_v3_fpd','br_v3_mob4','pboc_dpd20','dz_fpd','xz_fpd']
varsname = [f'{col}'.lower() for col in varsname]
print(len(varsname))


# In[27]:


df_sample.columns.to_list()[0:9]


# In[28]:


df_sample = df_sample[df_sample.columns.to_list()[0:9] + varsname]
df_sample = df_sample.reset_index(drop=True)
df_sample.info(show_counts=True)
df_sample.head()


# In[29]:


df_sample = df_sample.query("apply_date>='2024-08-01'")
df_sample = df_sample.reset_index(drop=True)


# In[30]:


df_sample['apply_month'] = df_sample['apply_date'].str[0:7]
df_sample.loc[df_sample.query("apply_date>='2024-08-01' & apply_date<='2024-10-31'").index, 'data_set']='1_train'
df_sample.loc[df_sample.query("apply_date>='2024-11-01' & apply_date<='2024-11-30'").index, 'data_set']='3_oot'


# In[31]:


df_sample.to_csv(result_path + 'model_授信全渠道高成本子分融合模型三期标签v2.csv',index=False)
print(result_path + 'model_授信全渠道高成本子分融合模型三期标签v2.csv')


# In[32]:


df_sample.info(show_counts=True)


# In[33]:


target = 'target_mob3dpd30'


# In[ ]:





# # 1. 样本概况

# In[34]:


def get_target_summary(df, target, groupby_col):
    """
    对 DataFrame 进行分组聚合，并添加一个汇总行。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 用于分组的列名
    - agg_cols: 字典，键是列名，值是聚合函数名称（如 'count', 'sum', 'mean'）
    - new_col_name: 字典,键是旧列的名称，值是新列的名称
    
    返回:
    - 包含分组聚合结果和汇总行的新 DataFrame
    """
    # 使用 groupby 和 agg 进行分组和聚合
    grouped = df.groupby(groupby_col)[target].agg(total=lambda x: len(x), 
            bad=lambda x: x.sum(), 
            good=lambda x: (x== 0).sum(), 
            bad_rate=lambda x: x.mean()).reset_index()
    
    # 将汇总行添加到分组结果中
    result = grouped.copy()
    result.rename(columns={groupby_col: 'bins'}, inplace=True)
    
    # 返回结果
    return result


# In[35]:


print(df_sample[target].value_counts())


# In[36]:


df_target_summary_month = get_target_summary(df_sample, target, 'apply_month')
print(df_target_summary_month)


# In[37]:


df_target_summary_set = get_target_summary(df_sample, target, 'data_set')
print(df_target_summary_set)


# In[40]:


task_name = '授信全渠道子分融合模型三期标签v2'


# In[41]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx") as writer:
    df_target_summary_month.to_excel(writer, sheet_name='df_target_summary_month')
    df_target_summary_set.to_excel(writer, sheet_name='df_target_summary_set')
    
print(f"数据存储完成: {timestamp}")
print(result_path + f"1_样本概况_{task_name}_{timestamp}.xlsx")


# # 2.数据探索性分析

# In[42]:


# 2.1 变量分布
df_explor = toad.detect(df_sample[varsname])
df_explor.head()


# ## 2.1缺失值处理

# In[43]:


for col in varsname:
    if df_sample[col].min()<0:
        print(f"--{col}--")
        df_sample.loc[df_sample[col]<0, col] = np.nan
gc.collect()


# In[44]:


# 2.1 变量分布
df_explor_v1 = toad.detect(df_sample[varsname])
df_explor_v1


# In[45]:


# 2.2 添加最高占比
for i, col in enumerate(varsname):
    if i>=100 and i%500==0:
        print(i)
    df_explor_v1.loc[col, 'mod_null'] = df_sample[col].value_counts(normalize=True, ascending=False, dropna=False).max()
    df_explor_v1.loc[col, 'mod_notna'] = df_sample[col].value_counts(normalize=True, ascending=False).max()


# In[46]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
df_explor.to_excel(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")

print(f"数据存储完成: {timestamp}")
print(result_path + f"2_变量分布_{task_name}_{timestamp}.xlsx")


# ## 2.2 数据探索

# In[47]:



def calculate_missing_rate_by_month(df, columns, groupby_col):
    """
    计算每个月每列的缺失率。
    
    参数:
    - df: 待处理的 DataFrame
    - groupby_col: 分组的列名
    - columns: 需要计算缺失率的列名列表
    
    返回:
    - 包含每个月每列缺失率的新 DataFrame
    """
    # 分组并计算缺失率
    missing_rates = df.groupby(groupby_col)[columns].apply(lambda x: x.isnull().sum()/len(x)).T

    return missing_rates


# In[48]:


# 2.2 缺失率按月分布
columns = varsname
groupby_col = 'apply_month'
df_miss_month = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_month.index.name = 'variable'
print(df_miss_month.head())


# In[49]:


# 2.2 缺失率按数据集分布
columns = varsname
groupby_col = 'data_set'
df_miss_set = calculate_missing_rate_by_month(df_sample, columns, groupby_col)
df_miss_set.index.name = 'variable'
print(df_miss_set.head())


# In[50]:


# 2.3 快速查看特征重要性
df_iv = toad.quality(df_sample[varsname+[target]], target, iv_only=True, 
                     method='dt', min_samples=0.05, n_bins=6)
df_iv.index.name = 'variable'
print(df_iv.head())


# In[51]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx') as writer:
        df_explor.to_excel(writer, sheet_name='df_explor')
        df_explor_v1.to_excel(writer, sheet_name='df_explor_v1')
        df_miss_month.to_excel(writer, sheet_name='df_miss_month')
        df_miss_set.to_excel(writer, sheet_name='df_miss_set')
        df_iv.to_excel(writer, sheet_name='iv') 
print(f"数据存储完成时间：{timestamp}")
print(result_path + f'2_数据探索性分析_{task_name}_{timestamp}.xlsx')


# # 3.特征粗筛选

# ## 3.1 基于自身属性删除变量

# In[52]:


# 删除近期不可使用的特征(最近月份的缺失率大于等于0.95)
to_drop_recent = list(df_miss_set[(df_miss_set>=0.90).any(axis=1)].index)
to_drop_recent = []
print("to_drop_recent:", len(to_drop_recent))

# 删除缺失率大于0.95/删除枚举值只有一个/删除方差等于0/删除集中度大于0.95
to_drop_missing = list(df_explor_v1[df_explor_v1.missing.str[:-1].astype(float)/100>=0.90].index)
print("to_drop_missing:", len(to_drop_missing))

to_drop_unique = list(df_explor_v1[df_explor_v1.unique==1].index)
print("to_drop_unique:", len(to_drop_unique))

to_drop_std = list(df_explor_v1[df_explor_v1.std_or_top2==0].index)
print("to_drop_std:", len(to_drop_std))

to_drop_mode = list(df_explor_v1[df_explor_v1.mod_notna>=0.90].index)
print("to_drop_mode:", len(to_drop_mode))

to_drop_iv = list(df_iv[df_iv.iv<=0.01].index)
print("to_drop_iv:", len(to_drop_iv))

to_drop1 = list(set(to_drop_recent + to_drop_missing +  to_drop_unique +  to_drop_std + to_drop_mode + to_drop_iv))
print(f"删除的变量有{len(to_drop1)}个")


# In[ ]:





# In[53]:


to_drop_iv


# In[54]:


to_drop_missing


# In[55]:


df_iv.loc[to_drop_iv,:]


# In[56]:


varsname_v1 = [col for col in varsname if col not in to_drop1]

print(f"保留的变量有{len(varsname_v1)}个")
print(varsname_v1[:10])


# ## 3.2 基于相关性删除变量
# 

# In[57]:


train_selected, dropped = toad.selection.select(df_sample[varsname_v1+[target]],
                                                target=target, 
                                                empty=0.90, iv=0.01, corr=0.85, 
                                                return_drop=True, exclude=None)
train_selected.shape


# In[58]:


to_drop2 = []
for k, v in dropped.items():
    print(k, ":", len(v))
    to_drop2.extend(list(v))
print(len(set(to_drop2)))


# In[59]:


df_iv.loc[to_drop2,:]


# In[60]:



varsname_v2 = [col for col in varsname_v1 if col not in to_drop2]

print(f"保留的变量有{len(varsname_v2)}个")


# # 4.特征细筛选

# ## 4.1 基于变量稳定性筛选

# In[61]:



def cal_psi_by_month(df_actual, df_expect, cols, month_col, combiner, return_frame = True):
    """
    计算每个月每的psi。
    
    参数:
    - df_actual: 测试集
    - df_expect: 训练集
    - cols: 需要计算稳定性的列名列表
    - month_col: 分组的列名

    返回:
    - 包含每个月的新 DataFrame
    """
    bins_df_list = []
    psi_list = []
    for month_, df_actual_group in df_actual.groupby(month_col):
        if return_frame:
            psi_, bins_df = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
            bins_df['month'] = month_
            bins_df_list.append(bins_df)
        else:
            psi_ = toad.metrics.PSI(df_actual_group[cols], df_expect[cols], 
                                            combiner = combiner, return_frame = return_frame)
            psi_ = pd.DataFrame({month_: psi_}, index=cols)
            psi_list.append(psi_)
        
    # 合并所有结果 DataFrame
    if return_frame:
        psi_df = pd.concat(psi_list, axis=1)
        bins_df = pd.concat(bins_df_list, axis=0)
        
        return (psi_df, bins_df)
    else:
        psi_df = pd.concat(psi_list, axis=1)
        
        return psi_df


def cal_iv_by_month(df, cols, target, month_col):
    """
    计算每个变量每个月的iv。
    
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要计算iv的列名列表
    - month_col：月份列名
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    df_ = df.copy()
    result = pd.DataFrame(columns=sorted(list(df_[month_col].unique())), index=cols)
    for col in cols:
        for month in sorted(list(df_[month_col].unique())):
            data = df_[df_[month_col] == month]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            result.loc[col, month] = regroup['iv'].sum()    
      
    return result


def calculate_vars_distribute(df, cols, target, group_col):    
    """
    参数:
    - df: 待处理的 DataFrame,已分箱
    - target: Y标签
    - cols: 需要分箱的列名列表
    - group_col：分组列名，如月份、渠道、数据类型
    
    返回:
    - 包含每个月每列iv的新 DataFrame
    """
    result = pd.DataFrame()
    vars_ = sorted(list(df[group_col].unique()))
    for col in cols:
        for var in vars_:
            data = df[df[group_col] == var]
            regroup = data.groupby(col)[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum())
            regroup['good'] = regroup['total'] - regroup['bad']
            regroup['bad_rate'] = regroup['bad']/regroup['total']
            regroup['bad_rate_cum'] = regroup['bad'].cumsum()/regroup['total'].cumsum()
            regroup['total_pct'] = regroup['total']/regroup['total'].sum()
            regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
            regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
            regroup['bad_pct_cum'] = regroup['bad_pct'].cumsum()
            regroup['good_pct_cum'] = regroup['good_pct'].cumsum()
            regroup['total_pct_cum'] = regroup['total_pct'].cumsum()
            regroup['ks_bin'] = regroup['bad_pct_cum'] - regroup['good_pct_cum']
            regroup['ks'] = regroup['ks_bin'].max()
            regroup['lift_cum'] = regroup['bad_rate_cum']/data[target].mean()
            regroup['lift'] = regroup['bad_rate']/data[target].mean()
            regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])
            regroup['iv_bins'] = (regroup['bad_pct']-regroup['good_pct'])*regroup['woe']
            regroup['iv'] = regroup['iv_bins'].sum()
            regroup['bins'] = regroup.index
                
            total_summary = data[target].agg(total=lambda x: x.count(), bad=lambda x: x.sum()).to_frame().T
            total_summary['good'] = regroup['total'] - regroup['bad']
            total_summary['bad_rate'] = total_summary['bad']/total_summary['total']
            total_summary['iv'] = regroup['iv_bins'].sum()
            total_summary['ks'] = regroup['ks_bin'].max()
            total_summary['bins'] = 'Total'
            
            regroup = pd.concat([regroup, total_summary], axis=0, ignore_index=True)
            regroup['varsname'] = col
            regroup['groupvars'] = var
            
            usecols = ['groupvars', 'varsname', 'bins', 'total', 'bad', 'good', 'bad_rate', 'bad_rate_cum', 'woe', 'iv', 'iv_bins', 
                       'ks', 'ks_bin', 'lift', 'lift_cum', 'total_pct', 'total_pct_cum', 'bad_pct', 'bad_pct_cum', 'good_pct','good_pct_cum']
            regroup = regroup[usecols]
            result = pd.concat([result, regroup], axis=0, ignore_index=True)

    return result


# In[62]:



# 删除当前索引值所在行的后一行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndexPlus1(np_regroup, index_value):
np_regroup[index_value,1] = np_regroup[index_value,1] + np_regroup[index_value+1,1]#坏客户
np_regroup[index_value,2] = np_regroup[index_value,2] + np_regroup[index_value+1,2]#好客户
np_regroup = np.delete(np_regroup, index_value+1, axis=0)

return np_regroup
 
# 删除当前索引值所在行(按从小到大排序,合并都是保留较小值)，配合左闭右开
def DelIndex(np_regroup, index_value):
np_regroup[index_value-1,1] = np_regroup[index_value,1] + np_regroup[index_value-1,1]#坏客户
np_regroup[index_value-1,2] = np_regroup[index_value,2] + np_regroup[index_value-1,2]#好客户
np_regroup = np.delete(np_regroup, index_value, axis=0)

return np_regroup 

# 删除/合并客户数为0的箱子
def MergeZero(np_regroup):
#合并好坏客户数连续都为0的箱子
i = 0
while i<=np_regroup.shape[0]-2:
    if (np_regroup[i,1]==0 and np_regroup[i+1,1]==0) or (np_regroup[i,2]==0 and np_regroup[i+1,2]==0):
        np_regroup = DelIndexPlus1(np_regroup,i)
    i = i+1
    
#合并坏客户数为0的箱子
while True:
    if all(np_regroup[:,1]>0) or np_regroup.shape[0]==2:
        break
    bad_zero_index = np.argwhere(np_regroup[:,1]==0)[0][0]
    if bad_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
    elif bad_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, bad_zero_index)
    else:
        bad_1 = np_regroup[bad_zero_index-1,1]
        good_1 = np_regroup[bad_zero_index-1,2]
        badplus1 = np_regroup[bad_zero_index+1,1]
        goodplus1 = np_regroup[bad_zero_index+1,2]           
        if (bad_1/(bad_1+good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndex(np_regroup, bad_zero_index)
        else:
            np_regroup = DelIndexPlus1(np_regroup, bad_zero_index)
#合并好客户数为0的箱子
while True:
    if all(np_regroup[:,2]>0) or np_regroup.shape[0]==2:
        break
    good_zero_index = np.argwhere(np_regroup[:,2]==0)[0][0]
    if good_zero_index==0:
        np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
    elif good_zero_index==np_regroup.shape[0]-1:
        np_regroup = DelIndex(np_regroup, good_zero_index)
    else:
        bad_1 = np_regroup[good_zero_index-1,1]
        good_1 = np_regroup[good_zero_index-1,2]
        badplus1 = np_regroup[good_zero_index+1,1]
        goodplus1 = np_regroup[good_zero_index+1,2]                
        if (bad_1/(bad_1 + good_1)) <= (badplus1/(badplus1 + goodplus1)):
            np_regroup = DelIndexPlus1(np_regroup, good_zero_index)
        else:
            np_regroup = DelIndex(np_regroup, good_zero_index)
            
return np_regroup

#箱子最小占比
def MinPct(np_regroup, threshold=0.05):
while True:
    bins_pct = [(np_regroup[i,1]+np_regroup[i,2])/np_regroup.sum() for i in range(np_regroup.shape[0])]
    min_pct = min(bins_pct)
    if np_regroup.shape[0]==2:
        print(f"箱子最小占比：箱子数达到最小值2个,最小箱子占比{min_pct}")
        break
    if min_pct>=threshold:
        print(f"箱子最小占比：各箱子的样本占比最小值: {threshold}，已满足要求")
        break
    else:
        min_pct_index = bins_pct.index(min(bins_pct))
        if min_pct_index==0:
            np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
        elif min_pct_index == np_regroup.shape[0]-1:
            np_regroup = DelIndex(np_regroup, min_pct_index)
        else:
            BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
            BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
            if BadRateDiffMin[min_pct_index-1]>=BadRateDiffMin[min_pct_index]:
                np_regroup = DelIndexPlus1(np_regroup, min_pct_index)
            else:
                np_regroup = DelIndex(np_regroup, min_pct_index)
return np_regroup


# 箱子的单调性
def MonTone(np_regroup):
while True:
    if np_regroup.shape[0]==2:
        print("箱子单调性：箱子数达到最小值2个")
        break
    BadRate = [np_regroup[i,1]/(np_regroup[i,1]+np_regroup[i,2]) for i in range(np_regroup.shape[0])]
    BadRateMonetone = [BadRate[i]<BadRate[i+1] for i in range(np_regroup.shape[0]-1)]
    #确定是否单调
    if_Montone = len(set(BadRateMonetone))
    #判断跳出循环
    if if_Montone==1:
        print("箱子单调性：各箱子的坏样本率单调")
        break
    else:
        BadRateDiffMin = [abs(BadRate[i]-BadRate[i+1]) for i in range(np_regroup.shape[0]-1)]
        Montone_index = BadRateDiffMin.index(min(BadRateDiffMin))
        np_regroup = DelIndexPlus1(np_regroup, Montone_index)
        
return np_regroup


# 变量分箱，返回分割点，特殊值不参与分箱
def Vars_Bins(data, target, col, cutbins=[]):
df = data[data[target]>=0][[target, col]]
df = df[df[col].notnull()].reset_index(drop=True)
#区间左闭右开
df['bins'] = pd.cut(df[col], cutbins, duplicates='drop', right=False, precision=4, labels=False)
regroup = pd.DataFrame()
regroup['bins'] = df.groupby(['bins'])[col].min()
regroup['total'] = df.groupby(['bins'])[target].count()
regroup['bad'] = df.groupby(['bins'])[target].sum()
regroup['good'] = regroup['total'] - regroup['bad']
regroup.drop(['total'], axis=1, inplace=True)
np_regroup = np.array(regroup)
np_regroup = MergeZero(np_regroup)
np_regroup = MinPct(np_regroup)
np_regroup = MonTone(np_regroup)
cutoffpoints = list(np_regroup[:,0])
# 判断重新分箱后最高集中度占比
mode = [(np_regroup[i,1] + np_regroup[i,2])/np_regroup.sum()>0.95 for i in range(np_regroup.shape[0])]
is_drop_mode = any(mode)
# 判断第一个分割点是否最小值
if df[col].min()==cutoffpoints[0]:
    print(f"变量{col}：最小值所在箱子没有被合并过")
    cutoffpoints=cutoffpoints[1:]

return (cutoffpoints, is_drop_mode)


# In[63]:


# 计算分布前先变量分箱
combiner = toad.transform.Combiner()
combiner.fit(df_sample[varsname_v2+[target]], y=target, 
             method='dt', n_bins=10, min_samples = 0.05, empty_separate=True) 


# In[64]:


existing_bins_dict = combiner.export()
existing_bins_dict


# In[65]:


new_bins_dict = {}
to_drop_mode = []
for i, col in enumerate(varsname_v2):
    print(f"======第{i+1}个变量：{col}=========")
    empty = [x for x in existing_bins_dict[col] if pd.isnull(x)]
    not_empty = [x for x in existing_bins_dict[col] if pd.notnull(x)]
    if len(not_empty)==0:
        continue
    
    cutbins = [float('-inf')] + not_empty + [float('inf')]
    # 确保分箱无0值，单调，最小占比符合要求
    cutbins,  is_drop_mode = Vars_Bins(df_sample, target, col, cutbins=cutbins)
    # 新的分箱分割点，符合toad包要求
    new_bins_dict[col] = cutbins + empty
    # 删除重新分箱后，高度集中的变量
    if is_drop_mode:
        print(f"{col}重新分箱后，集中度占比超95%")
        to_drop_mode.append(col)


# In[66]:


new_bins_dict


# In[67]:


combiner.update(new_bins_dict)


# In[68]:


combiner.export()


# In[69]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with open(result_path + f'变量分箱字典_{timestamp}.pkl', 'wb') as f:
    pickle.dump(new_bins_dict, f)
print(result_path + f'变量分箱字典_{timestamp}.pkl')


# In[70]:


# 计算psi
df_psi_by_month = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                    'apply_month', combiner, return_frame = False)
print("-------")
df_psi_by_set = cal_psi_by_month(df_sample, df_sample.query("data_set=='1_train'"), varsname_v2,                                  'data_set', combiner, return_frame = False)
print("-------")


# In[71]:


df_bins = combiner.transform(df_sample, labels=True)
selected_cols = ['groupvars', 'varsname', 'bins', 'total', 
                 'bad', 'good', 'total_pct', 'bad_pct', 
                 'good_pct', 'bad_rate', 'iv']


# In[72]:


# 计算iv
df_iv_by_month = cal_iv_by_month(df_bins, varsname_v2, target, 'apply_month')
print("-------")

df_iv_by_set = cal_iv_by_month(df_bins, varsname_v2, target, 'data_set')
print("-------")


# In[73]:


df_group_month = calculate_vars_distribute(df_bins, varsname_v2, target, 'apply_month')[selected_cols] 
print("-------")

df_group_set = calculate_vars_distribute(df_bins, varsname_v2, target, 'data_set')[selected_cols]   
print("-------")


# In[74]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad = pd.DataFrame()
pivot_df_iv = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_month.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad = pd.concat([df_total_bad, pivot_df], axis=0) 

    df_tmp = df_group_month.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv = pd.concat([pivot_df_iv, df_tmp_iv], axis=0)
    
print("-------")


# In[75]:


# 计算total_pct 和 # bad_rate 以及iv的时间分布
df_total_bad_set = pd.DataFrame()
pivot_df_iv_set = pd.DataFrame()
    
for i in varsname_v2:      
    df_tmp = df_group_set.query("varsname==@i & bins!='Total'")
    pivot_df_totalpct = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                           values='total_pct', fill_value=0).reset_index()
    pivot_df_badrate = df_tmp.pivot_table(index=['varsname','bins'], columns='groupvars', 
                                          values='bad_rate', fill_value=0).reset_index()
    pivot_df = pd.merge(pivot_df_totalpct, pivot_df_badrate, how='inner', 
                        on=['varsname','bins'], suffixes=('_total', '_bad'))
    df_total_bad_set = pd.concat([df_total_bad_set, pivot_df], axis=0)

    df_tmp = df_group_set.query("varsname==@i & bins=='Total'")
    df_tmp_iv = df_tmp.pivot_table(index='varsname', columns='groupvars', values='iv').reset_index()
    pivot_df_iv_set = pd.concat([pivot_df_iv_set, df_tmp_iv], axis=0)
    
    pivot_df_badrate_iv = pd.merge(pivot_df_badrate, pivot_df_iv_set, how='inner', 
                        on=['varsname'], suffixes=('_bad', '_iv'))
        
print("-------")


# ### 删除不稳定特征

# In[76]:


drop_by_psi_month = list(df_psi_by_month[df_psi_by_month>=0.10].dropna(how='all').index) 
drop_by_psi_set = list(df_psi_by_set[df_psi_by_set>=0.10].dropna(how='all').index)
print("drop_by_psi_month: ", len(drop_by_psi_month))
print("drop_by_psi_set: ", len(drop_by_psi_set))


drop_by_iv_month = list(df_iv_by_month[df_iv_by_month<=0.01].dropna(how='all').index)
drop_by_iv_set = list(df_iv_by_set[df_iv_by_set<=0.01].dropna(how='all').index)
print("drop_by_iv_month: ", len(drop_by_iv_month))
print("drop_by_iv_set: ", len(drop_by_iv_set))


# In[77]:



to_drop3 = list(set(drop_by_psi_month + drop_by_psi_set + drop_by_iv_month + drop_by_iv_set))
print("剔除的变量有: ", len(to_drop3))


# In[78]:


df_iv_by_set.loc[drop_by_iv_set,:]


# In[79]:


df_psi_by_set.loc[drop_by_psi_set,:]


# In[80]:


to_drop3 = ['ypy_pboc_a_fpd7_v1_prob_good']
varsname_v3 = [ col for col in varsname_v2 if col not in to_drop3]
print(f"保留的变量有{len(varsname_v3)}个: ")


# ## 4.2 Y标签相关性删除

# In[81]:


target


# In[82]:


df_bins.shape
df_bins.head()


# In[83]:



# def calculate_woe(df, col, target):
#     """
#     计算给定分箱列的WOE值，并返回一个字典，用于后续映射。
#     :param df: DataFrame 包含分箱和目标变量
#     :param binned_col: 分箱变量名
#     :param target_col: 目标变量名
#     :return: WOE值的字典
#     """
#     regroup = df.groupby(col)[target].agg(['sum', 'count']).rename(columns={'sum': 'bad', 'count': 'total'})
#     regroup['good'] = regroup['total'] - regroup['bad']
#     regroup['bad_pct'] = regroup['bad']/regroup['bad'].sum() + 1e-10
#     regroup['good_pct'] = regroup['good']/regroup['good'].sum() + 1e-10
#     regroup['woe'] = np.log(regroup['bad_pct']/regroup['good_pct'])

#     return regroup['woe'].to_dict()   


# In[84]:


# 计算相关性
exclude = [col for col in df_bins.columns if col not in varsname_v3]

transer = toad.transform.WOETransformer()
print('--------')
df_sample_woe = transer.fit_transform(df_bins, df_bins[target], exclude=exclude)
print(df_sample_woe.shape) 


# In[85]:


df_sample_woe.head()


# In[86]:


def find_high_correlation_pairs(df, iv_series, threshold=0.80):
    """
    找出相关系数大于指定阈值的变量对，并排除对角线。保留IV值较大的变量。

    :param df: 输入的DataFrame
    :param iv_series: 包含每个变量的IV值的Series，变量名为行索引
    :param threshold: 相关系数的阈值，默认为0.80
    :return: 包含高相关性变量对及其相关系数的DataFrame，以及保留的变量
    """
    # 计算相关系数矩阵
    corr_matrix = df.copy()
    # 初始化一个空列表来存储高相关性变量对
    high_corr_pairs = []
    # 遍历相关系数矩阵
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                high_corr_pairs.append((var1, var2, corr_value))
    
    # 将结果转换为DataFrame
    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Var1', 'Var2', 'Correlation'])
    # 初始化一个空列表来存储需要删除的变量
    to_remove = set()
    # 初始化一个空列表，用于记录被剔除的变量（对应每一行）
    removed_vars = []
    # 遍历高相关性变量对，保留IV值较大的变量，删除IV值较小的变量
    for _, row in high_corr_df.iterrows():
        var1 = row['Var1']
        var2 = row['Var2']
        iv1 = iv_series[var1]
        iv2 = iv_series[var2]
        
        if iv1 >= iv2:
            to_remove.add(var2)
            removed_vars.append(var2)
        else:
            to_remove.add(var1)
            removed_vars.append(var1)
    high_corr_df['Removed_Variable'] = removed_vars
    # 返回高相关性变量对及其相关系数，以及保留的变量
    return high_corr_df, list(to_remove)


# In[87]:


# param method: 计算相关系数的方法，可以是'pearson', 'kendall', 'spearman'
df_corr_matrix = df_sample_woe[varsname_v3].corr(method='spearman')
df_corr_matrix.info()


# In[88]:


df_corr_matrix.head()


# In[89]:


# 调用函数

df_high_corr, to_drop4 = find_high_correlation_pairs(df_corr_matrix,
                                                     df_iv_by_set['3_oot'],
                                                     threshold=0.70)

# 查看结果
print("删除的变量有：", len(to_drop4))


# In[90]:


df_high_corr


# In[91]:


print(to_drop4)


# In[92]:


varsname_v4 = [col for col in varsname_v3 if col not in to_drop4]
print(f"保留变量{len(varsname_v4)}个")
print(varsname_v4)


# In[93]:


def calculate_correlations(df, group_col, varsname, target, method='pointbiserialr'):
    from scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau
    
    # 按指定的分组列分组
    grouped = df.groupby(group_col)
    # 初始化一个空的DataFrame来存储所有分组的相关系数
    all_corrs = pd.DataFrame()
    all_pvalue = pd.DataFrame()
    # 遍历每个分组
    for name, group in grouped:      
        # 计算每个变量与目标变量的相关系数
        # 初始化一个空的字典来存储结果
        corr_series = pd.Series(index=varsname)
        corr_series.name = name
        result_pvalue = pd.Series(index=varsname)
        result_pvalue.name = name
        
        binary_var = group[target]
        # 计算每个连续变量与二分类变量之间的点二列相关系数
        for column in varsname:
            if method=='pointbiserialr':
                corr_series[column] =  pointbiserialr(binary_var, group[column])[0]
                result_pvalue[column] =  pointbiserialr(binary_var, group[column])[1]
            elif method=='pearsonr':
                corr_series[column] =  pearsonr(binary_var, group[column])[0]
                result_pvalue[column] =  pearsonr(binary_var, group[column])[1]
            elif method=='spearmanr':
                corr_series[column] =  spearmanr(binary_var, group[column])[0]
                result_pvalue[column] =  spearmanr(binary_var, group[column])[1]
            elif method=='kendalltau':
                corr_series[column] =  kendalltau(binary_var, group[column])[0]
                result_pvalue[column] =  kendalltau(binary_var, group[column])[1]
            else:
                raise ValueError("Invalid method. Choose from 'pointbiserialr','pearson', 'spearman', or 'kendall'.")

        # corr_series = group[varsname].corrwith(group[target], method=method)
        # 将结果添加到总的DataFrame中，并添加分组标识
        all_corrs = pd.concat([all_corrs, corr_series], axis=1)
        all_pvalue = pd.concat([all_pvalue, result_pvalue], axis=1)
    
    # 返回包含所有分组相关系数的DataFrame
    return (all_corrs, all_pvalue)


# In[94]:


# 调用函数
df_corr_vars_target, df_pvalue_vars_target = calculate_correlations(df_sample_woe,
                                                                    'apply_month',
                                                                    varsname_v4,
                                                                    target,
                                                                    method='pointbiserialr'
                                                                   )

# 查看前几行
# df_corr_vars_target


# In[95]:


df_corr_vars_target.info()
df_corr_vars_target.head()


# In[96]:


to_drop5 = list(df_corr_vars_target[df_corr_vars_target.apply(lambda row: (row > 0).any() and (row < 0).any(), axis=1)].index)
print("删除的变量有：", len(to_drop5))


# In[97]:


to_drop5


# In[98]:


varsname_v5 = [ col for col in varsname_v4 if col not in to_drop5]
print(f"保留的变量{len(varsname_v5)}个")


# In[99]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx') as writer:
        df_group_month.to_excel(writer, sheet_name='df_group_month') 
        df_group_set.to_excel(writer, sheet_name='df_group_set') 
        df_iv_by_month.to_excel(writer, sheet_name='df_iv_by_month')
        df_iv_by_set.to_excel(writer, sheet_name='df_iv_by_set')
        df_psi_by_month.to_excel(writer, sheet_name='df_psi_by_month')
        df_psi_by_set.to_excel(writer, sheet_name='df_psi_by_set')
        df_pvalue_vars_target.to_excel(writer, sheet_name='df_pvalue_vars_target')
        df_corr_vars_target.to_excel(writer, sheet_name='df_corr_vars_target')
        df_total_bad.to_excel(writer, sheet_name='df_total_bad')
        df_total_bad_set.to_excel(writer, sheet_name='df_total_bad_set')
        pivot_df_iv.to_excel(writer, sheet_name='pivot_df_iv')
        pivot_df_iv_set.to_excel(writer, sheet_name='pivot_df_iv_set') 
        pivot_df_badrate_iv.to_excel(writer, sheet_name='pivot_df_badrate_iv')
        df_corr_matrix.to_excel(writer, sheet_name='df_corr_matrix')
print(f"数据存储完成时间：{timestamp}！")        
print(result_path + f'3_变量分析_dis_iv_psi_{task_name}_{timestamp}.xlsx')


# # 5.模型训练

# ## 5.0 函数定义

# In[100]:



def model_ks_auc(df, target, y_pred, group_col):
    """
    Args:
        df (dataframe): 含有Y标签和预测分数的数据集
        target (string): Y标签列名
        y_pred (string): 坏概率分数列名
        group_col (string): 分组列名如月份，数据集

    Returns:
        dataframe: AUC和KS值的数据框
    """
    df_ks_auc = pd.DataFrame(index=['KS', 'AUC'])
    for col, group_df in df.groupby(group_col):  
        # 计算 AUC
        group_df = group_df[(group_df[target].notna())&(group_df[y_pred].notna())]
        auc_ = roc_auc_score(group_df[target], group_df[y_pred])      
        fpr, tpr, _ = roc_curve(group_df[target], group_df[y_pred], pos_label=1)
        ks_ = max(abs(tpr-fpr))
        df_ks_auc.loc['KS', col] = ks_
        df_ks_auc.loc['AUC', col] = auc_
#         print(f"{col}：KS值{ks_}，AUC值{auc_}")
    df_ks_auc = df_ks_auc.T
    
    return df_ks_auc



def feature_importance(model):
    if isinstance(model, lgb.Booster):
        print("这是原生接口的模型 (Booster)")
        # 获取特征重要性
        feature_importance_gain = model.feature_importance(importance_type='gain')
        feature_importance_split = model.feature_importance(importance_type='split')
        # 获取特征名称
        feature_names = model.feature_name()
        # 将特征重要性转换为数据框
        df_importance = pd.DataFrame({'gain': feature_importance_gain,
                                      'split': feature_importance_split}, 
                                     index=feature_names)
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.index.name = 'feature'
    elif isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor)):
        print("这是 sklearn 接口的模型")
        feature_names = model.feature_name_
        feature_importances_split = model.booster_.feature_importance(importance_type='split')
        importance_type_split = pd.DataFrame({'Feature': feature_names, 'split': feature_importances_split})
        importance_type_split = importance_type_split.sort_values('split', ascending=False)
        importance_type_split['split_pct'] = importance_type_split['split'] / importance_type_split['split'].sum()

        feature_importances_gain = model.booster_.feature_importance(importance_type='gain')
        importance_type_gain = pd.DataFrame({'Feature': feature_names, 'gain': feature_importances_gain})
        importance_type_gain = importance_type_gain.sort_values('gain', ascending=False)
        importance_type_gain['gain_pct'] = importance_type_gain['gain'] / importance_type_gain['gain'].sum()

        df_importance = pd.merge(importance_type_gain, importance_type_split, how='inner', on='Feature')
        df_importance = df_importance.sort_values('gain', ascending=False)
        df_importance.set_index('Feature', inplace=True)
        df_importance.index.name = 'feature'
    else:
        print("未知模型类型")
        df_importance = None
    
    return df_importance


# Pickle方式保存和读取模型
def save_model_as_pkl(model, path):
    """
    保存模型到路径path
    :param model: 训练完成的模型
    :param path: 保存的目标路径
    """
    import pickle
    with open(path, 'wb') as f:
        pickle.dump(model, f, protocol=2)
        
# xgb模型保存.bin 格式
def save_model_as_bin(model, save_file_path):
    #保存lgb模型为bin格式
    model.save_model(save_file_path)
    

def load_model_from_pkl(path):
    """
    从路径path加载模型
    :param path: 保存的目标路径
    """
    
    with open(path, 'rb') as f:
        model = pickle.load(f)
    return model

def channel_type(x):
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        channel='金科渠道'
    elif x==1:
        channel='桔子商城'
    else:
        channel='api渠道'
    return channel

def channel_rate(x): #(227,213,231,233,240,245,241,246)
    if x in (209, 213, 229, 233, 235, 236, 240, 241, 244, 226, 227, 231, 234, 245, 246, 247, 248, 249, 251):
        if x == 227:
            channel='227渠道'
        elif x in (209, 213, 229, 233, 235, 236, 240, 241, 244):
            channel='24利率'
        elif x in (226, 227, 231, 234, 245, 246, 247):
            channel='36利率'
        else:
            channel=None
    else:
        channel=None

    return channel


def calculate_ks_auc(df, modeltrian_target, target, y_pred, groupkeys):

    # 最初评估模型效果 
    tmp_df = df.query("channel_id!=1").reset_index(drop=True)
    df_ks_auc = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
    df_ks_auc['渠道'] = '全渠道'
    tmp = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
    df_ks_auc = pd.concat([tmp, df_ks_auc], axis=1)
    
    df_ks_auc_type = pd.DataFrame()
    for type_, tmp_df in df.groupby('channel_types'):
        print(f'--------{type_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = type_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_type = pd.concat([df_ks_auc_type, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    df_ks_auc_rate = pd.DataFrame()
    for rate_, tmp_df in df.groupby('channel_rates'):
        print(f'--------{rate_}----------')
        tmp1 = model_ks_auc(tmp_df, modeltrian_target, y_pred, groupkeys)
        tmp1['渠道'] = rate_
        tmp2 = get_target_summary(tmp_df, target, groupkeys).set_index('bins')
        df_ks_auc_rate = pd.concat([df_ks_auc_rate, pd.concat([tmp2, tmp1], axis=1)], axis=0)

    # 合并
    df_ks_auc = pd.concat([df_ks_auc, df_ks_auc_type, df_ks_auc_rate])
    
    return df_ks_auc


# ## 5.1 数据预处理

# In[101]:


# df_sample = pd.read_csv(r'提现全渠道无成本子分融合模型fpd30标签_2409_2411.csv')
# df_sample.info(show_counts=True)
# df_sample.head()


# In[102]:


target


# In[103]:


modeltrian_target = 'target_mob3dpd30_1'
df_sample[modeltrian_target] = 1 - df_sample[target]


# In[104]:


df_sample[target].value_counts()


# In[105]:


df_sample[modeltrian_target].value_counts()


# In[106]:


# 查看训练数据集
df_sample['data_set'].value_counts()


# In[107]:


# 查看训练数据集
df_sample.loc[df_sample.query("data_set not in ('3_oot')").index, 'data_set']='1_train'
df_sample['data_set'].value_counts()


# In[108]:


df_sample['channel_types'] = df_sample['channel_id'].apply(channel_type)
df_sample['channel_rates'] = df_sample['channel_id'].apply(channel_rate)


# In[109]:


df_sample['channel_types'].value_counts()


# In[110]:


df_sample['channel_rates'].value_counts()


# ## 5.2 模型训练

# ### 5.2.1 base模型

# In[111]:


# ### 模型参数
# opt_params = {}
# opt_params['boosting'] = 'gbdt'
# opt_params['objective'] = 'binary'
# opt_params['metric'] = 'auc'
# opt_params['bagging_freq'] = 1
# opt_params['scale_pos_weight'] = 1 
# opt_params['seed'] = 1 
# opt_params['num_threads'] = -1 
# # 调参时设置成不用调参的参数
# opt_params['learning_rate'] = 0.1
# ## 正则参数，防止过拟合
# opt_params['bagging_fraction'] = 0.8628008772208227     
# opt_params['feature_fraction'] = 0.6177619614753441
# opt_params['lambda_l1'] = 0
# opt_params['lambda_l2'] = 300
# opt_params['early_stopping_rounds'] = 50

# # 调参后的参数需要变成整数型
# opt_params['num_leaves'] = 21
# opt_params['min_data_in_leaf'] = 103
# opt_params['max_depth'] = 2
# # 调参后的其他参
# opt_params['min_gain_to_split'] = 10

### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 4
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.60


# In[112]:


print("最优参数opt_params: ", opt_params)


# In[113]:


print(len(varsname_v5))
print(varsname_v5)


# In[114]:


varsname_base = varsname_v5[:]


# In[115]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[116]:


df_sample['data_set'].value_counts()


# In[117]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[118]:


# 优化后评估模型效果
df_sample['y_prob_base_v1'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v1'].head()


# In[119]:


df_ks_auc_month_v1 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v1', 'apply_month')
df_ks_auc_month_v1


# In[120]:


df_ks_auc_set_v1 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v1', 'data_set')
df_ks_auc_set_v1['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v1 = pd.concat([tmp, df_ks_auc_set_v1], axis=1)
df_ks_auc_set_v1


# In[121]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v1 = feature_importance(lgb_model) 
df_importance_month_v1 = pd.merge(df_importance_month_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v1 = df_importance_month_v1.reset_index()
# df_importance_month_v1 = pd.merge(df_vars_list, df_importance_month_v1, how='right',left_on='name',right_on='feature')
df_importance_month_v1


# In[122]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v1 = feature_importance(lgb_model) 
df_importance_set_v1 = pd.merge(df_importance_set_v1, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v1 = df_importance_set_v1.reset_index()
# df_importance_set_v1 = pd.merge(df_vars_list, df_importance_set_v1, how='right',left_on='name',right_on='feature')
df_importance_set_v1


# In[123]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v1_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v1_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v1_{timestamp}.pkl')
print(result_path + f'{task_name}_v1_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx') as writer:
    df_importance_month_v1.to_excel(writer, sheet_name='df_importance_month_v1')
    df_importance_set_v1.to_excel(writer, sheet_name='df_importance_set_v1')
    df_ks_auc_month_v1.to_excel(writer, sheet_name='df_ks_auc_month_v1')
    df_ks_auc_set_v1.to_excel(writer, sheet_name='df_ks_auc_set_v1')      
print(result_path + f'4_模型训练_{task_name}_v1_{timestamp}.xlsx')


# ### 5.2 参数优化

# In[124]:


# ### 模型参数
# opt_params = {}
# opt_params['boosting'] = 'gbdt'
# opt_params['objective'] = 'binary'
# opt_params['metric'] = 'auc'
# opt_params['bagging_freq'] = 1
# opt_params['scale_pos_weight'] = 1 
# opt_params['seed'] = 1 
# opt_params['num_threads'] = -1 
# # 调参时设置成不用调参的参数
# opt_params['learning_rate'] = 0.1
# ## 正则参数，防止过拟合
# opt_params['bagging_fraction'] = 0.8628008772208227     
# opt_params['feature_fraction'] = 0.6177619614753441
# opt_params['lambda_l1'] = 0
# opt_params['lambda_l2'] = 300
# opt_params['early_stopping_rounds'] = 50

# # 调参后的参数需要变成整数型
# opt_params['num_leaves'] = 21
# opt_params['min_data_in_leaf'] = 103
# opt_params['max_depth'] = 2
# # 调参后的其他参
# opt_params['min_gain_to_split'] = 10

### 模型参数
opt_params = {}
opt_params['boosting'] = 'gbdt'
opt_params['objective'] = 'binary'
opt_params['metric'] = 'auc'
opt_params['bagging_freq'] = 1
opt_params['scale_pos_weight'] = 1 
opt_params['seed'] = 1 
opt_params['num_threads'] = -1 
# 调参时设置成不用调参的参数
opt_params['learning_rate'] = 0.02
## 正则参数，防止过拟合
opt_params['bagging_fraction'] = 0.6     
opt_params['feature_fraction'] = 0.99
opt_params['lambda_l1'] = 0
opt_params['lambda_l2'] = 300
opt_params['early_stopping_rounds'] = 50

# 调参后的参数需要变成整数型
opt_params['num_leaves'] = 59
opt_params['min_data_in_leaf'] = 115
opt_params['max_depth'] = 2
# 调参后的其他参
opt_params['min_gain_to_split'] = 0.60


# In[125]:


print("最优参数opt_params: ", opt_params)


# In[126]:


print(len(varsname_v5))
print(varsname_v5)


# In[127]:


varsname_base = varsname_v5[:]


# In[128]:


# 确定数据集参数后，训练模型

X_train_ = df_sample.query("data_set not in ('3_oot')")[varsname_base]
y_train_ = df_sample.query("data_set not in ('3_oot')")[modeltrian_target]
X_train, X_test, y_train, y_test = train_test_split(X_train_,
                                                    y_train_,
                                                    test_size=0.2, 
                                                    random_state=22, 
                                                    stratify=y_train_
                                                   )
df_sample.loc[X_train.index, 'data_set']='1_train'
df_sample.loc[X_test.index, 'data_set']='2_test'


# In[129]:


df_sample['data_set'].value_counts()


# In[130]:


# 6，训练/保存/评估模型
# 优化训练模型
train_set = lgb.Dataset(X_train, label=y_train)
valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)
lgb_model = lgb.train(opt_params, train_set, valid_sets=valid_set, num_boost_round=10000)


# In[131]:


# 优化后评估模型效果
df_sample['y_prob_base_v2'] = lgb_model.predict(df_sample[X_train.columns], num_iteration=lgb_model.best_iteration)
df_sample['y_prob_base_v2'].head()


# In[132]:


df_ks_auc_month_v2 = calculate_ks_auc(df_sample, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_v2


# In[133]:


df_ks_auc_set_v2 = model_ks_auc(df_sample, modeltrian_target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_v2['渠道'] = '全渠道'
tmp = get_target_summary(df_sample, target, 'data_set').set_index('bins')
df_ks_auc_set_v2 = pd.concat([tmp, df_ks_auc_set_v2], axis=1)
df_ks_auc_set_v2


# In[137]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_month, df_psi_by_month, df_miss_month, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_month.columns] + [f'{col}_psi' for col in df_psi_by_month.columns] + [f'{col}_na' for col in df_miss_month.columns] + ['total_na']
df_importance_month_v2 = feature_importance(lgb_model) 
df_importance_month_v2 = pd.merge(df_importance_month_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_month_v2 = df_importance_month_v2.reset_index()
# df_importance_month_v2 = pd.merge(df_vars_list, df_importance_month_v2, how='right',left_on='name',right_on='feature')
df_importance_month_v2


# In[138]:


# 模型变量重要性
tmp = pd.concat([df_iv_by_set, df_psi_by_set, df_miss_set, df_explor.loc[:,'missing']], axis=1)
tmp.columns = [f'{col}_iv' for col in df_iv_by_set.columns] + [f'{col}_psi' for col in df_psi_by_set.columns] + [f'{col}_na' for col in df_miss_set.columns] + ['total_na']
df_importance_set_v2 = feature_importance(lgb_model) 
df_importance_set_v2 = pd.merge(df_importance_set_v2, tmp, how='left', left_index=True, right_index=True)
df_importance_set_v2 = df_importance_set_v2.reset_index()
# df_importance_set_v2 = pd.merge(df_vars_list, df_importance_set_v2, how='right',left_on='name',right_on='feature')
df_importance_set_v2


# In[139]:


# 效果评估后保存模型
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
save_model_as_pkl(lgb_model, result_path + f'{task_name}_v2_{timestamp}.pkl')
save_model_as_bin(lgb_model, result_path + f'{task_name}_v2_{timestamp}.bin')
print(f"模型保存完成！：{timestamp}")
print(result_path + f'{task_name}_v2_{timestamp}.pkl')
print(result_path + f'{task_name}_v2_{timestamp}.bin')

with pd.ExcelWriter(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx') as writer:
    df_importance_month_v2.to_excel(writer, sheet_name='df_importance_month_v2')
    df_importance_set_v2.to_excel(writer, sheet_name='df_importance_set_v2')
    df_ks_auc_month_v2.to_excel(writer, sheet_name='df_ks_auc_month_v2')
    df_ks_auc_set_v2.to_excel(writer, sheet_name='df_ks_auc_set_v2')      
print(result_path + f'4_模型训练_{task_name}_v2_{timestamp}.xlsx')


# ## 5.3 模型效果对比

# In[140]:


df_sample.info(show_counts=True)


# ### 5.3.1数据处理

# In[142]:


df_sample_all = pd.read_csv(result_path + '授信全渠道高成本子分融合模型三期标签.csv')
df_sample_all = df_sample_all.query("target_mob3dpd30>=0 & channel_id != 1").reset_index(drop=True)


# In[143]:


usecols = ['order_no','sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'mix_pboc_dpd20'
,'gen7_fpd'
,'gen7_mob4'
,'free_v1_fpd']


# In[193]:


sql = """
select 
 t.order_no
,t.user_id
,t.id_no_des
,t.channel_id
,t.apply_date
,t.target_fpd30
,t.target_cpd30
,t.target_mob4dpd30
,t.target_mob3dpd30
-- 深圳团队子分
,good_score as M1A0030_g_p
,M1A0029_g_p

-- 北京团队模型子分
,sf_mob4_1_v2
,gen4_fpd
,gen4_mob4
,sf_simple_fpd
,sf_simple_mob4
,simple2_fpd
,simple2_mob4
,gen5_fpd
,gen5_mob4
,gen6_fpd
,gen6_mob4
,mix_pboc_dpd20
,gen7_fpd
,gen7_mob4
,free_v1_fpd
,low_v2_fpd30

from 
    (
    select 
     t2.order_no
    ,t1.user_id
    ,t1.id_no_des
    ,t1.channel_id
    ,t2.apply_date
    ,target_fpd30
    ,target_cpd30
    ,target_mob4dpd30
    ,target_mob3dpd30
    from znzz_fintech_ads.dm_f_zzj_test_user_target as t1 
    inner join znzz_fintech_dwd.dwd_beforeloan_auth_examine_fd as t2
    on t1.user_id=t2.user_id and t1.channel_id=t2.channel_id and t1.dt=t2.dt
    where t1.dt = date_sub(current_date(), 1) 
      and t2.dt = date_sub(current_date(), 1) 
      and t2.auth_status = 6
      and t2.apply_date >= '2024-08-01'
      and t2.apply_date <= '2025-02-10'
    ) as t 
-- 北京团队的子分
left join 
    (
    select t.*
    from znzz_fintech_ads.dm_f_cnn_test_credit_ps_allc as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-02-10'
    ) as t1 on t.order_no=t1.order_no

-- 深圳团队的子分
left join 
    (
    select 
     order_no
    ,max(case when variable_code = 'M1A0029_g_p' then variable_value else null end) as M1A0029_g_p 
    from znzz_fintech_ads.apply_model01_scores_vars as t 
    where apply_time >= '2024-08-01'
      and apply_time <= '2025-02-10'
      and variable_value is not null 
      and variable_code = 'M1A0029_g_p'
    group by order_no
    ) as t2 on t.order_no=t2.order_no 
left join 
    (
    select 
     order_no
    ,good_score
    from znzz_fintech_ads.lxl_a_pboc_submodel_merge_model_mob3pdp30_score_auth_v2 as t 
    where apply_date >= '2024-08-01'
      and apply_date <= '2025-02-10'
    ) as t3 on t.order_no=t3.order_no    
;
"""
df_1 = get_data(sql)


# In[194]:


df_1.info(show_counts=True)


# In[185]:


df_evalue_copy = df_evalue.copy()


# In[186]:


df_evalue.shape


# In[187]:


df_evalue = pd.merge(df_evalue_copy, df_1[['order_no','m1a0029_g_p']],  how='inner', on='order_no')
df_evalue.shape


# In[202]:


df_evalue = df_1.query("channel_id!=1")
df_evalue = df_evalue.reset_index(drop=True)
df_evalue.info(show_counts=True)


# In[144]:


df_evalue = pd.merge(df_sample, df_sample_all[usecols],  how='inner', on='order_no')
df_evalue.shape


# ### 5.3.2 效果对比

# In[146]:


# 小数转换百分数
def to_percentage(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return f"{x * 100:.2f}%"
    return x

def float_format(x):
    if isinstance(x, (float)) and pd.notnull(x):
        return '%.3f' %x
    return x

def cal_data_item(df, label_col, score_col):
    from sklearn.metrics import auc
    fpr, tpr, _ = roc_curve(df[label_col], df[score_col], pos_label=1)
    auc_value = auc(fpr, tpr)
    ks_value = max(abs(tpr - fpr))
#     badrate = df[label_col].mean()
    
#     if percentile>=0.90:#概率分数是坏分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]>pct_n][label_col].mean()
#     elif percentile<=0.10:#概率分数是好分数，计算最坏5%客群的lift
#         pct_n = df[score_col].quantile(percentile)
#         pct_n_badrate = df[df[score_col]<pct_n][label_col].mean()
#     else:
#         print("请根据概率分数是好分数还是坏分数，决定分位数的位置")
    
#     if badrate>0 and pct_n_badrate>0:
#         lift_n = pct_n_badrate/badrate
#     else:
#         lift_n = np.nan
#     data = pd.Series({'KS': ks_value, 'AUC': auc_value, 'top5lift':lift_n})
    data = pd.Series({'KS': ks_value, 'AUC': auc_value})
    
    return data


# 计算KS
def cal_ks_auc(df, groupkeys, model_score_label_dict):
    # groupkeys: 分组字段
    # model_score_label_dict: value: score_list: 得分字段列表, key: label_list: 标签字段列表
    # df: 有标签和得分的数据框
    # 输出KS、AUC
    
    ks_auc_result = pd.DataFrame()
    if not isinstance(groupkeys, list):
        groupkeys = [groupkeys]
    for label_, score_list in model_score_label_dict.items():
        data1 = df[df[label_]>=0]
        total_bad = data1.groupby(groupkeys)[label_].agg(total=lambda x: len(x), 
                                                        bad=lambda x: x.sum(), 
                                                        badrate=lambda x: x.mean())
        total_bad['bad'] = total_bad['total'] - total_bad['bad']
        total_bad['badrate'] = 1 - total_bad['badrate']
        total_bad['badrate'] = total_bad['badrate'].apply(to_percentage)
        total_bad.insert(loc=0, column='target_type', value=label_,                          allow_duplicates=False)
        
        ks_auc_list = []
        for score_ in score_list:
            data = df[(df[label_]>=0) & (df[score_].notnull())]
            tmp_ks_auc = data.groupby(groupkeys).apply(cal_data_item,                                                        label_col=label_,                                                        score_col=score_)
#             tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
#                                                     'AUC':f'AUC_{score_}',
#                                                     'top5lift':f'top5lift_{score_}'})
            tmp_ks_auc = tmp_ks_auc.rename(columns={'KS':f'KS_{score_}',
                                                    'AUC':f'AUC_{score_}'
                                                   }
                                          )
            ks_auc_list.append(tmp_ks_auc)
        df_ks_auc = pd.concat(ks_auc_list, axis=1)
        ks_columns = [col for col in df_ks_auc.columns if 'KS' in col]
        AUC_columns = [col for col in df_ks_auc.columns if 'AUC' in col]
        lift_columns = [col for col in df_ks_auc.columns if 'top5lift' in col]
        df_ks_auc[ks_columns] = df_ks_auc[ks_columns].applymap(float_format)
        df_ks_auc[AUC_columns] = df_ks_auc[AUC_columns].applymap(float_format)
        df_ks_auc[lift_columns] = df_ks_auc[lift_columns].applymap(float_format)        
        df_ks_auc = df_ks_auc[ks_columns + AUC_columns + lift_columns]
        
        df_ks_auc = pd.concat([total_bad, df_ks_auc], axis=1)
        df_ks_auc = df_ks_auc.reset_index()
        
        ks_auc_result = pd.concat([ks_auc_result, df_ks_auc], axis=0, ignore_index=True)
        print(f'==============完成标签：{label_}===============')
    
    return ks_auc_result


# In[ ]:


df_ksauc_all = pd.DataFrame()
for col in varsname_base:
    model_score = ['y_prob_base_all']
    vars_score = [col]

    score_list = model_score + vars_score
    target_list = ['target_mob3dpd30_1']
    labels_models_dict = {target: score_list for target in target_list}
    
    tmp_df_evalue = df_sample.loc[df_sample[score_list].notna().all(axis=1),:]

    groupkeys2 = ['channel_types', 'apply_month']
    df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
    df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

    groupkeys4 = ['channel_rates', 'apply_month']
    df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
    df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

    df_ksauc_all_1 = pd.concat([df_ksauc_all_v2, df_ksauc_all_v4], axis=0)
    df_ksauc_all = pd.concat([df_ksauc_all, df_ksauc_all_1], axis=0)


# In[147]:


model_score = ['y_prob_base_v2']
vars_score = ['br_fpd'
,'br_mob4'
,'br_fpd_2'
,'br_mob4_2'
,'br_v3_fpd'
,'br_v3_mob4'
,'sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'gen7_fpd'
,'gen7_mob4'
,'xz_fpd'
,'all_a_br_derived_fpd30_202408_g_p'
,'all_a_br_derived_v1_mob4dpd30_202502_st_p'
,'all_a_br_derived_v2_fpd30_202411_g_p'
,'all_a_br_derived_v3_fpd30_202412_g_p'
,'ypy_bhxz_a_fpd30_v1_prob_good'
,'m1a0026_g_p'
,'m1a0027_g_p'
,'m1a0029_g_p']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_2 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_2.head()


# In[215]:


df_evalue.info()


# In[218]:


df_evalue['channel_types'] = df_evalue['channel_id'].apply(channel_type)
df_evalue['channel_rates'] = df_evalue['channel_id'].apply(channel_rate)


# In[219]:


model_score = ['m1a0030_g_p']
vars_score = ['m1a0029_g_p','gen7_fpd','gen7_mob4','low_v2_fpd30']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[220]:


df_ksauc_all_3.to_csv(r'效果对比_补充数据.csv',encoding='gbk')


# In[203]:


df_evalue['target_mob3dpd30'].value_counts()


# In[204]:


df_evalue['target_fpd30'].value_counts()


# In[205]:


df_evalue['apply_month'] = df_evalue['apply_date'].str[0:7]
df_evalue.loc[df_evalue['target_fpd30']==-1, 'target_fpd30']=np.nan
df_evalue.loc[df_evalue['target_mob3dpd30']==-1, 'target_mob3dpd30']=np.nan


# In[206]:


df_evalue['target_mob3dpd30'].value_counts()


# In[211]:


df_evalue['target_mob3dpd30_1'].value_counts()


# In[212]:


df_evalue['target_fpd30_1'].value_counts()


# In[207]:


df_evalue['target_fpd30'].value_counts()


# In[208]:


df_evalue['target_fpd30_1'] = 1-df_evalue['target_fpd30']
df_evalue['target_mob3dpd30_1'] = 1-df_evalue['target_mob3dpd30']


# In[213]:


model_score = ['y_prob_base_v2']
vars_score = ['m1a0022_g_p','m1a0023_g_p','m1a0026_g_p',',gen7_fpd','gen7_mob4','low_v2_fpd30']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_3 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_3.head()


# In[ ]:





# In[151]:


model_score = ['y_prob_base_v2']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_4 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_4.head()


# In[153]:


model_score = ['y_prob_base_v2']
vars_score = ['pboc_dpd20','mix_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.loc[df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_all_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_all_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_all_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_all_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_all_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_all_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_all_5 = pd.concat([df_ksauc_all_v1,df_ksauc_all_v2,df_ksauc_all_v4], axis=0)
df_ksauc_all_5.head()


# In[154]:



timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx') as writer:
    df_ksauc_all_2.to_excel(writer, sheet_name='df_ksauc_all_2')
    df_ksauc_all_3.to_excel(writer, sheet_name='df_ksauc_all_3')
    df_ksauc_all_4.to_excel(writer, sheet_name='df_ksauc_all_4')
    df_ksauc_all_5.to_excel(writer, sheet_name='df_ksauc_all_5')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_{timestamp}.xlsx')


# ##### 调用征信的渠道

# In[221]:


tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue.info(show_counts=True)


# In[156]:


df_ks_auc_month_pboc = calculate_ks_auc(tmp_df_evalue, modeltrian_target, target, 'y_prob_base_v2', 'apply_month')
df_ks_auc_month_pboc


# In[158]:


df_ks_auc_set_pboc = model_ks_auc(tmp_df_evalue, modeltrian_target, 'y_prob_base_v2', 'data_set')
df_ks_auc_set_pboc['渠道'] = '全渠道'
tmp = get_target_summary(tmp_df_evalue, target, 'data_set').set_index('bins')
df_ks_auc_set_pboc = pd.concat([tmp, df_ks_auc_set_pboc], axis=1)
df_ks_auc_set_pboc


# In[222]:


model_score = ['m1a0030_g_p']
vars_score = ['m1a0029_g_p','low_v2_fpd30','gen7_fpd','gen7_mob4']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1','target_fpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2_v2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2_v2.head()


# In[225]:


df_ksauc_pboc_2_v2.to_csv('效果对比_征信渠道_补充数据.csv',encoding='gbk')


# In[161]:


model_score = ['y_prob_base_v2']
vars_score = ['br_fpd'
,'br_mob4'
,'br_fpd_2'
,'br_mob4_2'
,'br_v3_fpd'
,'br_v3_mob4'
,'sf_mob4_1_v2'
,'gen4_fpd'
,'gen4_mob4'
,'sf_simple_fpd'
,'sf_simple_mob4'
,'simple2_fpd'
,'simple2_mob4'
,'gen5_fpd'
,'gen5_mob4'
,'gen6_fpd'
,'gen6_mob4'
,'gen7_fpd'
,'gen7_mob4'
,'xz_fpd'
,'all_a_br_derived_fpd30_202408_g_p'
,'all_a_br_derived_v1_mob4dpd30_202502_st_p'
,'all_a_br_derived_v2_fpd30_202411_g_p'
,'all_a_br_derived_v3_fpd30_202412_g_p'
,'ypy_bhxz_a_fpd30_v1_prob_good'
,'m1a0026_g_p'
,'m1a0027_g_p'
,'m1a0028_g_p'
,'all_a_rh_fpd0_v1_p'
,'all_a_rh_fpd10_v1_p'
,'all_a_rh_fpd10_v2_p'
,'all_a_rh_fpd30_v1_p'
,'all_a_rh_fpd6_v1_p'       
,'pboc_dpd20'
,'mix_pboc_dpd20']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_2 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_2.head()


# In[162]:


model_score = ['y_prob_base_v2']
vars_score = ['all_a_dz_derived_v1_fpd30_202502_g_p','all_a_dz_derived_v2_fpd30_202502_g_p','dz_fpd','all_a_bhdj_fpd10_v1_p']

score_list = model_score + vars_score
print(len(score_list))

target_list = ['target_mob3dpd30_1']
labels_models_dict = {target: score_list for target in target_list}

tmp_df_evalue = df_evalue.query("channel_id in (227,213,231,233,240,245,241,246)")
tmp_df_evalue = tmp_df_evalue.loc[tmp_df_evalue[score_list].notna().all(axis=1),:]

groupkeys2 = ['channel_types', 'apply_month']
df_ksauc_pboc_v2 = cal_ks_auc(tmp_df_evalue, groupkeys2, labels_models_dict)
df_ksauc_pboc_v2.rename(columns={'channel_types':'channel'},inplace=True)

groupkeys4 = ['channel_rates', 'apply_month']
df_ksauc_pboc_v4 = cal_ks_auc(tmp_df_evalue, groupkeys4, labels_models_dict)
df_ksauc_pboc_v4.rename(columns={'channel_rates':'channel'},inplace=True)

groupkeys1 = ['apply_month']
df_ksauc_pboc_v1 = cal_ks_auc(tmp_df_evalue, groupkeys1, labels_models_dict)
df_ksauc_pboc_v1.insert(0, 'channel', value='全渠道', allow_duplicates=False)

df_ksauc_pboc_4 = pd.concat([df_ksauc_pboc_v1,df_ksauc_pboc_v2,df_ksauc_pboc_v4], axis=0)
df_ksauc_pboc_4.head()


# In[177]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
with pd.ExcelWriter(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx') as writer:
    df_ksauc_pboc_2.to_excel(writer, sheet_name='df_ksauc_pboc_2')
    df_ksauc_pboc_4.to_excel(writer, sheet_name='df_ksauc_pboc_4')
    df_ks_auc_month_pboc.to_excel(writer, sheet_name='df_ks_auc_month_pboc')
    df_ks_auc_set_pboc.to_excel(writer, sheet_name='df_ks_auc_set_pboc')
print(f"数据存储完成！{timestamp}")
print(result_path + f'6_模型效果对比_{task_name}_pboc_{timestamp}.xlsx')


# # 6. 评分分布

# In[ ]:





# In[163]:


df_sample['apply_month'].value_counts()


# In[164]:


score = 'y_prob_base_v2'


# In[165]:


c = toad.transform.Combiner()
c.fit(df_sample.query("data_set=='1_train'")[[score, target]], y=target, method='quantile', n_bins=20) 
df_sample['score_bins'] = c.transform(df_sample[score], labels=True)


# In[166]:


df_sample['score_bins'].head()


# In[167]:


def get_model_psi(df, cols, month_col, combiner):
    # 获取所有唯一的月份
    months = sorted(list(set(df[month_col])))
    # 初始化一个空的 DataFrame 来存储 PSI 值
    psi_matrix = pd.DataFrame(index=months, columns=months, dtype=float)
    # 循环计算每个月份与其他月份之间的PSI
    for i, month_i in enumerate(months):
        for j, month_j in enumerate(months):
            if i != j:
                # 从原始数据集中提取特定月份的数据
                df_actual_i = df[df[month_col] == month_i]
                df_expect_j = df[df[month_col] == month_j]
                # 调用函数计算PSI
                psi_ = toad.metrics.PSI(df_actual_i[cols], df_expect_j[cols], 
                                        combiner = combiner, return_frame = False)
                # 将结果存入矩阵
                psi_matrix.loc[month_i, month_j] = psi_
            else:
                # 对角线上的值设为 NaN 或 0，表示同一月份的 PSI
                psi_matrix.loc[month_i, month_j] = 0.0
    
    return psi_matrix


# In[169]:


df_psi_matrix = get_model_psi(df_sample, score, 'apply_month', c)

# 打印最终的 PSI 矩阵
print(df_psi_matrix)


# In[170]:


score_group_by_dataset = calculate_vars_distribute(df_sample, ['score_bins'], target, 'data_set') 
score_group_by_dataset = score_group_by_dataset[['groupvars', 'bins', 'total', 'bad',
                                                 'good', 'bad_rate', 'bad_rate_cum',
                                                 'total_pct_cum', 'ks_bin', 'lift', 'lift_cum']]


# In[173]:


score_group_by_dataset.query("groupvars=='3_oot' & bins!='Total'")


# In[174]:


timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

with pd.ExcelWriter(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx') as writer:
    df_psi_matrix.to_excel(writer, sheet_name='df_psi_matrix')
    score_group_by_dataset.to_excel(writer, sheet_name='score_group_by_dataset')
print(f"数据存储完成！:{timestamp}")
print(result_path + f'6_评分分布_{task_name}_{timestamp}.xlsx')


# In[176]:


df_sample.to_csv(result_path + '授信全渠道子分融合模型三期标签v2.csv',index=False)
print(result_path + '授信全渠道子分融合模型三期标签v2.csv')


